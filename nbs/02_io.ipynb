{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input / Output\n",
    "\n",
    "AlphaPept can read input from different files. The most important files are those that contain raw MS data. Currently, AlphaPept supports the following raw input formats:\n",
    "\n",
    "* `.raw` files for `Thermo` raw data.\n",
    "* `.d` folders for `Bruker` raw data.\n",
    "* `.mzML` for generic input.\n",
    "\n",
    "After reading these input files, AlphaPept uses several preprocessing functions to prepare it for subsequent analysis. Functions include, for instance:\n",
    "\n",
    "* Centroiding of profile data to centroided data.\n",
    "* Trimming spectra to retain only the `n` most intense peaks.\n",
    "\n",
    "Finally, AlphaPept exports raw data as a `.ms_data.hdf` file. All subsequent results can be added to this file to ensure ease of use/access, full transparency, and reproducibility using a single well-structured file for the whole analysis instead of using randomly scattered files that all relate to a single dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading raw input\n",
    "\n",
    "Input can be read from three different files formats:\n",
    "\n",
    "* `.raw` files describing `Thermo` raw data.\n",
    "* `.d` folders describing `Bruker` raw data.\n",
    "* `.mzML` files describing generic input.\n",
    "\n",
    "All reading functions return a `query_data` dictionary similar to the following structure:\n",
    "\n",
    "```\n",
    "query_data = {\n",
    "    \"scan_list_ms1\":   np.array(...)\n",
    "    \"rt_list_ms1\":     np.array(...)\n",
    "    \"mass_list_ms1\":   np.array(...)\n",
    "    \"int_list_ms1\":    np.array(...)\n",
    "    \"ms_list_ms1\":     np.array(...)\n",
    "    \"scan_list_ms2\":   np.array(...)\n",
    "    \"rt_list_ms2\":     np.array(...)\n",
    "    \"mass_list_ms2\":   mass_list_ms2\n",
    "    \"int_list_ms2\":    int_list_ms2\n",
    "    \"ms_list_ms2\":     np.array(...)\n",
    "    \"prec_mass_list2\": np.array(...)\n",
    "    \"mono_mzs2\":       np.array(...)\n",
    "    \"charge2\":         np.array(...)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thermo\n",
    "\n",
    "To read Thermo files, AlphaPept uses the `pyrawfilereader` package, a Python implementation of the commonly used `rawfilereader` tool. By using the custom python version, Thermo files can be read without having to install `MSFileReader`.\n",
    "\n",
    "The user can pass an additional flag `use_profile_ms1`. This will then use the profile data which is not centroided already an peform centroiding. Note that this will lead to slightly different intensities, as the centroided data uses the apex and the centroid algorithm the summed intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_thermo_raw(\n",
    "    raw_file_name: str,\n",
    "    n_most_abundant: int,\n",
    "    use_profile_ms1: bool = False,\n",
    "    callback: callable = None,\n",
    ") -> tuple:\n",
    "    \"\"\"Load raw thermo data as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        raw_file_name (str): The name of a Thermo .raw file.\n",
    "        n_most_abundant (int): The maximum number of peaks to retain per MS2 spectrum.\n",
    "        use_profile_ms1 (bool): Use profile data or centroid it beforehand. Defaults to False.\n",
    "        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A dictionary with all the raw data and a string with the acquisition_date_time\n",
    "\n",
    "    \"\"\"\n",
    "    from alphapept.pyrawfilereader import RawFileReader\n",
    "    rawfile = RawFileReader(raw_file_name)\n",
    "\n",
    "    spec_indices = range(rawfile.FirstSpectrumNumber, rawfile.LastSpectrumNumber + 1)\n",
    "\n",
    "    scan_list = []\n",
    "    rt_list = []\n",
    "    mass_list = []\n",
    "    int_list = []\n",
    "    ms_list = []\n",
    "    prec_mzs_list = []\n",
    "    mono_mzs_list = []\n",
    "    charge_list = []\n",
    "\n",
    "    for idx, i in enumerate(spec_indices):\n",
    "        try:\n",
    "            ms_order = rawfile.GetMSOrderForScanNum(i)\n",
    "            rt = rawfile.RTFromScanNum(i)\n",
    "\n",
    "            if ms_order == 2:\n",
    "                prec_mz = rawfile.GetPrecursorMassForScanNum(i, 0)\n",
    "\n",
    "                mono_mz, charge = rawfile.GetMS2MonoMzAndChargeFromScanNum(i)\n",
    "            else:\n",
    "                prec_mz, mono_mz, charge = 0,0,0\n",
    "\n",
    "            if use_profile_ms1:\n",
    "                if ms_order == 2:\n",
    "                    masses, intensity = rawfile.GetCentroidMassListFromScanNum(i)\n",
    "                    masses, intensity = get_most_abundant(masses, intensity, n_most_abundant)\n",
    "                else:\n",
    "                    masses, intensity = rawfile.GetProfileMassListFromScanNum(i)\n",
    "                    masses, intensity = centroid_data(masses, intensity)\n",
    "\n",
    "            else:\n",
    "                masses, intensity = rawfile.GetCentroidMassListFromScanNum(i)\n",
    "                if ms_order == 2:\n",
    "                    masses, intensity = get_most_abundant(masses, intensity, n_most_abundant)\n",
    "\n",
    "            scan_list.append(i)\n",
    "            rt_list.append(rt)\n",
    "            mass_list.append(np.array(masses))\n",
    "            int_list.append(np.array(intensity, dtype=np.int64))\n",
    "            ms_list.append(ms_order)\n",
    "            prec_mzs_list.append(prec_mz)\n",
    "            mono_mzs_list.append(mono_mz)\n",
    "            charge_list.append(charge)\n",
    "        except KeyboardInterrupt as e:\n",
    "            raise e\n",
    "        except SystemExit as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Bad scan={i} in raw file '{raw_file_name}': {e}\")\n",
    "\n",
    "        if callback:\n",
    "            callback((idx+1)/len(spec_indices))\n",
    "\n",
    "    scan_list_ms1 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    rt_list_ms1 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    mass_list_ms1 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    int_list_ms1 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    ms_list_ms1 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "\n",
    "    scan_list_ms2 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    rt_list_ms2 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    mass_list_ms2 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    int_list_ms2 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    ms_list_ms2 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    mono_mzs2 = [mono_mzs_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    charge2 = [charge_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "\n",
    "    prec_mass_list2 = [\n",
    "        calculate_mass(mono_mzs_list[i], charge_list[i])\n",
    "        for i, _ in enumerate(ms_list)\n",
    "        if _ == 2\n",
    "    ]\n",
    "\n",
    "    check_sanity(mass_list)\n",
    "\n",
    "    query_data = {}\n",
    "\n",
    "    query_data[\"scan_list_ms1\"] = np.array(scan_list_ms1)\n",
    "    query_data[\"rt_list_ms1\"] = np.array(rt_list_ms1)\n",
    "    query_data[\"mass_list_ms1\"] = np.array(mass_list_ms1, dtype=object)\n",
    "    query_data[\"int_list_ms1\"] = np.array(int_list_ms1, dtype=object)\n",
    "    query_data[\"ms_list_ms1\"] = np.array(ms_list_ms1)\n",
    "\n",
    "    query_data[\"scan_list_ms2\"] = np.array(scan_list_ms2)\n",
    "    query_data[\"rt_list_ms2\"] = np.array(rt_list_ms2)\n",
    "    query_data[\"mass_list_ms2\"] = mass_list_ms2\n",
    "    query_data[\"int_list_ms2\"] = int_list_ms2\n",
    "    query_data[\"ms_list_ms2\"] = np.array(ms_list_ms2)\n",
    "    query_data[\"prec_mass_list2\"] = np.array(prec_mass_list2)\n",
    "    query_data[\"mono_mzs2\"] = np.array(mono_mzs2)\n",
    "#     TODO: Refactor charge2 to be consistent: charge_ms2\n",
    "    query_data[\"charge2\"] = np.array(charge2)\n",
    "\n",
    "    acquisition_date_time = rawfile.GetCreationDate()\n",
    "\n",
    "    rawfile.Close()\n",
    "\n",
    "    return query_data, acquisition_date_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruker\n",
    "\n",
    "To access Bruker files, AlphaPept relies on the external `timsdata` library from Bruker (available in the `alphatims\\ext` folder, licenses are applicable). Unfortunately, these libraries are only available on Windows and Linux. As a result, the reading of raw data is not available on macOS. However, once raw data is converted to `.ms_data.hdf` output, other workflow steps (besides feature feating) are possible without problems on macOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_bruker_raw(\n",
    "    d_folder_name: str,\n",
    "    n_most_abundant: int,\n",
    "    callback: callable = None,\n",
    "    **kwargs\n",
    ") -> tuple:\n",
    "    \"\"\"Load raw Bruker data as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        d_folder_name (str): The name of a Bruker .d folder.\n",
    "        n_most_abundant (int): The maximum number of peaks to retain per MS2 spectrum.\n",
    "        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A dictionary with all the raw data and a string with the acquisition_date_time\n",
    "\n",
    "    \"\"\"\n",
    "    import sqlalchemy as db\n",
    "    import pandas as pd\n",
    "    from alphapept.constants import mass_dict\n",
    "    from alphapept.ext.bruker import timsdata\n",
    "    from alphapept.io import list_to_numpy_f32, get_most_abundant\n",
    "\n",
    "    tdf = os.path.join(d_folder_name, 'analysis.tdf')\n",
    "    engine = db.create_engine('sqlite:///{}'.format(tdf))\n",
    "    prec_data = pd.read_sql_table('Precursors', engine)\n",
    "    frame_data = pd.read_sql_table('Frames', engine)\n",
    "    frame_data = frame_data.set_index('Id')\n",
    "\n",
    "    global_metadata = pd.read_sql_table('GlobalMetadata', engine)\n",
    "    global_metadata = global_metadata.set_index('Key').to_dict()['Value']\n",
    "    acquisition_date_time = global_metadata['AcquisitionDateTime']\n",
    "\n",
    "    tdf = timsdata.TimsData(d_folder_name)\n",
    "\n",
    "    M_PROTON = mass_dict['Proton']\n",
    "\n",
    "    prec_data['Mass'] = prec_data['MonoisotopicMz'].values * prec_data['Charge'].values - prec_data['Charge'].values*M_PROTON\n",
    "\n",
    "    mass_list_ms2 = []\n",
    "    int_list_ms2 = []\n",
    "    scan_list_ms2 = []\n",
    "\n",
    "    prec_data = prec_data.sort_values(by='Mass', ascending=True)\n",
    "\n",
    "    precursor_ids = prec_data['Id'].tolist()\n",
    "\n",
    "    for idx, key in enumerate(precursor_ids):\n",
    "\n",
    "        ms2_data = tdf.readPasefMsMs([key])\n",
    "        masses, intensity = ms2_data[key]\n",
    "        masses, intensity = get_most_abundant(np.array(masses), np.array(intensity), n_most_abundant)\n",
    "        mass_list_ms2.append(masses)\n",
    "        int_list_ms2.append(intensity)\n",
    "        scan_list_ms2.append(key)\n",
    "\n",
    "        if callback:\n",
    "            callback((idx+1)/len(precursor_ids))\n",
    "\n",
    "    check_sanity(mass_list_ms2)\n",
    "\n",
    "    query_data = {}\n",
    "    query_data['prec_mass_list2'] = prec_data['Mass'].values\n",
    "    query_data['prec_id2'] = prec_data['Id'].values\n",
    "    query_data['mono_mzs2'] = prec_data['MonoisotopicMz'].values\n",
    "    query_data['rt_list_ms2'] = frame_data.loc[prec_data['Parent'].values]['Time'].values / 60 #convert to minutes\n",
    "    query_data['scan_list_ms2'] = prec_data['Parent'].values\n",
    "    query_data['charge2'] = prec_data['Charge'].values\n",
    "    query_data['mobility2'] = tdf.scanNumToOneOverK0(1, prec_data['ScanNumber'].to_list()) #check if its okay to always use first frame\n",
    "    query_data[\"mass_list_ms2\"] = mass_list_ms2\n",
    "    query_data[\"int_list_ms2\"] = int_list_ms2\n",
    "\n",
    "    return query_data, acquisition_date_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `ccs` (i.e., ion mobility) values, we need additional functions from the Bruker library. As the live feature-finder might not be able to determine some charge values, it is intended to perform this calculation at a later stage once we have charge values from the post-processing feature finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import alphapept\n",
    "import numpy as np\n",
    "\n",
    "def one_over_k0_to_CCS(\n",
    "    one_over_k0s: np.ndarray,\n",
    "    charges: np.ndarray,\n",
    "    mzs: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Retrieve collisional cross section (CCS) values from (mobility, charge, mz) arrays.\n",
    "\n",
    "    Args:\n",
    "        one_over_k0s (np.ndarray): The ion mobilities (1D-np.float).\n",
    "        charges (np.ndarray): The charges (1D-np.int).\n",
    "        mzs (np.ndarray): The mz values (1D-np.float).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The CCS values.\n",
    "\n",
    "    \"\"\"\n",
    "    from alphapept.ext.bruker import timsdata\n",
    "\n",
    "    ccs = np.empty(len(one_over_k0s))\n",
    "    ccs[:] = np.nan\n",
    "\n",
    "    for idx, (one_over, charge, mz) in enumerate(zip(one_over_k0s, charges, mzs)):\n",
    "        try:\n",
    "            ccs[idx] = timsdata.oneOverK0ToCCSforMz(one_over, int(charge), mz)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return ccs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to availbility of Bruker libraries, this can only be tested on Windows and Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_one_over_k0_to_CCS():\n",
    "    one_over_k0_to_CCS([1], [1], [1])\n",
    "\n",
    "# NOTE, test was not enable for a long time and might be outdated by now\n",
    "# import sys\n",
    "# if sys.platform[:6] != \"darwin\":\n",
    "#     test_one_over_k0_to_CCS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic `.mzML` files\n",
    "\n",
    "To access `.mzML` files, we rely on the `pyteomics` package. For using an mzml format for performing a search, Peak Picking (data centroiding) should be applied to all MS levels of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_sanity(mass_list: np.ndarray) -> None:\n",
    "    \"\"\"Sanity check for mass list to make sure the masses are sorted.\n",
    "\n",
    "    Args:\n",
    "        mass_list (np.ndarray): The mz values (1D-np.float).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: When the mz values are not sorted.\n",
    "\n",
    "    \"\"\"\n",
    "    if not all(\n",
    "        mass_list[0][i] <= mass_list[0][i + 1] for i in range(len(mass_list[0]) - 1)\n",
    "    ):\n",
    "        raise ValueError(\"Masses are not sorted.\")\n",
    "\n",
    "\n",
    "def extract_mzml_info(input_dict: dict) -> tuple:\n",
    "    \"\"\"Extract basic MS coordinate arrays from a dictionary.\n",
    "\n",
    "    Args:\n",
    "        input_dict (dict): A dictionary obtained by iterating over a Pyteomics mzml.read function.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The rt, masses, intensities, ms_order, prec_mass, mono_mz, charge arrays retrieved from the input_dict.\n",
    "            If the `ms level` in the input dict does not equal 2, the charge, mono_mz and prec_mass will be equal to 0.\n",
    "\n",
    "    \"\"\"\n",
    "    from alphapept.chem import calculate_mass\n",
    "    rt = float(input_dict.get('scanList').get('scan')[0].get('scan start time'))  # rt_list_ms1/2\n",
    "    masses = input_dict.get('m/z array')\n",
    "    intensities = input_dict.get('intensity array').astype(int)\n",
    "    ms_order = input_dict.get('ms level')\n",
    "    prec_mass = mono_mz = charge = 0\n",
    "    if ms_order == 2:\n",
    "        try:\n",
    "            charge = int(input_dict.get('precursorList').get('precursor')[0].get('selectedIonList').get('selectedIon')[0].get(\n",
    "                'charge state'))\n",
    "        except TypeError:\n",
    "            charge = 0\n",
    "        mono_mz = input_dict.get('precursorList').get('precursor')[0].get('selectedIonList').get('selectedIon')[0].get(\n",
    "                'selected ion m/z')\n",
    "        prec_mass = calculate_mass(mono_mz, charge)\n",
    "    return rt, masses, intensities, ms_order, prec_mass, mono_mz, charge\n",
    "\n",
    "\n",
    "def load_mzml_data(\n",
    "    filename: str,\n",
    "    n_most_abundant: int,\n",
    "    callback: callable = None,\n",
    "    **kwargs\n",
    ") -> tuple:\n",
    "    \"\"\"Load data from an mzml file as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of a .mzml file.\n",
    "        n_most_abundant (int): The maximum number of peaks to retain per MS2 spectrum.\n",
    "        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A dictionary with all the raw data, a string with the acquisition_date_time and a string with the vendor.\n",
    "\n",
    "    \"\"\"\n",
    "    from pyteomics import mzml\n",
    "    import os\n",
    "    import re\n",
    "    import logging\n",
    "    import datetime\n",
    "    import pathlib\n",
    "    np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "\n",
    "    try:\n",
    "        reader = mzml.read(filename, use_index=True)\n",
    "        spec_indices = np.array(range(1, len(reader) + 1))\n",
    "    except OSError:\n",
    "        logging.info('Could not open the file. Please, specify the correct path to the file.')\n",
    "        sys.exit(1)\n",
    "\n",
    "    scan_list = []\n",
    "    rt_list = []\n",
    "    mass_list = []\n",
    "    int_list = []\n",
    "    ms_list = []\n",
    "    prec_mzs_list = []\n",
    "    mono_mzs_list = []\n",
    "    charge_list = []\n",
    "    \n",
    "    vendor = \"Unknown\"\n",
    "    \n",
    "    for idx, i in enumerate(spec_indices):\n",
    "        try:\n",
    "            spec = next(reader)\n",
    "\n",
    "            if idx == 0:\n",
    "                ext = re.findall(r\"File:\\\".+\\.(\\w+)\\\"\", spec['spectrum title'])[0]\n",
    "                if ext.lower() == 'raw':\n",
    "                    vendor = \"Thermo\"\n",
    "\n",
    "            scan_list.append(i)\n",
    "            rt, masses, intensities, ms_order, prec_mass, mono_mz, charge = extract_mzml_info(spec)\n",
    "            \n",
    "            sortindex = np.argsort(masses)\n",
    "            \n",
    "            masses = masses[sortindex]\n",
    "            intensities = intensities[sortindex]\n",
    "            \n",
    "            if ms_order == 2:\n",
    "                masses, intensities = get_most_abundant(masses, intensities, n_most_abundant)\n",
    "            rt_list.append(rt)\n",
    "            \n",
    "            #Remove zero intensities\n",
    "            to_keep = intensities>0\n",
    "            masses = masses[to_keep]\n",
    "            intensities = intensities[to_keep]\n",
    "            \n",
    "            mass_list.append(masses)\n",
    "            int_list.append(intensities)\n",
    "            ms_list.append(ms_order)\n",
    "            prec_mzs_list.append(prec_mass)\n",
    "            mono_mzs_list.append(mono_mz)\n",
    "            charge_list.append(charge)\n",
    "            \n",
    "        except KeyboardInterrupt as e:\n",
    "            raise e\n",
    "        except SystemExit as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Bad scan={i} in mzML file '{filename}' {e}\")\n",
    "\n",
    "        if callback:\n",
    "            callback((idx+1)/len(spec_indices))\n",
    "\n",
    "    check_sanity(mass_list)\n",
    "\n",
    "    scan_list_ms1 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    rt_list_ms1 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    mass_list_ms1 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    int_list_ms1 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    ms_list_ms1 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "\n",
    "    scan_list_ms2 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    rt_list_ms2 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    mass_list_ms2 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    int_list_ms2 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    ms_list_ms2 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    prec_mass_list2 = [prec_mzs_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    mono_mzs2 = [mono_mzs_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    charge_ms2 = [charge_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "\n",
    "    prec_mass_list2 = [\n",
    "        calculate_mass(mono_mzs_list[i], charge_list[i])\n",
    "        for i, _ in enumerate(ms_list)\n",
    "        if _ == 2\n",
    "    ]\n",
    "\n",
    "    query_data = {}\n",
    "\n",
    "    query_data[\"scan_list_ms1\"] = np.array(scan_list_ms1)\n",
    "    query_data[\"rt_list_ms1\"] = np.array(rt_list_ms1)\n",
    "    query_data[\"mass_list_ms1\"] = np.array(mass_list_ms1)\n",
    "    query_data[\"int_list_ms1\"] = np.array(int_list_ms1)\n",
    "    query_data[\"ms_list_ms1\"] = np.array(ms_list_ms1)\n",
    "\n",
    "    query_data[\"scan_list_ms2\"] = np.array(scan_list_ms2)\n",
    "    query_data[\"rt_list_ms2\"] = np.array(rt_list_ms2)\n",
    "    query_data[\"mass_list_ms2\"] = mass_list_ms2\n",
    "    query_data[\"int_list_ms2\"] = int_list_ms2\n",
    "    query_data[\"ms_list_ms2\"] = np.array(ms_list_ms2)\n",
    "    query_data[\"prec_mass_list2\"] = np.array(prec_mass_list2)\n",
    "    query_data[\"mono_mzs2\"] = np.array(mono_mzs2)\n",
    "    query_data[\"charge2\"] = np.array(charge_ms2)\n",
    "\n",
    "    fname = pathlib.Path(filename)\n",
    "    acquisition_date_time = datetime.datetime.fromtimestamp(fname.stat().st_mtime).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    return query_data, acquisition_date_time, vendor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading other files\n",
    "\n",
    "Benchmarking proteomics software against each other is not straightforward as various naming conventions exist and different algorithms are implemented. In this section, we define some helper functions that allow us to read results from other tools and facilitate the comparison of different tools against AlphaPept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading MaxQuant .xml settings file\n",
    "\n",
    "One of the most commonly used tools to analyze MS data is MaxQuant. AlphaPept reads MaxQuant .xml files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def __extract_nested(child):\n",
    "    \"\"\"\n",
    "    Helper function to extract nested entries\n",
    "    \"\"\"\n",
    "    if len(child) > 0:\n",
    "        temp_dict = {}\n",
    "        for xx in child:\n",
    "            temp_dict[xx.tag] = __extract_nested(xx)\n",
    "        return temp_dict\n",
    "    else:\n",
    "        if child.text == 'True':\n",
    "            info = True\n",
    "        elif child.text == 'False':\n",
    "            info = False\n",
    "        else:\n",
    "            info = child.text\n",
    "        return info\n",
    "\n",
    "def extract_mq_settings(path: str) -> dict:\n",
    "    \"\"\"Function to return MaxQuant values as a dictionary for a given xml file.\n",
    "\n",
    "    Args:\n",
    "        path (str): File name of an xml file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with MaxQuant info.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: When path is not a valid xml file.\n",
    "\n",
    "    \"\"\"\n",
    "    if not path.endswith('.xml'):\n",
    "        raise ValueError(\"Path {} is not a valid xml file.\".format(path))\n",
    "\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    mq_dict = {}\n",
    "\n",
    "    for child in root:\n",
    "\n",
    "        mq_dict[child.tag] = __extract_nested(child)\n",
    "\n",
    "    return mq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FastaFileInfo': {'fastaFilePath': 'testfile.fasta',\n",
       "  'identifierParseRule': '>([^\\\\s]*)',\n",
       "  'descriptionParseRule': '>(.*)',\n",
       "  'taxonomyParseRule': None,\n",
       "  'variationParseRule': None,\n",
       "  'modificationParseRule': None,\n",
       "  'taxonomyId': None}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mq_dict = extract_mq_settings('../testfiles/test_mqpar.xml')\n",
    "mq_dict['fastaFiles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlphaPept incorporates PTMs directly in amino acid sequences with lower case identifiers. Parsing MaxQuant sequences with PTMs is done with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_mq_seq(peptide: str) -> str:\n",
    "    \"\"\"Replaces maxquant convention to alphapept convention.\n",
    "\n",
    "    ToDo: include more sequences\n",
    "\n",
    "    Args:\n",
    "        peptide (str): A peptide sequence from MaxQuant.\n",
    "\n",
    "    Returns:\n",
    "        str: A parsed peptide sequence compatible with AlphaPept.\n",
    "\n",
    "    \"\"\"\n",
    "    peptide = peptide[1:-1] #Remove _\n",
    "\n",
    "    peptide = peptide.replace('(Acetyl (Protein N-term))','a')\n",
    "    peptide = peptide.replace('M(Oxidation (M))','oxM')\n",
    "    peptide = peptide.replace('C','cC') #This is fixed and not indicated in MaxQuant\n",
    "\n",
    "    return peptide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test shows the results are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AFQPFFVELToxMPYSVIR'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_mq_seq('_AFQPFFVELTM(Oxidation (M))PYSVIR_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "Raw data needs to be preprocessed to simplify it for AlphaPept analysis. Currently, this requires:\n",
    "\n",
    "* Profiling of the data\n",
    "* Trimming of spectra to retain only the 'n'-most intense peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroiding profile data\n",
    "\n",
    "Importing raw data frequently results in profile data. When having profile data, Alphapept first needs to perform centroiding to use this data properly. For this, it needs to search local maxima (\"peaks\") of the intensity as a function of m/z. For this AlphaPept uses the function `get_peaks`. A peak is described by three points, the `start` of the peak, the `center`, and the `end`. The function accepts an intensity array and calculates the delta (gradient) between consecutive data points to determine the start, center, and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "\n",
    "@njit\n",
    "def get_peaks(int_array: np.ndarray) -> list:\n",
    "    \"\"\"Detects peaks in an array.\n",
    "\n",
    "    Args:\n",
    "        int_array (np.ndarray): An array with intensity values.\n",
    "\n",
    "    Returns:\n",
    "        list: A regular Python list with all peaks.\n",
    "            A peak is a triplet of the form (start, center, end)\n",
    "\n",
    "    \"\"\"\n",
    "    peaklist = []\n",
    "    gradient = np.diff(int_array)\n",
    "    start, center, end = -1, -1, -1\n",
    "\n",
    "    for i in range(len(gradient)):\n",
    "\n",
    "        grad = gradient[i]\n",
    "\n",
    "        if (end == -1) & (center == -1):  # No end and no center yet\n",
    "            if grad <= 0:  # If decreasing, move start point\n",
    "                start = i\n",
    "            else:  # If increasing set as center point\n",
    "                center = i\n",
    "\n",
    "        if (end == -1) & (\n",
    "            center != -1\n",
    "        ):  # If we have a centerpoint and it is still increasing set as new center\n",
    "            if grad >= 0:\n",
    "                center = i\n",
    "            else:  # If it is decreasing set as endpoint\n",
    "                end = i\n",
    "\n",
    "        if end != -1:  # If we have and endpoint and it is going down\n",
    "            if grad < 0:\n",
    "                end = i  # Set new Endpoint\n",
    "            else:  # if it stays the same or goes up set a new peak\n",
    "                peaklist.append((start + 1, center + 1, end + 1))\n",
    "                start, center, end = end, -1, -1  # Reset start, center, end\n",
    "\n",
    "    if end != -1:\n",
    "        peaklist.append((start + 1, center + 1, end + 1))\n",
    "\n",
    "    return peaklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test to illustrate the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 6, 9), (9, 13, 16)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_array = np.array([0,0,0,1,2,3,3,2,1,0,0,1,2,3,2,1,0])\n",
    "get_peaks(int_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_get_peaks():\n",
    "    int_array = np.array([0,0,0,1,2,3,3,2,1,0,0,1,2,3,2,1,0])\n",
    "    assert get_peaks(int_array) == [(2, 6, 9), (9, 13, 16)]\n",
    "    \n",
    "test_get_peaks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the center of the peak, we distinguish based on the number of raw data points that are contained in the peak:\n",
    "\n",
    "* One data point: m/z value of this point is taken as the center\n",
    "* Two datapoints: Average of the two m/z values, weighted by the intensities\n",
    "* Three or more data points: Gaussian estimation of the center position.\n",
    "\n",
    "For the Gaussian estimation, only the three central points are used to fit a Gaussian Peak shape. The Gaussian is then approximated with the logarithm. \n",
    "\n",
    "The gaussian estimator is defined in `gaussian_estimator` and is used by the wrapper `get_centroid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def get_centroid(\n",
    "    peak: tuple,\n",
    "    mz_array: np.ndarray,\n",
    "    int_array: np.ndarray\n",
    ") -> tuple:\n",
    "    \"\"\"Wrapper to estimate centroid center positions.\n",
    "\n",
    "    Args:\n",
    "        peak (tuple): A triplet of the form (start, center, end)\n",
    "        mz_array (np.ndarray): An array with mz values.\n",
    "        int_array (np.ndarray): An array with intensity values.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of the form (center, intensity)\n",
    "    \"\"\"\n",
    "    start, center, end = peak\n",
    "    mz_int = np.sum(int_array[start + 1 : end])\n",
    "\n",
    "    peak_size = end - start - 1\n",
    "\n",
    "    if peak_size == 1:\n",
    "        mz_cent = mz_array[center]\n",
    "    elif peak_size == 2:\n",
    "        mz_cent = (\n",
    "            mz_array[start + 1] * int_array[start + 1]\n",
    "            + mz_array[end - 1] * int_array[end - 1]\n",
    "        ) / (int_array[start + 1] + int_array[end - 1])\n",
    "    else:\n",
    "        mz_cent = gaussian_estimator(peak, mz_array, int_array)\n",
    "\n",
    "    return mz_cent, mz_int\n",
    "\n",
    "@njit\n",
    "def gaussian_estimator(\n",
    "    peak: tuple,\n",
    "    mz_array: np.ndarray,\n",
    "    int_array: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"Three-point gaussian estimator.\n",
    "\n",
    "    Args:\n",
    "        peak (tuple): A triplet of the form (start, center, end)\n",
    "        mz_array (np.ndarray): An array with mz values.\n",
    "        int_array (np.ndarray): An array with intensity values.\n",
    "\n",
    "    Returns:\n",
    "        float: The gaussian estimate of the center.\n",
    "    \"\"\"\n",
    "    start, center, end = peak\n",
    "\n",
    "    m1, m2, m3 = mz_array[center - 1], mz_array[center], mz_array[center + 1]\n",
    "    i1, i2, i3 = int_array[center - 1], int_array[center], int_array[center + 1]\n",
    "\n",
    "    if i1 == 0:  # Case of sharp flanks\n",
    "        m = (m2 * i2 + m3 * i3) / (i2 + i3)\n",
    "    elif i3 == 0:\n",
    "        m = (m1 * i1 + m2 * i2) / (i1 + i2)\n",
    "    else:\n",
    "        l1, l2, l3 = np.log(i1), np.log(i2), np.log(i3)\n",
    "        m = (\n",
    "            ((l2 - l3) * (m1 ** 2) + (l3 - l1) * (m2 ** 2) + (l1 - l2) * (m3 ** 2))\n",
    "            / ((l2 - l3) * (m1) + (l3 - l1) * (m2) + (l1 - l2) * (m3))\n",
    "            * 1\n",
    "            / 2\n",
    "        )\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function performs as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.499999999999999\n",
      "(4.499999999999999, 12)\n"
     ]
    }
   ],
   "source": [
    "int_array = np.array([0,0,0,1,2,3,3,2,1,0,0,1,2,3,2,1,0])\n",
    "mz_array = np.array([0,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "peak = (2, 6, 9)\n",
    "\n",
    "print(gaussian_estimator(peak, mz_array, int_array))\n",
    "print(get_centroid(peak, mz_array, int_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_get_centroid():\n",
    "    \n",
    "    int_array = np.array([0,0,0,1,2,3,3,2,1,0,0,1,2,3,2,1,0])\n",
    "    mz_array = np.array([0,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "    \n",
    "    peak = (2, 6, 9)\n",
    "    \n",
    "    mz, intsum = get_centroid(peak, mz_array, int_array)\n",
    "    \n",
    "    assert np.allclose(mz, 4.499999999999999)\n",
    "    assert np.allclose(intsum, 12)\n",
    "    \n",
    "    def gaussian(x, mu, sigma):\n",
    "        return np.exp(-np.power(x - mu, 2.0) / (2 * np.power(sigma, 2.0)))\n",
    "\n",
    "    test_values = np.arange(0, 200)\n",
    "    gauss_trace = np.zeros(len(test_values))\n",
    "\n",
    "    centers = [30, 60, 120]\n",
    "    sigmas = [1, 2, 3]\n",
    "\n",
    "    for center, sigma in zip(centers, sigmas):\n",
    "        gauss_trace += np.round(gaussian(test_values, center, sigma), 3)\n",
    "\n",
    "    peaks = get_peaks(gauss_trace)\n",
    "    centers_found = [_[1] for _ in peaks]\n",
    "\n",
    "    assert set(centers) == set(centers_found)\n",
    "\n",
    "    for i, peak in enumerate(peaks):\n",
    "        center_est, a = get_centroid(peak, test_values, gauss_trace)\n",
    "        sigma_est = (a * 0.3989) / 1\n",
    "\n",
    "        assert np.round(center_est, 2) == centers[i]\n",
    "        assert np.round(sigma_est, 2) == sigmas[i]\n",
    "        \n",
    "test_get_centroid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detected centroid can also easily be visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_centroid(peak, mz_array, int_array):\n",
    "\n",
    "    plt.figure(figsize=(3, 3))\n",
    "\n",
    "    start, center, end = peak\n",
    "\n",
    "    centroid = get_centroid((start, center, end), mz_array, int_array)\n",
    "\n",
    "    plt.plot(mz_array[start - 2 : end + 2], int_array[start - 2 : end + 2])\n",
    "    plt.plot(mz_array[start - 2 : end + 2], int_array[start - 2 : end + 2], \"b.\")\n",
    "    plt.axvline(mz_array[start], color=\"r\")\n",
    "    plt.axvline(mz_array[end], color=\"r\")\n",
    "\n",
    "    plt.axvline(centroid[0], color=\"g\", label='Center')\n",
    "    plt.title(\"Centroid\")\n",
    "    plt.xlabel(\"m/z\")\n",
    "    plt.ylabel(\"Intensity\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "int_array = np.array([0,0,0,1,2,3,3,2,1,0,0,1,2,3,2,1,0])\n",
    "mz_array = np.array([0,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "\n",
    "peak = (2, 6, 9)\n",
    "    \n",
    "plot_centroid(peak, mz_array, int_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@njit\n",
    "def centroid_data(\n",
    "    mz_array: np.ndarray,\n",
    "    int_array: np.ndarray\n",
    ") -> tuple:\n",
    "    \"\"\"Estimate centroids and intensities from profile data.\n",
    "\n",
    "    Args:\n",
    "        mz_array (np.ndarray): An array with mz values.\n",
    "        int_array (np.ndarray): An array with intensity values.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of the form (mz_array_centroided, int_array_centroided)\n",
    "    \"\"\"\n",
    "    peaks = get_peaks(int_array)\n",
    "\n",
    "    mz_array_centroided = np.zeros(len(peaks))\n",
    "    int_array_centroided = np.zeros(len(peaks))\n",
    "\n",
    "\n",
    "    for i in range(len(peaks)):\n",
    "        mz_, int_ = get_centroid(peaks[i], mz_array, int_array)\n",
    "        mz_array_centroided[i] = mz_\n",
    "        int_array_centroided[i] = int_\n",
    "\n",
    "    return mz_array_centroided, int_array_centroided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming spectra to retain the `n` most intense peaks\n",
    "\n",
    "`get_most_abundant`: In order to save spectra in a more memory-efficient form, we only keep the n most abundant peaks. This allows us to save data in a fast, accessible matrix format. \n",
    "\n",
    "`get_local_intensity`: This calculates the local intensity to get local maxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from alphapept.chem import calculate_mass\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from numba.typed import List\n",
    "from numba import njit\n",
    "import gzip\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "@njit\n",
    "def get_local_intensity(intensity, window=10):\n",
    "    \"\"\"\n",
    "    Calculate the local intensity for a spectrum.\n",
    "\n",
    "    Args:\n",
    "        intensity (np.ndarray): An array with intensity values.\n",
    "        window (int): Window Size\n",
    "    Returns:\n",
    "        nop.ndarray: local intensity\n",
    "    \"\"\"\n",
    "\n",
    "    local_intensity = np.zeros(len(intensity))\n",
    "\n",
    "    for i in range(len(intensity)):\n",
    "        start = max(0, i-window) \n",
    "        end = min(len(intensity), i+window)\n",
    "        local_intensity[i] = intensity[i]/np.max(intensity[start:end])\n",
    "        \n",
    "    return local_intensity\n",
    "\n",
    "def get_most_abundant(\n",
    "    mass: np.ndarray,\n",
    "    intensity: np.ndarray,\n",
    "    n_max: int,\n",
    "    window: int = 10,\n",
    ") -> tuple:\n",
    "    \"\"\"Returns the n_max most abundant peaks of a spectrum.\n",
    "\n",
    "    Args:\n",
    "        mass (np.ndarray): An array with mz values.\n",
    "        intensity (np.ndarray): An array with intensity values.\n",
    "        n_max (int): The maximum number of peaks to retain.\n",
    "            Setting `n_max` to -1 returns all peaks.\n",
    "        window (int): Use local maximum in a window\n",
    "\n",
    "    Returns:\n",
    "        tuple: the filtered mass and intensity arrays.\n",
    "\n",
    "    \"\"\"\n",
    "    if n_max == -1:\n",
    "        return mass, intensity\n",
    "    if len(mass) < n_max:\n",
    "        return mass, intensity\n",
    "    else:\n",
    "        \n",
    "        if window > 0:\n",
    "            sortindex = np.argsort(get_local_intensity(intensity, window))[::-1][:n_max]\n",
    "        else:\n",
    "            sortindex = np.argsort(intensity)[::-1][:n_max]\n",
    "        \n",
    "        sortindex.sort()\n",
    "\n",
    "    return mass[sortindex], intensity[sortindex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For saving, we are currently relying on the hdf-container (see below).\n",
    "\n",
    "While we could, in principle, store the mz and int arrays as a list of variable length, this will come at a performance decrease. We, therefore, create an array of the dimensions of the n most abundant peaks and the number of spectra with the function `list_to_numpy_f32` and fill the unoccupied cells with `-1`. This allows an increase in accessing times at the cost of additional disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def list_to_numpy_f32(\n",
    "    long_list: list\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Function to convert a list to np.float32 array.\n",
    "\n",
    "    Args:\n",
    "        long_list (list): A regular Python list with values that can be converted to floats.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A np.float32 array.\n",
    "\n",
    "    \"\"\"\n",
    "    np_array = (\n",
    "        np.zeros(\n",
    "            [len(max(long_list, key=lambda x: len(x))), len(long_list)],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        - 1\n",
    "    )\n",
    "    for i, j in enumerate(long_list):\n",
    "        np_array[0 : len(j), i] = j\n",
    "\n",
    "    return np_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving output\n",
    "\n",
    "As MS hardware has continued to improve over the years, MS data has become more complex. To deal with this complexity, the MS community has already used many different [data formats](https://onlinelibrary.wiley.com/doi/full/10.1002/mas.21522) to store and access data. [HDF](https://www.hdfgroup.org/solutions/hdf5/) containers are one option, but they have not yet gained widespread support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF containers in general\n",
    "\n",
    "In general, an HDF container can be viewed as a compressed folder with metadata (i.e., attributes) associated to each single subfolder or file (i.e., data arrays of various types and sizes) within this container. A container might, for instance have contents that look like, e.g.:\n",
    "```\n",
    "HDF_Container\n",
    "    {\n",
    "        meta_data_1: \"Some string\",\n",
    "        meta_data_2: 1234567890,\n",
    "        ...\n",
    "    }\n",
    "    array_1\n",
    "        {\n",
    "            meta_data_of_array1_1: \"Some other string\",\n",
    "            ...\n",
    "        },\n",
    "        100x2 int8\n",
    "    array_2\n",
    "        1000x2 float64\n",
    "    subfolder_1\n",
    "        {\n",
    "            meta_data_of_subfolder_1_1: \"Really any string of any length\",\n",
    "            ...\n",
    "        }\n",
    "        array_1_of_subfolder_1\n",
    "        subfolder_1_1\n",
    "        ...\n",
    "    subfolder_n\n",
    "    ...\n",
    "```\n",
    "\n",
    "\n",
    "A few of the advantages of HDF are, e.g.:\n",
    "\n",
    "* It has no upper limit with regards to file size\n",
    "* It can be used on disk without consuming RAM memory\n",
    "* It is fully portable on different machines and systems\n",
    "* It is capable of fast IO operations\n",
    "* It allows data to be structured very transparent while still providing a flexible way to store metadata\n",
    "* ...\n",
    "\n",
    "For these reasons, HDF containers have gained popularity in several scientific fields, including (astro)physics and geology. It is, therefore, no surprise that Python has excellent support for HDF containers. The two most used packages are `h5py` and `tables`, where the former has a generic API and the second is frequently used with `pandas` dataframes. An excellent viewer for HDF files is [HDF Compass](https://support.hdfgroup.org/projects/compass/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using HDF containers in AlphaPept\n",
    "\n",
    "AlphaPept uses the python `h5py` package to store MS data in HDF containers, inspired by the [ion_networks](https://github.com/swillems/ion_networks) repository.\n",
    "\n",
    "* First, a generic class is defined that will serve as an API for HDF containers. To ensure full transparency, this will include immutable metadata such as `creation time`, `original_file_name` and `version`.\n",
    "* The constructor of an HDF_File will be passed the `file_name` of an HDF container, an `is_read_only` flag, an `is_overwritable` flag and `is_new_file` flag.\n",
    "* To compare HDF_Files, several (magic) functions need to be defined.\n",
    "* Traceability and reproducibility are ensured by storing a `last_updated` and a `check` function to warn users about potential compatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "from alphapept.__main__ import VERSION_NO\n",
    "\n",
    "\n",
    "class HDF_File(object):\n",
    "    '''\n",
    "    A generic class to store and retrieve on-disk\n",
    "    data with an HDF container.\n",
    "    '''\n",
    "\n",
    "    @property\n",
    "    def original_file_name(self):\n",
    "        return self.read(\n",
    "            attr_name=\"original_file_name\"\n",
    "        )  # See below for function definition\n",
    "\n",
    "    @property\n",
    "    def file_name(self):\n",
    "        return self.__file_name\n",
    "\n",
    "    @property\n",
    "    def directory(self):\n",
    "        return os.path.dirname(self.file_name)\n",
    "\n",
    "    @property\n",
    "    def creation_time(self):\n",
    "        return self.read(\n",
    "            attr_name=\"creation_time\"\n",
    "        )  # See below for function definition\n",
    "\n",
    "    @property\n",
    "    def last_updated(self):\n",
    "        return self.read(\n",
    "            attr_name=\"last_updated\"\n",
    "        )  # See below for function definition\n",
    "\n",
    "    @property\n",
    "    def version(self):\n",
    "        return self.read(\n",
    "            attr_name=\"version\"\n",
    "        )  # See below for function definition\n",
    "\n",
    "    @property\n",
    "    def is_read_only(self):\n",
    "        return self.__is_read_only\n",
    "\n",
    "    @property\n",
    "    def is_overwritable(self):\n",
    "        return self.__is_overwritable\n",
    "\n",
    "    def read(self):\n",
    "        pass\n",
    "\n",
    "    def write(self):\n",
    "        pass\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_name: str,\n",
    "        is_read_only: bool = True,\n",
    "        is_new_file: bool = False,\n",
    "        is_overwritable: bool = False,\n",
    "    ):\n",
    "        \"\"\"Create/open a wrapper object to access HDF data.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): The file_name of the HDF file.\n",
    "            is_read_only (bool): If True, the HDF file cannot be modified. Defaults to True.\n",
    "            is_new_file (bool): If True, an already existing file will be completely removed. Defaults to False.\n",
    "            is_overwritable (bool): If True, already existing arrays will be overwritten. If False, only new data can be appended. Defaults to False.\n",
    "\n",
    "        \"\"\"\n",
    "        self.__file_name = os.path.abspath(file_name)\n",
    "        if is_new_file:\n",
    "            is_read_only = False\n",
    "            if not os.path.exists(self.directory):\n",
    "                os.makedirs(self.directory)\n",
    "            with h5py.File(self.file_name, \"w\") as hdf_file:\n",
    "                current_time = time.asctime()\n",
    "                hdf_file.attrs[\"creation_time\"] = current_time\n",
    "                hdf_file.attrs[\"original_file_name\"] = self.__file_name\n",
    "                hdf_file.attrs[\"version\"] = VERSION_NO\n",
    "                hdf_file.attrs[\"last_updated\"] = current_time\n",
    "        else:\n",
    "            with h5py.File(self.file_name, \"r\") as hdf_file:\n",
    "                self.check()\n",
    "        if is_overwritable:\n",
    "            is_read_only = False\n",
    "        self.__is_read_only = is_read_only\n",
    "        self.__is_overwritable = is_overwritable\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.file_name == other.file_name\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.file_name)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<HDF_File {self.file_name}>\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def check(\n",
    "        self,\n",
    "        version: bool = True,\n",
    "        file_name: bool = True,\n",
    "    ) -> list:\n",
    "        \"\"\"Check if the `version` or `file_name` of this HDF_File have changed.\n",
    "\n",
    "        Args:\n",
    "            version (bool): If False, do not check the version. Defaults to True.\n",
    "            file_name (bool): If False, do not check the file_name. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of warning messages stating any issues.\n",
    "\n",
    "        \"\"\"\n",
    "        warning_messages = []\n",
    "        if version:\n",
    "            current_version = VERSION_NO\n",
    "            creation_version = self.version\n",
    "            if creation_version != current_version:\n",
    "                warning_messages.append(\n",
    "                    f\"{self} was created with version \"\n",
    "                    f\"{creation_version} instead of {current_version}.\"\n",
    "                )\n",
    "        if file_name:\n",
    "            if self.file_name != self.original_file_name:\n",
    "                warning_messages.append(\n",
    "                    f\"The file name of {self} has been changed from\"\n",
    "                    f\"{self.original_file_name} to {self.file_name}.\"\n",
    "                )\n",
    "        return warning_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents of HDF containers come in three variants:\n",
    "\n",
    "1. `Groups`: folders\n",
    "2. `Datasets`: arrays\n",
    "3. `Attributes`: metadata associated with individual datasets or groups (with the root folder also considered as a normal group)\n",
    "\n",
    "These contents can be accessed with `read` and `write` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "\n",
    "@patch\n",
    "def read(\n",
    "    self: HDF_File,\n",
    "    group_name: str = None,\n",
    "    dataset_name: str = None,\n",
    "    attr_name: str = None,\n",
    "    return_dataset_shape: bool = False,\n",
    "    return_dataset_dtype: bool = False,\n",
    "    return_dataset_slice: slice = slice(None),\n",
    "    swmr: bool = False,\n",
    "):\n",
    "    \"\"\"Read contents of an HDF_File.\n",
    "\n",
    "    Args:\n",
    "        group_name (str): The group_name from where to read data.\n",
    "            If no `group_name` has been provided, read directly from the root group.\n",
    "            Defaults to None.\n",
    "        dataset_name (str): The dataset to read.\n",
    "            If no `dataset_name` has been provided, read directly from the group.\n",
    "            If the `dataset_name` refers to a group, it is assumed to be\n",
    "            pd.DataFrame and returned as such.\n",
    "            Defaults to None.\n",
    "        attr_name (str): The attribute to read.\n",
    "            If `attr_name` is not None, read the attribute value instead of the contents of a group or dataset.\n",
    "            If `attr_name` == \"\", read all attributes as a dict.\n",
    "            Defaults to None.\n",
    "        return_dataset_shape (bool): Do not read complete dataset to minimize RAM and IO usage.\n",
    "            Defaults to False.\n",
    "        return_dataset_dtype (bool): Do not read complete dataset to minimize RAM and IO usage.\n",
    "            Defaults to False.\n",
    "        return_dataset_slice (slice): Do not read complete dataset to minimize RAM and IO usage.\n",
    "            Defaults to slice(None).\n",
    "        swmr (bool): Use swmr mode to read data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        type: Depending on what is requested, a dict, value, np.ndarray or pd.dataframe is returned.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: When the group_name does not exist.\n",
    "        KeyError: When the attr_name does not exist in the group or dataset.\n",
    "        KeyError: When the dataset_name does not exist in the group.\n",
    "        ValueError: When the requested dataset is not a np.ndarray or pd.dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    with h5py.File(self.file_name, \"r\", swmr=swmr) as hdf_file:\n",
    "        if group_name is None:\n",
    "            group = hdf_file\n",
    "            group_name = \"/\"\n",
    "        else:\n",
    "            try:\n",
    "                group = hdf_file[group_name]\n",
    "            except KeyError as k:\n",
    "                raise KeyError(\n",
    "                    f\"Group {group_name} does not exist in {self}. Error {k}\"\n",
    "                )\n",
    "        if dataset_name is None:\n",
    "            if attr_name is None:\n",
    "                return sorted(group)\n",
    "            elif attr_name != \"\":\n",
    "                try:\n",
    "                    return group.attrs[attr_name]\n",
    "                except KeyError:\n",
    "                    raise KeyError(\n",
    "                        f\"Attribute {attr_name} does not exist for \"\n",
    "                        f\"group {group_name} of {self}.\"\n",
    "                    )\n",
    "            else:\n",
    "                return dict(group.attrs)\n",
    "        else:\n",
    "            try:\n",
    "                dataset = group[dataset_name]\n",
    "            except KeyError:\n",
    "                raise KeyError(\n",
    "                    f\"Dataset {dataset_name} does not exist for \"\n",
    "                    f\"group {group_name} of {self}.\"\n",
    "                )\n",
    "            if attr_name is None:\n",
    "                if isinstance(dataset, h5py.Dataset):\n",
    "                    if return_dataset_shape:\n",
    "                        return dataset.shape\n",
    "                    elif return_dataset_dtype:\n",
    "                        return dataset.dtype\n",
    "                    else:\n",
    "                        array = dataset[return_dataset_slice]\n",
    "                        # TODO: This assumes any object array is a string array\n",
    "                        if array.dtype == object:\n",
    "                            array = array.astype(str)\n",
    "                        return array\n",
    "                elif dataset.attrs[\"is_pd_dataframe\"]:\n",
    "                    if return_dataset_shape:\n",
    "                        columns = list(dataset)\n",
    "                        return (\n",
    "                            len(dataset[columns[0]]),\n",
    "                            len(columns)\n",
    "                        )\n",
    "                    elif return_dataset_dtype:\n",
    "                        return [\n",
    "                            dataset[column].dtype for column in sorted(\n",
    "                                dataset\n",
    "                            )\n",
    "                        ]\n",
    "                    else:\n",
    "                        df = pd.DataFrame(\n",
    "                            {\n",
    "                                column: dataset[column][\n",
    "                                    return_dataset_slice\n",
    "                                ] for column in sorted(dataset)\n",
    "                            }\n",
    "                        )\n",
    "                        # TODO: This assumes any object array is a string array\n",
    "                        for column in dataset:\n",
    "                            if df[column].dtype == object:\n",
    "                                df[column] = df[column].apply(\n",
    "                                    lambda x: x if isinstance(x, str) else x.decode('UTF-8')\n",
    "                                )\n",
    "                        return df\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"{dataset_name} is not a valid dataset in \"\n",
    "                        f\"group {group_name} of {self}.\"\n",
    "                    )\n",
    "            elif attr_name != \"\":\n",
    "                try:\n",
    "                    return dataset.attrs[attr_name]\n",
    "                except KeyError:\n",
    "                    raise KeyError(\n",
    "                        f\"Attribute {attr_name} does not exist for \"\n",
    "                        f\"dataset {dataset_name} of group \"\n",
    "                        f\"{group_name} of {self}.\"\n",
    "                    )\n",
    "            else:\n",
    "                return dict(dataset.attrs)\n",
    "\n",
    "\n",
    "@patch\n",
    "def write(\n",
    "    self:HDF_File,\n",
    "    value,\n",
    "    group_name:str=None,\n",
    "    dataset_name:str=None,\n",
    "    attr_name:str=None,\n",
    "    overwrite:bool=None,\n",
    "    dataset_compression:str=None,\n",
    "    swmr:bool=False,\n",
    ") -> None:\n",
    "    \"\"\"Write a `value` to an HDF_File.\n",
    "\n",
    "    Args:\n",
    "        value (type): The name of the data to write.\n",
    "            If the `value` is pd.DataFrame, a `dataset_name` must be provided.\n",
    "        group_name (str): The group where to write data.\n",
    "            If no `group_name` is provided, write directly to the root group.\n",
    "            Defaults to None.\n",
    "        dataset_name (str): If no `dataset_name` is provided,\n",
    "            create a new group with `value` as name.\n",
    "            The dataset where to write data. Defaults to None.\n",
    "        attr_name (str): The attr where to write data. Defaults to None.\n",
    "        overwrite (bool): Overwrite pre-existing data and truncate existing groups.\n",
    "            If the False, ignore the is_overwritable flag of this HDF_File.\n",
    "            Defaults to None.\n",
    "        dataset_compression (str): The compression type to use for datasets.\n",
    "            Defaults to None.\n",
    "        swmr (bool): Open files in swmr mode. Defaults to False.\n",
    "\n",
    "    Raises:\n",
    "        IOError: When the object is read-only.\n",
    "        KeyError: When the group_name or attr_name does not exist.\n",
    "        ValueError: When trying to overwrite something while overwiting is disabled.\n",
    "\n",
    "    \"\"\"\n",
    "    if self.is_read_only:\n",
    "        raise IOError(\n",
    "            f\"Trying to write to {self}, which is read_only.\"\n",
    "        )\n",
    "    if overwrite is None:\n",
    "        overwrite = self.is_overwritable\n",
    "    with h5py.File(self.file_name, \"a\", swmr=swmr) as hdf_file:\n",
    "\n",
    "        if group_name is None:\n",
    "            group = hdf_file\n",
    "            group_name = \"/\"\n",
    "        else:\n",
    "            try:\n",
    "                group = hdf_file[group_name]\n",
    "            except KeyError:\n",
    "                raise KeyError(\n",
    "                    f\"Group {group_name} does not exist in {self}.\"\n",
    "                )\n",
    "        if dataset_name is None:\n",
    "            if attr_name is None:\n",
    "                if value in group:\n",
    "                    if overwrite:\n",
    "                        del group[value]\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            f\"New group {value} already exists in group \"\n",
    "                            f\"{group_name} of {self}.\"\n",
    "                        )\n",
    "                group.create_group(value)\n",
    "            else:\n",
    "                if (attr_name in group.attrs) and not overwrite:\n",
    "                    raise ValueError(\n",
    "                        f\"Attribute {attr_name} already exists in group \"\n",
    "                        f\"{group_name} of {self}.\"\n",
    "                    )\n",
    "                try:\n",
    "                    group.attrs[attr_name] = value\n",
    "                except TypeError:\n",
    "                    group.attrs[attr_name] = str(value)\n",
    "        else:\n",
    "            if attr_name is None:\n",
    "                if dataset_name in group:\n",
    "                    if overwrite:\n",
    "                        del group[dataset_name]\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            f\"Dataset {dataset_name} already exists in group \"\n",
    "                            f\"{group_name} of {self}.\"\n",
    "                        )\n",
    "                if isinstance(value, pd.core.frame.DataFrame):\n",
    "                    new_group_name = f\"{group_name}/{dataset_name}\"\n",
    "                    self.write(\n",
    "                        dataset_name,\n",
    "                        group_name=group_name,\n",
    "                        overwrite=overwrite,\n",
    "                    )\n",
    "                    self.write(\n",
    "                        True,\n",
    "                        group_name=new_group_name,\n",
    "                        attr_name=\"is_pd_dataframe\",\n",
    "                        overwrite=overwrite,\n",
    "                    )\n",
    "                    for column in value.columns:\n",
    "                        self.write(\n",
    "                            value[column].values,\n",
    "                            group_name=new_group_name,\n",
    "                            dataset_name=column,\n",
    "                            overwrite=overwrite,\n",
    "                            dataset_compression=dataset_compression,\n",
    "                        )\n",
    "                else:\n",
    "                    dtype = value.dtype\n",
    "                    if value.dtype == np.dtype('O'):\n",
    "                        dtype = h5py.string_dtype()\n",
    "                    try:\n",
    "                        hdf_dataset = group.create_dataset(\n",
    "                            dataset_name,\n",
    "                            data=value,\n",
    "                            compression=dataset_compression,\n",
    "                            dtype=dtype\n",
    "                        )\n",
    "                    except TypeError:\n",
    "                        # TODO\n",
    "                        # print(f\"Cannot save array {value} to HDF, skipping it...\")\n",
    "                        pass\n",
    "            else:\n",
    "                try:\n",
    "                    dataset = group[dataset_name]\n",
    "                except KeyError:\n",
    "                    raise KeyError(\n",
    "                        f\"Dataset {dataset_name} does not exist for \"\n",
    "                        f\"group {group_name} of {self}.\"\n",
    "                    )\n",
    "                if (attr_name in dataset.attrs) and not overwrite:\n",
    "                    raise ValueError(\n",
    "                        f\"Attribute {attr_name} already exists in \"\n",
    "                        f\"dataset {dataset_name} of group \"\n",
    "                        f\"{group_name} of {self}.\"\n",
    "                    )\n",
    "                try:\n",
    "                    dataset.attrs[attr_name] = value\n",
    "                except TypeError:\n",
    "                    dataset.attrs[attr_name] = str(value) # e.g. dicts\n",
    "        hdf_file.attrs[\"last_updated\"] = time.asctime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit tests for this generic HDF class include:\n",
    "\n",
    "* Creation and truncation of files with various access.\n",
    "* Writing and reading data from the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import numpy as np\n",
    "import unittest\n",
    "import os\n",
    "import alphapept.io\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def define_new_test_files(test_folder):\n",
    "    test_file_names = [\n",
    "        os.path.abspath(\n",
    "            os.path.join(test_folder, f\"{file_name}.hdf\")\n",
    "        ) for file_name in [\n",
    "            \"test0\",\n",
    "            \"test1\",\n",
    "            \"test2\",\n",
    "        ]\n",
    "    ]\n",
    "    for file_name in test_file_names:\n",
    "        if os.path.isfile(file_name):\n",
    "            os.remove(file_name)\n",
    "    return test_file_names\n",
    "\n",
    "def test_hdf_file_creation(test_folder):\n",
    "    test_file_names = define_new_test_files(test_folder)\n",
    "    try:\n",
    "        f0 = HDF_File(test_file_names[0])\n",
    "    except OSError:\n",
    "        assert True\n",
    "    else:\n",
    "        assert False, \"Non-existing file should raise an error\"\n",
    "    f0 = HDF_File(test_file_names[0], is_new_file=True)\n",
    "    assert f0.is_read_only == False, \"New files should never be read-only\"\n",
    "    del f0\n",
    "    try:\n",
    "        f0 = HDF_File(test_file_names[0])\n",
    "    except OSError:\n",
    "        assert False, \"Newly created file should exist on disk\"\n",
    "    else:\n",
    "        assert True\n",
    "    assert f0.is_read_only == True, \"Existing files should be read-only\"\n",
    "    assert f0.file_name == test_file_names[0], \"File name should match given file name\"\n",
    "    assert f0.original_file_name == test_file_names[0], \"Original file name should match given file name\"\n",
    "    assert f0.version == VERSION_NO, \"Versions should match\"\n",
    "    assert str(f0) == f\"<HDF_File {test_file_names[0]}>\", \"File name should match\"\n",
    "    del f0\n",
    "    os.rename(test_file_names[0], test_file_names[1])\n",
    "    f1 = HDF_File(test_file_names[1])\n",
    "    assert f1.original_file_name != test_file_names[1], \"Original file name should not match given file name\"\n",
    "    assert f1.directory == os.path.abspath(test_folder), \"Directory should match\"\n",
    "    f0 = HDF_File(test_file_names[0], is_new_file=True)\n",
    "    assert f0 != f1, \"Different file names should be different HDF_Files\"\n",
    "    del f0\n",
    "    f1_copy = HDF_File(test_file_names[1], is_read_only=False)\n",
    "    assert f1 == f1_copy, \"Same file names should be same HDF_Files\"\n",
    "    assert f1_copy.is_read_only == False, \"File should not be read-only\"\n",
    "\n",
    "\n",
    "def test_hdf_file_read_and_write(test_folder):\n",
    "    test_file_names = define_new_test_files(test_folder)\n",
    "    f0 = HDF_File(test_file_names[0], is_new_file=True)\n",
    "    try:\n",
    "        f0.read(group_name=\"subgroup\")\n",
    "    except KeyError:\n",
    "        assert True\n",
    "    else:\n",
    "        assert False, \"subgroup should not exist\"\n",
    "    f0.write(\"subgroup\")\n",
    "    try:\n",
    "        f0.read(group_name=\"subgroup\")\n",
    "    except KeyError:\n",
    "        assert False, \"Subgroup should exist\"\n",
    "    else:\n",
    "        assert True\n",
    "    z = np.random.random((100, 4))\n",
    "    f0.write(z, group_name=\"subgroup\", dataset_name=\"random\")\n",
    "    f0_copy = HDF_File(test_file_names[0])\n",
    "    assert f0_copy.read(\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        return_dataset_shape=True\n",
    "    ) == z.shape, \"Shape of dataset is not correct\"\n",
    "    assert f0_copy.read(\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        return_dataset_dtype=True\n",
    "    ) == z.dtype, \"Type of dataset is not correct\"\n",
    "    assert np.all(\n",
    "        f0_copy.read(dataset_name=\"random\", group_name=\"subgroup\") == z\n",
    "    ), \"Contents of dataset are not correct\"\n",
    "    try:\n",
    "        f0.write(z, group_name=\"subgroup\", dataset_name=\"random\")\n",
    "    except ValueError:\n",
    "        assert True\n",
    "    else:\n",
    "        assert False, \"Should not overwrite dataset\"\n",
    "    try:\n",
    "        f0.write(z, group_name=\"subgroup\", dataset_name=\"random\", overwrite=True)\n",
    "    except ValueError:\n",
    "        assert False, \"Should be able to overwrite dataset\"\n",
    "    else:\n",
    "        assert True\n",
    "    f0.write(\n",
    "        4,\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"numeric_attr\",\n",
    "    )\n",
    "    assert f0.read(\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"numeric_attr\",\n",
    "    ) == 4, \"Attr shoud match\"\n",
    "    f0.write(\n",
    "        \"test\",\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"string_attr\",\n",
    "    )\n",
    "    assert f0.read(\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"string_attr\",\n",
    "    ) == \"test\", \"String attr shoud match\"\n",
    "    f0.write(\n",
    "        list(range(5)),\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"int_list_attr\",\n",
    "    )\n",
    "    assert np.all(\n",
    "        f0.read(\n",
    "            dataset_name=\"random\",\n",
    "            group_name=\"subgroup\",\n",
    "            attr_name=\"int_list_attr\",\n",
    "        ) == list(range(5))\n",
    "    ), \"Attr shoud match\"\n",
    "    mixed_list =  [\"test\", \"mixed\", 2, 4.9]\n",
    "    try:\n",
    "        f0.write(\n",
    "            mixed_list,\n",
    "            dataset_name=\"random\",\n",
    "            group_name=\"subgroup\",\n",
    "            attr_name=\"int_list_attr\",\n",
    "        )\n",
    "    except ValueError:\n",
    "        assert True\n",
    "    else:\n",
    "        assert False, \"Should not be able to overwrite attr\"\n",
    "    f0.write(\n",
    "        mixed_list,\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"int_list_attr\",\n",
    "        overwrite=True\n",
    "    )\n",
    "#     assert np.all(\n",
    "#         f0.read(\n",
    "#             dataset_name=\"random\",\n",
    "#             group_name=\"subgroup\",\n",
    "#             attr_name=\"int_list_attr\",\n",
    "#         ) == mixed_list\n",
    "#     ), \"Attr of mixed list should match\"\n",
    "    f0.write(\n",
    "        {\"t\": 1},\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"dict_attr\",\n",
    "        overwrite=True\n",
    "    )\n",
    "    assert np.all(\n",
    "        f0.read(\n",
    "            dataset_name=\"random\",\n",
    "            group_name=\"subgroup\",\n",
    "            attr_name=\"dict_attr\",\n",
    "        ) == str({\"t\": 1})\n",
    "    ), \"Attr shoud match\"\n",
    "    \n",
    "\n",
    "def test_hdf_file_data_frames(test_folder):\n",
    "    test_file_names = define_new_test_files(test_folder)\n",
    "    f0 = HDF_File(test_file_names[0], is_new_file=True)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"col1\": np.arange(10, dtype=np.float16) / 2,\n",
    "            \"col2\": np.arange(10),\n",
    "        }\n",
    "    )\n",
    "    f0.write(df, dataset_name=\"df\")\n",
    "    z = f0.read(dataset_name=\"df\")\n",
    "    assert z.equals(df)\n",
    "    \n",
    "test_hdf_file_creation(test_folder=\"tmp\")\n",
    "test_hdf_file_read_and_write(test_folder=\"tmp\")\n",
    "test_hdf_file_data_frames(test_folder=\"tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final `.ms_data.hdf` file\n",
    "\n",
    "Based on the generic HDF_File, a subclass that acts as an MS data container can be implemented. This class should contain all (centroided) fragment ions, all their coordinates, and all the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MS_Data_File(HDF_File):\n",
    "    \"\"\" A class to store and retrieve on-disk MS data with an HDF container.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single generic function should allow to read raw data and store spectra. Different arguments allow different vendor formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def import_raw_DDA_data(\n",
    "    self:MS_Data_File,\n",
    "    file_name:str,\n",
    "    n_most_abundant:int=-1,\n",
    "    callback:callable=None,\n",
    "    query_data:dict=None,\n",
    "    vendor:str=None\n",
    ") -> None:\n",
    "    \"\"\"Load centroided data and save it to this object.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The file name with raw data (Thermo, Bruker or mzml).\n",
    "        n_most_abundant (int): The maximum number of peaks to retain per MS2 spectrum.\\\n",
    "            Defaults to -1.\n",
    "        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\n",
    "        query_data (dict):\n",
    "            A dictionary with raw data.\n",
    "            If this is not None, data will only be saved and not imported.\n",
    "            Defaults to None.\n",
    "        vendor (str): The vendor name, must be Thermo or Bruker if provided.\n",
    "            Defaults to None.\n",
    "\n",
    "    \"\"\"\n",
    "    base, ext = os.path.splitext(file_name)\n",
    "    if query_data is None:\n",
    "        query_data, vendor, acquisition_date_time = _read_DDA_query_data(\n",
    "            file_name,\n",
    "            n_most_abundant=n_most_abundant,\n",
    "            callback=callback\n",
    "        )\n",
    "    self._save_DDA_query_data(query_data, vendor, acquisition_date_time)\n",
    "    \n",
    "    \n",
    "def index_ragged_list(ragged_list: list)  -> np.ndarray:\n",
    "    \"\"\"Create lookup indices for a list of arrays for concatenation.\n",
    "\n",
    "    Args:\n",
    "        value (list): Input list of arrays.\n",
    "\n",
    "    Returns:\n",
    "        indices: A numpy array with indices.\n",
    "    \"\"\"\n",
    "    indices = np.zeros(len(ragged_list) + 1, np.int64)\n",
    "    indices[1:] = [len(i) for i in ragged_list]\n",
    "    indices = np.cumsum(indices)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def _read_DDA_query_data(\n",
    "    file_name:str,\n",
    "    n_most_abundant:int=-1,\n",
    "    callback:callable=None\n",
    ") -> tuple:\n",
    "    \"\"\"Read raw data and return as query dictionary.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The file name with raw data (Thermo, Bruker or mzml).\n",
    "        n_most_abundant (int): The maximum number of peaks to retain per MS2 spectrum.\\\n",
    "            Defaults to -1.\n",
    "        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple with (query_data, vendor, acquisition_date_time).\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: If the raw data is no Bruker, Thermo or mzml.\n",
    "\n",
    "    \"\"\"\n",
    "    base, ext = os.path.splitext(file_name)\n",
    "    if ext.lower() == '.raw':\n",
    "        if os.path.isdir(file_name):\n",
    "            vendor = \"Waters\"\n",
    "            acquisition_date_time = None\n",
    "            raise NotImplementedError(\n",
    "                f'File extension {ext} indicates Waters, which is not implemented.'\n",
    "            )\n",
    "        else:\n",
    "            vendor = \"Thermo\"\n",
    "            logging.info(f'File {base} has extension {ext} - converting from {vendor}.')\n",
    "            query_data, acquisition_date_time = load_thermo_raw(\n",
    "                file_name,\n",
    "                n_most_abundant,\n",
    "                callback=callback,\n",
    "            )\n",
    "    elif ext.lower() == '.d':\n",
    "        vendor = \"Bruker\"\n",
    "        logging.info(f'File {base} has extension {ext} - converting from {vendor}.')\n",
    "        query_data, acquisition_date_time = load_bruker_raw(\n",
    "            file_name,\n",
    "            n_most_abundant,\n",
    "            callback=callback,\n",
    "        )\n",
    "    elif ext.lower() == '.mzml':\n",
    "        logging.info(f'File {base} has extension {ext} - converting from {ext[1:]}.')\n",
    "        query_data, acquisition_date_time, vendor = load_mzml_data(\n",
    "            file_name,\n",
    "            n_most_abundant,\n",
    "            callback=callback,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f'File extension {ext} not understood.')\n",
    "    logging.info(\n",
    "        f'File conversion complete. Extracted {len(query_data[\"prec_mass_list2\"]):,} precursors.'\n",
    "    )\n",
    "    return query_data, vendor, acquisition_date_time\n",
    "\n",
    "\n",
    "@patch\n",
    "def _save_DDA_query_data(\n",
    "    self:MS_Data_File,\n",
    "    query_data:dict,\n",
    "    vendor:str,\n",
    "    acquisition_date_time:str,\n",
    "    overwrite:bool=False\n",
    ") -> None:\n",
    "    \"\"\"Save a query dict to this ms_data object.\n",
    "\n",
    "    Args:\n",
    "        query_data (dict): A dictionary with data for MS1 and MS2 scans.\n",
    "        vendor (str): The vendor name, must be Thermo or Bruker if provided.\n",
    "        acquisition_date_time (str): A string that indicates when the data was acquired.\n",
    "        overwrite (bool): Overwrite pre-existing data and truncate existing groups.\n",
    "            If the False, ignore the is_overwritable flag of this HDF_File.\n",
    "            Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the query_dict contains keys that do not end with 1 or 2.\n",
    "            i.e. are not MS1 or MS2 spectra.\n",
    "\n",
    "    \"\"\"\n",
    "#     if vendor == \"Bruker\":\n",
    "#         raise NotImplementedError(\"Unclear what are ms1 and ms2 attributes for bruker\")\n",
    "    if \"Raw\" not in self.read():\n",
    "        self.write(\"Raw\")\n",
    "    self.write(vendor, group_name=\"Raw\", attr_name=\"vendor\")\n",
    "    self.write(acquisition_date_time, group_name=\"Raw\", attr_name=\"acquisition_date_time\")\n",
    "    if \"MS1_scans\" not in self.read(group_name=\"Raw\"):\n",
    "        self.write(\"MS1_scans\", group_name=\"Raw\")\n",
    "    if \"MS2_scans\" not in self.read(group_name=\"Raw\"):\n",
    "        self.write(\"MS2_scans\", group_name=\"Raw\")\n",
    "    for key, value in query_data.items():\n",
    "        if key.endswith(\"1\"):\n",
    "#             TODO: Weak check for ms2, imporve to _ms1 if consistency in naming is guaranteed\n",
    "            if key == \"mass_list_ms1\":\n",
    "                indices = index_ragged_list(value)\n",
    "                self.write(\n",
    "                    indices,\n",
    "                    dataset_name=\"indices_ms1\",\n",
    "                    group_name=f\"Raw/MS1_scans\"\n",
    "                )\n",
    "                value = np.concatenate(value)\n",
    "            elif key == \"int_list_ms1\":\n",
    "                value = np.concatenate(value)\n",
    "            self.write(\n",
    "                value,\n",
    "#                 TODO: key should be trimmed: xxx_ms1 should just be e.g. xxx\n",
    "                dataset_name=key,\n",
    "                group_name=f\"Raw/MS1_scans\"\n",
    "            )\n",
    "        elif key.endswith(\"2\"):\n",
    "#             TODO: Weak check for ms2, imporve to _ms2 if consistency in naming is guaranteed\n",
    "            if key == \"mass_list_ms2\":\n",
    "                indices = index_ragged_list(value)\n",
    "                self.write(\n",
    "                    indices,\n",
    "                    dataset_name=\"indices_ms2\",\n",
    "                    group_name=f\"Raw/MS2_scans\"\n",
    "                )\n",
    "                if len(value) > 1: #in case there is no MS2\n",
    "                    value = np.concatenate(value)\n",
    "                else:\n",
    "                    value = np.array(value)\n",
    "            elif key == \"int_list_ms2\":\n",
    "                if len(value) > 1: #in case there is no MS2\n",
    "                    value = np.concatenate(value)\n",
    "                else:\n",
    "                    value = np.array(value)\n",
    "            self.write(\n",
    "                value,\n",
    "#                 TODO: key should be trimmed: xxx_ms2 should just be e.g. xxx\n",
    "                dataset_name=key,\n",
    "                group_name=f\"Raw/MS2_scans\"\n",
    "            )\n",
    "        else:\n",
    "            raise KeyError(\"Unspecified scan type\")\n",
    "    return\n",
    "#     to_save[\"bounds\"] = np.sum(to_save['mass_list_ms2']>=0,axis=0).astype(np.int64)\n",
    "#     logging.info('Converted file saved to {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing of the MS_Data_File container includes reading and writing from different file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def test_ms_data_file_import_thermo(file_name):\n",
    "    local_file_name = os.path.basename(file_name)\n",
    "    output_path = os.path.dirname(file_name)\n",
    "    base_file_name, ext = os.path.splitext(local_file_name)\n",
    "    output_file_name = os.path.join(output_path, base_file_name+\".ms_data.hdf\")\n",
    "    ms_data_file = MS_Data_File(\n",
    "        output_file_name,\n",
    "        is_new_file=True\n",
    "    )\n",
    "    ms_data_file.import_raw_DDA_data(\n",
    "        file_name,\n",
    "        n_most_abundant = settings[\"raw\"][\"n_most_abundant\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# print(time.asctime())\n",
    "# test_ms_data_file_import_thermo(\n",
    "#     \"/Users/swillems/Documents/sandbox/alphapept_projects/09-07-18_EcoliSpikeIn_1xF1R1.raw\"\n",
    "# )\n",
    "# print(time.asctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While that HDF data structure could be used directly, it is often easier to read it and return a `query_data` dictionary similar to those that are returned by the readers of `Thermo`, `Bruker`, `mzML` and `mzXML` raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def read_DDA_query_data(\n",
    "    self:MS_Data_File,\n",
    "    calibrated_fragments:bool=False,\n",
    "    force_recalibrate:bool=False,\n",
    "    swmr:bool=False,\n",
    "    **kwargs\n",
    ") -> dict:\n",
    "    \"\"\"Read query data from this ms_data object and return it as a query_dict.\n",
    "\n",
    "    Args:\n",
    "        calibrated_fragments (bool): If True, calibrated fragments are retrieved.\n",
    "            Calibration offsets can already be present in the ms_data or recalculated.\n",
    "            Defaults to False.\n",
    "        force_recalibrate (bool): If calibrated fragments is True,\n",
    "            recalibrate mzs values even if a recalibration is already provided.\n",
    "            Defaults to False.\n",
    "        swmr (bool): Open the file in swmr mode. Defaults to False.\n",
    "        **kwargs (type): Can contain a database file name that was used for recalibration.\n",
    "\n",
    "    Returns:\n",
    "        dict: A query_dict with data for MS1 and MS2 scans.\n",
    "\n",
    "    \"\"\"\n",
    "    query_data = {}\n",
    "    for dataset_name in self.read(group_name=\"Raw/MS1_scans\"):\n",
    "        values = self.read(\n",
    "            dataset_name=dataset_name,\n",
    "            group_name=\"Raw/MS1_scans\",\n",
    "            swmr=swmr,\n",
    "        )\n",
    "        query_data[dataset_name] = values\n",
    "    for dataset_name in self.read(group_name=\"Raw/MS2_scans\"):\n",
    "        values = self.read(\n",
    "            dataset_name=dataset_name,\n",
    "            group_name=\"Raw/MS2_scans\",\n",
    "            swmr=swmr\n",
    "        )\n",
    "        query_data[dataset_name] = values\n",
    "#     indices_ms1 = query_data[\"indices_ms1\"]\n",
    "#     mz_ms1 = query_data[\"mass_list_ms1\"]\n",
    "#     query_data[\"mass_list_ms1\"] = np.array(\n",
    "#         [mz_ms1[s:e] for s,e in zip(indices_ms1[:-1], indices_ms1[1:])]\n",
    "#     )\n",
    "#     int_ms1 = query_data[\"int_list_ms1\"]\n",
    "#     query_data[\"int_list_ms1\"] = np.array(\n",
    "#         [int_ms1[s:e] for s,e in zip(indices_ms1[:-1], indices_ms1[1:])]\n",
    "#     )\n",
    "    indices_ms2 = query_data[\"indices_ms2\"]\n",
    "#     mz_ms2 = query_data[\"mass_list_ms2\"]\n",
    "#     query_data[\"mass_list_ms2\"] = np.array(\n",
    "#         [mz_ms2[s:e] for s,e in zip(indices_ms2[:-1], indices_ms2[1:])]\n",
    "#     )\n",
    "#     int_ms2 = query_data[\"int_list_ms2\"]\n",
    "#     query_data[\"int_list_ms2\"] = np.array(\n",
    "#         [int_ms2[s:e] for s,e in zip(indices_ms2[:-1], indices_ms2[1:])]\n",
    "#     )\n",
    "    if self.read(attr_name=\"vendor\", group_name=\"Raw\") == \"Bruker\":\n",
    "        query_data[\"mobility\"] = query_data[\"mobility2\"]\n",
    "        query_data[\"prec_id\"] = query_data[\"prec_id2\"]\n",
    "    if calibrated_fragments:\n",
    "        if (\"corrected_fragment_mzs\" not in self.read()) or force_recalibrate:\n",
    "#         if True:\n",
    "            logging.info(\"Calibrating fragments\")\n",
    "            import alphapept.recalibration\n",
    "            alphapept.recalibration.calibrate_fragments(\n",
    "                kwargs[\"database_file_name\"],\n",
    "                self.file_name,\n",
    "            )\n",
    "        query_data[\"mass_list_ms2\"] *= (\n",
    "            1 - self.read(\n",
    "                dataset_name=\"corrected_fragment_mzs\", swmr=swmr\n",
    "            ) / 10**6\n",
    "        )\n",
    "    return query_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def test_get_query_datafrom_thermo_ms_file(file_name):\n",
    "    local_file_name = os.path.basename(file_name)\n",
    "    output_path = os.path.dirname(file_name)\n",
    "    base_file_name, ext = os.path.splitext(local_file_name)\n",
    "    output_file_name = os.path.join(output_path, base_file_name+\".ms_data.hdf\")\n",
    "    ms_data_file = MS_Data_File(\n",
    "        output_file_name,\n",
    "    )\n",
    "    return ms_data_file.read_DDA_query_data()\n",
    "\n",
    "\n",
    "# print(time.asctime())\n",
    "# qd = test_get_query_datafrom_thermo_ms_file(\n",
    "#     \"/Users/swillems/Documents/sandbox/alphapept_projects/09-07-18_EcoliSpikeIn_1xF1R1.raw\"\n",
    "# )\n",
    "# print(time.asctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage in workflows\n",
    "\n",
    "To use all the above functionality from a workflow with several parameters, the following functions are defined. These functions also allow parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def raw_conversion(\n",
    "    to_process: dict,\n",
    "    callback: callable = None,\n",
    "    parallel:bool = False\n",
    ") -> bool:\n",
    "    \"\"\"Wrapper function to convert raw to ms_data_file hdf.\n",
    "\n",
    "    Args:\n",
    "        to_process (dict): A dictionary with settings indicating which files are to be processed and how.\n",
    "        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\n",
    "        parallel (bool): If True, process multiple files in parallel.\n",
    "            This is not implemented yet!\n",
    "            Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if and only if the conversion was succesful.\n",
    "\n",
    "    \"\"\"\n",
    "    index, settings = to_process\n",
    "    file_name = settings['experiment']['file_paths'][index]\n",
    "    try:\n",
    "        local_file_name = os.path.basename(file_name)\n",
    "        output_path = os.path.dirname(file_name)\n",
    "        base_file_name, ext = os.path.splitext(local_file_name)\n",
    "        output_file_name = os.path.join(output_path, base_file_name+\".ms_data.hdf\")\n",
    "\n",
    "        if not os.path.isfile(output_file_name):\n",
    "            ms_data_file = MS_Data_File(\n",
    "                output_file_name,\n",
    "                is_new_file=True\n",
    "            )\n",
    "            ms_data_file.import_raw_DDA_data(\n",
    "                file_name,\n",
    "                n_most_abundant = settings[\"raw\"][\"n_most_abundant\"]\n",
    "            )\n",
    "\n",
    "        logging.info(f'File conversion of file {file_name} complete.')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f'File conversion of file {file_name} failed. Exception {e}')\n",
    "        return f\"{e}\" #Can't return exception object, cast as string\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands are bookkeeping to make sure this and other notebooks are properly parsed to python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted 12_performance.ipynb.\n",
      "Converted 13_export.ipynb.\n",
      "Converted 14_display.ipynb.\n",
      "Converted 15_label.ipynb.\n",
      "Converted additional_code.ipynb.\n",
      "Converted contributing.ipynb.\n",
      "Converted file_formats.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphapept",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
