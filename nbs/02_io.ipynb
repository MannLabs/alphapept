{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input / Output\n",
    "\n",
    "> Functions related to input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all functions related to importing and exporting files. To access proprietary data formats, we have import functions to access `Bruker` and `Thermo` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing and accessing MS data\n",
    "\n",
    "As MS hardware has continued to improve over the years, MS data has become more complex. To deal with this complexity, the MS community hase already used many different [data formats](https://onlinelibrary.wiley.com/doi/full/10.1002/mas.21522) to store and access. [HDF](https://www.hdfgroup.org/solutions/hdf5/) containers are one option, but they have not yet gained widespread support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF containers\n",
    "In general, an HDF container can be viewed as compressed folder with metadata (i.e. attributes) associated to each single subfolder or file (i.e. data arrays of various types and sizes) within this container. A container might for instance have contents that look like e.g.:\n",
    "```\n",
    "HDF_Container\n",
    "    {\n",
    "        meta_data_1: \"Some string\",\n",
    "        meta_data_2: 1234567890,\n",
    "        ...\n",
    "    }\n",
    "    array_1\n",
    "        {\n",
    "            meta_data_of_array1_1: \"Some other string\",\n",
    "            ...\n",
    "        },\n",
    "        100x2 int8\n",
    "    array_2\n",
    "        1000x2 float64\n",
    "    subfolder_1\n",
    "        {\n",
    "            meta_data_of_subfolder_1_1: \"Really any string of any length\",\n",
    "            ...\n",
    "        }\n",
    "        array_1_of_subfolder_1\n",
    "        subfolder_1_1\n",
    "        ...\n",
    "    subfolder_n\n",
    "    ...\n",
    "```\n",
    "\n",
    "\n",
    "A few of the advantages of HDF are e.g.:\n",
    "\n",
    "* It has no upper limit with regards to file size\n",
    "* It can be used on disk without consuming RAM memory\n",
    "* It is fully portable on different machines and systems\n",
    "* It is capable of fast IO operations\n",
    "* It allows data to be structured very transparent, while still providing a flexible way to store metadata\n",
    "* ...\n",
    "\n",
    "For these reasons, HDF containers have gained popularity in several scientific fields, including (astro)physics and geology. It is therefore no surprise that python has excellent support for HDF containers. The two most used packages are `h5py` and `tables`, where the former has generic API and the second is frequently used with `pandas` dataframes. An excellent viewer for HDF files is [HDF Compass](https://support.hdfgroup.org/projects/compass/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using HDF containers for MS data\n",
    "We will use `h5py` to store MS data in HDF containers, inspired by the [ion_networks](https://github.com/swillems/ion_networks) repository.\n",
    "\n",
    "* First we define a generic class that will serve as an API for HDF containers. To ensure full transparancy, we will include immutable metadata such as `creation time`, `original_file_name` and `version`.\n",
    "* The constructor of an HDF_File will be passed the `file_name` of an HDF container, an `is_read_only` flag and `is_new_file` flag.\n",
    "* To compare HDF_Files, several (magic) functions need to be defined.\n",
    "* Traceabilty and reproducibility are ensured by storing a `last_updated` and a `check` function to warn users about potential compatability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "from alphapept.__main__ import VERSION_NO\n",
    "\n",
    "\n",
    "class HDF_File(object):\n",
    "    '''\n",
    "    A generic class to store and retrieve on-disk\n",
    "    data with an HDF container.\n",
    "    '''\n",
    "\n",
    "    @property\n",
    "    def original_file_name(self):\n",
    "        return self.read(\n",
    "            attr_name=\"original_file_name\"\n",
    "        )  # See below for function definition\n",
    "\n",
    "    @property\n",
    "    def file_name(self):\n",
    "        return self.__file_name\n",
    "    \n",
    "    @property\n",
    "    def directory(self):\n",
    "        return os.path.dirname(self.file_name)\n",
    "    \n",
    "    @property\n",
    "    def creation_time(self):\n",
    "        return self.read(\n",
    "            attr_name=\"creation_time\"\n",
    "        )  # See below for function definition\n",
    "\n",
    "    @property\n",
    "    def last_updated(self):\n",
    "        return self.read(\n",
    "            attr_name=\"last_updated\"\n",
    "        )  # See below for function definition\n",
    "\n",
    "    @property\n",
    "    def version(self):\n",
    "        return self.read(\n",
    "            attr_name=\"version\"\n",
    "        )  # See below for function definition\n",
    "        \n",
    "    @property\n",
    "    def is_read_only(self):\n",
    "        return self.__is_read_only\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        file_name:str,\n",
    "        is_read_only:bool=True,\n",
    "        is_new_file:bool=False,\n",
    "    ):\n",
    "        self.__file_name = os.path.abspath(file_name)\n",
    "        if is_new_file:\n",
    "            is_read_only = False\n",
    "            if not os.path.exists(self.directory):\n",
    "                os.makedirs(self.directory)\n",
    "            with h5py.File(self.file_name, \"w\") as hdf_file:\n",
    "                current_time = time.asctime()\n",
    "                hdf_file.attrs[\"creation_time\"] = current_time\n",
    "                hdf_file.attrs[\"original_file_name\"] = self.__file_name\n",
    "                hdf_file.attrs[\"version\"] = VERSION_NO\n",
    "                hdf_file.attrs[\"last_updated\"] = current_time\n",
    "        else:\n",
    "            with h5py.File(self.file_name, \"r\") as hdf_file:\n",
    "                self.check()\n",
    "        self.__is_read_only = is_read_only\n",
    "               \n",
    "    def __eq__(self, other):\n",
    "        return self.file_name == other.file_name\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.file_name)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<HDF_File {self.file_name}>\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def check(\n",
    "        self,\n",
    "        version:bool=True,\n",
    "        file_name:bool=True,\n",
    "    ):\n",
    "        '''\n",
    "        Check if the `version` or `file_name` of this HDF_File have changed.\n",
    "        This requires to define a global LOGGER and VERSION_NO variable.\n",
    "        '''\n",
    "        warning_messages = []\n",
    "        if version:\n",
    "            current_version = VERSION_NO\n",
    "            creation_version = self.version\n",
    "            if creation_version != current_version:\n",
    "                warning_messages.append(\n",
    "                    f\"{self} was created with version \"\n",
    "                    f\"{creation_version} instead of {current_version}.\"\n",
    "                )\n",
    "        if file_name:\n",
    "            if self.file_name != self.original_file_name:\n",
    "                warning_messages.append(\n",
    "                    f\"The file name of {self} has been changed from\"\n",
    "                    f\"{self.original_file_name} to {self.file_name}.\"\n",
    "                )\n",
    "        if len(warning_messages) != 0:\n",
    "            try:\n",
    "                printer = LOGGER.warning\n",
    "            except NameError:\n",
    "                printer = print\n",
    "                warning_messages.append(\n",
    "                    \"No LOGGER has been defined, using normal print instead.\"\n",
    "                )   \n",
    "            printer(\"\\n\".join(warning_messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents of HDF containers come in three variants:\n",
    "\n",
    "1. `Groups`: folders\n",
    "2. `Datasets`: arrays\n",
    "3. `Attributes`: metadata associated to individual datasets or groups (with the root folder also considered as a normal group)\n",
    "\n",
    "These contents can be accessed with `read` and `write` funtions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "\n",
    "@patch\n",
    "def read(\n",
    "    self:HDF_File,\n",
    "    group_name:str=None,\n",
    "    dataset_name:str=None,\n",
    "    attr_name:str=None,\n",
    "    return_dataset_shape:bool=False,\n",
    "    return_dataset_dtype:bool=False,\n",
    "    return_dataset_slice:slice=slice(None),\n",
    "):\n",
    "    '''\n",
    "    Read the contents of an HDF_File. If no `group_name` has been provided,\n",
    "    read directly from the root group. If no `dataset_name` has been provided,\n",
    "    read directly from the group. If `attr_name` is not None,\n",
    "    read the attribute value instead of the contents of a group or dataset.\n",
    "    If `attr_name` == \"\", read all attributes as a dict.\n",
    "    The options `return_dataset_shape`, `return_dataset_dtype` and\n",
    "    `return_dataset_slice` allow to minimize IO and RAM usage by reading\n",
    "    datasets only partially.\n",
    "    '''\n",
    "    with h5py.File(self.file_name, \"r\") as hdf_file:\n",
    "        if group_name is None:\n",
    "            group = hdf_file\n",
    "            group_name = \"/\"\n",
    "        else:\n",
    "            try:\n",
    "                group = hdf_file[group_name]\n",
    "            except KeyError:\n",
    "                raise KeyError(\n",
    "                    f\"Group {group_name} does not exist in {self}.\"\n",
    "                )\n",
    "        if dataset_name is None:\n",
    "            if attr_name is None:\n",
    "                return sorted(group)\n",
    "            elif attr_name != \"\":\n",
    "                try:\n",
    "                    return group.attrs[attr_name]\n",
    "                except KeyError:\n",
    "                    raise keyError(\n",
    "                        f\"Attribute {attr_name} does not exist for \"\n",
    "                        f\"group {group_name} of {self}.\"\n",
    "                    )\n",
    "            else:\n",
    "                return dict(group.attrs)\n",
    "        else:\n",
    "            try:\n",
    "                dataset = group[dataset_name]\n",
    "            except KeyError:\n",
    "                raise KeyError(\n",
    "                    f\"Dataset {dataset_name} does not exist for \"\n",
    "                    f\"group {group_name} of {self}.\"\n",
    "                )\n",
    "            if attr_name is None:\n",
    "                if isinstance(dataset, h5py.Dataset):\n",
    "                    if return_dataset_shape:\n",
    "                        return dataset.shape\n",
    "                    elif return_dataset_dtype:\n",
    "                        return dataset.dtype\n",
    "                    else:\n",
    "                        return dataset[return_dataset_slice]\n",
    "                else:\n",
    "                    raise NotImplementedError(\n",
    "                        \"Use group as pandas dataframe container?\"\n",
    "                    )\n",
    "            elif attr_name != \"\":\n",
    "                try:\n",
    "                    return dataset.attrs[attr_name]\n",
    "                except KeyError:\n",
    "                    raise KeyError(\n",
    "                        f\"Attribute {attr_name} does not exist for \"\n",
    "                        f\"dataset {dataset_name} of group \"\n",
    "                        f\"{group_name} of {self}.\"\n",
    "                    )\n",
    "            else:\n",
    "                return dict(dataset.attrs)\n",
    "\n",
    "\n",
    "@patch\n",
    "def write(\n",
    "    self:HDF_File,\n",
    "    value,\n",
    "    group_name:str=None,\n",
    "    dataset_name:str=None,\n",
    "    attr_name:str=None,\n",
    "    overwrite:bool=False,\n",
    "    dataset_compression=None\n",
    "):\n",
    "    '''\n",
    "    Write a `value` to an HDF_File. If an 'attr_name' is provided,\n",
    "    `value` will be stored for this attribute.\n",
    "    If no `group_name` is provided, write directly to the root group.\n",
    "    If no `dataset_name` is provided, create a new group with `value`\n",
    "    as name. If a 'dataset_name' is provided, a 'dataset_compression`\n",
    "    can be defined to minimize disk usage, at the cost of slower IO.\n",
    "    If the `overwrite` flag is True, overwrite the given attribute\n",
    "    or dataset and truncate groups.\n",
    "    '''\n",
    "    if self.is_read_only:\n",
    "        raise IOError(\n",
    "            f\"Trying to write to {self}, which is read_only.\"\n",
    "        )\n",
    "    with h5py.File(self.file_name, \"a\") as hdf_file:\n",
    "        if group_name is None:\n",
    "            group = hdf_file\n",
    "            group_name = \"/\"\n",
    "        else:\n",
    "            try:\n",
    "                group = hdf_file[group_name]\n",
    "            except KeyError:\n",
    "                raise KeyError(\n",
    "                    f\"Group {group_name} does not exist in {self}.\"\n",
    "                )\n",
    "        if dataset_name is None:\n",
    "            if attr_name is None:\n",
    "                if value in group:\n",
    "                    if overwrite:\n",
    "                        del group[value]\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            f\"New group {value} already exists in group \"\n",
    "                            f\"{group_name} of {self}.\"\n",
    "                        )\n",
    "                group.create_group(value)\n",
    "            else:\n",
    "                if (attr_name in group.attrs) and not overwrite:\n",
    "                    raise ValueError(\n",
    "                        f\"Attribute {attr_name} already exists in group \"\n",
    "                        f\"{group_name} of {self}.\"\n",
    "                    )\n",
    "                try:\n",
    "                    group.attrs[attr_name] = value\n",
    "                except TypeError:\n",
    "                    group.attrs[attr_name] = str(value)\n",
    "        else:\n",
    "            if attr_name is None:\n",
    "                if dataset_name in group:\n",
    "                    if overwrite:\n",
    "                        del group[dataset_name]\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            f\"Dataset {dataset_name} already exists in group \"\n",
    "                            f\"{group_name} of {self}.\"\n",
    "                        )\n",
    "                if isinstance(value, pd.core.frame.DataFrame):\n",
    "                    raise NotImplementedError(\n",
    "                        \"Use group as pandas dataframe container?\"\n",
    "                    )\n",
    "                if value.dtype.type == np.str_:\n",
    "                    value = value.astype(np.dtype('O'))\n",
    "                if value.dtype == np.dtype('O'):\n",
    "                    hdf_dataset = group.create_dataset(\n",
    "                        dataset_name,\n",
    "                        data=value,\n",
    "                        compression=dataset_compression,\n",
    "                        dtype=h5py.string_dtype()\n",
    "                    )\n",
    "                else:\n",
    "                    hdf_dataset = group.create_dataset(\n",
    "                        dataset_name,\n",
    "                        data=value,\n",
    "                        compression=dataset_compression,\n",
    "                    )\n",
    "            else:\n",
    "                try:\n",
    "                    dataset = group[dataset_name]\n",
    "                except KeyError:\n",
    "                    raise KeyError(\n",
    "                        f\"Dataset {dataset_name} does not exist for \"\n",
    "                        f\"group {group_name} of {self}.\"\n",
    "                    )\n",
    "                if (attr_name in dataset.attrs) and not overwrite:\n",
    "                    raise ValueError(\n",
    "                        f\"Attribute {attr_name} already exists in \"\n",
    "                        f\"dataset {dataset_name} of group \"\n",
    "                        f\"{group_name} of {self}.\"\n",
    "                    )\n",
    "                try:\n",
    "                    dataset.attrs[attr_name] = value\n",
    "                except TypeError:\n",
    "                    dataset.attrs[attr_name] = str(value) # e.g. dicts\n",
    "        hdf_file.attrs[\"last_updated\"] = time.asctime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit tests for this generic HDF class include:\n",
    "\n",
    "* Creation and truncation of file with various access.\n",
    "* Writing and reading data from the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file name of <HDF_File /Users/swillems/Documents/software/alphapept/nbs/tmp/test1.hdf> has been changed from/Users/swillems/Documents/software/alphapept/nbs/tmp/test0.hdf to /Users/swillems/Documents/software/alphapept/nbs/tmp/test1.hdf.\n",
      "No LOGGER has been defined, using normal print instead.\n",
      "The file name of <HDF_File /Users/swillems/Documents/software/alphapept/nbs/tmp/test1.hdf> has been changed from/Users/swillems/Documents/software/alphapept/nbs/tmp/test0.hdf to /Users/swillems/Documents/software/alphapept/nbs/tmp/test1.hdf.\n",
      "No LOGGER has been defined, using normal print instead.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "\n",
    "def define_new_test_files(test_folder):\n",
    "    test_file_names = [\n",
    "        os.path.abspath(\n",
    "            os.path.join(test_folder, f\"{file_name}.hdf\")\n",
    "        ) for file_name in [\n",
    "            \"test0\",\n",
    "            \"test1\",\n",
    "            \"test2\",\n",
    "        ]\n",
    "    ]\n",
    "    for file_name in test_file_names:\n",
    "        if os.path.isfile(file_name):\n",
    "            os.remove(file_name)\n",
    "    return test_file_names\n",
    "\n",
    "def test_hdf_file_creation(test_folder):\n",
    "    test_file_names = define_new_test_files(test_folder)\n",
    "    try:\n",
    "        f0 = HDF_File(test_file_names[0])\n",
    "    except OSError:\n",
    "        assert True\n",
    "    else:\n",
    "        assert False, \"Non-existing file should raise an error\"\n",
    "    f0 = HDF_File(test_file_names[0], is_new_file=True)\n",
    "    assert f0.is_read_only == False, \"New files should never be read-only\"\n",
    "    del f0\n",
    "    try:\n",
    "        f0 = HDF_File(test_file_names[0])\n",
    "    except OSError:\n",
    "        assert False, \"Newly created file should exist on disk\"\n",
    "    else:\n",
    "        assert True\n",
    "    assert f0.is_read_only == True, \"Existing files should be read-only\"\n",
    "    assert f0.file_name == test_file_names[0], \"File name should match given file name\"\n",
    "    assert f0.original_file_name == test_file_names[0], \"Original file name should match given file name\"\n",
    "    assert f0.version == VERSION_NO, \"Versions should match\"\n",
    "    assert str(f0) == f\"<HDF_File {test_file_names[0]}>\", \"File name should match\"\n",
    "    del f0\n",
    "    os.rename(test_file_names[0], test_file_names[1])\n",
    "    f1 = HDF_File(test_file_names[1])\n",
    "    assert f1.original_file_name != test_file_names[1], \"Original file name should not match given file name\"\n",
    "    assert f1.directory == os.path.abspath(test_folder), \"Directory should match\"\n",
    "    f0 = HDF_File(test_file_names[0], is_new_file=True)\n",
    "    assert f0 != f1, \"Different file names should be different HDF_Files\"\n",
    "    del f0\n",
    "    f1_copy = HDF_File(test_file_names[1], is_read_only=False)\n",
    "    assert f1 == f1_copy, \"Same file names should be same HDF_Files\"\n",
    "    assert f1_copy.is_read_only == False, \"File should not be read-only\"\n",
    "\n",
    "\n",
    "def test_hdf_file_read_and_write(test_folder):\n",
    "    test_file_names = define_new_test_files(test_folder)\n",
    "    f0 = HDF_File(test_file_names[0], is_new_file=True)\n",
    "    try:\n",
    "        f0.read(group_name=\"subgroup\")\n",
    "    except KeyError:\n",
    "        assert True\n",
    "    else:\n",
    "        assert False, \"subgroup should not exist\"\n",
    "    f0.write(\"subgroup\")\n",
    "    try:\n",
    "        f0.read(group_name=\"subgroup\")\n",
    "    except KeyError:\n",
    "        assert False, \"Subgroup should exist\"\n",
    "    else:\n",
    "        assert True\n",
    "    z = np.random.random((100, 4))\n",
    "    f0.write(z, group_name=\"subgroup\", dataset_name=\"random\")\n",
    "    f0_copy = HDF_File(test_file_names[0])\n",
    "    assert f0_copy.read(\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        return_dataset_shape=True\n",
    "    ) == z.shape, \"Shape of dataset is not correct\"\n",
    "    assert f0_copy.read(\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        return_dataset_dtype=True\n",
    "    ) == z.dtype, \"Type of dataset is not correct\"\n",
    "    assert np.all(\n",
    "        f0_copy.read(dataset_name=\"random\", group_name=\"subgroup\") == z\n",
    "    ), \"Contents of dataset are not correct\"\n",
    "    try:\n",
    "        f0.write(z, group_name=\"subgroup\", dataset_name=\"random\")\n",
    "    except ValueError:\n",
    "        assert True\n",
    "    else:\n",
    "        assert False, \"Should not overwrite dataset\"\n",
    "    try:\n",
    "        f0.write(z, group_name=\"subgroup\", dataset_name=\"random\", overwrite=True)\n",
    "    except ValueError:\n",
    "        assert False, \"Should be able to overwrite dataset\"\n",
    "    else:\n",
    "        assert True\n",
    "    f0.write(\n",
    "        4,\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"numeric_attr\",\n",
    "    )\n",
    "    assert f0.read(\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"numeric_attr\",\n",
    "    ) == 4, \"Attr shoud match\"\n",
    "    f0.write(\n",
    "        \"test\",\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"string_attr\",\n",
    "    )\n",
    "    assert f0.read(\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"string_attr\",\n",
    "    ) == \"test\", \"String attr shoud match\"\n",
    "    f0.write(\n",
    "        list(range(5)),\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"int_list_attr\",\n",
    "    )\n",
    "    assert np.all(\n",
    "        f0.read(\n",
    "            dataset_name=\"random\",\n",
    "            group_name=\"subgroup\",\n",
    "            attr_name=\"int_list_attr\",\n",
    "        ) == list(range(5))\n",
    "    ), \"Attr shoud match\"\n",
    "    mixed_list =  [\"test\", \"mixed\", 2, 4.9]\n",
    "    try:\n",
    "        f0.write(\n",
    "            mixed_list,\n",
    "            dataset_name=\"random\",\n",
    "            group_name=\"subgroup\",\n",
    "            attr_name=\"int_list_attr\",\n",
    "        )\n",
    "    except ValueError:\n",
    "        assert True\n",
    "    else:\n",
    "        assert False, \"Should not be able to overwrite attr\"\n",
    "    f0.write(\n",
    "        mixed_list,\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"int_list_attr\",\n",
    "        overwrite=True\n",
    "    )\n",
    "    assert np.all(\n",
    "        f0.read(\n",
    "            dataset_name=\"random\",\n",
    "            group_name=\"subgroup\",\n",
    "            attr_name=\"int_list_attr\",\n",
    "        ) == mixed_list\n",
    "    ), \"Attr of mixed list should match\"\n",
    "    f0.write(\n",
    "        {\"t\": 1},\n",
    "        dataset_name=\"random\",\n",
    "        group_name=\"subgroup\",\n",
    "        attr_name=\"dict_attr\",\n",
    "        overwrite=True\n",
    "    )\n",
    "    assert np.all(\n",
    "        f0.read(\n",
    "            dataset_name=\"random\",\n",
    "            group_name=\"subgroup\",\n",
    "            attr_name=\"dict_attr\",\n",
    "        ) == str({\"t\": 1})\n",
    "    ), \"Attr shoud match\"\n",
    "    \n",
    "\n",
    "    \n",
    "test_hdf_file_creation(test_folder=\"tmp\")\n",
    "test_hdf_file_read_and_write(test_folder=\"tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion functions\n",
    "\n",
    "`get_most_abundant`: In order to save spectra in a more memory efficient form, we only keep the n most abundant peaks. This allows us to save data in a fast accessible matrix format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swillems/miniconda3/envs/alphapept/lib/python3.8/site-packages/alphapept/chem.py:18: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\n",
      "  @jitclass(spec)\n",
      "/Users/swillems/miniconda3/envs/alphapept/lib/python3.8/site-packages/alphapept/constants.py:197: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\n",
      "  @jitclass(spec)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from alphapept.chem import calculate_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from numba.typed import List\n",
    "from numba import njit\n",
    "import gzip\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "def get_most_abundant(mass, intensity, n_max):\n",
    "    \"\"\"\n",
    "    Returns the n_max most abundant peaks of a spectrum\n",
    "    \"\"\"\n",
    "    if len(mass) < n_max:\n",
    "        return mass, intensity\n",
    "    else:\n",
    "        sortindex = np.argsort(intensity)[::-1][:n_max]\n",
    "        sortindex.sort()\n",
    "\n",
    "    return mass[sortindex], intensity[sortindex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Thermo Files\n",
    "\n",
    "> The current implementation uses a lot of lists and fills them with list comprehensions. This creates a lot of variables but seems to work reasonably fast. This code could be refactored as all variables end up in a dictionary-type container anyhow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_thermo_raw(raw_file, most_abundant, callback=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Load thermo raw file and extract spectra\n",
    "    \"\"\"\n",
    "\n",
    "    from .pyrawfilereader import RawFileReader\n",
    "    rawfile = RawFileReader(raw_file)\n",
    "\n",
    "    spec_indices = np.array(\n",
    "        range(rawfile.FirstSpectrumNumber, rawfile.LastSpectrumNumber + 1)\n",
    "    )\n",
    "\n",
    "    scan_list = []\n",
    "    rt_list = []\n",
    "    mass_list = []\n",
    "    int_list = []\n",
    "    ms_list = []\n",
    "    prec_mzs_list = []\n",
    "    mono_mzs_list = []\n",
    "    charge_list = []\n",
    "\n",
    "    for idx, i in enumerate(spec_indices):\n",
    "        ms_order = rawfile.GetMSOrderForScanNum(i)\n",
    "        rt = rawfile.RTFromScanNum(i)\n",
    "\n",
    "        prec_mz = rawfile.GetPrecursorMassForScanNum(i, 0)\n",
    "\n",
    "        trailer_extra = rawfile.GetTrailerExtraForScanNum(i)\n",
    "        mono_mz = float(trailer_extra[\"Monoisotopic M/Z:\"])\n",
    "        charge = int(trailer_extra[\"Charge State:\"])\n",
    "        # if mono_mz == 0: mono_mz = prec_mz\n",
    "        # if mono_mz != 0 and abs(mono_mz - prec_mz) > 0.1:\n",
    "        #    print(f'MSn={ms_order}, mono_mz={mono_mz}, perc_mz={prec_mz}, charge={charge}')\n",
    "\n",
    "        # may be centroid for MS2 and profile for MS1 is betterï¼Ÿ\n",
    "        masses, intensity = rawfile.GetCentroidMassListFromScanNum(i)\n",
    "\n",
    "        if ms_order == 2:\n",
    "            masses, intensity = get_most_abundant(masses, intensity, most_abundant)\n",
    "\n",
    "        scan_list.append(i)\n",
    "        rt_list.append(rt)\n",
    "        mass_list.append(np.array(masses))\n",
    "        int_list.append(np.array(intensity, dtype=np.int64))\n",
    "        ms_list.append(ms_order)\n",
    "        prec_mzs_list.append(prec_mz)\n",
    "        mono_mzs_list.append(mono_mz)\n",
    "        charge_list.append(charge)\n",
    "\n",
    "        if callback:\n",
    "            callback((idx+1)/len(spec_indices))\n",
    "\n",
    "    scan_list_ms1 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    rt_list_ms1 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    mass_list_ms1 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    int_list_ms1 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    ms_list_ms1 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "\n",
    "    scan_list_ms2 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    rt_list_ms2 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    mass_list_ms2 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    int_list_ms2 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    ms_list_ms2 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    mono_mzs2 = [mono_mzs_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    charge2 = [charge_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "\n",
    "    prec_mass_list2 = [\n",
    "        calculate_mass(mono_mzs_list[i], charge_list[i])\n",
    "        for i, _ in enumerate(ms_list)\n",
    "        if _ == 2\n",
    "    ]\n",
    "\n",
    "    check_sanity(mass_list)\n",
    "\n",
    "    query_data = {}\n",
    "\n",
    "    query_data[\"scan_list_ms1\"] = np.array(scan_list_ms1)\n",
    "    query_data[\"rt_list_ms1\"] = np.array(rt_list_ms1)\n",
    "    query_data[\"mass_list_ms1\"] = np.array(mass_list_ms1)\n",
    "    query_data[\"int_list_ms1\"] = np.array(int_list_ms1)\n",
    "    query_data[\"ms_list_ms1\"] = np.array(ms_list_ms1)\n",
    "\n",
    "    query_data[\"scan_list_ms2\"] = np.array(scan_list_ms2)\n",
    "    query_data[\"rt_list_ms2\"] = np.array(rt_list_ms2)\n",
    "    query_data[\"mass_list_ms2\"] = mass_list_ms2\n",
    "    query_data[\"int_list_ms2\"] = int_list_ms2\n",
    "    query_data[\"ms_list_ms2\"] = np.array(ms_list_ms2)\n",
    "    query_data[\"prec_mass_list2\"] = np.array(prec_mass_list2)\n",
    "    query_data[\"mono_mzs2\"] = np.array(mono_mzs2)\n",
    "    query_data[\"charge2\"] = np.array(charge2)\n",
    "\n",
    "    return query_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# TODO: Is there any really small (<20MB) raw files that we can upload and test, instead of our local raw files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_thermo_raw_MSFileReader(raw_file, most_abundant, callback=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Load thermo raw file and extract spectra\n",
    "    \"\"\"\n",
    "\n",
    "    from pymsfilereader import MSFileReader\n",
    "    rawfile = MSFileReader(raw_file)\n",
    "\n",
    "    spec_indices = np.array(\n",
    "        range(rawfile.FirstSpectrumNumber, rawfile.LastSpectrumNumber + 1)\n",
    "    )\n",
    "\n",
    "    scan_list = []\n",
    "    rt_list = []\n",
    "    mass_list = []\n",
    "    int_list = []\n",
    "    ms_list = []\n",
    "    prec_mzs_list = []\n",
    "    mono_mzs_list = []\n",
    "    charge_list = []\n",
    "\n",
    "    for idx, i in enumerate(spec_indices):\n",
    "        ms_order = rawfile.GetMSOrderForScanNum(i)\n",
    "        rt = rawfile.RTFromScanNum(i)\n",
    "\n",
    "        prec_mz = rawfile.GetPrecursorMassForScanNum(i, 2)\n",
    "\n",
    "        trailer_extra = rawfile.GetTrailerExtraForScanNum(i)\n",
    "        mono_mz = trailer_extra[\"Monoisotopic M/Z\"]\n",
    "        charge = trailer_extra[\"Charge State\"]\n",
    "\n",
    "        label_data = rawfile.GetLabelData(i)\n",
    "\n",
    "        # if labeled data is not available extract else\n",
    "        # Todo: check for centroided or not \n",
    "        \n",
    "        if label_data[0][0] == ():\n",
    "            mlist = rawfile.GetMassListFromScanNum(i)\n",
    "            masses = np.array(mlist[0][0])\n",
    "            intensity = np.array(mlist[0][1])\n",
    "        else:\n",
    "            intensity = np.array(label_data[0][1])\n",
    "            masses = np.array(label_data[0][0])\n",
    "\n",
    "        if ms_order == 2:\n",
    "            masses, intensity = get_most_abundant(masses, intensity, most_abundant)\n",
    "\n",
    "        scan_list.append(i)\n",
    "        rt_list.append(rt)\n",
    "        mass_list.append(np.array(masses))\n",
    "        int_list.append(np.array(intensity, dtype=np.int64))\n",
    "        ms_list.append(ms_order)\n",
    "        prec_mzs_list.append(prec_mz)\n",
    "        mono_mzs_list.append(mono_mz)\n",
    "        charge_list.append(charge)\n",
    "        \n",
    "        if callback:\n",
    "            callback((idx+1)/len(spec_indices))\n",
    "\n",
    "    scan_list_ms1 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    rt_list_ms1 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    mass_list_ms1 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    int_list_ms1 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    ms_list_ms1 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "\n",
    "    scan_list_ms2 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    rt_list_ms2 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    mass_list_ms2 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    int_list_ms2 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    ms_list_ms2 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    mono_mzs2 = [mono_mzs_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    charge2 = [charge_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "\n",
    "    prec_mass_list2 = [\n",
    "        calculate_mass(mono_mzs_list[i], charge_list[i])\n",
    "        for i, _ in enumerate(ms_list)\n",
    "        if _ == 2\n",
    "    ]\n",
    "\n",
    "    check_sanity(mass_list)\n",
    "    \n",
    "    query_data = {}\n",
    "\n",
    "    query_data[\"scan_list_ms1\"] = np.array(scan_list_ms1)\n",
    "    query_data[\"rt_list_ms1\"] = np.array(rt_list_ms1)\n",
    "    query_data[\"mass_list_ms1\"] = np.array(mass_list_ms1)\n",
    "    query_data[\"int_list_ms1\"] = np.array(int_list_ms1)\n",
    "    query_data[\"ms_list_ms1\"] = np.array(ms_list_ms1)\n",
    "\n",
    "    query_data[\"scan_list_ms2\"] = np.array(scan_list_ms2)\n",
    "    query_data[\"rt_list_ms2\"] = np.array(rt_list_ms2)\n",
    "    query_data[\"mass_list_ms2\"] = mass_list_ms2\n",
    "    query_data[\"int_list_ms2\"] = int_list_ms2\n",
    "    query_data[\"ms_list_ms2\"] = np.array(ms_list_ms2)\n",
    "    query_data[\"prec_mass_list2\"] = np.array(prec_mass_list2)\n",
    "    query_data[\"mono_mzs2\"] = np.array(mono_mzs2)\n",
    "    query_data[\"charge2\"] = np.array(charge2)\n",
    "    \n",
    "    return query_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper\n",
    "\n",
    "We use `multiprocessing - pool` to be able to convert multiple files to raw in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def raw_to_npz(to_process, callback = None):\n",
    "    \"\"\"\n",
    "    Wrapper function to convert raw to npz\n",
    "    \"\"\"\n",
    "\n",
    "    path, settings = to_process\n",
    "\n",
    "    base, ext = os.path.splitext(path)\n",
    "    \n",
    "    if ext.lower() == '.raw':\n",
    "        logging.info('File {} has extension {} - converting from Thermo.'.format(base, ext))\n",
    "        query_data = load_thermo_raw(path, callback=callback, **settings['raw'])\n",
    "    elif ext.lower() == '.d':\n",
    "        logging.info('File {} has extension {} - converting from Bruker.'.format(base, ext))\n",
    "        query_data = load_bruker_raw(path, callback=callback, **settings['raw'])\n",
    "    else:\n",
    "        raise NotImplementedError('File extension {} not understood.'.format(ext))\n",
    "        \n",
    "    logging.info('File conversion complete. Extracted {:,} precursors.'.format(len(query_data['prec_mass_list2'])))\n",
    "        \n",
    "    save_path = base + \".npz\"\n",
    "    save_query_as_npz(save_path, query_data)\n",
    "    logging.info('Converted file saved to {}'.format(save_path))\n",
    "    \n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def raw_to_npz_parallel(path_list, settings, callback=None):\n",
    "    \n",
    "    n_processes = settings['general']['n_processes']\n",
    "    \n",
    "    to_process = [(_, settings) for _ in path_list]\n",
    "    \n",
    "    if len(to_process) == 1:\n",
    "        raw_to_npz(to_process[0], callback=callback)\n",
    "    \n",
    "    else:\n",
    "        with Pool(n_processes) as p:\n",
    "            max_ = len(to_process)\n",
    "            for i, _ in enumerate(p.imap_unordered(raw_to_npz, to_process)):\n",
    "                if callback:\n",
    "                    callback((i+1)/max_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bruker\n",
    "\n",
    "For accessing Bruker files, we rely on the external `timsdata` library. \n",
    "For `ccs` values, we need some functions from this library. As the live feature-finder might not be able to determine some charge values, it is intended to perform this calculation at a later stage once we have charge values from the post-processing feature finder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_bruker_raw(raw_file, most_abundant, callback=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Load bruker raw file and extract spectra\n",
    "    \"\"\"\n",
    "    import sqlalchemy as db\n",
    "    import pandas as pd\n",
    "    \n",
    "    from alphapept.ext.bruker import timsdata\n",
    "\n",
    "    tdf = os.path.join(raw_file, 'analysis.tdf')\n",
    "    engine = db.create_engine('sqlite:///{}'.format(tdf))\n",
    "    prec_data = pd.read_sql_table('Precursors', engine)\n",
    "    frame_data = pd.read_sql_table('Frames', engine)\n",
    "    frame_data = frame_data.set_index('Id')\n",
    "    \n",
    "    from alphapept.constants import mass_dict\n",
    "\n",
    "    tdf = timsdata.TimsData(raw_file)\n",
    "\n",
    "    M_PROTON = mass_dict['Proton']\n",
    "\n",
    "    prec_data['Mass'] = prec_data['MonoisotopicMz'].values * prec_data['Charge'].values - prec_data['Charge'].values*M_PROTON\n",
    "\n",
    "    from alphapept.io import list_to_numpy_f32, get_most_abundant\n",
    "\n",
    "    mass_list_ms2 = []\n",
    "    int_list_ms2 = []\n",
    "    scan_list_ms2 = []\n",
    "    \n",
    "    prec_data = prec_data.sort_values(by='Mass', ascending=True)\n",
    "    \n",
    "    precursor_ids = prec_data['Id'].tolist()\n",
    "\n",
    "    for idx, key in enumerate(precursor_ids):\n",
    "\n",
    "        ms2_data = tdf.readPasefMsMs([key])\n",
    "        masses, intensity = ms2_data[key]\n",
    "\n",
    "        masses, intensity = get_most_abundant(np.array(masses), np.array(intensity), most_abundant)\n",
    "\n",
    "        mass_list_ms2.append(masses)\n",
    "        int_list_ms2.append(intensity)\n",
    "        scan_list_ms2.append(key)\n",
    "        \n",
    "        if callback:\n",
    "            callback((idx+1)/len(precursor_ids))\n",
    "            \n",
    "\n",
    "    check_sanity(mass_list_ms2)\n",
    "                               \n",
    "    query_data = {}\n",
    "\n",
    "    query_data['prec_mass_list2'] = prec_data['Mass'].values\n",
    "    query_data['prec_id'] = prec_data['Id'].values\n",
    "    query_data['mono_mzs2'] = prec_data['MonoisotopicMz'].values\n",
    "    query_data['rt_list_ms2'] = frame_data.loc[prec_data['Parent'].values]['Time'].values / 60 #convert to minutes\n",
    "    query_data['scan_list_ms2'] = prec_data['Parent'].values\n",
    "    query_data['charge2'] = prec_data['Charge'].values\n",
    "    query_data['mobility'] = tdf.scanNumToOneOverK0(1, prec_data['ScanNumber'].to_list()) #check if its okay to always use first frame\n",
    "    query_data[\"mass_list_ms2\"] = mass_list_ms2\n",
    "    query_data[\"int_list_ms2\"] = int_list_ms2\n",
    "    \n",
    "    \n",
    "    return query_data\n",
    "\n",
    "def one_over_k0_to_CCS(one_over_k0s, charges, mzs):\n",
    "    \"\"\"\n",
    "    convert one_over_k0 to CCS\n",
    "    \"\"\"\n",
    "    from alphapept.ext.bruker import timsdata\n",
    "    \n",
    "    ccs = np.empty(len(one_over_k0s))\n",
    "    ccs[:] = np.nan\n",
    "    \n",
    "    for idx, (one_over, charge, mz) in enumerate(zip(one_over_k0s, charges, mzs)):\n",
    "        try:\n",
    "            ccs[idx] =timsdata.oneOverK0ToCCSforMz(one_over, int(charge), mz)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return ccs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Test if timsdata can be called\n",
    "def test_one_over_k0_to_CCS():\n",
    "    one_over_k0_to_CCS([1], [1], [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MZML \n",
    "\n",
    "To access mzml files, we rely on the pyteomics package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def check_sanity(mass_list):\n",
    "    \"\"\"\n",
    "    Sanity check for mass list to make sure the masses are sorted\n",
    "    \"\"\"\n",
    "    \n",
    "    if not all(\n",
    "        mass_list[0][i] <= mass_list[0][i + 1] for i in range(len(mass_list[0]) - 1)\n",
    "    ):\n",
    "        raise ValueError(\"Masses are not sorted.\")\n",
    "        \n",
    "        \n",
    "def extract_mzml_info(input_dict):\n",
    "    rt = float(input_dict.get('scanList').get('scan')[0].get('scan start time'))  # rt_list_ms1/2\n",
    "    masses = input_dict.get('m/z array')\n",
    "    intensities = input_dict.get('intensity array')\n",
    "    ms_order = input_dict.get('ms level')  # ms_list_ms1/2\n",
    "    prec_mass = 0\n",
    "    if ms_order == 2:\n",
    "        charge = int(\n",
    "            input_dict.get('precursorList').get('precursor')[0].get('selectedIonList').get('selectedIon')[0].get(\n",
    "                'charge state'))\n",
    "        mono_mz = round(\n",
    "            input_dict.get('precursorList').get('precursor')[0].get('selectedIonList').get('selectedIon')[0].get(\n",
    "                'selected ion m/z'), 4)\n",
    "        prec_mass = calculate_mass(mono_mz, charge)\n",
    "    return rt, masses, intensities, ms_order, prec_mass\n",
    "\n",
    "\n",
    "def extract_mzxml_info(input_dict):\n",
    "    rt = float(input_dict.get('retentionTime'))\n",
    "    masses = input_dict.get('m/z array')\n",
    "    intensities = input_dict.get('intensity array')\n",
    "    ms_order = input_dict.get('msLevel')  # ms_list_ms1/2\n",
    "    prec_mass = 0\n",
    "    if ms_order == 2:\n",
    "        charge = int(input_dict.get('precursorMz')[0].get('precursorCharge'))\n",
    "        mono_mz = round(input_dict.get('precursorMz')[0].get('precursorMz'), 4)\n",
    "        prec_mass = calculate_mass(mono_mz, charge)\n",
    "    return rt, masses, intensities, ms_order, prec_mass\n",
    "\n",
    "\n",
    "def read_mzML(filename, most_abundant):\n",
    "    \"\"\"\n",
    "    Read spectral data from an mzML file and return various lists separately for ms1 and ms2 data.\n",
    "    \"\"\"\n",
    "    from pyteomics import mzml, mzxml\n",
    "\n",
    "    try:\n",
    "        if os.path.splitext(filename)[1] == '.gz':\n",
    "            reader = mzml.read(gzip.open(filename), use_index=True)\n",
    "        else:\n",
    "            reader = mzml.read(filename, use_index=True)\n",
    "        spec_indices = np.array(range(1, len(reader) + 1))\n",
    "\n",
    "    except OSError:\n",
    "        logging('Could not open the file. Please, specify the correct path to the file.')\n",
    "        sys.exit(1)\n",
    "\n",
    "    scan_list = []\n",
    "    rt_list = []\n",
    "    mass_list = []\n",
    "    int_list = []\n",
    "    ms_list = []\n",
    "    prec_mzs_list = []\n",
    "\n",
    "    logging('Start reading mzML file...')\n",
    "    if reader:\n",
    "        for i in tqdm(spec_indices):\n",
    "            spec = next(reader)\n",
    "            scan_list.append(i)\n",
    "            rt, masses, intensities, ms_order, prec_mass = extract_mzml_info(spec, min_charge, max_charge)\n",
    "            if ms_order == 2:\n",
    "                masses, intensities = get_most_abundant(masses, intensities, most_abundant)\n",
    "            rt_list.append(rt)\n",
    "            mass_list.append(masses)\n",
    "            int_list.append(intensities)\n",
    "            ms_list.append(ms_order)\n",
    "            prec_mzs_list.append(prec_mass)\n",
    "\n",
    "    check_sanity(mass_list)\n",
    "\n",
    "    scan_list_ms1 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    rt_list_ms1 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    mass_list_ms1 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    int_list_ms1 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    ms_list_ms1 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "\n",
    "    scan_list_ms2 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    rt_list_ms2 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    mass_list_ms2 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    int_list_ms2 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    ms_list_ms2 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    prec_mass_list2 = [prec_mzs_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    \n",
    "    query_data = {}\n",
    "\n",
    "    query_data[\"scan_list_ms1\"] = np.array(scan_list_ms1)\n",
    "    query_data[\"rt_list_ms1\"] = np.array(rt_list_ms1)\n",
    "    query_data[\"mass_list_ms1\"] = np.array(mass_list_ms1)\n",
    "    query_data[\"int_list_ms1\"] = np.array(int_list_ms1)\n",
    "    query_data[\"ms_list_ms1\"] = np.array(ms_list_ms1)\n",
    "\n",
    "    query_data[\"scan_list_ms2\"] = np.array(scan_list_ms2)\n",
    "    query_data[\"rt_list_ms2\"] = np.array(rt_list_ms2)\n",
    "    query_data[\"mass_list_ms2\"] = mass_list_ms2\n",
    "    query_data[\"int_list_ms2\"] = int_list_ms2\n",
    "    query_data[\"ms_list_ms2\"] = np.array(ms_list_ms2)\n",
    "    query_data[\"prec_mass_list2\"] = np.array(prec_mass_list2)\n",
    "    query_data[\"mono_mzs2\"] = np.array(mono_mzs2)\n",
    "    query_data[\"charge2\"] = np.array(charge2)\n",
    "    \n",
    "    return query_data\n",
    "\n",
    "\n",
    "def read_mzXML(filename, most_abundant):\n",
    "    \"\"\"\n",
    "    Read spectral data from an mzXML file and return various lists separately for ms1 and ms2 data.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if os.path.splitext(filename)[1] == '.gz':\n",
    "            reader = mzxml.read(gzip.open(filename), use_index=True)\n",
    "        else:\n",
    "            reader = mzxml.read(filename, use_index=True)\n",
    "        spec_indices = np.array(range(1, len(reader) + 1))\n",
    "\n",
    "    except OSError:\n",
    "        print('Could not open the file. Please, specify the correct path to the file.')\n",
    "        sys.exit(1)\n",
    "\n",
    "    scan_list = []\n",
    "    rt_list = []\n",
    "    mass_list = []\n",
    "    int_list = []\n",
    "    ms_list = []\n",
    "    prec_mzs_list = []\n",
    "\n",
    "    print('Start reading mzXML file...')\n",
    "    if reader:\n",
    "        for i in tqdm(spec_indices):\n",
    "            spec = next(reader)\n",
    "            scan_list.append(i)\n",
    "            rt, masses, intensities, ms_order, prec_mass = extract_mzxml_info(spec, min_charge, max_charge)\n",
    "            if ms_order == 2:\n",
    "                masses, intensities = get_most_abundant(masses, intensities, most_abundant)\n",
    "            rt_list.append(rt)\n",
    "            mass_list.append(masses)\n",
    "            int_list.append(intensities)\n",
    "            ms_list.append(ms_order)\n",
    "            prec_mzs_list.append(prec_mass)\n",
    "\n",
    "    check_sanity(mass_list)\n",
    "\n",
    "    scan_list_ms1 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    rt_list_ms1 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    mass_list_ms1 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    int_list_ms1 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "    ms_list_ms1 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 1]\n",
    "\n",
    "    scan_list_ms2 = [scan_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    rt_list_ms2 = [rt_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    mass_list_ms2 = [mass_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    int_list_ms2 = [int_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    ms_list_ms2 = [ms_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "    prec_mass_list2 = [prec_mzs_list[i] for i, _ in enumerate(ms_list) if _ == 2]\n",
    "\n",
    "    check_sanity(mass_list)\n",
    "    \n",
    "    query_data = {}\n",
    "\n",
    "    query_data[\"scan_list_ms1\"] = np.array(scan_list_ms1)\n",
    "    query_data[\"rt_list_ms1\"] = np.array(rt_list_ms1)\n",
    "    query_data[\"mass_list_ms1\"] = np.array(mass_list_ms1)\n",
    "    query_data[\"int_list_ms1\"] = np.array(int_list_ms1)\n",
    "    query_data[\"ms_list_ms1\"] = np.array(ms_list_ms1)\n",
    "\n",
    "    query_data[\"scan_list_ms2\"] = np.array(scan_list_ms2)\n",
    "    query_data[\"rt_list_ms2\"] = np.array(rt_list_ms2)\n",
    "    query_data[\"mass_list_ms2\"] = mass_list_ms2\n",
    "    query_data[\"int_list_ms2\"] = int_list_ms2\n",
    "    query_data[\"ms_list_ms2\"] = np.array(ms_list_ms2)\n",
    "    query_data[\"prec_mass_list2\"] = np.array(prec_mass_list2)\n",
    "    query_data[\"mono_mzs2\"] = np.array(mono_mzs2)\n",
    "    query_data[\"charge2\"] = np.array(charge2)\n",
    "    \n",
    "    return query_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "For saving, we are currently relying on the NumPy-native npz-container. It offers reasonable speed, dictionary-type access, and does not need individual type definitions.\n",
    "\n",
    "While we could, in principle, store the mz and int arrays as a list of variable length, this will come at a performance decrease. We, therefore, create an array of the dimensions of the n most abundant peaks and the number of spectra with the function `list_to_numpy_f32` and fill the unoccupied cells with `-1`. This allows an increase in accessing times at the cost of additional disk space.\n",
    "\n",
    "Implementation Note: For large files (e.g., choosing a large number of peaks that should be kept, the npz array can fail and trigger an ZIP64 error. This is supposed to be fixed in a later NumPy version.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def list_to_numpy_f32(long_list):\n",
    "    \"\"\"\n",
    "    Function to convert a list to float32 array\n",
    "    \"\"\"\n",
    "    np_array = (\n",
    "        np.zeros(\n",
    "            [len(max(long_list, key=lambda x: len(x))), len(long_list)],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        - 1\n",
    "    )\n",
    "    for i, j in enumerate(long_list):\n",
    "        np_array[0 : len(j), i] = j\n",
    "\n",
    "    return np_array\n",
    "\n",
    "        \n",
    "def save_query_as_npz(raw_file_npz, query_data):\n",
    "    \"\"\"\n",
    "    Saves query_data as npz\n",
    "    \"\"\"\n",
    "    \n",
    "    to_save = {}\n",
    "    \n",
    "    for key in query_data.keys():\n",
    "        if key in ['mass_list_ms2','int_list_ms2']:\n",
    "            to_save[key] = list_to_numpy_f32(query_data[key])\n",
    "        else:\n",
    "            to_save[key] = query_data[key]\n",
    "            \n",
    "    to_save[\"bounds\"] = np.sum(to_save['mass_list_ms2']>=0,axis=0).astype(np.int64)\n",
    "            \n",
    "    np.savez(raw_file_npz, **to_save)\n",
    "    \n",
    "    return raw_file_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing other Files\n",
    "\n",
    "Benchmarking proteomics software against each other is not straightforward as various naming conventions exist, and different algorithms are implemented. In this section, we define some helper functions that allow us to facilitate the comparison of different tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading MaxQuant xml settings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_nested(child):\n",
    "    \"\"\"\n",
    "    Helper function to extract nested entries\n",
    "    \"\"\"\n",
    "    if len(child) > 0:\n",
    "        temp_dict = {}\n",
    "        for xx in child:\n",
    "            temp_dict[xx.tag] = extract_nested(xx)\n",
    "        return temp_dict\n",
    "    else:\n",
    "        if child.text == 'True':\n",
    "            info = True\n",
    "        elif child.text == 'False':\n",
    "            info = False\n",
    "        else:\n",
    "            info = child.text\n",
    "        return info\n",
    "\n",
    "def extract_mq_settings(path):\n",
    "    \"\"\"\n",
    "    Function to return MaxQuant values as a dictionary for a given xml file\n",
    "    \"\"\"\n",
    "    if not path.endswith('.xml'):\n",
    "        raise ValueError(\"Path {} is not a valid xml file.\".format(path))\n",
    "    \n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    mq_dict = {}\n",
    "\n",
    "    for child in root:  \n",
    "\n",
    "        mq_dict[child.tag] = extract_nested(child)\n",
    "        \n",
    "    return mq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FastaFileInfo': {'fastaFilePath': 'testfile.fasta',\n",
       "  'identifierParseRule': '>([^\\\\s]*)',\n",
       "  'descriptionParseRule': '>(.*)',\n",
       "  'taxonomyParseRule': None,\n",
       "  'variationParseRule': None,\n",
       "  'modificationParseRule': None,\n",
       "  'taxonomyId': None}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mq_dict = extract_mq_settings('../testfiles/test_mqpar.xml')\n",
    "mq_dict['fastaFiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_mq_seq(peptide):\n",
    "    \"\"\"\n",
    "    Replaces maxquant convention to alphapept convention\n",
    "    ToDo: include more sequences\n",
    "    \"\"\"\n",
    "    peptide = peptide[1:-1] #Remove _\n",
    "\n",
    "    peptide = peptide.replace('(Acetyl (Protein N-term))','a')\n",
    "    peptide = peptide.replace('M(Oxidation (M))','oxM')\n",
    "    peptide = peptide.replace('C','cC') #This is fixed and not indicated in MaxQuant\n",
    "    \n",
    "    return peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AFQPFFVELToxMPYSVIR'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_mq_seq('_AFQPFFVELTM(Oxidation (M))PYSVIR_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted io_utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphapept]",
   "language": "python",
   "name": "conda-env-alphapept-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
