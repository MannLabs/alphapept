{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface\n",
    "\n",
    "This notebook describes/implements the interface to use alphapept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "The implemented functions are as follows:\n",
    "\n",
    "* Create database\n",
    "* Import raw data\n",
    "* Perform feature finding\n",
    "* Search data with fasta\n",
    "* Recalibrate\n",
    "* Score data with fasta\n",
    "* Perform LFQ\n",
    "* Export results\n",
    "* Run whole workflow\n",
    "\n",
    "The last command allows to run the whole pipeline at once.\n",
    "\n",
    "Helper functions include:\n",
    "\n",
    "* Callback function to track progress\n",
    "* Logging function\n",
    "* Version/hardware/settings checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def tqdm_wrapper(pbar, update):\n",
    "    current_value = pbar.n\n",
    "    delta = update - current_value\n",
    "    pbar.update(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "def set_logger():\n",
    "    root = logging.getLogger()\n",
    "    while root.hasHandlers():\n",
    "        root.removeHandler(root.handlers[0])\n",
    "    root.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s %(levelname)-s - %(message)s', \"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    root.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def check_version_and_hardware(settings):\n",
    "    import alphapept.utils\n",
    "    alphapept.utils.check_hardware()\n",
    "    alphapept.utils.check_python_env()\n",
    "    settings = alphapept.utils.check_settings(settings)\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import os\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "def create_database(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    import alphapept.fasta\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "    if 'database_path' not in settings['fasta']:\n",
    "        database_path = ''\n",
    "    else:\n",
    "        database_path = settings['fasta']['database_path']\n",
    "        \n",
    "    if not settings['fasta']['save_db']:\n",
    "        logging.info('Creating small database w/o modifications for first search.')\n",
    "        temp_settings = copy.deepcopy(settings)\n",
    "        temp_settings['fasta']['mods_fixed'] = []\n",
    "        temp_settings['fasta']['mods_fixed_terminal'] = []\n",
    "        temp_settings['fasta']['mods_fixed_terminal_prot'] = []\n",
    "        temp_settings['fasta']['mods_variable'] = []\n",
    "        temp_settings['fasta']['mods_variable_terminal'] = []\n",
    "        temp_settings['fasta']['mods_variable_terminal_prot'] = []\n",
    "    else:\n",
    "        temp_settings = settings\n",
    "                \n",
    "    if os.path.isfile(database_path):\n",
    "        logging.info(\n",
    "            'Database path set and exists. Using {} as database.'.format(\n",
    "                database_path\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        logging.info(\n",
    "            'Database path {} is not a file.'.format(database_path)\n",
    "        )\n",
    "\n",
    "        if len(settings['fasta']['fasta_paths']) == 0:\n",
    "            raise FileNotFoundError(\"No FASTA files set.\")\n",
    "\n",
    "        for fasta_file in settings['fasta']['fasta_paths']:\n",
    "            if os.path.isfile(fasta_file):\n",
    "                logging.info(\n",
    "                    'Found FASTA file {} with size {:.2f} Mb.'.format(\n",
    "                        fasta_file,\n",
    "                        os.stat(fasta_file).st_size/(1024**2)\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    'File {} not found'.format(fasta_file)\n",
    "                )\n",
    "\n",
    "        logging.info('Creating a new database from FASTA.')\n",
    "\n",
    "        if not callback:\n",
    "            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "        else:\n",
    "            cb = callback\n",
    "\n",
    "        (\n",
    "            spectra,\n",
    "            pept_dict,\n",
    "            fasta_dict\n",
    "        ) = alphapept.fasta.generate_database_parallel(\n",
    "            temp_settings,\n",
    "            callback=cb\n",
    "        )\n",
    "        logging.info(\n",
    "            'Digested {:,} proteins and generated {:,} spectra'.format(\n",
    "                len(fasta_dict),\n",
    "                len(spectra)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        alphapept.fasta.save_database(\n",
    "            spectra,\n",
    "            pept_dict,\n",
    "            fasta_dict,\n",
    "            **settings['fasta']\n",
    "        )\n",
    "        logging.info(\n",
    "            'Database saved to {}. Filesize of database is {:.2f} GB'.format(\n",
    "                database_path,\n",
    "                os.stat(database_path).st_size/(1024**3)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        settings['fasta']['database_path'] = database_path\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def import_raw_data(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.io\n",
    "    \n",
    "    files_ms_data_hdf = []\n",
    "    to_convert = []\n",
    "\n",
    "    for file_name in settings['experiment']['file_paths']:\n",
    "        base, ext = os.path.splitext(file_name)\n",
    "        ms_data_file_path = f'{base}.ms_data.hdf'\n",
    "        files_ms_data_hdf.append(ms_data_file_path)\n",
    "        if os.path.isfile(ms_data_file_path):\n",
    "            logging.info(f'Found *.ms_data.hdf file for {file_name}')\n",
    "        else:\n",
    "            to_convert.append(file_name)\n",
    "            logging.info(f'No *.ms_data.hdf file found for {file_name}. Adding to conversion list.')\n",
    "    files_ms_data_hdf.sort()\n",
    "\n",
    "    if len(to_convert) > 0:\n",
    "        logging.info('Starting file conversion.')\n",
    "        if not callback:\n",
    "            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "        else:\n",
    "            cb = callback\n",
    "        alphapept.io.raw_to_ms_data_file_parallel(to_convert, settings, callback = cb)\n",
    "        logging.info('File conversion complete.')\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def feature_finding(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.feature_finding\n",
    "    import alphapept.io\n",
    "    \n",
    "        \n",
    "    to_convert = []\n",
    "    for file_name in settings['experiment']['file_paths']:\n",
    "        base, ext = os.path.splitext(file_name)\n",
    "        hdf_path = base+'.ms_data.hdf'\n",
    "\n",
    "        if os.path.isfile(hdf_path):\n",
    "            try:\n",
    "                alphapept.io.MS_Data_File(\n",
    "                    hdf_path\n",
    "                ).read(dataset_name=\"features\")\n",
    "                logging.info(\n",
    "                    'Found *.hdf with features for {}'.format(file_name)\n",
    "                )\n",
    "            except KeyError:\n",
    "                to_convert.append(file_name)\n",
    "                logging.info(\n",
    "                    'No *.hdf file with features found for {}. Adding to feature finding list.'.format(file_name)\n",
    "                )\n",
    "        else:\n",
    "            to_convert.append(file_name)\n",
    "            logging.info(\n",
    "                'No *.hdf file with features found for {}. Adding to feature finding list.'.format(file_name)\n",
    "            )\n",
    "\n",
    "    if len(to_convert) > 0:\n",
    "        logging.info(\n",
    "            'Feature extraction for {} file(s).'.format(len(to_convert))\n",
    "        )\n",
    "        if not callback:\n",
    "            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "        else:\n",
    "            cb = callback\n",
    "\n",
    "        alphapept.feature_finding.find_and_save_features_parallel(\n",
    "            to_convert,\n",
    "            settings,\n",
    "            callback=cb\n",
    "        )\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def search_data(\n",
    "    settings,\n",
    "    recalibrated=False,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.search\n",
    "    import alphapept.io\n",
    "        \n",
    "    if not recalibrated:\n",
    "\n",
    "        logging.info('Starting first search with DB.')\n",
    "\n",
    "        if not callback:\n",
    "            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "        else:\n",
    "            cb = callback\n",
    "\n",
    "        fasta_dict, pept_dict = alphapept.search.search_parallel_db(\n",
    "            settings,\n",
    "            callback=cb\n",
    "        )\n",
    "\n",
    "        logging.info('First search complete.')\n",
    "    else:\n",
    "        ms_files = []\n",
    "        for _ in settings['experiment']['file_paths']:\n",
    "            base, ext = os.path.splitext(_)\n",
    "            ms_files.append(base + '.ms_data.hdf')\n",
    "            \n",
    "        offsets = [\n",
    "            alphapept.io.MS_Data_File(\n",
    "                ms_file_name\n",
    "            ).read(\n",
    "                dataset_name=\"corrected_mass\",\n",
    "                group_name=\"features\",\n",
    "                attr_name=\"estimated_max_precursor_ppm\"\n",
    "            ) * settings['search']['calibration_std'] for ms_file_name in ms_files\n",
    "        ]\n",
    "        if settings['fasta']['save_db']:\n",
    "            logging.info('Starting second search with DB.')\n",
    "\n",
    "            if not callback:\n",
    "                cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "            else:\n",
    "                cb = callback\n",
    "\n",
    "            fasta_dict, pept_dict = alphapept.search.search_parallel_db(\n",
    "                settings,\n",
    "                calibration=offsets,\n",
    "                callback=cb\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            logging.info('Starting second search.')\n",
    "\n",
    "            if not callback:\n",
    "                cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "            else:\n",
    "                cb = callback\n",
    "\n",
    "            fasta_dict = alphapept.search.search_parallel(\n",
    "                settings,\n",
    "                calibration=offsets,\n",
    "                callback=cb\n",
    "            )\n",
    "            pept_dict = None\n",
    "\n",
    "        logging.info('Second search complete.')\n",
    "    return settings, pept_dict, fasta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def recalibrate_data(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.recalibration\n",
    "    \n",
    "    if settings['search']['calibrate']:\n",
    "        logging.info('Performing recalibration.')\n",
    "\n",
    "        if not callback:\n",
    "            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "        else:\n",
    "            cb = callback\n",
    "\n",
    "        offsets = alphapept.recalibration.calibrate_hdf_parallel(\n",
    "            settings,\n",
    "            callback=cb\n",
    "        )\n",
    "\n",
    "        logging.info('Recalibration complete.')\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def score(\n",
    "    settings,\n",
    "    pept_dict=None,\n",
    "    fasta_dict=None,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):    \n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.score\n",
    "    import alphapept.fasta\n",
    "    \n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "\n",
    "    if (fasta_dict is None) or (pept_dict is None):\n",
    "        db_data = alphapept.fasta.read_database(\n",
    "            settings['fasta']['database_path']\n",
    "        )\n",
    "        fasta_dict = db_data['fasta_dict'].item()\n",
    "        pept_dict = db_data['pept_dict'].item()\n",
    "    alphapept.score.score_hdf_parallel(settings, callback=cb)\n",
    "    logging.info('Scoring complete.')\n",
    "\n",
    "    if not settings['fasta']['save_db']:\n",
    "        pept_dict = alphapept.fasta.pept_dict_from_search(settings)\n",
    "\n",
    "    # Protein groups\n",
    "    logging.info('Extracting protein groups.')\n",
    "\n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "\n",
    "    # This is on each file individually -> when having repeats maybe\n",
    "    # use differently (if this matter at all )\n",
    "    alphapept.score.protein_groups_hdf_parallel(\n",
    "        settings,\n",
    "        pept_dict,\n",
    "        fasta_dict,\n",
    "        callback=cb\n",
    "    )\n",
    "    logging.info('Protein groups complete.')\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def align(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    \n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.matching\n",
    "    \n",
    "    alphapept.matching.align_datasets(settings, callback = callback)\n",
    "    \n",
    "    return settings\n",
    "\n",
    "def match(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.matching\n",
    "        \n",
    "    if settings['matching']['match_between_runs']:\n",
    "        alphapept.matching.match_datasets(settings)\n",
    "    \n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def lfq_quantification(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.quantification\n",
    "    \n",
    "    field = settings['quantification']['mode']\n",
    "\n",
    "    logging.info('Assembling dataframe.')\n",
    "    df = alphapept.utils.assemble_df(settings)\n",
    "    logging.info('Assembly complete.')\n",
    "    \n",
    "    if field in df.keys():  # Check if the quantification information exists.\n",
    "        # We could include another protein fdr in here..\n",
    "        if 'fraction' in df.keys():\n",
    "            logging.info('Delayed Normalization.')\n",
    "            df, normalization = alphapept.quantification.delayed_normalization(\n",
    "                df,\n",
    "                field\n",
    "            )\n",
    "            pd.DataFrame(normalization).to_hdf(\n",
    "                settings['experiment']['results_path'],\n",
    "                'fraction_normalization'\n",
    "            )\n",
    "            df_grouped = df.groupby(\n",
    "                ['shortname', 'precursor', 'protein', 'filename']\n",
    "            )[['{}_dn'.format(field)]].sum().reset_index()\n",
    "        else:\n",
    "            df_grouped = df.groupby(\n",
    "                ['shortname', 'precursor', 'protein', 'filename']\n",
    "            )[field].sum().reset_index()\n",
    "\n",
    "        df.to_hdf(\n",
    "            settings['experiment']['results_path'],\n",
    "            'combined_protein_fdr_dn'\n",
    "        )\n",
    "\n",
    "        logging.info('Complete. ')\n",
    "        logging.info('Starting profile extraction.')\n",
    "\n",
    "        if not callback:\n",
    "            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "        else:\n",
    "            cb = callback\n",
    "\n",
    "        protein_table = alphapept.quantification.protein_profile_parallel(\n",
    "            settings,\n",
    "            df_grouped,\n",
    "            callback=cb\n",
    "        )\n",
    "        protein_table.to_hdf(\n",
    "            settings['experiment']['results_path'],\n",
    "            'protein_table'\n",
    "        )\n",
    "        results_path = settings['experiment']['results_path']\n",
    "        base, ext = os.path.splitext(results_path)\n",
    "        protein_table.to_csv(base+'_proteins.csv')\n",
    "        \n",
    "        logging.info('LFQ complete.')\n",
    "        \n",
    "    logging.info('Exporting as csv.')\n",
    "    results_path = settings['experiment']['results_path']\n",
    "    base, ext = os.path.splitext(results_path)\n",
    "    df.to_csv(base+'.csv') \n",
    "        \n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import yaml\n",
    "\n",
    "\n",
    "def export(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "    base, ext = os.path.splitext(settings['experiment']['results_path'])\n",
    "    out_path_settings = base+'.yaml'\n",
    "\n",
    "    with open(out_path_settings, 'w') as file:\n",
    "        yaml.dump(settings, file)\n",
    "\n",
    "    logging.info('Settings saved to {}'.format(out_path_settings))\n",
    "    logging.info('Analysis complete.')\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback\n",
    "\n",
    "Return an overall progress\n",
    "And a task based progress.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from time import time, sleep\n",
    "from alphapept.__version__ import VERSION_NO\n",
    "\n",
    "def get_summary(settings, summary):\n",
    "\n",
    "    summary['file_sizes'] = {}\n",
    "\n",
    "    file_sizes = {}\n",
    "    for _ in settings['experiment']['file_paths']:\n",
    "\n",
    "        base, ext = os.path.splitext(_)\n",
    "        filename = os.path.split(base)[1]\n",
    "        file_sizes[base+\"_ms_data\"] = os.path.getsize(os.path.splitext(_)[0] + \".ms_data.hdf\")/1024**2\n",
    "        # file_sizes[base+\"_result\"] = os.path.getsize(os.path.splitext(_)[0] + \".hdf\")/1024**2\n",
    "\n",
    "        ms_data = alphapept.io.MS_Data_File(os.path.splitext(_)[0] + \".ms_data.hdf\")\n",
    "        n_ms2 = ms_data.read(group_name='Raw/MS2_scans', dataset_name='prec_mass_list2', return_dataset_shape=True)[0]\n",
    "        for key in ms_data.read():\n",
    "            if \"is_pd_dataframe\" in ms_data.read(\n",
    "                attr_name=\"\",\n",
    "                group_name=key\n",
    "            ):\n",
    "                f_summary = {}\n",
    "                for key in ms_data.read():\n",
    "                    if \"is_pd_dataframe\" in ms_data.read(\n",
    "                        attr_name=\"\",\n",
    "                        group_name=key\n",
    "                    ):\n",
    "                        df = ms_data.read(dataset_name=key)\n",
    "\n",
    "                        f_name = filename+'_'+key.lstrip('/')\n",
    "\n",
    "                        f_summary[key] = len(df)\n",
    "                        if key in 'protein_fdr':\n",
    "                            if 'type' in df.columns:\n",
    "                                f_summary['id_rate'] = df[df['type'] == 'msms']['raw_idx'].nunique() / n_ms2\n",
    "                            else:\n",
    "                                f_summary['id_rate'] = df['raw_idx'].nunique() / n_ms2\n",
    "                                \n",
    "                            for field in ['protein','protein_group','precursor','naked_sequence','sequence']:\n",
    "                                f_summary['n_'+field] = df[field].nunique()\n",
    "\n",
    "                    summary[filename] = f_summary\n",
    "\n",
    "    summary['file_sizes']['files'] = file_sizes\n",
    "    summary['file_sizes']['results'] = os.path.getsize(settings['experiment']['results_path'])/1024**2\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def run_complete_workflow(\n",
    "    settings,\n",
    "    progress = False,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None,\n",
    "    callback_overall = None,\n",
    "    callback_task = None\n",
    "):\n",
    "\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    steps = [create_database, import_raw_data, feature_finding, search_data, recalibrate_data, search_data, score, align, match,\n",
    "            lfq_quantification, export]\n",
    "        \n",
    "    n_steps = len(steps)\n",
    "    \n",
    "\n",
    "    if progress:\n",
    "        logging.info('Setting callback to logger.')\n",
    "        #log progress to be used \n",
    "        def cb_logger_o(x):\n",
    "            logging.info(f\"__progress_overall {x:.3f}\")\n",
    "        def cb_logger_c(x):\n",
    "            logging.info(f\"__progress_current {x:.3f}\")\n",
    "        def cb_logger_t(x):\n",
    "            logging.info(f\"__current_task {x}\")\n",
    "            \n",
    "        callback = cb_logger_c\n",
    "        callback_overall = cb_logger_o\n",
    "        callback_task = cb_logger_t\n",
    "        \n",
    "    \n",
    "    def progress_wrapper(step, n_steps, current):\n",
    "        if callback:\n",
    "            callback(current)\n",
    "        if callback_overall:\n",
    "            callback_overall((step/n_steps)+(current/n_steps))\n",
    "            \n",
    "\n",
    "    recalibrated = False\n",
    "    \n",
    "    time_dict = {}\n",
    "    \n",
    "    run_start = time()\n",
    "    \n",
    "    summary = {}\n",
    "    \n",
    "    for idx, step in enumerate(steps):\n",
    "        if callback_task:\n",
    "            callback_task(step.__name__)\n",
    "        \n",
    "        start = time()\n",
    "        \n",
    "        progress_wrapper(idx, n_steps, 0)\n",
    "        \n",
    "        cb = functools.partial(progress_wrapper, idx, n_steps)\n",
    "        \n",
    "        if step is search_data:\n",
    "            settings, pept_dict, fasta_dict = step(settings, recalibrated=recalibrated, logger_set = True, settings_parsed = True, callback = cb)\n",
    "        \n",
    "        elif step is score:\n",
    "            settings = step(settings, pept_dict=pept_dict, fasta_dict=fasta_dict, logger_set = True,  settings_parsed = True, callback = cb)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if step is export:\n",
    "                # Get summary information \n",
    "                summary = get_summary(settings, summary)\n",
    "                settings['summary'] = summary\n",
    "    \n",
    "            settings = step(settings, logger_set = True,  settings_parsed = True, callback = cb)\n",
    "            \n",
    "        if step is recalibrate_data:\n",
    "            recalibrated = True\n",
    "            \n",
    "        progress_wrapper(idx, n_steps, 1)\n",
    "        \n",
    "        end = time()\n",
    "        \n",
    "        if step.__name__ in time_dict:\n",
    "            time_dict[step.__name__+'_2'] = (end-start)/60 #minutes\n",
    "        else:\n",
    "            time_dict[step.__name__] = (end-start)/60 #minutes\n",
    "        \n",
    "        time_dict['total'] = (end-run_start)/60\n",
    "\n",
    "        summary['timing'] = time_dict\n",
    "        summary['version'] = VERSION_NO\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watcher\n",
    "Function to watch folders for new files and automatically process them. This can be useful as time-consuming file conversion and feature finding steps can be started immediately after acquisition. There is a hook to upload the settings of processed files to a mongo db. The file `file_watcher_config_sample.yaml` shows the required arguments for running the FileWatcher. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FileWatcher():\n",
    "    \"\"\"\n",
    "    Class to watch files and process\n",
    "    \"\"\"\n",
    "    def __init__(self, config_path):\n",
    "        db_set = True\n",
    "        try:\n",
    "            from pymongo import MongoClient\n",
    "        except:\n",
    "            print('DB upload requires pymongo - DB upload deactivated')\n",
    "            db_set = False\n",
    "            \n",
    "        try:\n",
    "            import dns\n",
    "        except:\n",
    "            print('DB upload requires dnspython - DB upload deactivated')\n",
    "            db_set = False\n",
    "        \n",
    "        if os.path.isfile(config_path):\n",
    "            watcher_config = alphapept.settings.load_settings(config_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(config_path)\n",
    "        \n",
    "        if os.path.isdir(watcher_config['path']):\n",
    "            self.path = watcher_config['path']\n",
    "        else:\n",
    "            raise FileNotFoundError(path)\n",
    "            \n",
    "            \n",
    "        db_config = {}\n",
    "        for db_field in ['db_user', 'db_password', 'db_url', 'db_database', 'db_collection']:\n",
    "            if watcher_config[db_field] == '':\n",
    "                print(f\"{db_field} not set.\")\n",
    "                db_set = False\n",
    "            else:\n",
    "                db_config[db_field] = watcher_config[db_field]\n",
    "                \n",
    "        if db_set:\n",
    "            self.set_db(**db_config)\n",
    "        else:\n",
    "            self.db_set = False\n",
    "        \n",
    "        if os.path.isfile(watcher_config['settings']):\n",
    "            self.settings = alphapept.settings.load_settings(watcher_config['settings'])\n",
    "            print(f\"Loaded settings from {watcher_config['settings']}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(watcher_config['settings'])\n",
    "            \n",
    "        self.update_rate = watcher_config['update_rate']\n",
    "        self.n_processed = 0\n",
    "        self.n_failed = 0\n",
    "        self.minimum_file_size = watcher_config['minimum_file_size']\n",
    "        self.tag = watcher_config['tag']\n",
    "    \n",
    "    def check_new_files(self):\n",
    "        \"\"\"\n",
    "        Check for new files in folder\n",
    "        \"\"\"\n",
    "        new_files = []\n",
    "\n",
    "        for dirpath, dirnames, filenames in os.walk(self.path):\n",
    "\n",
    "            for dirname in [d for d in dirnames if d.endswith(('.d','.d/'))]: #Bruker\n",
    "                new_file = os.path.join(dirpath, dirname)\n",
    "                base, ext = os.path.splitext(dirname)\n",
    "                hdf_path = os.path.join(dirpath, base+'.ms_data.hdf')\n",
    "\n",
    "                if not os.path.exists(hdf_path):\n",
    "                    new_files.append(new_file)\n",
    "\n",
    "            for filename in [f for f in filenames if f.lower().endswith(('.raw','.raw/'))]: #Thermo\n",
    "                new_file = os.path.join(dirpath, filename)\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                hdf_path = os.path.join(dirpath, base+'.ms_data.hdf')\n",
    "\n",
    "                if not os.path.exists(hdf_path):\n",
    "                    new_files.append(new_file)\n",
    "        return new_files\n",
    "    \n",
    "    def set_db(self, db_user = '', db_password = '', db_url = '', db_database= '', db_collection= ''):\n",
    "        \n",
    "        self.db_user = db_user\n",
    "        self.db_password = db_password\n",
    "        self.db_url = db_url\n",
    "        self.db_database = db_database\n",
    "        self.db_collection = db_collection\n",
    "        \n",
    "        self.db_set = True\n",
    "\n",
    "\n",
    "    def check_file_completion(self, list_of_files, sleep_time = 10):\n",
    "        to_analyze = []\n",
    "        file_dict = {}\n",
    "\n",
    "        for file in list_of_files:\n",
    "            if file.endswith('.d'):\n",
    "                to_check = os.path.join(file, 'analysis.tdf_bin')\n",
    "            else:\n",
    "                to_check = file\n",
    "\n",
    "            file_dict[file] = os.path.getsize(to_check)\n",
    "\n",
    "        sleep(sleep_time)\n",
    "\n",
    "        for file in list_of_files:\n",
    "            if file.endswith('.d'):\n",
    "                to_check = os.path.join(file, 'analysis.tdf_bin')\n",
    "            else:\n",
    "                to_check = file\n",
    "\n",
    "            filesize = os.path.getsize(to_check)\n",
    "            if (filesize == file_dict[file]) & (filesize/1024/1024 > self.minimum_file_size):\n",
    "                to_analyze.append(file)\n",
    "\n",
    "        return to_analyze\n",
    "    \n",
    "    def run(self):\n",
    "        print(f'Starting FileWatcher on {self.path}')\n",
    "        \n",
    "        while True:\n",
    "            unprocessed = self.check_file_completion(self.check_new_files()) \n",
    "            \n",
    "            if self.tag != '':\n",
    "                unprocessed = [_ for _ in unprocessed if self.tag.lower() in _.lower()]\n",
    "                \n",
    "            if len(unprocessed) > 0:\n",
    "                try:\n",
    "                    settings = self.settings.copy()\n",
    "                    settings['experiment']['file_paths'] =  [unprocessed[0]]\n",
    "                    settings_ = alphapept.interface.run_complete_workflow(settings)\n",
    "                    \n",
    "                    if self.db_set:\n",
    "                        self.upload_to_db(settings_)\n",
    "                    self.n_processed +=1\n",
    "                    print(f'--- File Watcher Status: Files processed {self.n_processed:,} - failed {self.n_failed:,} ---')\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except Exception as e:\n",
    "                    logging.info(e)\n",
    "                    self.n_failed +=1\n",
    "            else:\n",
    "                sleep(self.update_rate)\n",
    "                \n",
    "\n",
    "    def upload_to_db(self, settings):\n",
    "        from pymongo import MongoClient\n",
    "        logging.info('Uploading to DB')\n",
    "        string = f\"mongodb+srv://{self.db_user}:{self.db_password}@{self.db_url}\"\n",
    "        client = MongoClient(string)\n",
    "\n",
    "        post_id = client[self.db_database][self.db_collection].insert_one(settings).inserted_id\n",
    "\n",
    "        logging.info(f\"Uploaded {post_id}.\")\n",
    "\n",
    "        return post_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI\n",
    "\n",
    "All workflow functions can be called with the command line interface (CLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import click\n",
    "import os\n",
    "import alphapept.settings\n",
    "from alphapept.__version__ import VERSION_NO\n",
    "from alphapept.__version__ import COPYRIGHT\n",
    "from alphapept.__version__ import URL\n",
    "\n",
    "CONTEXT_SETTINGS = dict(help_option_names=['-h', '--help'])\n",
    "CLICK_SETTINGS_OPTION = click.argument(\n",
    "    \"settings_file\",\n",
    "#     help=\"A .yaml file with settings.\",\n",
    "    type=click.Path(exists=True, dir_okay=False),\n",
    "#     default=f\"{os.path.dirname(__file__)}/settings_template.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "def run_cli():\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                \"\\n\",\n",
    "                r\"     ___    __      __          ____             __ \",\n",
    "                r\"    /   |  / /___  / /_  ____  / __ \\___  ____  / /_\",\n",
    "                r\"   / /| | / / __ \\/ __ \\/ __ \\/ /_/ / _ \\/ __ \\/ __/\",\n",
    "                r\"  / ___ |/ / /_/ / / / / /_/ / ____/ ___/ /_/ / /_  \",\n",
    "                r\" /_/  |_/_/ .___/_/ /_/\\__,_/_/    \\___/ .___/\\__/  \",\n",
    "                r\"         /_/                          /_/           \",\n",
    "                '.'*52,\n",
    "                '.{}.'.format(URL.center(50)),\n",
    "                '.{}.'.format(COPYRIGHT.center(50)),\n",
    "                '.{}.'.format(VERSION_NO.center(50)),\n",
    "                '.'*52,\n",
    "                \"\\n\"\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    cli_overview.add_command(cli_database)\n",
    "    cli_overview.add_command(cli_import)\n",
    "    cli_overview.add_command(cli_feature_finding)\n",
    "    cli_overview.add_command(cli_search)\n",
    "    cli_overview.add_command(cli_recalibrate)\n",
    "    cli_overview.add_command(cli_score)\n",
    "    cli_overview.add_command(cli_quantify)\n",
    "    cli_overview.add_command(cli_export)\n",
    "    cli_overview.add_command(cli_workflow)\n",
    "    cli_overview.add_command(cli_gui)\n",
    "    cli_overview.add_command(cli_watcher)\n",
    "    cli_overview()\n",
    "\n",
    "\n",
    "@click.group(\n",
    "    context_settings=CONTEXT_SETTINGS,\n",
    "#     help=\"AlphaPept\"\n",
    ")\n",
    "def cli_overview():\n",
    "    pass\n",
    "\n",
    "\n",
    "@click.command(\n",
    "    \"database\",\n",
    "    help=\"Create a database from a fasta file.\",\n",
    "    short_help=\"Create a database from a fasta file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_database(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    create_database(settings)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"import\",\n",
    "    help=\"Import and convert raw data from vendor to `.ms_data.hdf` file.\",\n",
    "    short_help=\"Import and convert raw data from vendor to `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_import(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    import_raw_data(settings)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"features\",\n",
    "    help=\"Find features in a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Find features in a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_feature_finding(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    feature_finding(settings)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"search\",\n",
    "    help=\"Search and identify feature in a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Search and identify feature in a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "@click.option(\n",
    "    '--recalibrated_features',\n",
    "    '-r',\n",
    "    'recalibrated',\n",
    "    help=\"Use recalibrated features if present\",\n",
    "    is_flag=True,\n",
    "    default=False,\n",
    "    show_default=True,\n",
    ")\n",
    "def cli_search(settings_file, recalibrated):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    search_data(settings, recalibrated)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"recalibrate\",\n",
    "    help=\"Recalibrate a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Recalibrate a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_recalibrate(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    recalibrate_data(settings)\n",
    "    \n",
    "\n",
    "@click.command(\n",
    "    \"score\",\n",
    "    help=\"Score PSM from a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Score PSM from a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_score(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    score(settings)\n",
    "    \n",
    "@click.command(\n",
    "    \"align\",\n",
    "    help=\"Align multiple `.ms_data.hdf` files.\",\n",
    "    short_help=\"Align multiple `.ms_data.hdf` files.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_align(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    align(settings)\n",
    "    \n",
    "@click.command(\n",
    "    \"match\",\n",
    "    help=\"Perform match between run type analysis on multiple `.ms_data.hdf` files.\",\n",
    "    short_help=\"Perform match between run type analysis on multiple `.ms_data.hdf` files.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_match(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    align(settings)\n",
    "    match(settings)\n",
    "    \n",
    "@click.command(\n",
    "    \"quantify\",\n",
    "    help=\"Quantify and compare multiple `.ms_data.hdf` files.\",\n",
    "    short_help=\"Quantify and compare multiple `.ms_data.hdf` files.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_quantify(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    lfq_quantification(settings)\n",
    "    \n",
    "\n",
    "@click.command(\n",
    "    \"export\",\n",
    "    help=\"Export protein table from `.ms_data.hdf` files as `.csv`\",\n",
    "    short_help=\"Export protein table from `.ms_data.hdf` files as `.csv`.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_export(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    export(settings)\n",
    "    \n",
    "\n",
    "@click.command(\n",
    "    \"workflow\",\n",
    "    help=\"Run the complete AlphaPept workflow.\",\n",
    "    short_help=\"Run the complete AlphaPept workflow.\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--progress\",\n",
    "    \"-p\",\n",
    "    help=\"Log progress output.\",\n",
    "    is_flag=True,\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_workflow(settings_file, progress):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    run_complete_workflow(settings, progress = progress)\n",
    "\n",
    "    \n",
    "@click.command(\n",
    "    \"gui\",\n",
    "    help=\"Start graphical user interface for AlphaPept.\",\n",
    ")\n",
    "@click.option(\n",
    "    \"--test\",\n",
    "    \"test\",\n",
    "    help=\"Test\",\n",
    "    is_flag=True,\n",
    "    default=False,\n",
    "    show_default=True,\n",
    ")\n",
    "def cli_gui(test):\n",
    "    print('Launching GUI')\n",
    "    import alphapept.ui\n",
    "    if test:\n",
    "        alphapept.ui.main(close=True)\n",
    "    else:\n",
    "        alphapept.ui.main()\n",
    "      \n",
    "@click.command(\n",
    "    \"watcher\",\n",
    "    help=\"Watch folder for new files and automatically process them. Upload to MongoDB possible.\",\n",
    "    short_help=\"File watching and procesing.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_watcher(settings_file):\n",
    "    x = FileWatcher(settings_file)\n",
    "    x.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "No export destination, ignored:\n",
      "#export\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import logging\n",
      "import alphapept.io\n",
      "\n",
      "def filter_score(df, mode='multiple'):\n",
      "    \"\"\"\n",
      "    Filter df by score\n",
      "    TODO: PSMS could still have the same socre when having modifications at multiple positions that are not distinguishable.\n",
      "    Only keep one.\n",
      "\n",
      "    \"\"\"\n",
      "    df[\"rank\"] = df.groupby(\"query_idx\")[\"score\"].rank(\"dense\", ascending=False).astype(\"int\")\n",
      "    df = df[df[\"rank\"] == 1]\n",
      "\n",
      "    # in case two hits have the same score and therfore rank only accept the first one\n",
      "    df = df.drop_duplicates(\"query_idx\")\n",
      "\n",
      "    if 'dist' in df.columns:\n",
      "        df[\"feature_rank\"] = df.groupby(\"feature_idx\")[\"dist\"].rank(\"dense\", ascending=True).astype(\"int\")\n",
      "        df[\"raw_rank\"] = df.groupby(\"raw_idx\")[\"score\"].rank(\"dense\", ascending=False).astype(\"int\")\n",
      "\n",
      "        if mode == 'single':\n",
      "            df_filtered = df[(df[\"feature_rank\"] == 1) & (df[\"raw_rank\"] == 1) ]\n",
      "            df_filtered = df_filtered.drop_duplicates(\"raw_idx\")\n",
      "\n",
      "        elif mode == 'multiple':\n",
      "            df_filtered = df[(df[\"feature_rank\"] == 1)]\n",
      "\n",
      "        else:\n",
      "            raise NotImplementedError('Mode {} not implemented yet'.format(mode))\n",
      "\n",
      "    else:\n",
      "        df_filtered = df\n",
      "\n",
      "    # TOD: this needs to be sorted out, for modifications -> What if we have MoxM -> oxMM, this will screw up with the filter sequence part\n",
      "    return df_filtered\n",
      "\n",
      "def filter_precursor(df):\n",
      "    \"\"\"\n",
      "    Filter df by precursor\n",
      "    Allow each precursor only once.\n",
      "\n",
      "    \"\"\"\n",
      "    df[\"rank_precursor\"] = (\n",
      "        df.groupby(\"precursor\")[\"score\"].rank(\"dense\", ascending=False).astype(\"int\")\n",
      "    )\n",
      "    df_filtered = df[df[\"rank_precursor\"] == 1]\n",
      "\n",
      "    return df_filtered\n",
      "No export destination, ignored:\n",
      "#export\n",
      "from numba import njit\n",
      "@njit\n",
      "def get_q_values(fdr_values):\n",
      "    \"\"\"\n",
      "    Calculate q values from fdr_values\n",
      "    \"\"\"\n",
      "    q_values = np.zeros_like(fdr_values)\n",
      "    min_q_value = np.max(fdr_values)\n",
      "    for i in range(len(fdr_values) - 1, -1, -1):\n",
      "        fdr = fdr_values[i]\n",
      "        if fdr < min_q_value:\n",
      "            min_q_value = fdr\n",
      "        q_values[i] = min_q_value\n",
      "\n",
      "    return q_values\n",
      "No export destination, ignored:\n",
      "#export\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "def cut_fdr(df, fdr_level=0.01, plot=True):\n",
      "    \"\"\"\n",
      "    Cuts a dataframe with a given fdr level\n",
      "\n",
      "    Args:\n",
      "        fdr_level: fdr level that should be used\n",
      "        plot: flag to enable plot\n",
      "\n",
      "    Returns:\n",
      "        cutoff: df with psms within fdr\n",
      "        cutoff_value: numerical value of score cutoff\n",
      "\n",
      "    Raises:\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    df[\"target\"] = ~df[\"decoy\"]\n",
      "\n",
      "    df = df.sort_values(by=[\"score\",\"decoy\"], ascending=False)\n",
      "    df = df.reset_index()\n",
      "\n",
      "    df[\"target_cum\"] = np.cumsum(df[\"target\"])\n",
      "    df[\"decoys_cum\"] = np.cumsum(df[\"decoy\"])\n",
      "\n",
      "    df[\"fdr\"] = df[\"decoys_cum\"] / df[\"target_cum\"]\n",
      "    df[\"q_value\"] = get_q_values(df[\"fdr\"].values)\n",
      "\n",
      "    last_q_value = df[\"q_value\"].iloc[-1]\n",
      "    first_q_value = df[\"q_value\"].iloc[0]\n",
      "\n",
      "    if last_q_value <= fdr_level:\n",
      "        logging.info('Last q_value {:.3f} of dataset is smaller than fdr_level {:.3f}'.format(last_q_value, fdr_level))\n",
      "        cutoff_index = len(df)-1\n",
      "\n",
      "    elif first_q_value >= fdr_level:\n",
      "        logging.info('First q_value {:.3f} of dataset is larger than fdr_level {:.3f}'.format(last_q_value, fdr_level))\n",
      "        cutoff_index = 0\n",
      "\n",
      "    else:\n",
      "        cutoff_index = df[df[\"q_value\"].gt(fdr_level)].index[0] - 1\n",
      "\n",
      "    cutoff_value = df.loc[cutoff_index][\"score\"]\n",
      "    cutoff = df[df[\"score\"] >= cutoff_value]\n",
      "\n",
      "    targets = df.loc[cutoff_index, \"target_cum\"]\n",
      "    decoy = df.loc[cutoff_index, \"decoys_cum\"]\n",
      "\n",
      "    fdr = df.loc[cutoff_index, \"fdr\"]\n",
      "\n",
      "\n",
      "    logging.info(\n",
      "        \"{:,} target ({:,} decoy) of {} PSM. fdr {:.6f} for a cutoff of {:.2f} \".format(\n",
      "            targets, decoy, len(df), fdr, cutoff_value\n",
      "        )\n",
      "    )\n",
      "\n",
      "    if plot:\n",
      "        import matplotlib.pyplot as plt\n",
      "        import seaborn as sns\n",
      "        plt.figure(figsize=(10, 5))\n",
      "        plt.plot(df[\"score\"], df[\"fdr\"])\n",
      "        plt.axhline(0.01, color=\"k\", linestyle=\"--\")\n",
      "\n",
      "        plt.axvline(cutoff_value, color=\"r\", linestyle=\"--\")\n",
      "        plt.title(\"fdr vs Cutoff value\")\n",
      "        plt.xlabel(\"Score\")\n",
      "        plt.ylabel(\"fdr\")\n",
      "        # plt.savefig('fdr.png')\n",
      "        plt.show()\n",
      "\n",
      "        bins = np.linspace(np.min(df[\"score\"]), np.max(df[\"score\"]), 100)\n",
      "        plt.figure(figsize=(10, 5))\n",
      "        sns.distplot(df[df[\"decoy\"]][\"score\"].values, label=\"decoy\", bins=bins)\n",
      "        sns.distplot(df[~df[\"decoy\"]][\"score\"].values, label=\"target\", bins=bins)\n",
      "        plt.xlabel(\"Score\")\n",
      "        plt.ylabel(\"Frequency\")\n",
      "        plt.title(\"Score vs Class\")\n",
      "        plt.legend()\n",
      "        plt.show()\n",
      "\n",
      "    cutoff = cutoff.reset_index(drop=True)\n",
      "    return cutoff_value, cutoff\n",
      "No export destination, ignored:\n",
      "#export\n",
      "\n",
      "def cut_global_fdr(data, analyte_level='sequence', fdr_level=0.01, plot=True, **kwargs):\n",
      "    \"\"\"\n",
      "    Function to estimate and filter by global peptide or protein fdr\n",
      "\n",
      "    \"\"\"\n",
      "    logging.info('Global FDR on {}'.format(analyte_level))\n",
      "    data_sub = data[[analyte_level,'score','decoy']]\n",
      "    data_sub_unique = data_sub.groupby([analyte_level,'decoy'], as_index=False).agg({\"score\": \"max\"})\n",
      "\n",
      "    analyte_levels = ['precursor', 'sequence', 'protein']\n",
      "\n",
      "    if analyte_level in analyte_levels:\n",
      "        agg_score = data_sub_unique.groupby([analyte_level,'decoy'])['score'].max().reset_index()\n",
      "    else:\n",
      "        raise Exception('analyte_level should be either sequence or protein. The selected analyte_level was: {}'.format(analyte_level))\n",
      "\n",
      "    agg_cval, agg_cutoff = cut_fdr(agg_score, fdr_level=fdr_level, plot=plot)\n",
      "\n",
      "    agg_report = pd.merge(data,\n",
      "                          agg_cutoff,\n",
      "                          how = 'inner',\n",
      "                          on = [analyte_level,'decoy'],\n",
      "                          suffixes=('', '_'+analyte_level),\n",
      "                          validate=\"many_to_one\")\n",
      "    return agg_report\n",
      "No export destination, ignored:\n",
      "#export\n",
      "\n",
      "import networkx as nx\n",
      "\n",
      "def get_x_tandem_score(df):\n",
      "\n",
      "    b = df['b_hits'].astype('int').apply(lambda x: np.math.factorial(x)).values\n",
      "    y = df['y_hits'].astype('int').apply(lambda x: np.math.factorial(x)).values\n",
      "    x_tandem = np.log(b.astype('float')*y.astype('float')*df['matched_int'].values)\n",
      "\n",
      "    x_tandem[x_tandem==-np.inf] = 0\n",
      "\n",
      "    return x_tandem\n",
      "\n",
      "def score_x_tandem(df, fdr_level = 0.01, plot = True, **kwargs):\n",
      "    logging.info('Scoring using X-Tandem')\n",
      "    df['score'] = get_x_tandem_score(df)\n",
      "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
      "\n",
      "    df = filter_score(df)\n",
      "    df = filter_precursor(df)\n",
      "    cval, cutoff = cut_fdr(df, fdr_level, plot)\n",
      "\n",
      "    return cutoff\n",
      "\n",
      "def filter_with_x_tandem(df, fdr_level = 0.01):\n",
      "    \"\"\"\n",
      "    Filters a dataframe using an x_tandem score\n",
      "    \"\"\"\n",
      "    logging.info('Filter df with x_tandem score')\n",
      "\n",
      "    df['score'] = get_x_tandem_score(df)\n",
      "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
      "\n",
      "    df = filter_score(df)\n",
      "    df = filter_precursor(df)\n",
      "\n",
      "    return df\n",
      "No export destination, ignored:\n",
      "#export\n",
      "\n",
      "def score_psms(df, score = 'y_hits', fdr_level = 0.01, plot = True, **kwargs):\n",
      "    if score in df.columns:\n",
      "        df['score'] = df[score]\n",
      "    else:\n",
      "        raise ValueError(\"The specified 'score' {} is not available in 'df'.\".format(score))\n",
      "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
      "\n",
      "    df = filter_score(df)\n",
      "    df = filter_precursor(df)\n",
      "    cval, cutoff = cut_fdr(df, fdr_level, plot)\n",
      "\n",
      "    return cutoff\n",
      "No export destination, ignored:\n",
      "#export\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "from alphapept.fasta import count_missed_cleavages, count_internal_cleavages\n",
      "\n",
      "\n",
      "def get_ML_features(df, protease='trypsin', **kwargs):\n",
      "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
      "\n",
      "    df['abs_delta_m_ppm'] = np.abs(df['delta_m_ppm'])\n",
      "    df['naked_sequence'] = df['sequence'].str.replace('[a-z]|_', '')\n",
      "    df['n_AA']= df['naked_sequence'].str.len()\n",
      "    df['matched_ion_fraction'] = df['hits']/(2*df['n_AA'])\n",
      "\n",
      "    df['n_missed'] = df['naked_sequence'].apply(lambda x: count_missed_cleavages(x, protease))\n",
      "    df['n_internal'] = df['naked_sequence'].apply(lambda x: count_internal_cleavages(x, protease))\n",
      "    \n",
      "    df['x_tandem'] = get_x_tandem_score(df)\n",
      "\n",
      "    return df\n",
      "\n",
      "def train_RF(df,\n",
      "             exclude_features = ['ion_idx','fasta_index','feature_rank','raw_rank','rank','db_idx', 'feature_idx', 'precursor', 'query_idx', 'raw_idx','sequence','decoy','naked_sequence'],\n",
      "             train_fdr_level = 0.1,\n",
      "             ini_score = 'x_tandem',\n",
      "             min_train = 5000,\n",
      "             test_size = 0.8,\n",
      "             max_depth = [5,25,50],\n",
      "             max_leaf_nodes = [150,200,250],\n",
      "             n_jobs=1,\n",
      "             scoring='accuracy',\n",
      "             plot = False,\n",
      "             random_state = 42,\n",
      "             **kwargs):\n",
      "    \n",
      "    features = [_ for _ in df.columns if _ not in exclude_features]\n",
      "\n",
      "    # Setup ML pipeline\n",
      "    scaler = StandardScaler()\n",
      "    rfc = RandomForestClassifier(random_state=random_state) # class_weight={False:1,True:5},\n",
      "    ## Initiate scaling + classification pipeline\n",
      "    pipeline = Pipeline([('scaler', scaler), ('clf', rfc)])\n",
      "    parameters = {'clf__max_depth':(max_depth), 'clf__max_leaf_nodes': (max_leaf_nodes)}\n",
      "    ## Setup grid search framework for parameter selection and internal cross validation\n",
      "    cv = GridSearchCV(pipeline, param_grid=parameters, cv=5, scoring=scoring,\n",
      "                     verbose=0,return_train_score=True,n_jobs=n_jobs)\n",
      "\n",
      "    # Prepare target and decoy df\n",
      "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
      "    df['target'] = ~df['decoy']\n",
      "    df['score'] = df[ini_score]\n",
      "    dfT = df[~df.decoy]\n",
      "    dfD = df[df.decoy]\n",
      "\n",
      "    # Select high scoring targets (<= train_fdr_level)\n",
      "    df_prescore = filter_score(df)\n",
      "    df_prescore = filter_precursor(df_prescore)\n",
      "    scored = cut_fdr(df_prescore, fdr_level = train_fdr_level, plot=False)[1]\n",
      "    highT = scored[scored.decoy==False]\n",
      "    dfT_high = dfT[dfT['query_idx'].isin(highT.query_idx)]\n",
      "    dfT_high = dfT_high[dfT_high['db_idx'].isin(highT.db_idx)]\n",
      "\n",
      "    # Determine the number of psms for semi-supervised learning\n",
      "    n_train = int(dfT_high.shape[0])\n",
      "    if dfD.shape[0] < n_train:\n",
      "        n_train = int(dfD.shape[0])\n",
      "        logging.info(\"The total number of available decoys is lower than the initial set of high scoring targets.\")\n",
      "    if n_train < min_train:\n",
      "        raise ValueError(\"There are fewer high scoring targets or decoys than required by 'min_train'.\")\n",
      "\n",
      "    # Subset the targets and decoys datasets to result in a balanced dataset\n",
      "    df_training = dfT_high.sample(n=n_train, random_state=random_state).append(dfD.sample(n=n_train, random_state=random_state))\n",
      "\n",
      "    # Select training and test sets\n",
      "    X = df_training[features]\n",
      "    y = df_training['target'].astype(int)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=test_size, random_state=random_state, stratify=y.values)\n",
      "\n",
      "    # Train the classifier on the training set via 5-fold cross-validation and subsequently test on the test set\n",
      "    logging.info('Training & cross-validation on {} targets and {} decoys'.format(np.sum(y_train),X_train.shape[0]-np.sum(y_train)))\n",
      "    cv.fit(X_train,y_train)\n",
      "\n",
      "    logging.info('The best parameters selected by 5-fold cross-validation were {}'.format(cv.best_params_))\n",
      "    logging.info('The train {} was {}'.format(scoring, cv.score(X_train, y_train)))\n",
      "    logging.info('Testing on {} targets and {} decoys'.format(np.sum(y_test),X_test.shape[0]-np.sum(y_test)))\n",
      "    logging.info('The test {} was {}'.format(scoring, cv.score(X_test, y_test)))\n",
      "    \n",
      "    feature_importances=cv.best_estimator_.named_steps['clf'].feature_importances_\n",
      "    indices = np.argsort(feature_importances)[::-1][:40]\n",
      "    \n",
      "    top_features = X.columns[indices][:40]\n",
      "    top_score = feature_importances[indices][:40]\n",
      "    \n",
      "    feature_dict = dict(zip(top_features, top_score))\n",
      "    logging.info(f\"Top features {feature_dict}\")\n",
      "    \n",
      "    # Inspect feature importances\n",
      "    if plot:\n",
      "        g = sns.barplot(y=X.columns[indices][:40],\n",
      "                        x = feature_importances[indices][:40],\n",
      "                        orient='h', palette='RdBu')\n",
      "        g.set_xlabel(\"Relative importance\",fontsize=12)\n",
      "        g.set_ylabel(\"Features\",fontsize=12)\n",
      "        g.tick_params(labelsize=9)\n",
      "        g.set_title(\"Feature importance\")\n",
      "        plt.show()\n",
      "\n",
      "    return cv, features\n",
      "\n",
      "def score_ML(df,\n",
      "             trained_classifier,\n",
      "             features = None,\n",
      "            fdr_level = 0.01,\n",
      "            plot=True,\n",
      "             **kwargs):\n",
      "    \n",
      "    logging.info('Scoring using Machine Learning')\n",
      "    # Apply the classifier to the entire dataset\n",
      "    df_new = df.copy()\n",
      "    df_new['score'] = trained_classifier.predict_proba(df_new[features])[:,1]\n",
      "    df_new = filter_score(df_new)\n",
      "    df_new = filter_precursor(df_new)\n",
      "    cval, cutoff = cut_fdr(df_new, fdr_level, plot)\n",
      "\n",
      "    return cutoff\n",
      "\n",
      "\n",
      "def filter_with_ML(df,\n",
      "             trained_classifier,\n",
      "             features = None,\n",
      "            fdr_level = 0.01,\n",
      "            plot=True,\n",
      "             **kwargs):\n",
      "\n",
      "    \"\"\"\n",
      "    Filters a dataframe using ML\n",
      "    \"\"\"\n",
      "    logging.info('Filter df with x_tandem score')\n",
      "    # Apply the classifier to the entire dataset\n",
      "    df_new = df.copy()\n",
      "    df_new['score'] = trained_classifier.predict_proba(df_new[features])[:,1]\n",
      "    df_new = filter_score(df_new)\n",
      "    df_new = filter_precursor(df_new)\n",
      "\n",
      "    return df_new\n",
      "No export destination, ignored:\n",
      "#export\n",
      "import networkx as nx\n",
      "def get_protein_groups(data, pept_dict, fasta_dict, callback = None, **kwargs):\n",
      "    \"\"\"\n",
      "    Function to perform protein grouping by razor approach\n",
      "    ToDo: implement callback for solving\n",
      "    Each protein is indicated with a p -> protein index\n",
      "    \"\"\"\n",
      "    G=nx.Graph()\n",
      "\n",
      "    found_proteins = {}\n",
      "\n",
      "    for i in range(len(data)):\n",
      "        line = data.iloc[i]\n",
      "        seq = line['sequence']\n",
      "        score = line['score']\n",
      "        if seq in pept_dict:\n",
      "            proteins = pept_dict[seq]\n",
      "            if len(proteins) > 1:\n",
      "                for protein in proteins:\n",
      "                    G.add_edge(str(i), 'p'+str(protein), score=score)\n",
      "            else: #if there is only one PSM just add to this protein\n",
      "                if 'p'+str(proteins[0]) in found_proteins.keys():\n",
      "                    found_proteins['p'+str(proteins[0])] = found_proteins['p'+str(proteins[0])] + [str(i)]\n",
      "                else:\n",
      "                    found_proteins['p'+str(proteins[0])] = [str(i)]\n",
      "\n",
      "        if callback:\n",
      "            callback((i+1)/len(data))\n",
      "\n",
      "    logging.info('A total of {:,} proteins with unique PSMs found'.format(len(found_proteins)))\n",
      "\n",
      "    connected_groups = np.array([list(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)], dtype=object)\n",
      "    n_groups = len(connected_groups)\n",
      "\n",
      "\n",
      "    logging.info('A total of {} ambigious proteins'.format(len(connected_groups)))\n",
      "\n",
      "    #Solving with razor:\n",
      "    found_proteins_razor = {}\n",
      "    for a in connected_groups:\n",
      "        H = G.subgraph(a)\n",
      "\n",
      "        shared_proteins = list(np.array(a)[np.array(list(i[0] == 'p' for i in a))])\n",
      "\n",
      "        removed = []\n",
      "\n",
      "        while len(shared_proteins) > 0:\n",
      "\n",
      "            neighbors_list = []\n",
      "\n",
      "            for node in shared_proteins:\n",
      "                neighbors = list(H.neighbors(node))\n",
      "                n_neigbhors = len(neighbors)\n",
      "\n",
      "                if node in G:\n",
      "                    if node in found_proteins.keys():\n",
      "                        n_neigbhors+= len(found_proteins[node])\n",
      "\n",
      "                neighbors_list.append((n_neigbhors, node, neighbors))\n",
      "\n",
      "            neighbors_list.sort()\n",
      "\n",
      "            #Remove the last entry:\n",
      "\n",
      "            count, node, psms = neighbors_list[-1]\n",
      "\n",
      "            shared_proteins.remove(node)\n",
      "\n",
      "            psms = [_ for _ in psms if _ not in removed]\n",
      "\n",
      "            removed += psms\n",
      "\n",
      "            found_proteins_razor[node] = psms\n",
      "\n",
      "    #Put back in Df\n",
      "    report = data.copy()\n",
      "    report['protein'] = ''\n",
      "    report['protein_group'] = ''\n",
      "\n",
      "    for protein_str in found_proteins.keys():\n",
      "        protein = int(protein_str[1:])\n",
      "        indexes = [int(_) for _ in found_proteins[protein_str]]\n",
      "        report.loc[indexes, 'protein'] = fasta_dict[protein]['name']\n",
      "        report.loc[indexes, 'protein_group'] = fasta_dict[protein]['name']\n",
      "\n",
      "    report['razor'] = False\n",
      "    for protein_str in found_proteins_razor.keys():\n",
      "        protein = int(protein_str[1:])\n",
      "        indexes = [int(_) for _ in found_proteins_razor[protein_str]]\n",
      "\n",
      "        report.loc[indexes, 'protein'] = fasta_dict[protein]['name']\n",
      "        report.loc[indexes, 'razor'] = True\n",
      "\n",
      "    for a in connected_groups:\n",
      "        protein_group = list(np.array(a)[np.array(list(i[0] == 'p' for i in a))])\n",
      "        psms = [int(i) for i in a if i not in protein_group]\n",
      "        report.loc[psms, 'protein_group'] = ','.join([fasta_dict[int(_[1:])]['name'] for _ in protein_group])\n",
      "\n",
      "    return report\n",
      "\n",
      "def perform_protein_grouping(data, pept_dict, fasta_dict, **kwargs):\n",
      "    \"\"\"\n",
      "    Wrapper function to perform protein grouping by razor approach\n",
      "\n",
      "    \"\"\"\n",
      "    data_sub = data[['sequence','score','decoy']]\n",
      "    data_sub_unique = data_sub.groupby(['sequence','decoy'], as_index=False).agg({\"score\": \"max\"})\n",
      "\n",
      "    targets = data_sub_unique[data_sub_unique.decoy == False]\n",
      "    targets = targets.reset_index(drop=True)\n",
      "    protein_targets = get_protein_groups(targets, pept_dict, fasta_dict, **kwargs)\n",
      "\n",
      "    decoys = data_sub_unique[data_sub_unique.decoy == True]\n",
      "    decoys = decoys.reset_index(drop=True)\n",
      "    protein_decoys = get_protein_groups(decoys, pept_dict, fasta_dict, **kwargs)\n",
      "\n",
      "    protein_groups = protein_targets.append(protein_decoys)\n",
      "    protein_groups_app = protein_groups[['sequence','decoy','protein','protein_group','razor']]\n",
      "    protein_report = pd.merge(data,\n",
      "                                protein_groups_app,\n",
      "                                how = 'inner',\n",
      "                                on = ['sequence','decoy'],\n",
      "                                validate=\"many_to_one\")\n",
      "    return protein_report\n",
      "No export destination, ignored:\n",
      "#export \n",
      "import os\n",
      "from multiprocessing import Pool\n",
      "\n",
      "\n",
      "def score_hdf(to_process):\n",
      "\n",
      "    path, settings = to_process\n",
      "\n",
      "    skip = False\n",
      "\n",
      "    ms_file = alphapept.io.MS_Data_File(path, is_overwritable=True)\n",
      "    \n",
      "    try:\n",
      "        df = ms_file.read(dataset_name='second_search')\n",
      "\n",
      "        logging.info('Found second search psms for scoring.')\n",
      "    except KeyError:\n",
      "        df = ms_file.read(dataset_name='first_search')\n",
      "        logging.info('No second search psms for scoring found. Using first search.')\n",
      "\n",
      "    if len(df) == 0:\n",
      "        skip = True\n",
      "        logging.info('Dataframe does not contain data. Skipping scoring step.')\n",
      "\n",
      "    if not skip:\n",
      "        df = get_ML_features(df, **settings['fasta'])\n",
      "\n",
      "        if settings[\"general\"][\"score\"] == 'random_forest':\n",
      "            try:\n",
      "                cv, features = train_RF(df)\n",
      "                df = filter_with_ML(df, cv, features = features)\n",
      "            except ValueError as e:\n",
      "                logging.info('ML failed. Defaulting to x_tandem score')\n",
      "                logging.info(f\"{e}\")\n",
      "                df = filter_with_x_tandem(df)\n",
      "        elif settings[\"general\"][\"score\"] == 'x_tandem':\n",
      "            df = filter_with_x_tandem(df)\n",
      "        else:\n",
      "            raise NotImplementedError('Scoring method {} not implemented.'.format(settings[\"general\"][\"score\"]))\n",
      "\n",
      "        df = cut_global_fdr(df, analyte_level='precursor',  plot=False, **settings['search'])\n",
      "\n",
      "        ms_file.write(df, dataset_name=\"peptide_fdr\")\n",
      "        \n",
      "        logging.info('FDR on peptides complete. For {} FDR found {:,} targets and {:,} decoys.'.format(settings[\"search\"][\"peptide_fdr\"], df['target'].sum(), df['decoy'].sum()) )\n",
      "\n",
      "\n",
      "def score_hdf_parallel(settings, callback=None):\n",
      "\n",
      "    paths = []\n",
      "\n",
      "    for _ in settings['experiment']['file_paths']:\n",
      "        base, ext = os.path.splitext(_)\n",
      "        hdf_path = base+'.ms_data.hdf'\n",
      "        paths.append(hdf_path)\n",
      "\n",
      "    to_process = [(path, settings) for path in paths]\n",
      "\n",
      "    n_processes = settings['general']['n_processes']\n",
      "\n",
      "    if len(to_process) == 1:\n",
      "        score_hdf(to_process[0])\n",
      "    else:\n",
      "\n",
      "        with Pool(n_processes) as p:\n",
      "            max_ = len(to_process)\n",
      "            for i, _ in enumerate(p.imap_unordered(score_hdf, to_process)):\n",
      "                if callback:\n",
      "                    callback((i+1)/max_)\n",
      "No export destination, ignored:\n",
      "#export\n",
      "\n",
      "ion_dict = {}\n",
      "ion_dict[0] = ''\n",
      "ion_dict[1] = '-H20'\n",
      "ion_dict[2] = '-NH3'\n",
      "\n",
      "def get_ion(i, df, ions):\n",
      "    start = df['ion_idx'].iloc[i]\n",
      "    end = df['n_ions'].iloc[i]+start\n",
      "\n",
      "    ion = [('b'+str(int(_))).replace('b-','y') for _ in ions.iloc[start:end]['ion_index']]\n",
      "    losses = [ion_dict[int(_)] for _ in ions.iloc[start:end]['ion_type']]\n",
      "    ion = [a+b for a,b in zip(ion, losses)]\n",
      "    ints = ions.iloc[start:end]['ion_int'].astype('int').values\n",
      "    \n",
      "    return ion, ints\n",
      "\n",
      "def protein_groups_hdf(to_process):\n",
      "\n",
      "    skip = False\n",
      "    path, pept_dict, fasta_dict, settings = to_process\n",
      "    ms_file = alphapept.io.MS_Data_File(path, is_overwritable=True)\n",
      "    try:\n",
      "        df = ms_file.read(dataset_name='peptide_fdr')\n",
      "        ions = ms_file.read(dataset_name='ions')\n",
      "    except KeyError:\n",
      "        skip = True\n",
      "\n",
      "    if not skip:\n",
      "        df_pg = perform_protein_grouping(df, pept_dict, fasta_dict, callback = None)\n",
      "\n",
      "        df_pg = cut_global_fdr(df_pg, analyte_level='protein',  plot=False, **settings['search'])\n",
      "        logging.info('FDR on proteins complete. For {} FDR found {:,} targets and {:,} decoys. A total of {:,} proteins found.'.format(settings[\"search\"][\"protein_fdr\"], df_pg['target'].sum(), df_pg['decoy'].sum(), len(set(df_pg['protein']))))\n",
      "        \n",
      "        logging.info('Extracting ions')\n",
      "        \n",
      "        ion_list = []\n",
      "        ion_ints = []\n",
      "\n",
      "        for i in range(len(df_pg)):\n",
      "            ion, ints = get_ion(i, df_pg, ions)\n",
      "            ion_list.append(ion)\n",
      "            ion_ints.append(ints)\n",
      "\n",
      "        df_pg['ion_int'] = ion_ints\n",
      "        df_pg['ion_types'] = ion_list\n",
      "        \n",
      "        logging.info('Extracting ions complete.')\n",
      "        \n",
      "        ms_file.write(df_pg, dataset_name=\"protein_fdr\")\n",
      "        base, ext = os.path.splitext(path)\n",
      "        df_pg.to_csv(base+'_protein_fdr.csv')\n",
      "\n",
      "        logging.info('Saving complete.')\n",
      "\n",
      "\n",
      "def protein_groups_hdf_parallel(settings, pept_dict, fasta_dict, callback=None):\n",
      "\n",
      "    paths = []\n",
      "\n",
      "    for _ in settings['experiment']['file_paths']:\n",
      "        base, ext = os.path.splitext(_)\n",
      "        hdf_path = base+'.ms_data.hdf'\n",
      "        paths.append(hdf_path)\n",
      "\n",
      "    to_process = [(path, pept_dict.copy(), fasta_dict.copy(), settings) for path in paths]\n",
      "\n",
      "    n_processes = settings['general']['n_processes']\n",
      "\n",
      "    if len(to_process) == 1:\n",
      "        protein_groups_hdf(to_process[0])\n",
      "    else:\n",
      "\n",
      "        with Pool(n_processes) as p:\n",
      "            max_ = len(to_process)\n",
      "            for i, _ in enumerate(p.imap_unordered(protein_groups_hdf, to_process)):\n",
      "                if callback:\n",
      "                    callback((i+1)/max_)\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\installer_test\\\\alphapept\\\\alphapept\\\\None.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-09bc8eae79de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#hide\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnbdev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnotebook2script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\alphapept\\lib\\site-packages\\nbdev\\export.py\u001b[0m in \u001b[0;36mnotebook2script\u001b[1;34m(fname, silent, to_dict)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_mod_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_notebook2script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0madd_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lib_path\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\alphapept\\lib\\site-packages\\nbdev\\export.py\u001b[0m in \u001b[0;36m_notebook2script\u001b[1;34m(fname, modules, silent, to_dict)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_from_future_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_add2all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mf\"'{f}'\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m         \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' +$'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMULTILINE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\alphapept\\lib\\site-packages\\nbdev\\export.py\u001b[0m in \u001b[0;36m_add2all\u001b[1;34m(fname, names, line_width)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_add2all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[0mtw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_indent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsequent_indent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbreak_long_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0mre_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_re__all__def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\installer_test\\\\alphapept\\\\alphapept\\\\None.py'"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
