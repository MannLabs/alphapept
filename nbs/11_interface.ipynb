{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface\n",
    "\n",
    "This notebook describes/implements the interface to use alphapept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "The implemented functions are as follows:\n",
    "\n",
    "* Create database\n",
    "* Import raw data\n",
    "* Perform feature finding\n",
    "* Search data with fasta\n",
    "* Recalibrate\n",
    "* Score data with fasta\n",
    "* Perform LFQ\n",
    "* Export results\n",
    "* Run whole workflow\n",
    "\n",
    "The last command allows to run the whole pipeline at once.\n",
    "\n",
    "Helper functions include:\n",
    "\n",
    "* Callback function to track progress\n",
    "* Logging function\n",
    "* Version/hardware/settings checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import alphapept.utils\n",
    "from alphapept.utils import set_logger\n",
    "\n",
    "import alphapept.speed\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import psutil\n",
    "\n",
    "def parallel_execute(settings, step, callback=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generic function to parallel execute worklow steps on a file basis\n",
    "    \n",
    "    \"\"\"\n",
    "    n_processes = settings['general']['n_processes']\n",
    "    n_processes = alphapept.speed.set_max_process(n_processes)\n",
    "    \n",
    "    files = settings['experiment']['file_paths']\n",
    "    n_files = len(files)\n",
    "    logging.info(f'Processing {len(files)} files for step {step.__name__}')\n",
    "\n",
    "    if 'failed' not in settings:\n",
    "        settings['failed'] = {}\n",
    "\n",
    "    to_process = [(i, settings) for i in range(n_files)]\n",
    "    \n",
    "    failed = []\n",
    "    \n",
    "    if n_files == 1:\n",
    "        if not step(to_process[0], callback=callback, parallel=True):\n",
    "            failed.append(files[0])\n",
    "\n",
    "    else:\n",
    "        #Limit number of processes for Bruker FF\n",
    "        if step.__name__ == 'find_features':\n",
    "            base, ext = os.path.splitext(files[0])\n",
    "            if ext.lower() == '.d':\n",
    "                memory_available = psutil.virtual_memory().available/1024**3\n",
    "                n_processes = max((int(memory_available //25 ),1))\n",
    "                logging.info(f'Using Bruker Feature Finder. Setting Process limit to {n_processes}.')\n",
    "            elif ext.lower() == '.raw':\n",
    "                memory_available = psutil.virtual_memory().available/1024**3\n",
    "                n_processes = max((int(memory_available //8 ), 1))\n",
    "                logging.info(f'Setting Process limit to {n_processes}')\n",
    "            else:\n",
    "                raise NotImplementedError('File extension {} not understood.'.format(ext))\n",
    "                \n",
    "        if step.__name__ == 'search_db':\n",
    "            memory_available = psutil.virtual_memory().available/1024**3\n",
    "            n_processes = max((int(memory_available //8 ), 1)) # 8 gb per file: Todo: make this better\n",
    "            logging.info(f'Searching. Setting Process limit to {n_processes}.')\n",
    "        \n",
    "        \n",
    "        failed = []\n",
    "        rerun = []\n",
    "        rerun_map = {}\n",
    "        with alphapept.speed.AlphaPool(n_processes) as p:\n",
    "            for i, success in enumerate(p.imap(step, to_process)):\n",
    "                if success is not True:\n",
    "                    failed.append((i, files[i]))\n",
    "                    logging.error(f'Processing of {files[i]} for step {step.__name__} failed. Exception {success}')\n",
    "                    rerun_map[len(rerun)] = i\n",
    "                    rerun.append(to_process[i])\n",
    "                    \n",
    "                if callback:\n",
    "                    callback((i+1)/n_files)\n",
    "                    \n",
    "        ## Retry failed with more memory\n",
    "        n_processes_ = max((1, int(n_processes // 2)))\n",
    "        logging.info(f'Attempting to rerun failed runs with {n_processes_} processes')\n",
    "        \n",
    "        failed = []\n",
    "        with alphapept.speed.AlphaPool(n_processes_) as p:\n",
    "            for i, success in enumerate(p.imap(step, rerun)):\n",
    "                if success is not True:\n",
    "                    failed.append(files[rerun_map[i]])\n",
    "                    logging.error(f'Processing of {files[i]} for step {step.__name__} failed. Exception {success}')\n",
    "\n",
    "                if callback:\n",
    "                    callback((i+1)/n_files)\n",
    "        \n",
    "    if step.__name__ not in settings['failed']:\n",
    "        settings['failed'][step.__name__] = failed\n",
    "    else:\n",
    "        settings['failed'][step.__name__+'_2'] = failed\n",
    "\n",
    "    return settings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import tqdm\n",
    "\n",
    "def tqdm_wrapper(pbar, update):\n",
    "    current_value = pbar.n\n",
    "    delta = update - current_value\n",
    "    pbar.update(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def check_version_and_hardware(settings):\n",
    "    import alphapept.utils\n",
    "    #alphapept.utils.check_hardware()\n",
    "    #alphapept.utils.check_python_env()\n",
    "    alphapept.utils.show_platform_info()\n",
    "    alphapept.utils.show_python_info()\n",
    "    \n",
    "    \n",
    "    settings = alphapept.utils.check_settings(settings)\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import os\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "def create_database(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    import alphapept.fasta\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "    if 'database_path' not in settings['experiment']:\n",
    "        database_path = ''\n",
    "    else:\n",
    "        database_path = settings['experiment']['database_path']\n",
    "        \n",
    "    if database_path is None:\n",
    "        database_path = ''\n",
    "        \n",
    "        \n",
    "    if not settings['fasta']['save_db']: #Do not save DB\n",
    "        settings['experiment']['database_path'] = None\n",
    "        logging.info('Not saving Database.')\n",
    "\n",
    "        return settings\n",
    "        \n",
    "\n",
    "    temp_settings = settings\n",
    "                \n",
    "    if os.path.isfile(database_path):\n",
    "        logging.info(\n",
    "            'Database path set and exists. Using {} as database.'.format(\n",
    "                database_path\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        logging.info(\n",
    "            'Database path {} is not a file.'.format(database_path)\n",
    "        )\n",
    "\n",
    "        if len(settings['experiment']['fasta_paths']) == 0:\n",
    "            raise FileNotFoundError(\"No FASTA files set.\")\n",
    "            \n",
    "        total_fasta_size = 0\n",
    "\n",
    "        for fasta_file in settings['experiment']['fasta_paths']:\n",
    "            if os.path.isfile(fasta_file):\n",
    "                \n",
    "                fasta_size = os.stat(fasta_file).st_size/(1024**2)\n",
    "                \n",
    "                total_fasta_size += fasta_size\n",
    "                \n",
    "                logging.info(\n",
    "                    'Found FASTA file {} with size {:.2f} Mb.'.format(\n",
    "                        fasta_file,\n",
    "                        fasta_size\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    'File {} not found'.format(fasta_file)\n",
    "                )\n",
    "                \n",
    "        fasta_size_max = settings['fasta']['fasta_size_max']\n",
    "                \n",
    "        if total_fasta_size >= fasta_size_max:\n",
    "            logging.info(f'Total FASTA size {total_fasta_size:.2f} is larger than the set maximum size of {fasta_size_max:.2f} Mb')\n",
    "            \n",
    "            settings['experiment']['database_path'] = None\n",
    "            \n",
    "            return settings\n",
    "            \n",
    "        logging.info('Creating a new database from FASTA.')\n",
    "\n",
    "        if not callback:\n",
    "            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "        else:\n",
    "            cb = callback\n",
    "\n",
    "        (\n",
    "            spectra,\n",
    "            pept_dict,\n",
    "            fasta_dict\n",
    "        ) = alphapept.fasta.generate_database_parallel(\n",
    "            temp_settings,\n",
    "            callback=cb\n",
    "        )\n",
    "        logging.info(\n",
    "            'Digested {:,} proteins and generated {:,} spectra'.format(\n",
    "                len(fasta_dict),\n",
    "                len(spectra)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        alphapept.fasta.save_database(\n",
    "            spectra,\n",
    "            pept_dict,\n",
    "            fasta_dict,\n",
    "            database_path = database_path,\n",
    "            **settings['fasta']\n",
    "        )\n",
    "        logging.info(\n",
    "            'Database saved to {}. Filesize of database is {:.2f} GB'.format(\n",
    "                database_path,\n",
    "                os.stat(database_path).st_size/(1024**3)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        settings['experiment']['database_path'] = database_path\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def import_raw_data(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "        \n",
    "    import alphapept.io        \n",
    "    \n",
    "    settings = parallel_execute(settings, alphapept.io.raw_conversion, callback = cb)\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def feature_finding(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.feature_finding\n",
    "    \n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "\n",
    "    settings = parallel_execute(settings, alphapept.feature_finding.find_features, callback = cb)\n",
    "    \n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def wrapped_partial(func, *args, **kwargs):\n",
    "    partial_func = functools.partial(func, *args, **kwargs)\n",
    "    functools.update_wrapper(partial_func, func)\n",
    "    return partial_func\n",
    "\n",
    "def search_data(\n",
    "    settings,\n",
    "    first_search=True,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.search\n",
    "    import alphapept.io\n",
    "    \n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "        \n",
    "    if first_search:\n",
    "        logging.info('Starting first search.')\n",
    "        if settings['experiment']['database_path'] is not None:\n",
    "            settings = parallel_execute(settings, wrapped_partial(alphapept.search.search_db, first_search = first_search), callback = cb)\n",
    "\n",
    "            db_data = alphapept.fasta.read_database(settings['experiment']['database_path'])\n",
    "\n",
    "            fasta_dict = db_data['fasta_dict'].item()\n",
    "            pept_dict = db_data['pept_dict'].item()\n",
    "\n",
    "            \n",
    "        else:\n",
    "            ms_files = []\n",
    "            for _ in settings['experiment']['file_paths']:\n",
    "                base, ext = os.path.splitext(_)\n",
    "                ms_files.append(base + '.ms_data.hdf')\n",
    "            \n",
    "            fasta_dict = alphapept.search.search_parallel(\n",
    "                settings,\n",
    "                callback=cb\n",
    "            )\n",
    "            pept_dict = None\n",
    "            \n",
    "        logging.info('First search complete.')\n",
    "    else:\n",
    "        logging.info('Starting second search with DB.')\n",
    "        \n",
    "        if settings['experiment']['database_path'] is not None:\n",
    "            settings = parallel_execute(settings, wrapped_partial(alphapept.search.search_db, first_search = first_search), callback = cb)\n",
    "            \n",
    "            db_data = alphapept.fasta.read_database(settings['experiment']['database_path'])\n",
    "\n",
    "            fasta_dict = db_data['fasta_dict'].item()\n",
    "            pept_dict = db_data['pept_dict'].item()\n",
    "\n",
    "\n",
    "        else:\n",
    "            \n",
    "            ms_files = []\n",
    "            for _ in settings['experiment']['file_paths']:\n",
    "                base, ext = os.path.splitext(_)\n",
    "                ms_files.append(base + '.ms_data.hdf')\n",
    "\n",
    "            try:\n",
    "                offsets = [\n",
    "                    alphapept.io.MS_Data_File(\n",
    "                        ms_file_name\n",
    "                    ).read(\n",
    "                        dataset_name=\"corrected_mass\",\n",
    "                        group_name=\"features\",\n",
    "                        attr_name=\"estimated_max_precursor_ppm\"\n",
    "                    ) * settings['search']['calibration_std'] for ms_file_name in ms_files\n",
    "                ]\n",
    "            except KeyError:\n",
    "                logging.info('No calibration found.')\n",
    "                offsets = None\n",
    "            \n",
    "            logging.info('Starting second search.')\n",
    "\n",
    "            fasta_dict = alphapept.search.search_parallel(\n",
    "                settings,\n",
    "                calibration=offsets,\n",
    "                callback=cb\n",
    "            )\n",
    "            pept_dict = None\n",
    "\n",
    "        logging.info('Second search complete.')\n",
    "    return settings, pept_dict, fasta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def recalibrate_data(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.recalibration\n",
    "    \n",
    "    if settings['search']['calibrate']:\n",
    "        if not callback:\n",
    "            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "        else:\n",
    "            cb = callback\n",
    "\n",
    "        settings = parallel_execute(settings, alphapept.recalibration.calibrate_hdf, callback = cb)\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def score(\n",
    "    settings,\n",
    "    pept_dict=None,\n",
    "    fasta_dict=None,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):    \n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.score\n",
    "    import alphapept.fasta\n",
    "    \n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "        \n",
    "    if fasta_dict is None:\n",
    "                \n",
    "        db_data = alphapept.fasta.read_database(\n",
    "            settings['experiment']['database_path']\n",
    "        )\n",
    "        fasta_dict = db_data['fasta_dict'].item()\n",
    "        pept_dict = db_data['pept_dict'].item()\n",
    "               \n",
    "    settings = parallel_execute(settings, alphapept.score.score_hdf, callback = cb)\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def protein_grouping(\n",
    "    settings,\n",
    "    pept_dict=None,\n",
    "    fasta_dict=None,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):    \n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.score\n",
    "    import alphapept.fasta\n",
    "    \n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "        \n",
    "    if fasta_dict is None:\n",
    "                \n",
    "        db_data = alphapept.fasta.read_database(\n",
    "            settings['experiment']['database_path']\n",
    "        )\n",
    "        fasta_dict = db_data['fasta_dict'].item()\n",
    "        pept_dict = db_data['pept_dict'].item()\n",
    "               \n",
    "\n",
    "    if pept_dict is None: #Pept dict extractions needs scored\n",
    "        pept_dict = alphapept.fasta.pept_dict_from_search(settings)\n",
    "                \n",
    "    logging.info(f'Fasta dict with length {len(fasta_dict):,}, Pept dict with length {len(pept_dict):,}')\n",
    "    \n",
    "    # Protein groups\n",
    "    logging.info('Extracting protein groups.')\n",
    "\n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "    \n",
    "    alphapept.score.protein_grouping_all(settings, pept_dict, fasta_dict, callback=cb)\n",
    "    \n",
    "    logging.info('Protein groups complete.')\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def align(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    \n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.matching\n",
    "    \n",
    "    alphapept.matching.align_datasets(settings, callback = callback)\n",
    "    \n",
    "    return settings\n",
    "\n",
    "def match(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.matching\n",
    "        \n",
    " \n",
    "    alphapept.matching.match_datasets(settings)\n",
    "    \n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def quantification(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.quantification\n",
    "    \n",
    "    field = settings['quantification']['mode']\n",
    "    \n",
    "    df = pd.read_hdf(settings['experiment']['results_path'], 'protein_fdr')\n",
    "        \n",
    "    if settings[\"workflow\"][\"lfq_quantification\"]:\n",
    "\n",
    "        if field in df.keys():  # Check if the quantification information exists.\n",
    "            # We could include another protein fdr in here..\n",
    "\n",
    "            files = df['filename'].unique().tolist()\n",
    "            \n",
    "            if len(files) > 1:\n",
    "                logging.info('Delayed Normalization.')\n",
    "                df, normalization = alphapept.quantification.delayed_normalization(\n",
    "                    df,\n",
    "                    field\n",
    "                )\n",
    "                pd.DataFrame(normalization).to_hdf(\n",
    "                    settings['experiment']['results_path'],\n",
    "                    'fraction_normalization'\n",
    "                )\n",
    "                df_grouped = df.groupby(\n",
    "                    ['filename', 'precursor', 'protein_group']\n",
    "                )[['{}_dn'.format(field)]].sum().reset_index()\n",
    "            else:\n",
    "                df_grouped = df.groupby(\n",
    "                    ['filename', 'precursor', 'protein_group']\n",
    "                )[field].sum().reset_index()\n",
    "                \n",
    "\n",
    "            df.to_hdf(\n",
    "                settings['experiment']['results_path'],\n",
    "                'combined_protein_fdr_dn'\n",
    "            )\n",
    "\n",
    "            logging.info('Complete. ')\n",
    "            logging.info('Starting profile extraction.')\n",
    "\n",
    "            if not callback:\n",
    "                cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "            else:\n",
    "                cb = callback\n",
    "\n",
    "            protein_table = alphapept.quantification.protein_profile_parallel_ap(\n",
    "                settings,\n",
    "                df_grouped,\n",
    "                callback=cb\n",
    "            )\n",
    "            protein_table.to_hdf(\n",
    "                settings['experiment']['results_path'],\n",
    "                'protein_table'\n",
    "            )\n",
    "            results_path = settings['experiment']['results_path']\n",
    "            base, ext = os.path.splitext(results_path)\n",
    "            protein_table.to_csv(base+'_proteins.csv')\n",
    "\n",
    "            logging.info('LFQ complete.')\n",
    "            \n",
    "            protein_summary = pd.DataFrame(index = df['protein_group'].unique())\n",
    "\n",
    "            for field in ['sequence','precursor']:\n",
    "                col_ = 'n_'+ field+' '\n",
    "                m = df.groupby(['protein_group','filename'])[field].count().unstack()\n",
    "                m.columns = [col_ +_ for _ in m.columns]\n",
    "                protein_summary.loc[m.index, m.columns] = m.values\n",
    "\n",
    "            # Add intensity\n",
    "            new_cols = ['intensity ' + _[:-4] if _.endswith('_LFQ') else 'LFQ intensity '+_ for _ in protein_table.columns ]\n",
    "            protein_summary.loc[protein_table.index, new_cols] = protein_table.values\n",
    "            protein_summary.to_csv(base+'_protein_summary.csv')\n",
    "            logging.info(f'Saved df of length {len(df):,} saved to {base}')\n",
    "\n",
    "            #protein summary\n",
    "            protein_Summary.to_hdf(\n",
    "            settings['experiment']['results_path'],\n",
    "            'protein_Summary'\n",
    "                )\n",
    "            \n",
    "        \n",
    "    if len(df) > 0:\n",
    "        logging.info('Exporting as csv.')\n",
    "        results_path = settings['experiment']['results_path']\n",
    "        base, ext = os.path.splitext(results_path)\n",
    "        df.to_csv(base+'.csv') \n",
    "        logging.info(f'Saved df of length {len(df):,} saved to {base}')        \n",
    "        \n",
    "    else:\n",
    "        logging.info(f\"No Proteins found.\")\n",
    "        \n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import yaml\n",
    "\n",
    "def export(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "    base, ext = os.path.splitext(settings['experiment']['results_path'])\n",
    "    out_path_settings = base+'.yaml'\n",
    "\n",
    "    with open(out_path_settings, 'w') as file:\n",
    "        yaml.dump(settings, file)\n",
    "\n",
    "    logging.info('Settings saved to {}'.format(out_path_settings))\n",
    "    logging.info('Analysis complete.')\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback\n",
    "\n",
    "Return an overall progress\n",
    "And a task based progress.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from time import time, sleep\n",
    "from alphapept.__version__ import VERSION_NO\n",
    "import datetime\n",
    "import alphapept.utils\n",
    "\n",
    "\n",
    "def extract_median_unique(settings):\n",
    "    protein_fdr = pd.read_hdf(settings['experiment']['results_path'], 'protein_fdr')\n",
    "    cols = [_ for _ in ['protein','protein_group','precursor','naked_sequence','sequence'] if _ in protein_fdr.columns]\n",
    "    n_unique = protein_fdr.groupby('filename')[cols].nunique()\n",
    "    n_unique.index = [os.path.split(_)[1][:-12] for _ in n_unique.index]\n",
    "    cols = [_ for _ in ['fwhm','int_sum','rt_length','rt_tail','o_mass_ppm_raw'] if _ in protein_fdr.columns]\n",
    "    median = protein_fdr.groupby('filename')[cols].median()\n",
    "    median.index = [os.path.split(_)[1][:-12] for _ in median.index]\n",
    "    \n",
    "    return median, n_unique\n",
    "\n",
    "\n",
    "\n",
    "def get_file_summary(ms_data):\n",
    "    f_summary = {}\n",
    "\n",
    "    f_summary['acquisition_date_time'] = ms_data.read(group_name = 'Raw', attr_name = 'acquisition_date_time')\n",
    "    n_ms2 = ms_data.read(group_name='Raw/MS2_scans', dataset_name='prec_mass_list2', return_dataset_shape=True)[0]\n",
    "\n",
    "    for key in ms_data.read():\n",
    "\n",
    "        if \"is_pd_dataframe\" in ms_data.read(attr_name=\"\", group_name=key):\n",
    "            df = ms_data.read(dataset_name=key)\n",
    "\n",
    "            f_summary[f\"{key} (n in table)\"] = len(df)\n",
    "            \n",
    "            if key in ['peptide_fdr']:\n",
    "                if 'type' in df.columns:\n",
    "                    f_summary['id_rate (peptide_fdr)'] = float(df[df['type'] == 'msms']['raw_idx'].nunique() / n_ms2)\n",
    "                else:\n",
    "                    f_summary['id_rate (peptide_fdr)'] = float(df['raw_idx'].nunique() / n_ms2)\n",
    "\n",
    "            if key in ['feature_table','peptide_fdr']:                \n",
    "                for field in ['fwhm','int_sum','rt_length','rt_tail','o_mass_ppm_raw']:\n",
    "                    if field in df.columns:\n",
    "                        f_summary[f'{field} ({key}, median)'] = float(df[field].median())\n",
    "                        \n",
    "    return f_summary\n",
    "\n",
    "def get_summary(settings, summary):\n",
    "\n",
    "    summary['file_sizes'] = {}\n",
    "\n",
    "    file_sizes = {}\n",
    "    for _ in settings['experiment']['file_paths']:\n",
    "\n",
    "        base, ext = os.path.splitext(_)\n",
    "        filename = os.path.split(base)[1]\n",
    "        ms_file_name = os.path.splitext(_)[0] + \".ms_data.hdf\"\n",
    "        \n",
    "        file_sizes[ms_file_name] = os.path.getsize(ms_file_name)/1024**2\n",
    "\n",
    "        ms_data = alphapept.io.MS_Data_File(os.path.splitext(_)[0] + \".ms_data.hdf\")\n",
    "                                                        \n",
    "        summary[filename] = get_file_summary(ms_data)\n",
    "\n",
    "    summary['file_sizes']['files'] = file_sizes\n",
    "    if os.path.isfile(settings['experiment']['results_path']):\n",
    "        summary['file_sizes']['results'] = os.path.getsize(settings['experiment']['results_path'])/1024**2\n",
    "        \n",
    "        median, n_unique = extract_median_unique(settings)\n",
    "        \n",
    "        for col in median.columns:\n",
    "            for _ in range(len(median)): \n",
    "                summary[median.index[_]][f'{col} (protein_fdr, median)'] = float(median.iloc[_][col])\n",
    "        \n",
    "        for col in n_unique.columns:\n",
    "            for _ in range(len(n_unique)): \n",
    "                summary[n_unique.index[_]][f'{col} (protein_fdr, n unique)'] = int(n_unique.iloc[_][col])\n",
    "        \n",
    "    return summary\n",
    "\n",
    "\n",
    "def run_complete_workflow(\n",
    "    settings,\n",
    "    progress = False,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None,\n",
    "    callback_overall = None,\n",
    "    callback_task = None,\n",
    "    logfile = None\n",
    "):\n",
    "\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "        \n",
    "    if logfile is not None:\n",
    "        set_logger(log_file_name=logfile)\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    steps = []\n",
    "    \n",
    "    workflow = settings['workflow']\n",
    "    \n",
    "    if \"continue_runs\" in workflow:\n",
    "        if not workflow[\"continue_runs\"]:\n",
    "            for _ in settings['experiment']['file_paths']:\n",
    "                alphapept.utils.delete_file(os.path.splitext(_)[0]+\".ms_data.hdf\")\n",
    "    if workflow[\"create_database\"]:\n",
    "        steps.append(create_database)\n",
    "    if workflow[\"import_raw_data\"]:\n",
    "        steps.append(import_raw_data)\n",
    "    if workflow[\"find_features\"]:\n",
    "        steps.append(feature_finding)\n",
    "    if workflow[\"search_data\"]:\n",
    "        steps.append(search_data)\n",
    "    if workflow[\"recalibrate_data\"]:\n",
    "        steps.append(recalibrate_data)\n",
    "        steps.append(search_data)\n",
    "    steps.append(score)\n",
    "    if workflow[\"align\"]:\n",
    "        steps.append(align)\n",
    "    if workflow[\"match\"]:\n",
    "        if align not in steps:\n",
    "            steps.append(align)\n",
    "        steps.append(match)\n",
    "    steps.append(protein_grouping)\n",
    "    steps.append(quantification)\n",
    "    steps.append(export)\n",
    "             \n",
    "    n_steps = len(steps)\n",
    "    logging.info(f\"Workflow has {n_steps} steps\")\n",
    "    \n",
    "\n",
    "    if progress:\n",
    "        logging.info('Setting callback to logger.')\n",
    "        \n",
    "\n",
    "        #log progress to be used \n",
    "        def cb_logger_o(x):\n",
    "            logging.info(f\"__progress_overall {x:.3f}\")\n",
    "        def cb_logger_c(x):\n",
    "            logging.info(f\"__progress_current {x:.3f}\")\n",
    "        def cb_logger_t(x):\n",
    "            logging.info(f\"__current_task {x}\")\n",
    "            \n",
    "        callback = cb_logger_c\n",
    "        callback_overall = cb_logger_o\n",
    "        callback_task = cb_logger_t\n",
    "        \n",
    "    \n",
    "    def progress_wrapper(step, n_steps, current):\n",
    "        if callback:\n",
    "            callback(current)\n",
    "        if callback_overall:\n",
    "            callback_overall((step/n_steps)+(current/n_steps))\n",
    "            \n",
    "    pept_dict = None\n",
    "    fasta_dict = None\n",
    "\n",
    "    first_search = True\n",
    "    \n",
    "    time_dict = {}\n",
    "    \n",
    "    run_start = time()\n",
    "    \n",
    "    summary = {}\n",
    "    \n",
    "    for idx, step in enumerate(steps):\n",
    "        if callback_task:\n",
    "            callback_task(step.__name__)\n",
    "        \n",
    "        start = time()\n",
    "        \n",
    "        if callback_overall:\n",
    "            progress_wrapper(idx, n_steps, 0)\n",
    "            cb = functools.partial(progress_wrapper, idx, n_steps)\n",
    "        elif callback:\n",
    "            cb = callback\n",
    "        else:\n",
    "            cb = None\n",
    "        \n",
    "        if step is search_data:\n",
    "            settings, pept_dict, fasta_dict = step(settings, first_search=first_search, logger_set = True, settings_parsed = True, callback = cb)\n",
    "        \n",
    "        elif (step is score) or (step is protein_grouping):\n",
    "            \n",
    "            settings = step(settings, pept_dict=pept_dict, fasta_dict=fasta_dict, logger_set = True,  settings_parsed = True, callback = cb)\n",
    "            \n",
    "        else:\n",
    "            if step is export:\n",
    "                # Get summary information \n",
    "                summary = get_summary(settings, summary)\n",
    "                settings['summary'] = summary\n",
    "    \n",
    "            settings = step(settings, logger_set = True,  settings_parsed = True, callback = cb)\n",
    "            \n",
    "        if step is recalibrate_data:\n",
    "            first_search = False\n",
    "        \n",
    "        if callback_overall:\n",
    "            progress_wrapper(idx, n_steps, 1)\n",
    "        \n",
    "        end = time()\n",
    "        \n",
    "        if f\"{step.__name__} (min)\" in time_dict:\n",
    "            time_dict[f\"{step.__name__}_2 (min)\"] = (end-start)/60 #minutes\n",
    "        else:\n",
    "            time_dict[f\"{step.__name__} (min)\"] = (end-start)/60 #minutes\n",
    "        \n",
    "        time_dict['total (min)'] = (end-run_start)/60\n",
    "\n",
    "        summary['timing'] = time_dict\n",
    "        summary['version'] = VERSION_NO\n",
    "        summary['time'] = f\"{datetime.datetime.now()}\"\n",
    "        \n",
    "        processed_files = []\n",
    "        \n",
    "        for _ in settings['experiment']['file_paths']:\n",
    "            processed_files.append(os.path.split(_)[1])\n",
    "            \n",
    "        summary['processed_files'] = processed_files\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI\n",
    "\n",
    "All workflow functions can be called with the command line interface (CLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import click\n",
    "import os\n",
    "import alphapept.settings\n",
    "from alphapept.__version__ import VERSION_NO\n",
    "from alphapept.__version__ import COPYRIGHT\n",
    "from alphapept.__version__ import URL\n",
    "\n",
    "CONTEXT_SETTINGS = dict(help_option_names=['-h', '--help'])\n",
    "CLICK_SETTINGS_OPTION = click.argument(\n",
    "    \"settings_file\",\n",
    "#     help=\"A .yaml file with settings.\",\n",
    "    type=click.Path(exists=True, dir_okay=False),\n",
    "#     default=f\"{os.path.dirname(__file__)}/settings_template.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "def run_cli():\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                \"\\n\",\n",
    "                r\"     ___    __      __          ____             __ \",\n",
    "                r\"    /   |  / /___  / /_  ____  / __ \\___  ____  / /_\",\n",
    "                r\"   / /| | / / __ \\/ __ \\/ __ \\/ /_/ / _ \\/ __ \\/ __/\",\n",
    "                r\"  / ___ |/ / /_/ / / / / /_/ / ____/ ___/ /_/ / /_  \",\n",
    "                r\" /_/  |_/_/ .___/_/ /_/\\__,_/_/    \\___/ .___/\\__/  \",\n",
    "                r\"         /_/                          /_/           \",\n",
    "                '.'*52,\n",
    "                '.{}.'.format(URL.center(50)),\n",
    "                '.{}.'.format(COPYRIGHT.center(50)),\n",
    "                '.{}.'.format(VERSION_NO.center(50)),\n",
    "                '.'*52,\n",
    "                \"\\n\"\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    cli_overview.add_command(cli_database)\n",
    "    cli_overview.add_command(cli_import)\n",
    "    cli_overview.add_command(cli_feature_finding)\n",
    "    cli_overview.add_command(cli_search)\n",
    "    cli_overview.add_command(cli_recalibrate)\n",
    "    cli_overview.add_command(cli_score)\n",
    "    cli_overview.add_command(cli_quantify)\n",
    "    cli_overview.add_command(cli_export)\n",
    "    cli_overview.add_command(cli_workflow)\n",
    "    cli_overview.add_command(cli_gui)\n",
    "    cli_overview()\n",
    "\n",
    "\n",
    "@click.group(\n",
    "    context_settings=CONTEXT_SETTINGS,\n",
    "#     help=\"AlphaPept\"\n",
    ")\n",
    "def cli_overview():\n",
    "    pass\n",
    "\n",
    "\n",
    "@click.command(\n",
    "    \"database\",\n",
    "    help=\"Create a database from a fasta file.\",\n",
    "    short_help=\"Create a database from a fasta file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_database(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    create_database(settings)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"import\",\n",
    "    help=\"Import and convert raw data from vendor to `.ms_data.hdf` file.\",\n",
    "    short_help=\"Import and convert raw data from vendor to `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_import(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    import_raw_data(settings)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"features\",\n",
    "    help=\"Find features in a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Find features in a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_feature_finding(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    feature_finding(settings)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"search\",\n",
    "    help=\"Search and identify feature in a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Search and identify feature in a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "@click.option(\n",
    "    '--recalibrated_features',\n",
    "    '-r',\n",
    "    'recalibrated',\n",
    "    help=\"Use recalibrated features if present\",\n",
    "    is_flag=True,\n",
    "    default=False,\n",
    "    show_default=True,\n",
    ")\n",
    "def cli_search(settings_file, recalibrated):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    search_data(settings, recalibrated)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"recalibrate\",\n",
    "    help=\"Recalibrate a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Recalibrate a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_recalibrate(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    recalibrate_data(settings)\n",
    "    \n",
    "\n",
    "@click.command(\n",
    "    \"score\",\n",
    "    help=\"Score PSM from a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Score PSM from a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_score(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    score(settings)\n",
    "    \n",
    "@click.command(\n",
    "    \"align\",\n",
    "    help=\"Align multiple `.ms_data.hdf` files.\",\n",
    "    short_help=\"Align multiple `.ms_data.hdf` files.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_align(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    align(settings)\n",
    "    \n",
    "@click.command(\n",
    "    \"match\",\n",
    "    help=\"Perform match between run type analysis on multiple `.ms_data.hdf` files.\",\n",
    "    short_help=\"Perform match between run type analysis on multiple `.ms_data.hdf` files.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_match(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    align(settings)\n",
    "    match(settings)\n",
    "    \n",
    "@click.command(\n",
    "    \"quantify\",\n",
    "    help=\"Quantify and compare multiple `.ms_data.hdf` files.\",\n",
    "    short_help=\"Quantify and compare multiple `.ms_data.hdf` files.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_quantify(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    quantification(settings)\n",
    "    \n",
    "\n",
    "@click.command(\n",
    "    \"export\",\n",
    "    help=\"Export protein table from `.ms_data.hdf` files as `.csv`\",\n",
    "    short_help=\"Export protein table from `.ms_data.hdf` files as `.csv`.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_export(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    export(settings)\n",
    "    \n",
    "\n",
    "@click.command(\n",
    "    \"workflow\",\n",
    "    help=\"Run the complete AlphaPept workflow.\",\n",
    "    short_help=\"Run the complete AlphaPept workflow.\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--progress\",\n",
    "    \"-p\",\n",
    "    help=\"Log progress output.\",\n",
    "    is_flag=True,\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_workflow(settings_file, progress):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    run_complete_workflow(settings, progress = progress)\n",
    "\n",
    "    \n",
    "@click.command(\n",
    "    \"gui\",\n",
    "    help=\"Start graphical user interface for AlphaPept.\",\n",
    ")\n",
    "\n",
    "def cli_gui():\n",
    "    print('Starting AlphaPept Server')\n",
    "    print('This may take a second..')\n",
    "    \n",
    "    _this_file = os.path.abspath(__file__)\n",
    "    _this_directory = os.path.dirname(_this_file)\n",
    "    \n",
    "    file_path = os.path.join(_this_directory, 'webui.py')\n",
    "    \n",
    "    \n",
    "    #if __name__ == '__main__':\n",
    "    #    sys.argv = [\"streamlit\", \"run\", \"webui.py\"]\n",
    "    #    sys.exit(stcli.main())\n",
    "    \n",
    "    #args = '--theme.primaryColor #18212b --theme.backgroundColor #FFFFFF --theme.secondaryBackgroundColor #f0f2f6 --theme.textColor #262730 --theme.font \"sans serif\"' \n",
    "    \n",
    "    #sys.argv = [\"streamlit\", \"run\", file_path, args]\n",
    "    \n",
    "    HOME = os.path.expanduser(\"~\")\n",
    "    \n",
    "    ST_PATH = os.path.join(HOME, \".streamlit\")\n",
    "\n",
    "    for folder in [ST_PATH]:\n",
    "        if not os.path.isdir(folder):\n",
    "            os.mkdir(folder)\n",
    "\n",
    "    #Check if streamlit credentials exists\n",
    "    ST_CREDENTIALS = os.path.join(ST_PATH, 'credentials.toml')\n",
    "    if not os.path.isfile(ST_CREDENTIALS):\n",
    "        with open(ST_CREDENTIALS, 'w') as file:\n",
    "            file.write(\"[general]\\n\")\n",
    "            file.write('\\nemail = \"\"')\n",
    "            \n",
    "            \n",
    "    import sys\n",
    "    from streamlit import cli as stcli\n",
    "\n",
    "    theme = []\n",
    "    \n",
    "    theme.append(\"--theme.backgroundColor=#FFFFFF\")\n",
    "    theme.append(\"--theme.secondaryBackgroundColor=#f0f2f6\")\n",
    "    theme.append(\"--theme.textColor=#262730\")\n",
    "    theme.append(\"--theme.font='sans serif'\")\n",
    "    theme.append(\"--theme.primaryColor=#18212b\")\n",
    "\n",
    "    args = [\"streamlit\", \"run\", file_path, \"--global.developmentMode=false\", \"--server.port=8501\", \"--browser.gatherUsageStats=False\"]\n",
    "    \n",
    "    args.extend(theme)\n",
    "    \n",
    "    sys.argv = args\n",
    "    \n",
    "    sys.exit(stcli.main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted 12_speed.ipynb.\n",
      "Converted 13_export.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted XX_file_formats.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
