{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface\n",
    "\n",
    "This notebook describes/implements the interface to use alphapept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "The implemented functions are as follows:\n",
    "\n",
    "* Create database\n",
    "* Import raw data\n",
    "* Perform feature finding\n",
    "* Search data with fasta\n",
    "* Recalibrate\n",
    "* Score data with fasta\n",
    "* Perform LFQ\n",
    "* Export results\n",
    "* Run whole workflow\n",
    "\n",
    "The last command allows to run the whole pipeline at once.\n",
    "\n",
    "Helper functions include:\n",
    "\n",
    "* Callback function to track progress\n",
    "* Logging function\n",
    "* Version/hardware/settings checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import alphapept.utils\n",
    "from alphapept.utils import set_logger\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import psutil\n",
    "\n",
    "def parallel_execute(settings, step, callback=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generic function to parallel execute worklow steps on a file basis\n",
    "    \n",
    "    \"\"\"\n",
    "    n_processes = settings['general']['n_processes']\n",
    "    files = settings['experiment']['file_paths']\n",
    "    n_files = len(files)\n",
    "    logging.info(f'Processing {len(files)} files for step {step.__name__}')\n",
    "\n",
    "    if 'failed' not in settings:\n",
    "        settings['failed'] = {}\n",
    "\n",
    "    failed = []\n",
    "\n",
    "    to_process = [(i, settings) for i in range(n_files)]\n",
    "    \n",
    "    if n_files == 1:\n",
    "        if not step(to_process[0], callback=callback, parallel=True):\n",
    "            failed.append(files[0])\n",
    "\n",
    "    else:\n",
    "        #Limit number of processes for Bruker FF\n",
    "        if step.__name__ == 'find_features':\n",
    "            base, ext = os.path.splitext(files[0])\n",
    "            if ext.lower() == '.d':\n",
    "                memory_available = psutil.virtual_memory().available/1024**3\n",
    "                n_processes = np.max([int(np.floor(memory_available/25)),1])\n",
    "                logging.info(f'Using Bruker Feature Finder. Setting Process limit to {n_processes}.')\n",
    "            elif ext.lower() == '.raw':\n",
    "                memory_available = psutil.virtual_memory().available/1024**3\n",
    "                n_processes = np.max([int(np.floor(memory_available/8)),1]) #8 Gb per File\n",
    "                logging.info(f'Setting Process limit to {n_processes}')\n",
    "            else:\n",
    "                raise NotImplementedError('File extension {} not understood.'.format(ext))\n",
    "                \n",
    "        if step.__name__ == 'search_db':\n",
    "            memory_available = psutil.virtual_memory().available/1024**3\n",
    "            n_processes = np.max([int(np.floor(memory_available/6)),1]) # 8 gb per file: Todo: make this better\n",
    "            logging.info(f'Searching. Setting Process limit to {n_processes}.')\n",
    "\n",
    "        with Pool(n_processes) as p:\n",
    "            for i, success in enumerate(p.imap(step, to_process)):\n",
    "                if success is not True:\n",
    "                    failed.append(files[i])\n",
    "                    logging.error(f'Processing of {files[i]} for step {step.__name__} failed. Exception {success}')\n",
    "\n",
    "                if callback:\n",
    "                    callback((i+1)/n_files)\n",
    "\n",
    "    if step.__name__ in settings['failed']:\n",
    "        settings['failed'][step.__name__] = failed\n",
    "    else:\n",
    "        settings['failed'][step.__name__+'_2'] = failed\n",
    "\n",
    "    return settings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import tqdm\n",
    "\n",
    "def tqdm_wrapper(pbar, update):\n",
    "    current_value = pbar.n\n",
    "    delta = update - current_value\n",
    "    pbar.update(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def check_version_and_hardware(settings):\n",
    "    import alphapept.utils\n",
    "    #alphapept.utils.check_hardware()\n",
    "    #alphapept.utils.check_python_env()\n",
    "    alphapept.utils.show_platform_info()\n",
    "    alphapept.utils.show_python_info()\n",
    "    \n",
    "    \n",
    "    settings = alphapept.utils.check_settings(settings)\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import os\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "def create_database(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    import alphapept.fasta\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "    if 'database_path' not in settings['fasta']:\n",
    "        database_path = ''\n",
    "    else:\n",
    "        database_path = settings['fasta']['database_path']\n",
    "        \n",
    "    if not settings['fasta']['save_db']:\n",
    "        logging.info('Creating small database w/o modifications for first search.')\n",
    "        temp_settings = copy.deepcopy(settings)\n",
    "        temp_settings['fasta']['mods_fixed'] = []\n",
    "        temp_settings['fasta']['mods_fixed_terminal'] = []\n",
    "        temp_settings['fasta']['mods_fixed_terminal_prot'] = []\n",
    "        temp_settings['fasta']['mods_variable'] = []\n",
    "        temp_settings['fasta']['mods_variable_terminal'] = []\n",
    "        temp_settings['fasta']['mods_variable_terminal_prot'] = []\n",
    "    else:\n",
    "        temp_settings = settings\n",
    "                \n",
    "    if os.path.isfile(database_path):\n",
    "        logging.info(\n",
    "            'Database path set and exists. Using {} as database.'.format(\n",
    "                database_path\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        logging.info(\n",
    "            'Database path {} is not a file.'.format(database_path)\n",
    "        )\n",
    "\n",
    "        if len(settings['fasta']['fasta_paths']) == 0:\n",
    "            raise FileNotFoundError(\"No FASTA files set.\")\n",
    "\n",
    "        for fasta_file in settings['fasta']['fasta_paths']:\n",
    "            if os.path.isfile(fasta_file):\n",
    "                logging.info(\n",
    "                    'Found FASTA file {} with size {:.2f} Mb.'.format(\n",
    "                        fasta_file,\n",
    "                        os.stat(fasta_file).st_size/(1024**2)\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    'File {} not found'.format(fasta_file)\n",
    "                )\n",
    "\n",
    "        logging.info('Creating a new database from FASTA.')\n",
    "\n",
    "        if not callback:\n",
    "            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "        else:\n",
    "            cb = callback\n",
    "\n",
    "        (\n",
    "            spectra,\n",
    "            pept_dict,\n",
    "            fasta_dict\n",
    "        ) = alphapept.fasta.generate_database_parallel(\n",
    "            temp_settings,\n",
    "            callback=cb\n",
    "        )\n",
    "        logging.info(\n",
    "            'Digested {:,} proteins and generated {:,} spectra'.format(\n",
    "                len(fasta_dict),\n",
    "                len(spectra)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        alphapept.fasta.save_database(\n",
    "            spectra,\n",
    "            pept_dict,\n",
    "            fasta_dict,\n",
    "            **settings['fasta']\n",
    "        )\n",
    "        logging.info(\n",
    "            'Database saved to {}. Filesize of database is {:.2f} GB'.format(\n",
    "                database_path,\n",
    "                os.stat(database_path).st_size/(1024**3)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        settings['fasta']['database_path'] = database_path\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def import_raw_data(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "        \n",
    "    import alphapept.io        \n",
    "    \n",
    "    settings = parallel_execute(settings, alphapept.io.raw_conversion, callback = cb)\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def feature_finding(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.feature_finding\n",
    "    \n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "\n",
    "    settings = parallel_execute(settings, alphapept.feature_finding.find_features, callback = cb)\n",
    "    \n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def wrapped_partial(func, *args, **kwargs):\n",
    "    partial_func = functools.partial(func, *args, **kwargs)\n",
    "    functools.update_wrapper(partial_func, func)\n",
    "    return partial_func\n",
    "\n",
    "def search_data(\n",
    "    settings,\n",
    "    first_search=True,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.search\n",
    "    import alphapept.io\n",
    "    \n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "        \n",
    "    if first_search:\n",
    "\n",
    "        settings = parallel_execute(settings, wrapped_partial(alphapept.search.search_db, first_search = first_search), callback = cb)\n",
    "         \n",
    "        db_data = alphapept.fasta.read_database(settings['fasta']['database_path'])\n",
    "\n",
    "        fasta_dict = db_data['fasta_dict'].item()\n",
    "        pept_dict = db_data['pept_dict'].item()\n",
    "\n",
    "        logging.info('First search complete.')\n",
    "        \n",
    "    else:\n",
    "\n",
    "        if settings['fasta']['save_db']:\n",
    "            settings = parallel_execute(settings, wrapped_partial(alphapept.search.search_db, first_search = first_search), callback = cb)\n",
    "            \n",
    "            db_data = alphapept.fasta.read_database(settings['fasta']['database_path'])\n",
    "\n",
    "            fasta_dict = db_data['fasta_dict'].item()\n",
    "            pept_dict = db_data['pept_dict'].item()\n",
    "\n",
    "\n",
    "        else:\n",
    "            logging.info('Starting second search with DB.')\n",
    "            ms_files = []\n",
    "            for _ in settings['experiment']['file_paths']:\n",
    "                base, ext = os.path.splitext(_)\n",
    "                ms_files.append(base + '.ms_data.hdf')\n",
    "\n",
    "            offsets = [\n",
    "                alphapept.io.MS_Data_File(\n",
    "                    ms_file_name\n",
    "                ).read(\n",
    "                    dataset_name=\"corrected_mass\",\n",
    "                    group_name=\"features\",\n",
    "                    attr_name=\"estimated_max_precursor_ppm\"\n",
    "                ) * settings['search']['calibration_std'] for ms_file_name in ms_files\n",
    "            ]\n",
    "            logging.info('Starting second search.')\n",
    "\n",
    "            fasta_dict = alphapept.search.search_parallel(\n",
    "                settings,\n",
    "                calibration=offsets,\n",
    "                callback=cb\n",
    "            )\n",
    "            pept_dict = None\n",
    "\n",
    "        logging.info('Second search complete.')\n",
    "    return settings, pept_dict, fasta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def recalibrate_data(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.recalibration\n",
    "    \n",
    "    if settings['search']['calibrate']:\n",
    "        if not callback:\n",
    "            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "        else:\n",
    "            cb = callback\n",
    "\n",
    "        settings = parallel_execute(settings, alphapept.recalibration.calibrate_hdf, callback = cb)\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def score(\n",
    "    settings,\n",
    "    pept_dict=None,\n",
    "    fasta_dict=None,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):    \n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.score\n",
    "    import alphapept.fasta\n",
    "    \n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "\n",
    "    if (fasta_dict is None) or (pept_dict is None):\n",
    "        db_data = alphapept.fasta.read_database(\n",
    "            settings['fasta']['database_path']\n",
    "        )\n",
    "        fasta_dict = db_data['fasta_dict'].item()\n",
    "        pept_dict = db_data['pept_dict'].item()\n",
    "        \n",
    "        \n",
    "    settings = parallel_execute(settings, alphapept.score.score_hdf, callback = cb)\n",
    "        \n",
    "    if not settings['fasta']['save_db']:\n",
    "        pept_dict = alphapept.fasta.pept_dict_from_search(settings)\n",
    "\n",
    "    # Protein groups\n",
    "    logging.info('Extracting protein groups.')\n",
    "\n",
    "    if not callback:\n",
    "        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "    else:\n",
    "        cb = callback\n",
    "\n",
    "    # This is on each file individually -> when having repeats maybe\n",
    "    # use differently (if this matter at all )\n",
    "    alphapept.score.protein_groups_hdf_parallel(\n",
    "        settings,\n",
    "        pept_dict,\n",
    "        fasta_dict,\n",
    "        callback=cb\n",
    "    )\n",
    "    logging.info('Protein groups complete.')\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def align(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    \n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.matching\n",
    "    \n",
    "    alphapept.matching.align_datasets(settings, callback = callback)\n",
    "    \n",
    "    return settings\n",
    "\n",
    "def match(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.matching\n",
    "        \n",
    "    if settings['matching']['match_between_runs']:\n",
    "        alphapept.matching.match_datasets(settings)\n",
    "    \n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def lfq_quantification(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    import alphapept.quantification\n",
    "    \n",
    "    field = settings['quantification']['mode']\n",
    "\n",
    "    logging.info('Assembling dataframe.')\n",
    "    df = alphapept.utils.assemble_df(settings)\n",
    "    logging.info('Assembly complete.')\n",
    "    \n",
    "    if field in df.keys():  # Check if the quantification information exists.\n",
    "        # We could include another protein fdr in here..\n",
    "        if 'fraction' in df.keys():\n",
    "            logging.info('Delayed Normalization.')\n",
    "            df, normalization = alphapept.quantification.delayed_normalization(\n",
    "                df,\n",
    "                field\n",
    "            )\n",
    "            pd.DataFrame(normalization).to_hdf(\n",
    "                settings['experiment']['results_path'],\n",
    "                'fraction_normalization'\n",
    "            )\n",
    "            df_grouped = df.groupby(\n",
    "                ['shortname', 'precursor', 'protein', 'filename']\n",
    "            )[['{}_dn'.format(field)]].sum().reset_index()\n",
    "        else:\n",
    "            df_grouped = df.groupby(\n",
    "                ['shortname', 'precursor', 'protein', 'filename']\n",
    "            )[field].sum().reset_index()\n",
    "\n",
    "        df.to_hdf(\n",
    "            settings['experiment']['results_path'],\n",
    "            'combined_protein_fdr_dn'\n",
    "        )\n",
    "\n",
    "        logging.info('Complete. ')\n",
    "        logging.info('Starting profile extraction.')\n",
    "\n",
    "        if not callback:\n",
    "            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))\n",
    "        else:\n",
    "            cb = callback\n",
    "\n",
    "        protein_table = alphapept.quantification.protein_profile_parallel(\n",
    "            settings,\n",
    "            df_grouped,\n",
    "            callback=cb\n",
    "        )\n",
    "        protein_table.to_hdf(\n",
    "            settings['experiment']['results_path'],\n",
    "            'protein_table'\n",
    "        )\n",
    "        results_path = settings['experiment']['results_path']\n",
    "        base, ext = os.path.splitext(results_path)\n",
    "        protein_table.to_csv(base+'_proteins.csv')\n",
    "        \n",
    "        logging.info('LFQ complete.')\n",
    "        \n",
    "    logging.info('Exporting as csv.')\n",
    "    results_path = settings['experiment']['results_path']\n",
    "    base, ext = os.path.splitext(results_path)\n",
    "    df.to_csv(base+'.csv') \n",
    "        \n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import yaml\n",
    "\n",
    "\n",
    "def export(\n",
    "    settings,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None\n",
    "):\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "    base, ext = os.path.splitext(settings['experiment']['results_path'])\n",
    "    out_path_settings = base+'.yaml'\n",
    "\n",
    "    with open(out_path_settings, 'w') as file:\n",
    "        yaml.dump(settings, file)\n",
    "\n",
    "    logging.info('Settings saved to {}'.format(out_path_settings))\n",
    "    logging.info('Analysis complete.')\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback\n",
    "\n",
    "Return an overall progress\n",
    "And a task based progress.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from time import time, sleep\n",
    "from alphapept.__version__ import VERSION_NO\n",
    "\n",
    "def get_summary(settings, summary):\n",
    "\n",
    "    summary['file_sizes'] = {}\n",
    "\n",
    "    file_sizes = {}\n",
    "    for _ in settings['experiment']['file_paths']:\n",
    "\n",
    "        base, ext = os.path.splitext(_)\n",
    "        filename = os.path.split(base)[1]\n",
    "        file_sizes[base+\"_ms_data\"] = os.path.getsize(os.path.splitext(_)[0] + \".ms_data.hdf\")/1024**2\n",
    "        # file_sizes[base+\"_result\"] = os.path.getsize(os.path.splitext(_)[0] + \".hdf\")/1024**2\n",
    "\n",
    "        ms_data = alphapept.io.MS_Data_File(os.path.splitext(_)[0] + \".ms_data.hdf\")\n",
    "        n_ms2 = ms_data.read(group_name='Raw/MS2_scans', dataset_name='prec_mass_list2', return_dataset_shape=True)[0]\n",
    "        for key in ms_data.read():\n",
    "            if \"is_pd_dataframe\" in ms_data.read(\n",
    "                attr_name=\"\",\n",
    "                group_name=key\n",
    "            ):\n",
    "                f_summary = {}\n",
    "                for key in ms_data.read():\n",
    "                    if \"is_pd_dataframe\" in ms_data.read(\n",
    "                        attr_name=\"\",\n",
    "                        group_name=key\n",
    "                    ):\n",
    "                        df = ms_data.read(dataset_name=key)\n",
    "\n",
    "                        f_name = filename+'_'+key.lstrip('/')\n",
    "\n",
    "                        f_summary[key] = len(df)\n",
    "                        if key in 'protein_fdr':\n",
    "                            if 'type' in df.columns:\n",
    "                                f_summary['id_rate'] = df[df['type'] == 'msms']['raw_idx'].nunique() / n_ms2\n",
    "                            else:\n",
    "                                f_summary['id_rate'] = df['raw_idx'].nunique() / n_ms2\n",
    "                                \n",
    "                            for field in ['protein','protein_group','precursor','naked_sequence','sequence']:\n",
    "                                f_summary['n_'+field] = df[field].nunique()\n",
    "\n",
    "                    summary[filename] = f_summary\n",
    "\n",
    "    summary['file_sizes']['files'] = file_sizes\n",
    "    if os.path.isfile(settings['experiment']['results_path']):\n",
    "        summary['file_sizes']['results'] = os.path.getsize(settings['experiment']['results_path'])/1024**2\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def run_complete_workflow(\n",
    "    settings,\n",
    "    progress = False,\n",
    "    logger_set=False,\n",
    "    settings_parsed=False,\n",
    "    callback=None,\n",
    "    callback_overall = None,\n",
    "    callback_task = None\n",
    "):\n",
    "\n",
    "    if not logger_set:\n",
    "        set_logger()\n",
    "    if not settings_parsed:\n",
    "        settings = check_version_and_hardware(settings)\n",
    "        \n",
    "    steps = []\n",
    "    \n",
    "    general = settings['general']\n",
    "    \n",
    "    if general[\"create_database\"]:\n",
    "        steps.append(create_database)\n",
    "    if general[\"import_raw_data\"]:\n",
    "        steps.append(import_raw_data)\n",
    "    if general[\"feature_finding\"]:\n",
    "        steps.append(feature_finding)\n",
    "    if general[\"search_data\"]:\n",
    "        steps.append(search_data)\n",
    "    if general[\"recalibrate_data\"]:\n",
    "        steps.append(recalibrate_data)\n",
    "        steps.append(search_data)\n",
    "    steps.append(score)\n",
    "    if general[\"align\"]:\n",
    "        steps.append(align)\n",
    "    if general[\"match\"]:\n",
    "        if align not in steps:\n",
    "            steps.append(align)\n",
    "        steps.append(match)\n",
    "    if general[\"lfq_quantification\"]:\n",
    "        steps.append(lfq_quantification)\n",
    "    \n",
    "    steps.append(export)\n",
    "             \n",
    "    n_steps = len(steps)\n",
    "    logging.info(f\"Workflow has {n_steps} steps\")\n",
    "    \n",
    "\n",
    "    if progress:\n",
    "        logging.info('Setting callback to logger.')\n",
    "        #log progress to be used \n",
    "        def cb_logger_o(x):\n",
    "            logging.info(f\"__progress_overall {x:.3f}\")\n",
    "        def cb_logger_c(x):\n",
    "            logging.info(f\"__progress_current {x:.3f}\")\n",
    "        def cb_logger_t(x):\n",
    "            logging.info(f\"__current_task {x}\")\n",
    "            \n",
    "        callback = cb_logger_c\n",
    "        callback_overall = cb_logger_o\n",
    "        callback_task = cb_logger_t\n",
    "        \n",
    "    \n",
    "    def progress_wrapper(step, n_steps, current):\n",
    "        if callback:\n",
    "            callback(current)\n",
    "        if callback_overall:\n",
    "            callback_overall((step/n_steps)+(current/n_steps))\n",
    "            \n",
    "    pept_dict = None\n",
    "    fasta_dict = None\n",
    "\n",
    "    first_search = True\n",
    "    \n",
    "    time_dict = {}\n",
    "    \n",
    "    run_start = time()\n",
    "    \n",
    "    summary = {}\n",
    "    \n",
    "    for idx, step in enumerate(steps):\n",
    "        if callback_task:\n",
    "            callback_task(step.__name__)\n",
    "        \n",
    "        start = time()\n",
    "        \n",
    "        if callback_overall:\n",
    "            progress_wrapper(idx, n_steps, 0)\n",
    "            cb = functools.partial(progress_wrapper, idx, n_steps)\n",
    "        elif callback:\n",
    "            cb = callback\n",
    "        else:\n",
    "            cb = None\n",
    "        \n",
    "        if step is search_data:\n",
    "            settings, pept_dict, fasta_dict = step(settings, first_search=first_search, logger_set = True, settings_parsed = True, callback = cb)\n",
    "        \n",
    "        elif step is score:\n",
    "            \n",
    "            if fasta_dict is None or pept_dict is None:\n",
    "                \n",
    "                db_data = alphapept.fasta.read_database(settings['fasta']['database_path'])\n",
    "\n",
    "                fasta_dict = db_data['fasta_dict'].item()\n",
    "                pept_dict = db_data['pept_dict'].item()\n",
    "            \n",
    "            settings = step(settings, pept_dict=pept_dict, fasta_dict=fasta_dict, logger_set = True,  settings_parsed = True, callback = cb)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if step is export:\n",
    "                # Get summary information \n",
    "                summary = get_summary(settings, summary)\n",
    "                settings['summary'] = summary\n",
    "    \n",
    "            settings = step(settings, logger_set = True,  settings_parsed = True, callback = cb)\n",
    "            \n",
    "        if step is recalibrate_data:\n",
    "            first_search = False\n",
    "        \n",
    "        if callback_overall:\n",
    "            progress_wrapper(idx, n_steps, 1)\n",
    "        \n",
    "        end = time()\n",
    "        \n",
    "        if step.__name__ in time_dict:\n",
    "            time_dict[step.__name__+'_2'] = (end-start)/60 #minutes\n",
    "        else:\n",
    "            time_dict[step.__name__] = (end-start)/60 #minutes\n",
    "        \n",
    "        time_dict['total'] = (end-run_start)/60\n",
    "\n",
    "        summary['timing'] = time_dict\n",
    "        summary['version'] = VERSION_NO\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watcher\n",
    "Function to watch folders for new files and automatically process them. This can be useful as time-consuming file conversion and feature finding steps can be started immediately after acquisition. There is a hook to upload the settings of processed files to a mongo db. The file `file_watcher_config_sample.yaml` shows the required arguments for running the FileWatcher. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FileWatcher():\n",
    "    \"\"\"\n",
    "    Class to watch files and process\n",
    "    \"\"\"\n",
    "    def __init__(self, config_path):\n",
    "        db_set = True\n",
    "        try:\n",
    "            from pymongo import MongoClient\n",
    "        except:\n",
    "            print('DB upload requires pymongo - DB upload deactivated')\n",
    "            db_set = False\n",
    "            \n",
    "        try:\n",
    "            import dns\n",
    "        except:\n",
    "            print('DB upload requires dnspython - DB upload deactivated')\n",
    "            db_set = False\n",
    "        \n",
    "        if os.path.isfile(config_path):\n",
    "            watcher_config = alphapept.settings.load_settings(config_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(config_path)\n",
    "        \n",
    "        if os.path.isdir(watcher_config['path']):\n",
    "            self.path = watcher_config['path']\n",
    "        else:\n",
    "            raise FileNotFoundError(path)\n",
    "            \n",
    "            \n",
    "        db_config = {}\n",
    "        for db_field in ['db_user', 'db_password', 'db_url', 'db_database', 'db_collection']:\n",
    "            if watcher_config[db_field] == '':\n",
    "                print(f\"{db_field} not set.\")\n",
    "                db_set = False\n",
    "            else:\n",
    "                db_config[db_field] = watcher_config[db_field]\n",
    "                \n",
    "        if db_set:\n",
    "            self.set_db(**db_config)\n",
    "        else:\n",
    "            self.db_set = False\n",
    "        \n",
    "        if os.path.isfile(watcher_config['settings']):\n",
    "            self.settings = alphapept.settings.load_settings(watcher_config['settings'])\n",
    "            print(f\"Loaded settings from {watcher_config['settings']}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(watcher_config['settings'])\n",
    "            \n",
    "        self.update_rate = watcher_config['update_rate']\n",
    "        self.n_processed = 0\n",
    "        self.n_failed = 0\n",
    "        self.minimum_file_size = watcher_config['minimum_file_size']\n",
    "        self.tag = watcher_config['tag']\n",
    "    \n",
    "    def check_new_files(self):\n",
    "        \"\"\"\n",
    "        Check for new files in folder\n",
    "        \"\"\"\n",
    "        new_files = []\n",
    "\n",
    "        for dirpath, dirnames, filenames in os.walk(self.path):\n",
    "\n",
    "            for dirname in [d for d in dirnames if d.endswith(('.d','.d/'))]: #Bruker\n",
    "                new_file = os.path.join(dirpath, dirname)\n",
    "                base, ext = os.path.splitext(dirname)\n",
    "                hdf_path = os.path.join(dirpath, base+'.ms_data.hdf')\n",
    "\n",
    "                if not os.path.exists(hdf_path):\n",
    "                    new_files.append(new_file)\n",
    "\n",
    "            for filename in [f for f in filenames if f.lower().endswith(('.raw','.raw/'))]: #Thermo\n",
    "                new_file = os.path.join(dirpath, filename)\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                hdf_path = os.path.join(dirpath, base+'.ms_data.hdf')\n",
    "\n",
    "                if not os.path.exists(hdf_path):\n",
    "                    new_files.append(new_file)\n",
    "        return new_files\n",
    "    \n",
    "    def set_db(self, db_user = '', db_password = '', db_url = '', db_database= '', db_collection= ''):\n",
    "        \n",
    "        self.db_user = db_user\n",
    "        self.db_password = db_password\n",
    "        self.db_url = db_url\n",
    "        self.db_database = db_database\n",
    "        self.db_collection = db_collection\n",
    "        \n",
    "        self.db_set = True\n",
    "\n",
    "\n",
    "    def check_file_completion(self, list_of_files, sleep_time = 10):\n",
    "        to_analyze = []\n",
    "        file_dict = {}\n",
    "\n",
    "        for file in list_of_files:\n",
    "            if file.endswith('.d'):\n",
    "                to_check = os.path.join(file, 'analysis.tdf_bin')\n",
    "            else:\n",
    "                to_check = file\n",
    "\n",
    "            file_dict[file] = os.path.getsize(to_check)\n",
    "\n",
    "        sleep(sleep_time)\n",
    "\n",
    "        for file in list_of_files:\n",
    "            if file.endswith('.d'):\n",
    "                to_check = os.path.join(file, 'analysis.tdf_bin')\n",
    "            else:\n",
    "                to_check = file\n",
    "\n",
    "            filesize = os.path.getsize(to_check)\n",
    "            if (filesize == file_dict[file]) & (filesize/1024/1024 > self.minimum_file_size):\n",
    "                to_analyze.append(file)\n",
    "\n",
    "        return to_analyze\n",
    "    \n",
    "    def run(self):\n",
    "        print(f'Starting FileWatcher on {self.path}')\n",
    "        \n",
    "        while True:\n",
    "            unprocessed = self.check_file_completion(self.check_new_files()) \n",
    "            \n",
    "            if self.tag != '':\n",
    "                unprocessed = [_ for _ in unprocessed if self.tag.lower() in _.lower()]\n",
    "                \n",
    "            if len(unprocessed) > 0:\n",
    "                try:\n",
    "                    settings = self.settings.copy()\n",
    "                    settings['experiment']['file_paths'] =  [unprocessed[0]]\n",
    "                    settings_ = alphapept.interface.run_complete_workflow(settings)\n",
    "                    \n",
    "                    if self.db_set:\n",
    "                        self.upload_to_db(settings_)\n",
    "                    self.n_processed +=1\n",
    "                    print(f'--- File Watcher Status: Files processed {self.n_processed:,} - failed {self.n_failed:,} ---')\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except Exception as e:\n",
    "                    logging.info(e)\n",
    "                    self.n_failed +=1\n",
    "            else:\n",
    "                sleep(self.update_rate)\n",
    "                \n",
    "\n",
    "    def upload_to_db(self, settings):\n",
    "        from pymongo import MongoClient\n",
    "        logging.info('Uploading to DB')\n",
    "        string = f\"mongodb+srv://{self.db_user}:{self.db_password}@{self.db_url}\"\n",
    "        client = MongoClient(string)\n",
    "\n",
    "        post_id = client[self.db_database][self.db_collection].insert_one(settings).inserted_id\n",
    "\n",
    "        logging.info(f\"Uploaded {post_id}.\")\n",
    "\n",
    "        return post_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI\n",
    "\n",
    "All workflow functions can be called with the command line interface (CLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import click\n",
    "import os\n",
    "import alphapept.settings\n",
    "from alphapept.__version__ import VERSION_NO\n",
    "from alphapept.__version__ import COPYRIGHT\n",
    "from alphapept.__version__ import URL\n",
    "\n",
    "CONTEXT_SETTINGS = dict(help_option_names=['-h', '--help'])\n",
    "CLICK_SETTINGS_OPTION = click.argument(\n",
    "    \"settings_file\",\n",
    "#     help=\"A .yaml file with settings.\",\n",
    "    type=click.Path(exists=True, dir_okay=False),\n",
    "#     default=f\"{os.path.dirname(__file__)}/settings_template.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "def run_cli():\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                \"\\n\",\n",
    "                r\"     ___    __      __          ____             __ \",\n",
    "                r\"    /   |  / /___  / /_  ____  / __ \\___  ____  / /_\",\n",
    "                r\"   / /| | / / __ \\/ __ \\/ __ \\/ /_/ / _ \\/ __ \\/ __/\",\n",
    "                r\"  / ___ |/ / /_/ / / / / /_/ / ____/ ___/ /_/ / /_  \",\n",
    "                r\" /_/  |_/_/ .___/_/ /_/\\__,_/_/    \\___/ .___/\\__/  \",\n",
    "                r\"         /_/                          /_/           \",\n",
    "                '.'*52,\n",
    "                '.{}.'.format(URL.center(50)),\n",
    "                '.{}.'.format(COPYRIGHT.center(50)),\n",
    "                '.{}.'.format(VERSION_NO.center(50)),\n",
    "                '.'*52,\n",
    "                \"\\n\"\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    cli_overview.add_command(cli_database)\n",
    "    cli_overview.add_command(cli_import)\n",
    "    cli_overview.add_command(cli_feature_finding)\n",
    "    cli_overview.add_command(cli_search)\n",
    "    cli_overview.add_command(cli_recalibrate)\n",
    "    cli_overview.add_command(cli_score)\n",
    "    cli_overview.add_command(cli_quantify)\n",
    "    cli_overview.add_command(cli_export)\n",
    "    cli_overview.add_command(cli_workflow)\n",
    "    cli_overview.add_command(cli_gui)\n",
    "    cli_overview.add_command(cli_watcher)\n",
    "    cli_overview()\n",
    "\n",
    "\n",
    "@click.group(\n",
    "    context_settings=CONTEXT_SETTINGS,\n",
    "#     help=\"AlphaPept\"\n",
    ")\n",
    "def cli_overview():\n",
    "    pass\n",
    "\n",
    "\n",
    "@click.command(\n",
    "    \"database\",\n",
    "    help=\"Create a database from a fasta file.\",\n",
    "    short_help=\"Create a database from a fasta file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_database(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    create_database(settings)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"import\",\n",
    "    help=\"Import and convert raw data from vendor to `.ms_data.hdf` file.\",\n",
    "    short_help=\"Import and convert raw data from vendor to `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_import(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    import_raw_data(settings)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"features\",\n",
    "    help=\"Find features in a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Find features in a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_feature_finding(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    feature_finding(settings)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"search\",\n",
    "    help=\"Search and identify feature in a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Search and identify feature in a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "@click.option(\n",
    "    '--recalibrated_features',\n",
    "    '-r',\n",
    "    'recalibrated',\n",
    "    help=\"Use recalibrated features if present\",\n",
    "    is_flag=True,\n",
    "    default=False,\n",
    "    show_default=True,\n",
    ")\n",
    "def cli_search(settings_file, recalibrated):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    search_data(settings, recalibrated)\n",
    "    \n",
    "    \n",
    "@click.command(\n",
    "    \"recalibrate\",\n",
    "    help=\"Recalibrate a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Recalibrate a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_recalibrate(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    recalibrate_data(settings)\n",
    "    \n",
    "\n",
    "@click.command(\n",
    "    \"score\",\n",
    "    help=\"Score PSM from a `.ms_data.hdf` file.\",\n",
    "    short_help=\"Score PSM from a `.ms_data.hdf` file.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_score(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    score(settings)\n",
    "    \n",
    "@click.command(\n",
    "    \"align\",\n",
    "    help=\"Align multiple `.ms_data.hdf` files.\",\n",
    "    short_help=\"Align multiple `.ms_data.hdf` files.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_align(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    align(settings)\n",
    "    \n",
    "@click.command(\n",
    "    \"match\",\n",
    "    help=\"Perform match between run type analysis on multiple `.ms_data.hdf` files.\",\n",
    "    short_help=\"Perform match between run type analysis on multiple `.ms_data.hdf` files.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_match(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    align(settings)\n",
    "    match(settings)\n",
    "    \n",
    "@click.command(\n",
    "    \"quantify\",\n",
    "    help=\"Quantify and compare multiple `.ms_data.hdf` files.\",\n",
    "    short_help=\"Quantify and compare multiple `.ms_data.hdf` files.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_quantify(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    lfq_quantification(settings)\n",
    "    \n",
    "\n",
    "@click.command(\n",
    "    \"export\",\n",
    "    help=\"Export protein table from `.ms_data.hdf` files as `.csv`\",\n",
    "    short_help=\"Export protein table from `.ms_data.hdf` files as `.csv`.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_export(settings_file):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    export(settings)\n",
    "    \n",
    "\n",
    "@click.command(\n",
    "    \"workflow\",\n",
    "    help=\"Run the complete AlphaPept workflow.\",\n",
    "    short_help=\"Run the complete AlphaPept workflow.\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--progress\",\n",
    "    \"-p\",\n",
    "    help=\"Log progress output.\",\n",
    "    is_flag=True,\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_workflow(settings_file, progress):\n",
    "    settings = alphapept.settings.load_settings(settings_file)\n",
    "    run_complete_workflow(settings, progress = progress)\n",
    "\n",
    "    \n",
    "@click.command(\n",
    "    \"gui\",\n",
    "    help=\"Start graphical user interface for AlphaPept.\",\n",
    ")\n",
    "@click.option(\n",
    "    \"--test\",\n",
    "    \"test\",\n",
    "    help=\"Test\",\n",
    "    is_flag=True,\n",
    "    default=False,\n",
    "    show_default=True,\n",
    ")\n",
    "def cli_gui(test):\n",
    "    print('Launching GUI')\n",
    "    import alphapept.ui\n",
    "    if test:\n",
    "        alphapept.ui.main(close=True)\n",
    "    else:\n",
    "        alphapept.ui.main()\n",
    "      \n",
    "@click.command(\n",
    "    \"watcher\",\n",
    "    help=\"Watch folder for new files and automatically process them. Upload to MongoDB possible.\",\n",
    "    short_help=\"File watching and procesing.\"\n",
    ")\n",
    "@CLICK_SETTINGS_OPTION\n",
    "def cli_watcher(settings_file):\n",
    "    x = FileWatcher(settings_file)\n",
    "    x.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
