{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner\n",
    "\n",
    "> A function to run alphapept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo:\n",
    "* Multiple file handling\n",
    "* Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "\n",
    "def check_enviroment():\n",
    "    import numba \n",
    "    if float('.'.join(numba.__version__.split('.')[0:2])) < 0.46:\n",
    "        raise RuntimeError('Numba version {} not sufficient'.format(numba.__version__))\n",
    "\n",
    "def check_settings(settings):\n",
    "\n",
    "    \"\"\"\n",
    "    Consistency check for settings.\n",
    "\n",
    "    \"\"\"\n",
    "    check_enviroment()\n",
    "    \n",
    "    logging.info('Checking raw path.')\n",
    "\n",
    "    #Check if a valid raw file is provided. If a npz file is also provided do not convert\n",
    "\n",
    "    if os.path.isfile(settings[\"raw\"][\"raw_path_npz\"]):\n",
    "        if settings[\"general\"][\"convert_raw\"]:\n",
    "            settings[\"general\"][\"convert_raw\"] = False\n",
    "            logging.info('NPZ for raw file present. Skipping conversion step.')\n",
    "    else:\n",
    "        if os.path.isfile(settings[\"raw\"][\"raw_path\"]):\n",
    "            logging.info('No NPZ for raw present. Performing conversion step.')\n",
    "            settings[\"general\"][\"convert_raw\"] = True\n",
    "        else:\n",
    "            raise FileNotFoundError('No raw or converted raw file provided')\n",
    "            \n",
    "            \n",
    "    #check filetype -> todo what if we have an npz but no irignal file\n",
    "    \n",
    "    base, ext = os.path.splitext(settings[\"raw\"][\"raw_path\"])\n",
    "    \n",
    "    if ext == '.raw':\n",
    "        datatype='thermo'\n",
    "    elif ext == '.d':\n",
    "        datatype='bruker'\n",
    "    else:\n",
    "        raise NotImplementedError('File extension {} not understood.'.format(ext))\n",
    "\n",
    "    logging.info('Raw path okay.')\n",
    "\n",
    "    #Check library file\n",
    "    logging.info('Checking library path.')\n",
    "\n",
    "    if os.path.isfile(settings[\"fasta\"][\"library_path\"]):\n",
    "        if settings[\"general\"][\"create_library\"]:\n",
    "            settings[\"general\"][\"create_library\"] = False\n",
    "            logging.info('NPZ for library file present. Skipping library creation step.')\n",
    "    else:\n",
    "        if os.path.isfile(settings[\"fasta\"][\"fasta_path\"]):\n",
    "            logging.info('No NPZ for library present. Creating library from FASTA.')\n",
    "            settings[\"general\"][\"create_library\"] = True\n",
    "        else:\n",
    "            raise FileNotFoundError('No FASTA or library file provided')\n",
    "\n",
    "    logging.info('Library path okay.')\n",
    "    \n",
    "    \n",
    "    settings['general']['type'] = datatype\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import os\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "\n",
    "#alpha_runner(self.settings, self.update_global_progress, self.update_current_progress, self.update_task)\n",
    "\n",
    "def alpha_runner(settings, overall_progress = None, current_progress = None, CURRENT_TASK = print):\n",
    "    \"\"\"\n",
    "    A wrapper function to run alphapept\n",
    "    \"\"\"\n",
    "    \n",
    "    #assumes the dict is ordered \n",
    "\n",
    "    time_dict = {}\n",
    "\n",
    "    timings = [10,15,20,20,20,15]\n",
    "    total_time = 0\n",
    "    for idx, element in enumerate(['library_generation', 'file_conversion', 'feature_finding', 'search', 'combining_files', 'reporting']):\n",
    "        time_dict[element] = {}\n",
    "        time_dict[element]['step'] = timings[idx]\n",
    "\n",
    "        total_time += timings[idx]\n",
    "        time_dict[element]['total'] = total_time\n",
    "\n",
    "\n",
    "    def check_time_dict(time_dict):\n",
    "        \"\"\"\n",
    "        Function to check the validity of a time dict\n",
    "        \"\"\"\n",
    "\n",
    "        total_time = 0\n",
    "\n",
    "        for key in time_dict.keys():\n",
    "            total_time += time_dict[key]['step']\n",
    "\n",
    "        assert total_time == 100\n",
    "\n",
    "    check_time_dict(time_dict)\n",
    "    \n",
    "    progress = 0\n",
    "\n",
    "    def progress_wrapper(current, delta=1):\n",
    "        \"\"\"\n",
    "        Wrapper function to change the overall progress with the current progress\n",
    "        \"\"\"\n",
    "\n",
    "        global progress\n",
    "        \n",
    "        if current_progress:\n",
    "            current_progress(current)\n",
    "        \n",
    "        overall = progress + current*delta\n",
    "        \n",
    "        if overall_progress:\n",
    "            overall_progress(overall)\n",
    "\n",
    "        if current == 1.0:\n",
    "            progress += delta\n",
    "\n",
    "    def set_progress(overall):\n",
    "        global progress\n",
    "\n",
    "        overall_progress(overall)\n",
    "\n",
    "        progress = overall\n",
    "\n",
    "\n",
    "    from alphapept.constants import mass_dict\n",
    "\n",
    "    CURRENT_TASK('Checking Settings')\n",
    "    settings = check_settings(settings)\n",
    "\n",
    "\n",
    "    ### Library Generation ###\n",
    "\n",
    "    if settings[\"general\"][\"create_library\"]:\n",
    "        from alphapept.fasta import generate_library, generate_spectra, save_library\n",
    "\n",
    "        te = time_dict['library_generation']['step']\n",
    "        n_steps = 2\n",
    "\n",
    "        CURRENT_TASK('Digesting FASTA')\n",
    "        to_add, pept_dict, fasta_dict = generate_library(mass_dict, callback = partial(progress_wrapper, delta=1/n_steps*te), **settings['fasta'])\n",
    "        logging.info('Digested {:,} proteins and generated {:,} peptides'.format(len(fasta_dict), len(to_add)))\n",
    "\n",
    "        CURRENT_TASK('Generating Spectra')\n",
    "        spectra = generate_spectra(to_add, mass_dict, callback = partial(progress_wrapper, delta=1/n_steps*te))\n",
    "        logging.info('Generated {:,} spectra'.format(len(spectra)))\n",
    "\n",
    "        CURRENT_TASK('Saving library')\n",
    "        base, ext = os.path.splitext(settings['fasta']['fasta_path'])\n",
    "        settings['fasta']['library_path'] = base + '.npz'\n",
    "        library_path = save_library(spectra, pept_dict, fasta_dict, **settings['fasta'])             \n",
    "        logging.info('Database saved to {}. Filesize {:.2f} Gb'.format(library_path, os.stat(library_path).st_size/(1024**3)))\n",
    "\n",
    "    set_progress(time_dict['library_generation']['total'])\n",
    "    ### File Conversion ###\n",
    "\n",
    "    if settings[\"general\"][\"convert_raw\"]:\n",
    "\n",
    "        te = time_dict['file_conversion']['step']\n",
    "\n",
    "        from alphapept.io import raw_to_npz\n",
    "\n",
    "        CURRENT_TASK('Converting raw files')\n",
    "        out_path = raw_to_npz(settings[\"raw\"], callback=partial(progress_wrapper, delta=te))\n",
    "        settings[\"raw\"][\"query_path\"] = out_path[0]\n",
    "        logging.info('Raw file(s) saved to {}'.format(out_path))\n",
    "    else:\n",
    "        settings[\"raw\"][\"query_path\"] = settings[\"raw\"][\"raw_path_npz\"]\n",
    "\n",
    "    db_data = np.load(settings[\"fasta\"][\"library_path\"], allow_pickle=True)\n",
    "    query_data = np.load(settings[\"raw\"][\"query_path\"], allow_pickle=True)\n",
    "\n",
    "\n",
    "    set_progress(time_dict['file_conversion']['total'])\n",
    "\n",
    "    if settings[\"general\"][\"find_features\"]:\n",
    "\n",
    "        te = time_dict['feature_finding']['step']\n",
    "\n",
    "        logging.info('Finding Features')\n",
    "\n",
    "        if settings[\"general\"]['type'] == 'thermo':\n",
    "\n",
    "            n_steps = 8\n",
    "\n",
    "            logging.info('Fature Finding for Thermo')\n",
    "            #Feature Finding Part\n",
    "\n",
    "            from alphapept.feature_finding import raw_to_centroid, get_hills, split_hills, filter_hills, get_hill_data, get_edges, get_isotope_patterns, feature_finder_report\n",
    "            from alphapept.constants import averagine_aa, isotopes\n",
    "\n",
    "\n",
    "            CURRENT_TASK('Converting centroids')\n",
    "            centroids = raw_to_centroid(query_data, callback=partial(progress_wrapper, delta=te*1/n_steps))\n",
    "            logging.info('Loaded {:,} centroids.'.format(len(centroids)))\n",
    "\n",
    "            CURRENT_TASK('Exctracting hills')\n",
    "            completed_hills = get_hills(centroids, callback=partial(progress_wrapper, delta=te*1/n_steps))\n",
    "            logging.info('A total of {:,} hills extracted. Average hill length {:.2f}'.format(len(completed_hills), np.mean([len(_) for _ in completed_hills])))\n",
    "\n",
    "            CURRENT_TASK('Splitting hills')\n",
    "            splitted_hills = split_hills(completed_hills, centroids, smoothing=1, callback=partial(progress_wrapper, delta=te*1/n_steps))\n",
    "            logging.info('Split {:,} hills into {:,} hills'.format(len(completed_hills), len(splitted_hills)))\n",
    "\n",
    "            CURRENT_TASK('Refining hills')\n",
    "            filtered_hills = filter_hills(splitted_hills, centroids, callback=partial(progress_wrapper, delta=te*1/n_steps))\n",
    "            logging.info('Filtered {:,} hills. Remaining {:,} hills'.format(len(splitted_hills), len(filtered_hills)))\n",
    "\n",
    "            CURRENT_TASK('Calculating hill statistics')\n",
    "            sorted_hills, sorted_stats, sorted_data = get_hill_data(filtered_hills, centroids, callback=partial(progress_wrapper, delta=te*1/n_steps))\n",
    "            logging.info('Extracting hill stats complete')\n",
    "\n",
    "            CURRENT_TASK('Connecting pre isotope patterns')\n",
    "            pre_isotope_patterns = get_edges(sorted_stats, sorted_data, callback=partial(progress_wrapper, delta=te*1/n_steps))\n",
    "            logging.info('Found {} pre isotope patterns.'.format(len(pre_isotope_patterns)))\n",
    "\n",
    "            CURRENT_TASK('Deisotope patterns')\n",
    "            isotope_patterns, isotope_charges = get_isotope_patterns(pre_isotope_patterns, sorted_stats, sorted_data, averagine_aa, isotopes, callback=partial(progress_wrapper, delta=te*1/n_steps))\n",
    "            logging.info('Extracted {} isotope patterns.'.format(len(isotope_patterns)))\n",
    "\n",
    "            CURRENT_TASK('Calculating feature statistics')\n",
    "            feature_table = feature_finder_report(isotope_patterns, isotope_charges, sorted_stats, sorted_data, sorted_hills, query_data, callback=partial(progress_wrapper, delta=te*1/n_steps))\n",
    "            logging.info('Report complete.')\n",
    "\n",
    "        elif settings[\"general\"]['type'] == 'bruker':\n",
    "\n",
    "            logging.info('Fature Finding for Bruker')\n",
    "            from alphapept.feature_finding import extract_bruker, convert_bruker\n",
    "\n",
    "            feature_path = extract_bruker(settings['raw']['raw_path'])\n",
    "            feature_table = convert_bruker(feature_path)\n",
    "\n",
    "            logging.info('Extracted {:,} features.'.format(len(feature_table)))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Filetype {} not understood.'.format(settings[\"general\"][\"type\"]))\n",
    "\n",
    "        from alphapept.feature_finding import map_ms2\n",
    "        features = map_ms2(feature_table, query_data)\n",
    "\n",
    "    else:\n",
    "        logging.info('Using Features from raw data')\n",
    "        from alphapept.search import query_data_to_features\n",
    "        features = query_data_to_features(query_data)\n",
    "\n",
    "\n",
    "    set_progress(time_dict['feature_finding']['total'])\n",
    "\n",
    "    ### Search part ###\n",
    "\n",
    "    te = time_dict['search']['step']\n",
    "    n_steps = 2\n",
    "\n",
    "    from alphapept.search import get_psms, get_score_columns\n",
    "    from alphapept.score import score_x_tandem\n",
    "\n",
    "    CURRENT_TASK('Running first search.')\n",
    "    psms, num_specs_compared = get_psms(query_data, db_data, features, callback=partial(progress_wrapper, delta=te*1/n_steps), **settings[\"search\"])\n",
    "    logging.info('First search complete. Compared {:,} spectra and found {:,} psms.'.format(num_specs_compared, len(psms)))\n",
    "\n",
    "    CURRENT_TASK('Extracting columns for scoring.')\n",
    "    psms, num_specs_scored = get_score_columns(psms, query_data, db_data, features, **settings[\"search\"])\n",
    "    logging.info('Extracted columns for {:,} psms.'.format(num_specs_scored))\n",
    "\n",
    "\n",
    "    if settings['search']['calibrate']:\n",
    "\n",
    "        CURRENT_TASK('Scoring psms.')\n",
    "        df = score_x_tandem(pd.DataFrame(psms), plot=False, verbose=False, **settings[\"search\"])\n",
    "        logging.info('Scoring complete. For {} FDR found {:,} targets and {:,} decoys.'.format(settings[\"search\"][\"peptide_fdr\"], df['target'].sum(), df['decoy'].sum()) )\n",
    "\n",
    "\n",
    "        from alphapept.recalibration import get_calibration\n",
    "\n",
    "        CURRENT_TASK('Calibrating features.')\n",
    "        logging.info('Precursor Offset (PPM) is {:.2f} (mean), {:.2f} (std)'.format(df['o_mass_ppm'].mean(), df['o_mass_ppm'].std()))\n",
    "        'Calibrating MS1 spectra'\n",
    "        features_calib, df_sub = get_calibration(df, features, **settings[\"calibration\"])\n",
    "\n",
    "        o_mass_ppm_mean = df_sub['o_mass_ppm_calib'].mean()\n",
    "        o_mass_ppm_std = df_sub['o_mass_ppm_calib'].std()\n",
    "\n",
    "        logging.info('Calibration complete. Precursor Offset (PPM) is {:.2f} (mean), {:.2f} (std)'.format(o_mass_ppm_mean, o_mass_ppm_std))\n",
    "\n",
    "        logging.info('Adjusting search bound to {:.2f} ppm.'.format(3*o_mass_ppm_std))\n",
    "\n",
    "        settings[\"search\"][\"m_offset_calibrated\"] = float(3*o_mass_ppm_std)\n",
    "\n",
    "        CURRENT_TASK('Running second search.')\n",
    "        psms, num_specs_compared = get_psms(query_data, db_data, features_calib, callback=partial(progress_wrapper, delta=te*1/n_steps), **settings[\"search\"])\n",
    "        logging.info('Second search complete. Compared {:,} spectra and found {:,} psms.'.format(num_specs_compared, len(psms)))\n",
    "\n",
    "        CURRENT_TASK('Extracting columns for scoring.')\n",
    "        psms, num_specs_scored = get_score_columns(psms, query_data, db_data, features_calib, **settings[\"search\"])\n",
    "        logging.info('Extracted columns for {:,} psms.'.format(num_specs_scored))\n",
    "\n",
    "    ## Protein Groups and FDR control\n",
    "\n",
    "\n",
    "    set_progress(time_dict['search']['total'])\n",
    "\n",
    "    set_progress(time_dict['combining_files']['total'])\n",
    "\n",
    "\n",
    "    from alphapept.score import cut_global_fdr, perform_protein_grouping, cut_global_fdr, get_x_tandem_score, filter_score\n",
    "\n",
    "    CURRENT_TASK('Scoring')\n",
    "    df = pd.DataFrame(psms)\n",
    "    df['score'] = get_x_tandem_score(df)\n",
    "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "    df = filter_score(df)\n",
    "\n",
    "    CURRENT_TASK('FDR control on peptides')\n",
    "    df = cut_global_fdr(df, analyte_level='sequence',  plot=False, verbose=False)\n",
    "    logging.info('Scoring peptides complete. For {} FDR found {:,} targets and {:,} decoys.'.format(settings[\"search\"][\"peptide_fdr\"], df['target'].sum(), df['decoy'].sum()) )\n",
    "\n",
    "\n",
    "    CURRENT_TASK('Perform protein grouping')\n",
    "    df = perform_protein_grouping(df, db_data['pept_dict'].item(), db_data['fasta_dict'].item())\n",
    "    df = cut_global_fdr(df, analyte_level='protein',  plot=False, verbose=False)\n",
    "    logging.info('Scoring proteins complete. For {} FDR found {:,} targets and {:,} decoys. A total of {:,} proteins found.'.format(settings[\"search\"][\"protein_fdr\"], df['target'].sum(), df['decoy'].sum(), len(set(df['protein']))))\n",
    "\n",
    "    ## Quantification\n",
    "    if settings[\"quantification\"][\"max_lfq\"]:\n",
    "        raise NotImplementedError(\"MaxLFQ not implemented yet.\")\n",
    "        \n",
    "    ## Misc\n",
    "    if settings[\"misc\"][\"match_between_runs\"]:\n",
    "        raise NotImplementedError(\"Match beween runs not implemented yet.\")\n",
    "        \n",
    "    CURRENT_TASK('Saving.')\n",
    "\n",
    "    base, ext = os.path.splitext(settings['raw']['query_path'])\n",
    "    out_path = base+'_ap.csv'\n",
    "    df.to_csv(out_path, index = False)\n",
    "    logging.info('Saved to {}'.format(out_path))\n",
    "\n",
    "    import yaml\n",
    "\n",
    "    out_path_settings = base+'_ap.yaml'\n",
    "\n",
    "    with open(out_path_settings, 'w') as file:\n",
    "        yaml.dump(settings, file)\n",
    "\n",
    "    logging.info('Settings saved to {}'.format(out_path_settings))\n",
    "\n",
    "    set_progress(time_dict['reporting']['total'])\n",
    "\n",
    "    CURRENT_TASK('Complete.')\n",
    "    \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_settings.ipynb.\n",
      "Converted 13_runner.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
