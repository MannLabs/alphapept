{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp feature_finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Finding\n",
    "\n",
    "> Functions related to feature finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part describes the implementation of the feature-finding algorithm. The core of the algorithm is described in the [MaxQuant-Paper](https://www.nature.com/articles/nbt.1511).\n",
    "The supplementary material explains the underlying methodology in great detail and is the foundation of the theoretical background that is described here.\n",
    "A refined version of the algorithm was presented with [Dinosaur](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4933939/), which was also used as a reference for the Python implementation.\n",
    "\n",
    "For the algorithm, we need serval modules:\n",
    "\n",
    "1. Connecting Centroids to Hills\n",
    "\n",
    "2. Refinement of Hills\n",
    "\n",
    "3. Calculating Hill Statistics\n",
    "\n",
    "4. Combining Hills to Isotope Patterns\n",
    "\n",
    "5. Deconvolution of Isotope Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "From the `IO` library, we already have an `*.ms_data.hdf` container that contains centroided data. To use it in feature finding, we directly load the data.\n",
    "\n",
    "## Connecting Centroids to Hills\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "Feature finding relies heavily on the performance function decorator from the performance notebook: `@alphapept.performance.performance_function`. Part of this is that the functions will not have return values to be GPU compatible. Please check out this notebook for further information.\n",
    "\n",
    ":::\n",
    "\n",
    "### Connecting centroids\n",
    "\n",
    "Feature finding starts with connecting centroids. For this we look at subsequent scans and compare peaks that are withing a defined mass tolerance (`centroid_tol`). \n",
    "\n",
    "Imagine you have three scans with the following centroids:\n",
    "\n",
    "* Scan 0: 10, 20, 30\n",
    "* Scan 1: 10.2, 40.1\n",
    "* Scan 2: 40, 50, 60\n",
    "\n",
    "When comparing consecutive scans and defining the maximum delta mass to be 0.5 find the following connections: (Scan No, Centroid No) -> (Scan No, Centroid No). As we cannot easily store tuples in the matrix, we convert tuple containing the position of the connected centroid to an integer.\n",
    "* (0,0) -> (1,0) -> (3): 10 & 10.2 -> delta = 0.2\n",
    "* (1,1) -> (2,0) -> (6): 40.1 & 40 -> delta = 0.1\n",
    "\n",
    "Finally, we store this in the `results` matrix:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "3 & -1  & -1 \\\\ \n",
    "-1 & 6 & -1\\\\ \n",
    "-1 & -1 & -1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "The coressponding `scores` matrix will look as follows:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "0.2 & -1  & -1 \\\\ \n",
    "-1 & 0.1 & -1\\\\ \n",
    "-1 & -1 & -1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "This allows us to not only easily store connections between centroids but also perform a quick lookup for the delta of an existing connection. Note that it also only stores the best connection for each centroid. To extract the connected centroids, we can use `np.where(results >= 0)`. This implementation allows getting millions of connections within seconds. \n",
    "\n",
    "As we are also allowing gaps, refering to that we can have connections between Scan 0 and Scan 2, we make the aforementioned matrix multdimensional, so that e.g. a first matrix stores the conncetions for no gap, the second matrix the connections with a gap of 1.\n",
    "\n",
    "The functionality for this step is implemented in `connect_centroids_unidirection` and the wrapper `find_centroid_connections`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import alphapept.performance\n",
    "\n",
    "#This function is tested by being called from find_centroid_connections\n",
    "@alphapept.performance.performance_function\n",
    "def connect_centroids_unidirection(x:np.ndarray, row_borders:np.ndarray, connections:np.ndarray, scores:np.ndarray, centroids:np.ndarray, max_gap:int, centroid_tol:float):\n",
    "    \"\"\"Connect centroids.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Index to datapoint. Note that this using the performance_function, so one passes an ndarray.\n",
    "        row_borders (np.ndarray): Row borders of the centroids array.\n",
    "        connections (np.ndarray): Connections matrix to store the connections\n",
    "        scores (np.ndarray):  Score matrix to store the connections\n",
    "        centroids (np.ndarray): 1D Array containing the masses of the centroids data.\n",
    "        max_gap (int): Maximum gap when connecting centroids.\n",
    "        centroid_tol (float): Centroid tolerance.\n",
    "    \"\"\"\n",
    "    for gap in range(max_gap + 1):\n",
    "        y = x + gap + 1\n",
    "        if y >= row_borders.shape[0]:\n",
    "            return\n",
    "\n",
    "        start_index_f = 0\n",
    "        if x > 0:\n",
    "            start_index_f = row_borders[x - 1]\n",
    "\n",
    "        centroids_1 = centroids[start_index_f: row_borders[x]]\n",
    "        start_index_b = row_borders[y - 1]\n",
    "        centroids_2 = centroids[start_index_b: row_borders[y]]\n",
    "\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        while (i < len(centroids_1)) & (j < len(centroids_2)):\n",
    "            mz1, mz2 = centroids_1[i], centroids_2[j]\n",
    "            diff = mz1 - mz2\n",
    "            mz_sum = mz1 + mz2\n",
    "            delta = 2 * 1e6 * abs(diff) / mz_sum\n",
    "\n",
    "            if delta < centroid_tol:\n",
    "                if scores[x, i, gap] > delta:\n",
    "                    scores[x, i, gap] = delta\n",
    "                    connections[x, i, gap] = (connections.shape[1] * y) + j\n",
    "\n",
    "            if diff > 0:\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "\n",
    "def find_centroid_connections(rowwise_peaks:np.ndarray, row_borders:np.ndarray, centroids:np.ndarray, max_gap:int, centroid_tol:float):\n",
    "    \"\"\"Wrapper function to call connect_centroids_unidirection\n",
    "\n",
    "    Args:\n",
    "        rowwise_peaks (np.ndarray): Length of centroids with respect to the row borders.\n",
    "        row_borders (np.ndarray): Row borders of the centroids array.\n",
    "        centroids (np.ndarray): Array containing the centroids data.\n",
    "        max_gap (int): Maximum gap when connecting centroids.\n",
    "        centroid_tol (float): Centroid tolerance.\n",
    "    \"\"\"\n",
    "    if alphapept.performance.COMPILATION_MODE == \"cuda\":\n",
    "        import cupy\n",
    "        cupy = cupy\n",
    "    else:\n",
    "        import numpy\n",
    "        cupy = numpy\n",
    "\n",
    "    max_centroids = int(cupy.max(rowwise_peaks))\n",
    "    spectra_cnt = len(row_borders) - 1\n",
    "\n",
    "    connections = cupy.full((spectra_cnt, max_centroids, max_gap + 1), -1, dtype=np.int32)\n",
    "    score = cupy.full((spectra_cnt, max_centroids, max_gap + 1), np.inf)\n",
    "\n",
    "    connect_centroids_unidirection(range(len(row_borders)),\n",
    "                                    row_borders,\n",
    "                                   connections,\n",
    "                                   score,\n",
    "                                   centroids,\n",
    "                                   max_gap,\n",
    "                                   centroid_tol)\n",
    "\n",
    "    score = score[cupy.where(score < np.inf)]\n",
    "\n",
    "    score_median = cupy.median(score)\n",
    "    score_std = cupy.std(score)\n",
    "\n",
    "    del score, max_centroids, spectra_cnt\n",
    "\n",
    "    c_shape = connections.shape\n",
    "    from_r, from_c, from_g = cupy.where(connections >= 0)\n",
    "    to_r = connections[from_r, from_c, from_g] // c_shape[1]\n",
    "    to_c = connections[from_r, from_c, from_g] - to_r * c_shape[1]\n",
    "\n",
    "    del connections, from_g\n",
    "\n",
    "    return from_r, from_c, to_r, to_c, score_median, score_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_find_centroid_connections():\n",
    "    row_borders = np.array([3, 6, 9])\n",
    "    rowwise_peaks = np.array([3, 3, 3])\n",
    "    max_gap = 2\n",
    "\n",
    "    score = np.full((3,3, max_gap), np.inf)\n",
    "    connections = np.full((3,3, max_gap), -1)\n",
    "\n",
    "    centroids = np.array([10, 20, 30, 10.2, 40.1, 40, 50, 60])\n",
    "    centroid_tol = 0.5*1e6\n",
    "\n",
    "    from_r, from_c, to_r, to_c, score_median, score_std = find_centroid_connections(rowwise_peaks, row_borders, centroids, max_gap, centroid_tol)\n",
    "\n",
    "    assert np.allclose(from_r, np.array([0, 0, 1, 1])) #e.g. 0,0 is connected to 0,1 -> 10 to 10.2\n",
    "    assert np.allclose(from_c, np.array([0, 2, 1, 2]))\n",
    "    assert np.allclose(to_r, np.array([1, 1, 2, 2]))\n",
    "    assert np.allclose(to_c, np.array([0, 1, 0, 0]))\n",
    "\n",
    "test_find_centroid_connections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the centroid connections in the function `connect_centroids`. This function converts the connections into an usable array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#the performance functions are tested with the wrapper function connect_centroids\n",
    "@alphapept.performance.performance_function\n",
    "def convert_connections_to_array(x:np.ndarray, from_r:np.ndarray, from_c:np.ndarray, to_r:np.ndarray, to_c:np.ndarray, row_borders:np.ndarray, out_from_idx:np.ndarray, out_to_idx:np.ndarray):\n",
    "    \"\"\"Convert integer indices of a matrix to coordinates.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input index. Note that we are using the performance function so this is a range.\n",
    "        from_r (np.ndarray): From array with row coordinates.\n",
    "        from_c (np.ndarray): From array with column coordinates.\n",
    "        to_r (np.ndarray): To array with row coordinates. \n",
    "        to_c (np.ndarray): To array with column coordinates.\n",
    "        row_borders (np.ndarray): Row borders (for indexing).\n",
    "        out_from_idx (np.ndarray): Reporting array: 1D index from.\n",
    "        out_to_idx (np.ndarray): Reporting array: 1D index to.\n",
    "    \"\"\"    \n",
    "    row = from_r[x]\n",
    "    col = from_c[x]\n",
    "    start_index_f = 0\n",
    "    if row > 0:\n",
    "        start_index_f = row_borders[row - 1]\n",
    "    out_from_idx[x] = start_index_f + col\n",
    "\n",
    "    row = to_r[x]\n",
    "    col = to_c[x]\n",
    "    start_index_f = 0\n",
    "    if row > 0:\n",
    "        start_index_f = row_borders[row - 1]\n",
    "    out_to_idx[x] = start_index_f + col\n",
    "\n",
    "@alphapept.performance.performance_function\n",
    "def eliminate_overarching_vertex(x:np.ndarray, from_idx:np.ndarray, to_idx:np.ndarray):\n",
    "    \"\"\"Eliminate overacrhing vertex.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input index. Note that we are using the performance function so this is a range.\n",
    "        from_idx (np.ndarray): From index.\n",
    "        to_idx (np.ndarray): To index.\n",
    "    \"\"\"\n",
    "    if x == 0:\n",
    "        return\n",
    "\n",
    "    if from_idx[x - 1] == from_idx[x]:\n",
    "        to_idx[x] = -1\n",
    "\n",
    "def connect_centroids(rowwise_peaks:np.ndarray, row_borders:np.ndarray, centroids:np.ndarray, max_gap:int, centroid_tol:float)-> (np.ndarray, np.ndarray, float, float):\n",
    "    \"\"\"Function to connect centroids.\n",
    "\n",
    "    Args:\n",
    "        rowwise_peaks (np.ndarray): Indexes for centroids.\n",
    "        row_borders (np.ndarray): Row borders (for indexing).\n",
    "        centroids (np.ndarray): Centroid data.\n",
    "        max_gap: Maximum gap.\n",
    "        centroid_tol: Centroid tol for matching centroids.\n",
    "    Returns:\n",
    "        np.ndarray: From index.\n",
    "        np.ndarray: To index.\n",
    "        float: Median score.\n",
    "        float: Std deviation of the score.\n",
    "    \"\"\"    \n",
    "    if alphapept.performance.COMPILATION_MODE == \"cuda\":\n",
    "        import cupy\n",
    "        cupy = cupy\n",
    "    else:\n",
    "        import numpy\n",
    "        cupy = numpy\n",
    "\n",
    "    from_r, from_c, to_r, to_c, score_median, score_std = find_centroid_connections(rowwise_peaks,\n",
    "                                                           row_borders,\n",
    "                                                           centroids,\n",
    "                                                           max_gap,\n",
    "                                                           centroid_tol)\n",
    "\n",
    "    from_idx = cupy.zeros(len(from_r), np.int32)\n",
    "    to_idx = cupy.zeros(len(from_r), np.int32)\n",
    "\n",
    "    convert_connections_to_array(range(len(from_r)),\n",
    "                                    from_r,\n",
    "                                 from_c,\n",
    "                                 to_r,\n",
    "                                 to_c,\n",
    "                                 row_borders,\n",
    "                                 from_idx,\n",
    "                                 to_idx)\n",
    "\n",
    "    eliminate_overarching_vertex(range(len(from_idx)), from_idx, to_idx)\n",
    "\n",
    "    relavent_idx = cupy.where(to_idx >= 0)\n",
    "    from_idx = cupy.take(from_idx, relavent_idx)[0]\n",
    "    to_idx = cupy.take(to_idx, relavent_idx)[0]\n",
    "\n",
    "    del from_r, from_c, to_r, to_c, relavent_idx\n",
    "    return from_idx, to_idx, score_median, score_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Sample snippet to show centroid conncetions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "row_borders = np.array([3, 6, 9])\n",
    "rowwise_peaks = np.array([3, 3, 3])\n",
    "max_gap = 2\n",
    "\n",
    "score = np.full((3,3, max_gap), np.inf)\n",
    "connections = np.full((3,3, max_gap), -1)\n",
    "\n",
    "centroids = np.array([10, 20, 30, 10.2, 20, 10, 30, 40])\n",
    "centroid_tol = 0.5*1e5\n",
    "\n",
    "from_idx, to_idx, score_median, score_std = connect_centroids(rowwise_peaks, row_borders, centroids, max_gap, centroid_tol)\n",
    "\n",
    "scan_no = np.array([0, 0, 0, 1, 1, 2, 2, 2])\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "for i, _ in enumerate(row_borders):\n",
    "    ctrd = centroids[_-rowwise_peaks[i]:_]\n",
    "    plt.plot(ctrd, np.ones_like(ctrd)*i, 'o')\n",
    "    \n",
    "for i, _ in enumerate(from_idx):\n",
    "    from_ = _\n",
    "    to_ = to_idx[i]\n",
    "    plt.plot([centroids[from_], centroids[to_]], [scan_no[from_], scan_no[to_]], 'k:')\n",
    "    \n",
    "plt.ylabel('scan')\n",
    "plt.xlabel('m/z')\n",
    "plt.ylim(len(row_borders)+0.5, -1.5)\n",
    "plt.title('Peak connections')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_connect_centroids():\n",
    "    row_borders = np.array([3, 6, 9])\n",
    "    rowwise_peaks = np.array([3, 3, 3])\n",
    "    max_gap = 2\n",
    "\n",
    "    score = np.full((3,3, max_gap), np.inf)\n",
    "    connections = np.full((3,3, max_gap), -1)\n",
    "\n",
    "    centroids = np.array([10, 20, 30, 10.2, 20, 10, 30, 40])\n",
    "    centroid_tol = 0.5*1e5\n",
    "\n",
    "    from_idx, to_idx, score_median, score_std = connect_centroids(rowwise_peaks, row_borders, centroids, max_gap, centroid_tol)\n",
    "\n",
    "    assert np.allclose(from_idx, np.array([0, 1, 2]))\n",
    "    assert np.allclose(to_idx, np.array([3, 4, 6]))\n",
    "\n",
    "test_connect_centroids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting hills.\n",
    "\n",
    "To extract hills we extract connected components from the connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@alphapept.performance.performance_function\n",
    "def path_finder(x:np.ndarray, from_idx:np.ndarray, to_idx:np.ndarray, forward:np.ndarray, backward:np.ndarray):\n",
    "    \"\"\"Extracts path information and writes to path matrix.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input index. Note that we are using the performance function so this is a range.\n",
    "        from_idx (np.ndarray): Array containing from indices.\n",
    "        to_idx (np.ndarray): Array containing to indices.\n",
    "        forward (np.ndarray): Array to report forward connection.\n",
    "        backward (np.ndarray): Array to report backward connection.\n",
    "    \"\"\"\n",
    "\n",
    "    fr = from_idx[x]\n",
    "    to =  to_idx[x]\n",
    "\n",
    "    forward[fr] = to\n",
    "    backward[to] = fr\n",
    "\n",
    "@alphapept.performance.performance_function\n",
    "def find_path_start(x:np.ndarray, forward:np.ndarray, backward:np.ndarray, path_starts:np.ndarray):\n",
    "    \"\"\"Function to find the start of a path.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input index. Note that we are using the performance function so this is a range.\n",
    "        forward (np.ndarray):  Array to report forward connection.\n",
    "        backward (np.ndarray):  Array to report backward connection.\n",
    "        path_starts (np.ndarray): Array to report path starts.\n",
    "    \"\"\"\n",
    "    if forward[x] > -1 and backward[x] == -1:\n",
    "        path_starts[x] = 0\n",
    "\n",
    "@alphapept.performance.performance_function\n",
    "def find_path_length(x:np.ndarray, path_starts:np.ndarray, forward:np.ndarray, path_cnt:np.ndarray):\n",
    "    \"\"\"Function to extract the length of a path.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input index. Note that we are using the performance function so this is a range.\n",
    "        path_starts (np.ndarray): Array that stores the starts of the paths.\n",
    "        forward (np.ndarray): Array that stores forward information.\n",
    "        path_cnt (np.ndarray): Reporting array to count the paths.\n",
    "    \"\"\"    \n",
    "    ctr = 1\n",
    "    idx = path_starts[x]\n",
    "    while forward[idx] > -1:\n",
    "        ctr += 1\n",
    "        idx = forward[idx]\n",
    "    path_cnt[x] = ctr\n",
    "\n",
    "@alphapept.performance.performance_function\n",
    "def fill_path_matrix(x:np.ndarray, path_start:np.ndarray, forwards:np.ndarray, out_hill_data:np.ndarray, out_hill_ptr:np.ndarray):\n",
    "    \"\"\"Function to fill the path matrix.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input index. Note that we are using the performance function so this is a range.\n",
    "        path_starts (np.ndarray): Array that stores the starts of the paths.\n",
    "        forwards (np.ndarray): Forward array.\n",
    "        out_hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        out_hill_ptr (np.ndarray): Array containing the bounds to out_hill_data.\n",
    "    \"\"\"\n",
    "    path_position = 0\n",
    "    idx = path_start[x]\n",
    "    while idx > -1:\n",
    "        out_hill_data[out_hill_ptr[x] + path_position] = idx\n",
    "        idx = forwards[idx]\n",
    "        path_position += 1\n",
    "\n",
    "def get_hills(centroids:np.ndarray, from_idx:np.ndarray, to_idx:np.ndarray, hill_length_min:int=3)-> (np.ndarray, np.ndarray, int):\n",
    "    \"\"\"Function to get hills from centroid connections.\n",
    "\n",
    "    Args:\n",
    "        centroids (np.ndarray): 1D Array containing the masses of the centroids.\n",
    "        from_idx (np.ndarray): From index.\n",
    "        to_idx (np.ndarray): To index.\n",
    "        hill_length_min (int): Minimum hill length:\n",
    "\n",
    "    Returns:\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        path_node_cnt (int): Number of elements in this path.\n",
    "    \"\"\"\n",
    "    if alphapept.performance.COMPILATION_MODE == \"cuda\":\n",
    "        import cupy\n",
    "        cupy = cupy\n",
    "    else:\n",
    "        import numpy\n",
    "        cupy = numpy\n",
    "\n",
    "    forward = cupy.full(centroids.shape[0], -1)\n",
    "    backward = cupy.full(centroids.shape[0], -1)\n",
    "    path_starts = cupy.full(centroids.shape[0], -1)\n",
    "\n",
    "    path_finder(range(len(from_idx)), from_idx, to_idx, forward, backward)\n",
    "    find_path_start(range(len(forward)), forward, backward, path_starts)\n",
    "\n",
    "    # path_starts will now container the first index of all connected centroids\n",
    "    path_starts = cupy.where(path_starts == 0)[0]\n",
    "\n",
    "    path_node_cnt = cupy.full(path_starts.shape[0], -1)\n",
    "    find_path_length(range(len(path_starts)), path_starts, forward, path_node_cnt)\n",
    "\n",
    "    relavant_path_node = cupy.where(path_node_cnt >= hill_length_min)[0]\n",
    "    path_starts = cupy.take(path_starts, relavant_path_node)\n",
    "    path_node_cnt = cupy.take(path_node_cnt, relavant_path_node)\n",
    "    del relavant_path_node\n",
    "\n",
    "    # Generate the hill matix indice ptr data\n",
    "    hill_ptrs = cupy.empty((path_starts.shape[0] + 1), dtype=cupy.int32)\n",
    "\n",
    "    hill_ptrs[0] = 0\n",
    "    hill_ptrs[1:] = path_node_cnt.cumsum()\n",
    "    hill_data = cupy.empty((int(hill_ptrs[-1])), np.int32)\n",
    "\n",
    "    fill_path_matrix(range(len(path_starts)), path_starts, forward, hill_data, hill_ptrs)\n",
    "\n",
    "    del from_idx, to_idx, path_starts, forward, backward\n",
    "    return hill_ptrs, hill_data, path_node_cnt\n",
    "\n",
    "\n",
    "def extract_hills(query_data:dict, max_gap:int, centroid_tol:float)-> (np.ndarray, np.ndarray, int, float, float):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        query_data (dict): Data structure containing the query data.\n",
    "        max_gap (int): Maximum gap when connecting centroids.\n",
    "        centroid_tol (float): Centroid tolerance.\n",
    "\n",
    "    Returns:\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        path_node_cnt (int): Number of elements in this path.\n",
    "        score_median (float): Median score.\n",
    "        score_std (float): Std deviation of the score.\n",
    "    \"\"\"\n",
    "\n",
    "    if alphapept.performance.COMPILATION_MODE == \"cuda\":\n",
    "        import cupy\n",
    "        cupy = cupy\n",
    "    else:\n",
    "        import numpy\n",
    "        cupy = numpy\n",
    "\n",
    "    indices = cupy.array(query_data['indices_ms1'])\n",
    "    mass_data = cupy.array(query_data['mass_list_ms1'])\n",
    "\n",
    "    rowwise_peaks = indices[1:] - indices[:-1]\n",
    "    row_borders = indices[1:]\n",
    "\n",
    "    from_idx, to_idx, score_median, score_std = connect_centroids(rowwise_peaks, row_borders, mass_data, max_gap, centroid_tol)\n",
    "\n",
    "    hill_ptrs, hill_data, path_node_cnt = get_hills(mass_data, from_idx, to_idx)\n",
    "\n",
    "    del mass_data\n",
    "    del indices\n",
    "\n",
    "    if cupy.__name__ != 'numpy':\n",
    "        hill_ptrs = hill_ptrs.get()\n",
    "        hill_data = hill_data.get()\n",
    "        path_node_cnt = path_node_cnt.get()\n",
    "\n",
    "        score_median = score_median.get()\n",
    "        score_std = score_std.get()\n",
    "\n",
    "    return hill_ptrs, hill_data, path_node_cnt, score_median, score_std\n",
    "\n",
    "from numba import njit\n",
    "@njit\n",
    "def remove_duplicate_hills(hill_ptrs, hill_data, path_node_cnt):\n",
    "    \"\"\"\n",
    "    Removes hills that share datapoints. Starts from the largest hills.\n",
    "    \n",
    "    \"\"\"\n",
    "    taken_points = np.zeros(hill_data.max()+1)\n",
    "\n",
    "    c = 0\n",
    "    current_idx = 0\n",
    "\n",
    "    hill_ptrs_new = np.zeros_like(hill_ptrs)\n",
    "    hill_data_new = np.zeros_like(hill_data)\n",
    "\n",
    "    for i, _ in enumerate(np.argsort(path_node_cnt)[::-1]):\n",
    "        s, e = hill_ptrs[_], hill_ptrs[_+1]\n",
    "\n",
    "        point_idx = hill_data[s:e]\n",
    "\n",
    "        hill_pts = taken_points[point_idx]\n",
    "\n",
    "        if hill_pts.sum() == 0:\n",
    "            hill_data_new[current_idx:current_idx+len(hill_pts)] = point_idx\n",
    "            current_idx += len(hill_pts) \n",
    "            hill_ptrs_new[c+1] = current_idx\n",
    "            c +=1\n",
    "\n",
    "        taken_points[point_idx] +=1\n",
    "\n",
    "    hill_data_new = hill_data_new[:current_idx]\n",
    "    hill_ptrs_new = hill_ptrs_new[:c]\n",
    "\n",
    "    return hill_ptrs_new, hill_data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Splitting\n",
    "When having a hill with two or more maxima, we would like to split it at the minimum position. For this, we use a recursive approach. First, the minimum of a hill is detected. A hill is split at this minimum if the smaller of the surrounding maxima is at least the factor `hill_split_level` larger than the minimum. For each split, the process is repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def fast_minima(y:np.ndarray)->np.ndarray:\n",
    "    \"\"\"Function to calculate the local minimas of an array.\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): Input array.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array containing minima positions.\n",
    "    \"\"\"\n",
    "    minima = np.zeros(len(y))\n",
    "\n",
    "    start = 0\n",
    "    end = len(y)\n",
    "\n",
    "    for i in range(start + 2, end - 2):\n",
    "        if ((y[i - 1] > y[i]) & (y[i + 1] > y[i])) \\\n",
    "            or ((y[i - 1] > y[i]) & (y[i + 1] == y[i]) & (y[i + 2] > y[i])) \\\n",
    "            or ((y[i - 2] > y[i]) & (y[i - 1] == y[i]) & (y[i + 1] > y[i])) \\\n",
    "            or (((y[i - 2] > y[i]) & (y[i - 1] == y[i]) & (y[i + 1] == y[i]) & \\\n",
    "                (y[i + 2] > y[i]))):\n",
    "            minima[i] = 1\n",
    "\n",
    "    minima = minima.nonzero()[0]\n",
    "\n",
    "    return minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_fast_minima():\n",
    "    assert fast_minima(np.array([3,2,1,0,1,2,3])) == 3\n",
    "    assert fast_minima(np.array([4,3,2,1,0,1,2])) == 4\n",
    "    assert len(fast_minima(np.array([5,4,3,2,1,0,1]))) == 0\n",
    "    assert len(fast_minima(np.array([6,5,4,3,2,1,0]))) == 0\n",
    "\n",
    "test_fast_minima()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@alphapept.performance.performance_function(compilation_mode=\"numba-multithread\")\n",
    "def split(k:np.ndarray, hill_ptrs:np.ndarray, int_data:np.ndarray, hill_data:np.ndarray, splits:np.ndarray, hill_split_level:float, window:int):\n",
    "    \"\"\"Function to split hills.\n",
    "\n",
    "    Args:\n",
    "        k (np.ndarray): Input index. Note that we are using the performance function so this is a range.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        splits (np.ndarray): Array containing splits.\n",
    "        hill_split_level (float): Split level for hills.\n",
    "        window (int): Smoothing window.\n",
    "    \"\"\"\n",
    "\n",
    "    start = hill_ptrs[k]\n",
    "    end = hill_ptrs[k + 1]\n",
    "\n",
    "    int_idx = hill_data[start:end] #index to hill data\n",
    "\n",
    "    int_trace = int_data[int_idx]\n",
    "\n",
    "    for i in range(len(int_idx)):\n",
    "        min_index = max(0, i - window)\n",
    "        max_index = min(len(int_idx), i + window + 1)\n",
    "        int_trace[i] = np.median(int_trace[min_index:max_index])\n",
    "\n",
    "    for i in range(len(int_idx)):\n",
    "        min_index = max(0, i - window)\n",
    "        max_index = min(len(int_idx), i + window + 1)\n",
    "        int_trace[i] = np.mean(int_trace[min_index:max_index])\n",
    "\n",
    "    #minima = (np.diff(np.sign(np.diff(int_trace))) > 0).nonzero()[0] + 1 #This works also but is slower\n",
    "\n",
    "    minima = fast_minima(int_trace)\n",
    "\n",
    "    sorted_minima = np.argsort(int_trace[minima])\n",
    "\n",
    "    minima = minima[sorted_minima]\n",
    "\n",
    "    for min_ in minima:\n",
    "\n",
    "        minval = int_trace[min_]\n",
    "\n",
    "        left_max = max(int_trace[:min_])\n",
    "        right_max = max(int_trace[min_:])\n",
    "\n",
    "        min_max = min(left_max, right_max)\n",
    "\n",
    "        if (minval == 0) or ((min_max / minval) > hill_split_level):\n",
    "            splits[k] = start+min_\n",
    "            break # Split only once per iteration\n",
    "\n",
    "def split_hills(hill_ptrs:np.ndarray, hill_data:np.ndarray, int_data:np.ndarray, hill_split_level:float, window:int)->np.ndarray:\n",
    "    \"\"\"Wrapper function to split hills\n",
    "\n",
    "    Args:\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        hill_split_level (float): Split level for hills.\n",
    "        window (int): Smoothing window.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array containing the bounds to the hill_data with splits.\n",
    "    \"\"\"\n",
    "\n",
    "    splits = np.zeros(len(int_data), dtype=np.int32)\n",
    "    to_check = np.arange(len(hill_ptrs)-1)\n",
    "\n",
    "    while len(to_check) > 0:\n",
    "        split(to_check, hill_ptrs, int_data, hill_data, splits, hill_split_level, window)\n",
    "        splitpoints = splits.nonzero()[0]\n",
    "\n",
    "        to_check = np.zeros(len(hill_ptrs))\n",
    "        to_check[splitpoints] = 1\n",
    "\n",
    "        to_check = np.insert(to_check, splitpoints+1, np.ones(len(splitpoints))).nonzero()[0] #array, index, what\n",
    "        hill_ptrs = np.insert(hill_ptrs, splitpoints+1, splits[splitpoints]) #array, index, what\n",
    "\n",
    "        splits = np.zeros(len(hill_ptrs), dtype=np.int32) #was cupy np.int32\n",
    "\n",
    "    return hill_ptrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Hills\n",
    "\n",
    "To filter hills, we define a minimum length `hill_min_length`. All peaks below the threshold `hill_peak_min_length` are accepted as is. For longer hills, the intensity at the start and the end are compared to the maximum intensity. If the ratio of the maximum raw intensity to the smoothed intensity and the beginning and end are larger than `hill_peak_factor` the hills are accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@alphapept.performance.performance_function(compilation_mode=\"numba-multithread\")\n",
    "def check_large_hills(idx:np.ndarray, large_peaks:np.ndarray, hill_ptrs:np.ndarray, hill_data:np.ndarray, int_data:np.ndarray, to_remove:np.ndarray, large_peak:int = 40, hill_peak_factor:float = 2, window:int=1):\n",
    "    \"\"\"Function to check large hills and flag them for removal.\n",
    "\n",
    "    Args:\n",
    "        idx (np.ndarray): Input index. Note that we are using the performance function so this is a range.\n",
    "        large_peaks (np.ndarray): Array containing large peaks.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        to_remove (np.ndarray): Array with indexes to remove.\n",
    "        large_peak (int, optional): Length criterion when a peak is large. Defaults to 40.\n",
    "        hill_peak_factor (float, optional): Hill maximum criterion. Defaults to 2.\n",
    "        window (int, optional): Smoothing window.. Defaults to 1.\n",
    "    \"\"\"\n",
    "    k = large_peaks[idx]\n",
    "\n",
    "    start = hill_ptrs[k]\n",
    "    end = hill_ptrs[k + 1]\n",
    "\n",
    "    int_idx = hill_data[start:end] #index to hill data\n",
    "\n",
    "    int_smooth_ = int_data[int_idx]\n",
    "\n",
    "    for i in range(len(int_idx)):\n",
    "        min_index = max(0, i - window)\n",
    "        max_index = min(len(int_idx), i + window + 1)\n",
    "        int_smooth_[i] = np.median(int_smooth_[min_index:max_index])\n",
    "\n",
    "    for i in range(len(int_idx)):\n",
    "        min_index = max(0, i - window)\n",
    "        max_index = min(len(int_idx), i + window + 1)\n",
    "        int_smooth_[i] = np.mean(int_smooth_[min_index:max_index])\n",
    "\n",
    "    int_ = int_data[int_idx]\n",
    "\n",
    "    max_ = np.max(int_)\n",
    "\n",
    "    if (max_ / int_smooth_[0] > hill_peak_factor) & (max_ / int_smooth_[-1] > hill_peak_factor):\n",
    "        to_remove[idx] = 0\n",
    "\n",
    "\n",
    "def filter_hills(hill_data:np.ndarray, hill_ptrs:np.ndarray, int_data:np.ndarray, hill_check_large:int =40, window:int = 1) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"Filters large hills.\n",
    "\n",
    "    Args:\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        hill_check_large (int, optional): Length criterion when a hill is considered large.. Defaults to 40.\n",
    "        window (int, optional): Smoothing window. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Filtered hill data.\n",
    "        np.ndarray: Filtered hill points.\n",
    "    \"\"\"\n",
    "\n",
    "    large_peaks = np.where(np.diff(hill_ptrs)>=hill_check_large)[0]\n",
    "\n",
    "    to_remove = np.ones(len(large_peaks), dtype=np.int32)\n",
    "    check_large_hills(range(len(large_peaks)), large_peaks, hill_ptrs, hill_data, int_data, to_remove, window)\n",
    "\n",
    "    idx_ = np.ones(len(hill_data), dtype = np.int32)\n",
    "    keep = np.ones(len(hill_ptrs)-1, dtype = np.int32)\n",
    "\n",
    "    to_remove = to_remove.nonzero()[0]\n",
    "\n",
    "    for _ in to_remove:\n",
    "        idx_[hill_ptrs[_]:hill_ptrs[_+1]] = 0\n",
    "        keep[_] = 0\n",
    "\n",
    "    hill_lens = np.diff(hill_ptrs)\n",
    "    keep_ = hill_lens[keep.nonzero()[0]]\n",
    "\n",
    "    hill_data_ = hill_data[idx_.nonzero()[0]]\n",
    "    hill_ptrs_ = np.empty((len(keep_) + 1), dtype=np.int32)\n",
    "    hill_ptrs_[0] = 0\n",
    "    hill_ptrs_[1:] = keep_.cumsum()\n",
    "\n",
    "    return hill_data_, hill_ptrs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mass estimate min the equation above is more complicated than just an average of the mj, a standard deviation based estimate of the error would not be appropriate. Therefore we calculate the error as a bootstrap2 estimate over B=150 bootstrap replications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Hill Statistics\n",
    "\n",
    "Next, we calculate summary statistics for the connected centroids. We can obtain a high precision mass estimate for each hill by taking the average of the the masses and weighting this by their intensiteis:\n",
    "\n",
    "$$\n",
    "\\overline{m} = \\frac{\\sum_{j=1}^nm_jI_j}{\\sum_{j=1}^nI_j}\n",
    "$$\n",
    "\n",
    "To estimate the mass error, we calculate the error as a boostrap estimate:\n",
    " \n",
    "$$\\Delta \\overline{m} = \\sqrt{\\frac{\\sum_{b=1}^{B}(\\overline{m}_b - \\overline{m} )}{(B-1)}}$$\n",
    "\n",
    "The calculation of hill statistics for a single hill is implemented in `get_hill_stats`. To calculate the hill stats for a list of hills, we can call the wrapper `get_hill_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@alphapept.performance.performance_function(compilation_mode=\"numba-multithread\")\n",
    "def hill_stats(idx:np.ndarray, hill_range:np.ndarray, hill_ptrs:np.ndarray, hill_data:np.ndarray, int_data:np.ndarray, mass_data:np.ndarray, rt_:np.ndarray, rt_idx:np.ndarray, stats:np.ndarray, hill_nboot_max:int, hill_nboot:int):\n",
    "    \"\"\"Function to calculate hill stats.\n",
    "\n",
    "    Args:\n",
    "        idx (np.ndarray): Input index. Note that we are using the performance function so this is a range.\n",
    "        hill_range (np.ndarray): Hill range.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        mass_data (np.ndarray): Array containing mass data.\n",
    "        rt_ (np.ndarray): Array with retention time information for each scan.\n",
    "        rt_idx (np.ndarray): Lookup array to match centroid idx to rt.\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        hill_nboot_max (int): Maximum number of bootstrap comparisons.\n",
    "        hill_nboot (int): Number of bootstrap comparisons\n",
    "    \"\"\"    \n",
    "    np.random.seed(42)\n",
    "\n",
    "    start = hill_ptrs[idx]\n",
    "    end = hill_ptrs[idx + 1]\n",
    "\n",
    "    idx_ = hill_data[start:end]\n",
    "\n",
    "    int_ = int_data[idx_]\n",
    "    mz_ = mass_data[idx_]\n",
    "\n",
    "    ms1_int_apex = np.max(int_)\n",
    "    ms1_int_area = np.abs(np.trapz(int_, rt_[rt_idx[idx_]])) #Area\n",
    "\n",
    "    rt_min = rt_[rt_idx[idx_]].min()\n",
    "    rt_max = rt_[rt_idx[idx_]].max()\n",
    "\n",
    "    if len(idx_) > hill_nboot_max:\n",
    "        bootsize = hill_nboot_max\n",
    "    else:\n",
    "        bootsize = len(idx_)\n",
    "\n",
    "    averages = np.zeros(hill_nboot)\n",
    "    average = 0\n",
    "\n",
    "    for i in range(hill_nboot):\n",
    "        boot = np.random.choice(len(int_), bootsize, replace=True)\n",
    "        boot_mz = np.sum((mz_[boot] * int_[boot])) / np.sum(int_[boot])\n",
    "        averages[i] = boot_mz\n",
    "        average += boot_mz\n",
    "\n",
    "    average_mz = average/hill_nboot\n",
    "\n",
    "    delta = 0\n",
    "    for i in range(hill_nboot):\n",
    "        delta += (average_mz - averages[i]) ** 2 #maybe easier?\n",
    "    delta_m = np.sqrt(delta / (hill_nboot - 1))\n",
    "\n",
    "    stats[idx,0] = average_mz\n",
    "    stats[idx,1] = delta_m\n",
    "    stats[idx,2] = ms1_int_area\n",
    "    stats[idx,3] = ms1_int_apex\n",
    "    stats[idx,4] = rt_min\n",
    "    stats[idx,5] = rt_max\n",
    "\n",
    "def remove_duplicates(stats:np.ndarray, hill_data:np.ndarray, hill_ptrs:np.ndarray)-> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    \"\"\"Remove duplicate hills.\n",
    "\n",
    "    Args:\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Filtered hill data.\n",
    "        np.ndarray: Filtered hill points.\n",
    "        np.ndarray: Filtered hill stats.\n",
    "    \"\"\"\n",
    "\n",
    "    dups = pd.DataFrame(stats).duplicated() #all duplicated hills\n",
    "\n",
    "    idx_ = np.ones(len(hill_data), dtype = np.int32) #keep all\n",
    "    keep = np.ones(len(hill_ptrs)-1, dtype = np.int32)\n",
    "\n",
    "    for _ in np.arange(len(stats))[dups]: #duplicates will be assigned zeros\n",
    "        idx_[hill_ptrs[_]:hill_ptrs[_+1]] = 0\n",
    "        keep[_] = 0\n",
    "\n",
    "    hill_lens = np.diff(hill_ptrs)\n",
    "    keep_ = hill_lens[keep.nonzero()[0]]\n",
    "\n",
    "    hill_data_ = hill_data[idx_.nonzero()[0]]\n",
    "    hill_ptrs_ = np.empty((len(keep_) + 1), dtype=np.int32)\n",
    "    hill_ptrs_[0] = 0\n",
    "    hill_ptrs_[1:] = keep_.cumsum()\n",
    "\n",
    "    return hill_data_, hill_ptrs_, stats[~dups]\n",
    "\n",
    "def get_hill_data(query_data:dict, hill_ptrs:np.ndarray, hill_data:np.ndarray, hill_nboot_max:int = 300, hill_nboot:int = 150) -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    \"\"\"Wrapper function to get the hill data.\n",
    "\n",
    "    Args:\n",
    "        query_data (dict): Data structure containing the query data.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        hill_nboot_max (int): Maximum number of bootstrap comparisons.\n",
    "        hill_nboot (int): Number of bootstrap comparisons\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Hill stats.\n",
    "        np.ndarray: Sortindex.\n",
    "        np.ndarray: Upper index.\n",
    "        np.ndarray: Scan index.\n",
    "        np.ndarray: Hill data.\n",
    "        np.ndarray: Hill points.\n",
    "    \"\"\"\n",
    "    indices_ = np.array(query_data['indices_ms1'])\n",
    "    rt_ = np.array(query_data['rt_list_ms1'])\n",
    "    mass_data = np.array(query_data['mass_list_ms1'])\n",
    "    scan_idx = np.searchsorted(indices_, np.arange(len(mass_data)), side='right') - 1\n",
    "    int_data = np.array(query_data['int_list_ms1'])\n",
    "\n",
    "    stats = np.zeros((len(hill_ptrs)-1, 6)) #mz, delta, rt_min, rt_max, sum_max\n",
    "    hill_stats(range(len(hill_ptrs)-1), np.arange(len(hill_ptrs)-1), hill_ptrs, hill_data, int_data, mass_data, rt_, scan_idx, stats, hill_nboot_max, hill_nboot)\n",
    "\n",
    "    # sort the stats\n",
    "    sortindex = np.argsort(stats[:,4]) #Sorted by rt_min\n",
    "    stats = stats[sortindex,:]\n",
    "    idxs_upper = stats[:,4].searchsorted(stats[:,5], side=\"right\")\n",
    "    sortindex_ = np.arange(len(sortindex))[sortindex]\n",
    "\n",
    "    return stats, sortindex_, idxs_upper, scan_idx, hill_data, hill_ptrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Hills to Isotope Patterns\n",
    "\n",
    "After obtaining summary statistics of hills, the next step is to check whether they belong together to form an isotope pattern. For this, we check wheter it is possible that they are neighbors in an isotope pattern, e.g. one having a 12C atom that has been replaced by a 13C version. The detailed criterion for the check is implemented in `check_isotope_pattern` and is as follows:\n",
    "\n",
    "\n",
    "$$\\left | \\Delta m-\\frac{\\Delta M}{z} \\right |\\leq \\sqrt{\\left ( \\frac{\\Delta S}{z}  \\right )^{2}+\\Delta {m_{1}}^{2} +\\Delta {m_{2}}^{2}}$$\n",
    "\n",
    "The left side contains $\\Delta m$, being the delta of the precise mass estimates from the summary statistics and $\\Delta M = 1.00286864$, which is the mass difference ebtween the 13C peak and the monoisotopic peak in an averagine molecule of 1500 Da mass divided by the charge $z$.\n",
    "\n",
    "The right side contains $\\Delta S = 0.0109135$, which is the maximum shift that a sulphur atom can cause ($\\Delta S = 2m(^{13}C) - 2m(^{12}C) - m(^{34}S) + m(^{32}S)$) and $\\Delta {m_{1}}$ and $\\Delta {m_{2}}$, which are the bootstrapped mass standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from alphapept.constants import mass_dict\n",
    "\n",
    "DELTA_M = mass_dict['delta_M']\n",
    "DELTA_S = mass_dict['delta_S']\n",
    "maximum_offset = DELTA_M + DELTA_S\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def check_isotope_pattern(mass1:float, mass2:float, delta_mass1:float, delta_mass2:float, charge:int, iso_mass_range:int = 5)-> bool:\n",
    "    \"\"\"Check if two masses could belong to the same isotope pattern.\n",
    "\n",
    "    Args:\n",
    "        mass1 (float): Mass of the first pattern.\n",
    "        mass2 (float): Mass of the second pattern.\n",
    "        delta_mass1 (float): Delta mass of the first pattern.\n",
    "        delta_mass2 (float): Delta mass of the second pattern.\n",
    "        charge (int): Charge.\n",
    "        iso_mass_range (int, optional): Mass range. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        bool: Flag to see if pattern belongs to the same pattern.\n",
    "    \"\"\"\n",
    "    delta_mass1 = delta_mass1 * iso_mass_range\n",
    "    delta_mass2 = delta_mass2 * iso_mass_range\n",
    "\n",
    "    delta_mass = np.abs(mass1 - mass2)\n",
    "\n",
    "    left_side = np.abs(delta_mass - DELTA_M / charge)\n",
    "    right_side = np.sqrt((DELTA_S / charge) ** 2 + delta_mass1 ** 2 + delta_mass2 ** 2)\n",
    "\n",
    "    return left_side <= right_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_check_isotope_pattern():\n",
    "    charge = 1\n",
    "\n",
    "    mass1, delta_mass1 = 100, 0.1\n",
    "    mass2, delta_mass2 = 101.1, 0.05\n",
    "\n",
    "    assert check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge) == True\n",
    "\n",
    "    mass2, delta_mass2 = 102.1, 0.05\n",
    "\n",
    "    assert check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge) == False\n",
    "\n",
    "test_check_isotope_pattern()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "charge = 1\n",
    "\n",
    "mass1, delta_mass1 = 100, 0.1\n",
    "mass2, delta_mass2 = 101.1, 0.05\n",
    "\n",
    "print(check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge))\n",
    "\n",
    "mass2, delta_mass2 = 102.1, 0.05\n",
    "\n",
    "print(check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Correlation of two hills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional criterion that is being checked is that the intensity profiles have sufficient overalp in retention time. This is validated by ensuring that two hills have a cosine correlation of at least 0.6.\n",
    "\n",
    "$$\\frac{\\sum_{s=s_{min}}^{s_{max}}I_sJ_s}{\\sum_{s=s_{min}}^{s_{max}}I_s^{2} \\sum_{s=s_{min}}^{s_{max}}J_s^{2}} \\geq 0.6$$\n",
    "\n",
    "The intensities of two hills are only compared if both have an intensity value in a particular scan. Otherwise, the intensity is set to zero. Additionally, an overlap of at least three elements is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def correlate(scans_:np.ndarray, scans_2:np.ndarray, int_:np.ndarray, int_2:np.ndarray)->float:\n",
    "    \"\"\"Correlate two scans.\n",
    "\n",
    "    Args:\n",
    "        scans_ (np.ndarray): Masses of the first scan.\n",
    "        scans_2 (np.ndarray): Masses of the second scan.\n",
    "        int_ (np.ndarray): Intensity of the first scan.\n",
    "        int_2 (np.ndarray): Intensity of the second scan.\n",
    "\n",
    "    Returns:\n",
    "        float: Correlation.\n",
    "    \"\"\"\n",
    "\n",
    "    min_one, max_one = scans_[0], scans_[-1]\n",
    "    min_two, max_two = scans_2[0], scans_2[-1]\n",
    "\n",
    "    if min_one + 3 > max_two:  # at least an overlap of 3 elements\n",
    "        corr = 0\n",
    "    elif min_two + 3 > max_one:\n",
    "        corr = 0\n",
    "    else:\n",
    "        min_s = min(min_one, min_two)\n",
    "        max_s = max(max_one, max_two)\n",
    "\n",
    "        int_one_scaled = np.zeros(int(max_s - min_s + 1))\n",
    "        int_two_scaled = np.zeros(int(max_s - min_s + 1))\n",
    "\n",
    "        int_one_scaled[scans_ - min_s] = int_\n",
    "        int_two_scaled[scans_2 - min_s] = int_2\n",
    "\n",
    "        corr = np.sum(int_one_scaled * int_two_scaled) / np.sqrt(\n",
    "            np.sum(int_one_scaled ** 2) * np.sum(int_two_scaled ** 2)\n",
    "        )\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting pre-Isotope Patterns\n",
    "\n",
    "Now having two criteria to check whether hills could, in principle, belong together, we define the wrapper functions `extract_edge` and `get_edges` to extract the connected hills. To minimize the number of comparisons we need to perform, we only compare the hills that overlap in time (i.e., the start of one hill `rt_min` needs to be before the end of the other hill `rt_max`) and are less than the sum of $\\Delta M$ and $\\Delta S$ apart. \n",
    "\n",
    "To extract all hills that belong together, we again rely on the `NetworkX`-package to extract the connected components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def extract_edge(stats:np.ndarray, idxs_upper:np.ndarray, runner:int, max_index:int, maximum_offset:float,  iso_charge_min:int = 1, iso_charge_max:int = 6, iso_mass_range:int=5)->list:\n",
    "    \"\"\"Extract edges.\n",
    "\n",
    "    Args:\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        idxs_upper (np.ndarray): Upper index for comparing.\n",
    "        runner (int): Index.\n",
    "        max_index (int): Unused.\n",
    "        maximum_offset (float): Maximum offset when comparing edges.\n",
    "        iso_charge_min (int, optional): Minimum isotope charge. Defaults to 1.\n",
    "        iso_charge_max (int, optional): Maximum isotope charge. Defaults to 6.\n",
    "        iso_mass_range (float, optional): Mass search range. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: List of edges. \n",
    "    \"\"\"    \n",
    "    edges = []\n",
    "\n",
    "    mass1 = stats[runner, 0]\n",
    "    delta_mass1 = stats[runner, 1]\n",
    "\n",
    "    for j in range(runner+1, idxs_upper[runner]):\n",
    "        mass2 = stats[j, 0]\n",
    "        if np.abs(mass2 - mass1) <= maximum_offset:\n",
    "            delta_mass2 = stats[j, 1]\n",
    "            for charge in range(iso_charge_min, iso_charge_max + 1):\n",
    "                if check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge, iso_mass_range):\n",
    "                    edges.append((runner, j))\n",
    "                    break\n",
    "\n",
    "    return edges\n",
    "\n",
    "@alphapept.performance.performance_function(compilation_mode=\"numba-multithread\")\n",
    "def edge_correlation(idx:np.ndarray, to_keep:np.ndarray, sortindex_:np.ndarray, pre_edges:np.ndarray, hill_ptrs:np.ndarray, hill_data:np.ndarray, int_data:np.ndarray, scan_idx:np.ndarray, cc_cutoff:float):\n",
    "    \"\"\"Correlates two edges and flag them it they should be kept.\n",
    "\n",
    "    Args:\n",
    "        idx (np.ndarray): Input index. Note that we are using the performance function so this is a range.\n",
    "        to_keep (np.ndarray): Array with indices which edges should be kept.\n",
    "        sortindex_ (np.ndarray): Sortindex to access the hills from stats.\n",
    "        pre_edges (np.ndarray): Array with pre edges.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        scan_idx (np.ndarray): Array containing the scan index for a centroid.\n",
    "        cc_cutoff (float): Cutoff value for what is considered correlating.\n",
    "    \"\"\"\n",
    "    edge = pre_edges[idx,:]\n",
    "\n",
    "    y = sortindex_[edge[0]]\n",
    "    start = hill_ptrs[y]\n",
    "    end = hill_ptrs[y + 1]\n",
    "    idx_ = hill_data[start:end]\n",
    "    int_ = int_data[idx_]\n",
    "    scans_ = scan_idx[idx_]\n",
    "\n",
    "    con = sortindex_[edge[1]]\n",
    "    start = hill_ptrs[con]\n",
    "    end = hill_ptrs[con + 1]\n",
    "    idx_2 = hill_data[start:end]\n",
    "    int_2 = int_data[idx_2]\n",
    "    scans_2 = scan_idx[idx_2]\n",
    "\n",
    "    if correlate(scans_, scans_2, int_, int_2) > cc_cutoff:\n",
    "        to_keep[idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import networkx as nx\n",
    "\n",
    "def get_pre_isotope_patterns(stats:np.ndarray, idxs_upper:np.ndarray, sortindex_:np.ndarray, hill_ptrs:np.ndarray, hill_data:np.ndarray, int_data:np.ndarray, scan_idx:np.ndarray, maximum_offset:float, iso_charge_min:int=1, iso_charge_max:int=6, iso_mass_range:float=5, cc_cutoff:float=0.6)->list:\n",
    "    \"\"\"Function to extract pre isotope patterns.\n",
    "\n",
    "    Args:\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        idxs_upper (np.ndarray): Upper index for comparison.\n",
    "        sortindex_ (np.ndarray): Sortindex to access the hills from stats.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        scan_idx (np.ndarray): Array containing the scan index for a centroid.\n",
    "        maximum_offset (float): Maximum offset when matching.\n",
    "        iso_charge_min (int, optional): Minimum isotope charge. Defaults to 1.\n",
    "        iso_charge_max (int, optional): Maximum isotope charge. Defaults to 6.\n",
    "        iso_mass_range (float, optional): Mass search range. Defaults to 5.\n",
    "        cc_cutoff (float, optional): Correlation cutoff. Defaults to 0.6.\n",
    "\n",
    "    Returns:\n",
    "        list: List of pre isotope patterns.\n",
    "    \"\"\"    \n",
    "    pre_edges = []\n",
    "\n",
    "    # Step 1\n",
    "    for runner in range(len(stats)):\n",
    "        pre_edges.extend(extract_edge(stats, idxs_upper, runner, idxs_upper[runner], maximum_offset, iso_charge_min, iso_charge_max, iso_mass_range))\n",
    "\n",
    "    to_keep = np.zeros(len(pre_edges), dtype='int')\n",
    "    pre_edges = np.array(pre_edges)\n",
    "    edge_correlation(range(len(to_keep)), to_keep, sortindex_, pre_edges, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)\n",
    "    edges = pre_edges[to_keep.nonzero()]\n",
    "\n",
    "    G2 = nx.Graph()\n",
    "    for i in range(len(edges)):\n",
    "        G2.add_edge(edges[i][0], edges[i][1])\n",
    "\n",
    "    pre_isotope_patterns = [\n",
    "        sorted(list(c))\n",
    "        for c in sorted(nx.connected_components(G2), key=len, reverse=True)\n",
    "    ]\n",
    "\n",
    "    return pre_isotope_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Isotope Patterns\n",
    "\n",
    "The extracted pre-isotope patterns may not be consistent because their pair-wise mass differences may not correspond to the same charge. To extract isotope patterns from pre-isotope patterns, we need to ensure that they are consistent for a single charge. \n",
    "\n",
    "To do this, we start with the 100 most intense peaks from a pre-isotope pattern to be used as a seed. For each seed and charge we then try to extract the longest consistent isotope pattern. To check wheter a hill is consistent with the seed we employ a modified checking criterion (`check_isotope_pattern_directed`) to be as follows:\n",
    "\n",
    "$$\\left | m-m_j-\\frac{j\\Delta M}{z} \\right |\\leq \\sqrt{\\left ( \\frac{\\Delta S}{z}  \\right )^{2}+\\Delta {m}^{2} +\\Delta {m_{j}}^{2}}$$\n",
    "\n",
    "Here $m$ is the mass of a seed peak, and $m_{j}$ refers to a peak relative to the seed. $j$ refers to the peaks to the left or right (negative or positive index) within the pattern. $j$ needs to run over consecutive values so that gaps are not allowed. Besides this consistency check, two hills are also checked to have a cosine correlation of at least 0.6.\n",
    "\n",
    "Programmatically, this is implemented in `grow_trail` and `grow`. These function uses a recursive approach that adds matching hills to the seed on the left and right side until no more hills can be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from numba.typed import List\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def check_isotope_pattern_directed(mass1:float, mass2:float, delta_mass1:float, delta_mass2:float, charge:int, index:int, iso_mass_range:float)->bool:\n",
    "    \"\"\"Check if two masses could belong to the same isotope pattern.\n",
    "\n",
    "    Args:\n",
    "        mass1 (float): Mass of the first pattern.\n",
    "        mass2 (float): Mass of the second pattern.\n",
    "        delta_mass1 (float): Delta mass of the first pattern.\n",
    "        delta_mass2 (float): Delta mass of the second pattern.\n",
    "        charge (int): Charge.\n",
    "        index (int): Index (unused).\n",
    "        iso_mass_range (float): Isotope mass ranges.\n",
    "    Returns:\n",
    "        bool: Flag if two isotope patterns belong together.\n",
    "    \"\"\"\n",
    "    delta_mass1 = delta_mass1 * iso_mass_range\n",
    "    delta_mass2 = delta_mass2 * iso_mass_range\n",
    "\n",
    "    left_side = np.abs(mass1 - mass2 - index * DELTA_M / charge)\n",
    "    right_side = np.sqrt((DELTA_S / charge) ** 2 + delta_mass1 ** 2 + delta_mass2 ** 2)\n",
    "\n",
    "    return left_side <= right_side\n",
    "\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def grow(trail:List, seed:int, direction:int, relative_pos:int, index:int, stats:np.ndarray, pattern:np.ndarray, charge:int, iso_mass_range:float, sortindex_:np.ndarray, hill_ptrs:np.ndarray, hill_data:np.ndarray, int_data:np.ndarray, scan_idx:np.ndarray, cc_cutoff:float)->List:\n",
    "    \"\"\"Grows isotope pattern based on a seed and direction.\n",
    "\n",
    "    Args:\n",
    "        trail (List): List of hills belonging to a pattern.\n",
    "        seed (int): Seed position.\n",
    "        direction (int): Direction in which to grow the trail\n",
    "        relative_pos (int): Relative position.\n",
    "        index (int): Index.\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        pattern (np.ndarray): Isotope pattern.\n",
    "        charge (int): Charge.\n",
    "        iso_mass_range (float): Mass range for checking isotope patterns.\n",
    "        sortindex_ (np.ndarray): Sortindex to access the hills from stats.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        scan_idx (np.ndarray): Array containing the scan index for a centroid.\n",
    "        cc_cutoff (float): Cutoff value for what is considered correlating.\n",
    "\n",
    "    Returns:\n",
    "        List: List of hills belonging to a pattern.\n",
    "    \"\"\"    \n",
    "    x = pattern[seed]  # This is the seed\n",
    "    mass1 = stats[x,0]\n",
    "    delta_mass1 = stats[x,1]\n",
    "\n",
    "    k = sortindex_[x]\n",
    "    start = hill_ptrs[k]\n",
    "    end = hill_ptrs[k + 1]\n",
    "    idx_ = hill_data[start:end]\n",
    "    int_ = int_data[idx_]\n",
    "    scans_ = scan_idx[idx_]\n",
    "\n",
    "    growing = True\n",
    "\n",
    "    while growing:\n",
    "        if direction == 1:\n",
    "            if seed + relative_pos == len(pattern):\n",
    "                growing = False\n",
    "                break\n",
    "        else:\n",
    "            if seed + relative_pos < 0:\n",
    "                growing = False\n",
    "                break\n",
    "\n",
    "        y = pattern[seed + relative_pos]  # This is a reference peak\n",
    "\n",
    "        l = sortindex_[y]\n",
    "\n",
    "        mass2 = stats[y,0]\n",
    "        delta_mass2 = stats[y,1]\n",
    "\n",
    "        start = hill_ptrs[l]\n",
    "        end = hill_ptrs[l + 1]\n",
    "        idx_ = hill_data[start:end]\n",
    "        int_2 = int_data[idx_]\n",
    "        scans_2 = scan_idx[idx_]\n",
    "\n",
    "        if correlate(scans_, scans_2, int_, int_2) > cc_cutoff:\n",
    "            if check_isotope_pattern_directed(mass1, mass2, delta_mass1, delta_mass2, charge, -direction * index, iso_mass_range):\n",
    "                if direction == 1:\n",
    "                    trail.append(y)\n",
    "                else:\n",
    "                    trail.insert(0, y)\n",
    "                index += (\n",
    "                    1\n",
    "                )  # Greedy matching: Only one edge for a specific distance, will not affect the following matches\n",
    "\n",
    "        delta_mass = np.abs(mass1 - mass2)\n",
    "\n",
    "        if (delta_mass > (DELTA_M+DELTA_S) * index):  # the pattern is sorted so there is a maximum to look back\n",
    "            break\n",
    "\n",
    "        relative_pos += direction\n",
    "\n",
    "    return trail\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def grow_trail(seed:int, pattern:np.ndarray, stats:np.ndarray, charge:int, iso_mass_range:float, sortindex_:np.ndarray, hill_ptrs:np.ndarray, hill_data:np.ndarray, int_data:np.ndarray, scan_idx:np.ndarray, cc_cutoff:float)->List:\n",
    "    \"\"\"Wrapper to grow an isotope pattern to the left and right side.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed position.\n",
    "        pattern (np.ndarray): Isotope pattern.\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        charge (int): Charge.\n",
    "        iso_mass_range (float): Mass range for checking isotope patterns.\n",
    "        sortindex_ (np.ndarray): Sortindex to access the hills from stats.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        scan_idx (np.ndarray): Array containing the scan index for a centroid.\n",
    "        cc_cutoff (float): Cutoff value for what is considered correlating.\n",
    "\n",
    "    Returns:\n",
    "        List: Isotope pattern.\n",
    "    \"\"\"\n",
    "    x = pattern[seed]\n",
    "    trail = List()\n",
    "    trail.append(x)\n",
    "    trail = grow(trail, seed, -1, -1, 1, stats, pattern, charge, iso_mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)\n",
    "    trail = grow(trail, seed, 1, 1, 1, stats, pattern, charge, iso_mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)\n",
    "\n",
    "    return trail\n",
    "\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def get_trails(seed:int, pattern:np.ndarray, stats:np.ndarray, charge_range:List, iso_mass_range:float, sortindex_:np.ndarray, hill_ptrs:np.ndarray, hill_data:np.ndarray, int_data:np.ndarray, scan_idx:np.ndarray, cc_cutoff:float)->List:\n",
    "    \"\"\"Wrapper to extract trails for a given charge range.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed index.\n",
    "        pattern (np.ndarray): Pre isotope pattern.\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        charge_range (List): Charge range.\n",
    "        iso_mass_range (float): Mass range for checking isotope patterns.\n",
    "        sortindex_ (np.ndarray): Sortindex to access the hills from stats.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        scan_idx (np.ndarray): Array containing the scan index for a centroid.\n",
    "        cc_cutoff (float): Cutoff value for what is considered correlating.\n",
    "\n",
    "    Returns:\n",
    "        List: Trail of consistent hills.\n",
    "    \"\"\"\n",
    "    trails = []\n",
    "    for charge in charge_range:\n",
    "        trail = grow_trail(seed, pattern, stats, charge, iso_mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)\n",
    "\n",
    "        trails.append(trail)\n",
    "\n",
    "    return trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def plot_pattern(pattern:np.ndarray, sorted_hills:np.ndarray, centroids:np.ndarray, hill_data:np.ndarray):\n",
    "    \"\"\"Helper function to plot a pattern.\n",
    "\n",
    "    Args:\n",
    "        pattern (np.ndarray): Pre isotope pattern.\n",
    "        sorted_hills (np.ndarray): Hills, sorted.\n",
    "        centroids (np.ndarray): 1D Array containing the masses of the centroids.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "    \"\"\"\n",
    "    f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,10))\n",
    "    centroid_dtype = [(\"mz\", float), (\"int\", float), (\"scan_no\", int), (\"rt\", float)]\n",
    "\n",
    "    mzs = []\n",
    "    rts = []\n",
    "    ints = []\n",
    "    for entry in pattern:\n",
    "        hill = sorted_hills[entry]\n",
    "        hill_data = np.array([centroids[_[0]][_[1]] for _ in hill], dtype=centroid_dtype)\n",
    "\n",
    "        int_profile = hill_data[\"int\"]\n",
    "        ax1.plot(hill_data[\"rt\"], hill_data[\"int\"])\n",
    "        ax2.scatter(hill_data[\"rt\"], hill_data[\"mz\"], s = hill_data[\"int\"]/5e5 )\n",
    "\n",
    "\n",
    "    ax1.set_title('Pattern')\n",
    "    ax1.set_xlabel('RT (min)')\n",
    "    ax1.set_ylabel('Intensity')\n",
    "\n",
    "    ax2.set_xlabel('RT (min)')\n",
    "    ax2.set_ylabel('m/z')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def get_minpos(y:np.ndarray, iso_split_level:float)->List:\n",
    "    \"\"\"Function to get a list of minima in a trace.\n",
    "    A minimum is returned if the ratio of lower of the surrounding maxima to the minimum is larger than the splitting factor.\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): Input array.\n",
    "        iso_split_level (float): Isotope split level.\n",
    "\n",
    "    Returns:\n",
    "        List: List with min positions.\n",
    "    \"\"\"\n",
    "    minima = get_local_minima(y)\n",
    "    minima_list = List()\n",
    "\n",
    "    for minpos in minima:\n",
    "\n",
    "        minval = y[minpos]\n",
    "\n",
    "        left_max = (y[:minpos]).max()\n",
    "        right_max = (y[minpos:]).max()\n",
    "\n",
    "        minimum_max = min(left_max, right_max)\n",
    "\n",
    "        if minimum_max / minval >= iso_split_level:\n",
    "            minima_list.append(minpos)\n",
    "\n",
    "    return minima_list\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def get_local_minima(y:np.ndarray)->List:\n",
    "    \"\"\"Function to return all local minima of a array\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): Input array.\n",
    "\n",
    "    Returns:\n",
    "        List: List with indices to minima.\n",
    "    \"\"\"\n",
    "    minima = List()\n",
    "    for i in range(1, len(y) - 1):\n",
    "        if is_local_minima(y, i):\n",
    "            minima.append(i)\n",
    "    return minima\n",
    "\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def is_local_minima(y:np.ndarray, i:int)->bool:\n",
    "    \"\"\"Check if position is a local minima. \n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): Input array.\n",
    "        i (int): Position to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: Flag if position is minima or not.\n",
    "    \"\"\"\n",
    "    return (y[i - 1] > y[i]) & (y[i + 1] > y[i])\n",
    "\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def truncate(array:np.ndarray, intensity_profile:np.ndarray, seedpos:int, iso_split_level:float)->np.ndarray:\n",
    "    \"\"\"Function to truncate an intensity profile around its seedposition.\n",
    "\n",
    "    Args:\n",
    "        array (np.ndarray):  Input array.\n",
    "        intensity_profile (np.ndarray): Intensities for the input array.\n",
    "        seedpos (int): Seedposition.\n",
    "        iso_split_level (float): Split level.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Truncated array.\n",
    "    \"\"\"\n",
    "    minima = int_list_to_array(get_minpos(intensity_profile, iso_split_level))\n",
    "\n",
    "    if len(minima) > 0:\n",
    "        left_minima = minima[minima < seedpos]\n",
    "        right_minima = minima[minima > seedpos]\n",
    "\n",
    "        # If the minimum is smaller than the seed\n",
    "        if len(left_minima) > 0:\n",
    "            minpos = left_minima[-1]\n",
    "        else:\n",
    "            minpos = 0\n",
    "\n",
    "        if len(right_minima) > 0:\n",
    "            maxpos = right_minima[0]\n",
    "        else:\n",
    "            maxpos = len(array)\n",
    "\n",
    "        array = array[minpos:maxpos+1]\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_get_minpos():\n",
    "    \"\"\"\n",
    "    Generate an intensity profile with local minima\n",
    "    Check that the minima are found\n",
    "\n",
    "    \"\"\"\n",
    "    intensity_profile = np.ones(20) * 10\n",
    "\n",
    "    minima_ref = [3, 7, 10, 17]\n",
    "\n",
    "    for minimum in minima_ref:\n",
    "        intensity_profile[minimum] = 1\n",
    "\n",
    "    minima = get_minpos(intensity_profile, 2)\n",
    "\n",
    "    minima_list = [_ for _ in minima]\n",
    "\n",
    "    assert minima_list == minima_ref\n",
    "\n",
    "test_get_minpos() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolating Isotope_patterns\n",
    "\n",
    "The extraction of the longest consistent isotope pattern is implemented in `isolate_isotope_pattern`. Here, three additional checks for an isotope pattern are implemented. \n",
    "\n",
    "The first one is `truncate`. Here, one checks the seed position, whether it has a minimum to its left or right side. If a minimum is found, the isotope pattern is cut off at this position.\n",
    "\n",
    "The second one is a mass filter. If the seed has a mass of smaller than 1000, the intensity maximum is detected, and all smaller masses are discarded. This reflects the averagine distribution for small masses where no minimum on the left side can be found.\n",
    "\n",
    "The third one is `check_averagine` that relies on `pattern_to_mz` and `cosine_averagine`. It is used to ensure that the extracted isotope pattern has a cosine correlation of the averagine isotope pattern of the same mass of at least 0.6.\n",
    "\n",
    "After the longest consistent isotope pattern is found, the hills are removed from the pre-isotope pattern, and the process is repeated until no more isotope patterns can be extracted from the pre-isotope patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from alphapept.chem import mass_to_dist\n",
    "from alphapept.constants import averagine_aa, isotopes, Isotope\n",
    "from numba.typed import Dict\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def check_averagine(stats:np.ndarray, pattern:np.ndarray, charge:int, averagine_aa:Dict, isotopes:Dict)->float:\n",
    "    \"\"\"Function to compare a pattern to an averagine model.\n",
    "\n",
    "    Args:\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        pattern (np.ndarray): Isotope pattern.\n",
    "        charge (int): Charge.\n",
    "        averagine_aa (Dict): Dict containing averagine masses.\n",
    "        isotopes (Dict): Dict containing isotopes.\n",
    "\n",
    "    Returns:\n",
    "        float: Averagine correlation.\n",
    "    \"\"\"\n",
    "    masses, intensity = pattern_to_mz(stats, pattern, charge)\n",
    "\n",
    "    spec_one = np.floor(masses).astype(np.int64)\n",
    "    int_one = intensity\n",
    "\n",
    "    spec_two, int_two = mass_to_dist(np.min(masses), averagine_aa, isotopes) # maybe change to no rounded version\n",
    "\n",
    "    spec_two = np.floor(spec_two).astype(np.int64)\n",
    "\n",
    "    return cosine_averagine(int_one, int_two, spec_one, spec_two)\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def pattern_to_mz(stats:np.ndarray, pattern:np.ndarray, charge:int)-> (np.ndarray, np.ndarray):\n",
    "    \"\"\"Function to calculate masses and intensities from pattern for a given charge.\n",
    "\n",
    "    Args:\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        pattern (np.ndarray): Isotope pattern.\n",
    "        charge (int): Charge of the pattern.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: masses\n",
    "        np.ndarray: intensity\n",
    "    \"\"\"\n",
    "\n",
    "    mzs = np.zeros(len(pattern))\n",
    "    ints = np.zeros(len(pattern))\n",
    "\n",
    "    for i in range(len(pattern)):\n",
    "        entry = pattern[i]\n",
    "        mzs[i] = mz_to_mass(stats[entry,0], charge)\n",
    "        ints[i] = stats[entry,2]\n",
    "\n",
    "    sortindex = np.argsort(mzs)\n",
    "\n",
    "    masses = mzs[sortindex]\n",
    "    intensity = ints[sortindex]\n",
    "\n",
    "    return masses, intensity\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def cosine_averagine(int_one:np.ndarray, int_two:np.ndarray, spec_one:np.ndarray, spec_two:np.ndarray)-> float:\n",
    "    \"\"\"Calculate the cosine correlation of two hills. \n",
    "\n",
    "    Args:\n",
    "        int_one (np.ndarray): Intensity of the first hill.\n",
    "        int_two (np.ndarray): Intensity of the second hill.\n",
    "        spec_one (np.ndarray): Scan numbers of the first hill.\n",
    "        spec_two (np.ndarray): Scan numbers of the second hill.\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine \n",
    "    \"\"\"\n",
    "\n",
    "    min_one, max_one = spec_one[0], spec_one[-1]\n",
    "    min_two, max_two = spec_two[0], spec_two[-1]\n",
    "\n",
    "    min_s = np.min(np.array([min_one, min_two]))\n",
    "    max_s = np.max(np.array([max_one, max_two]))\n",
    "\n",
    "    int_one_scaled = np.zeros(int(max_s - min_s + 1))\n",
    "    int_two_scaled = np.zeros(int(max_s - min_s + 1))\n",
    "\n",
    "    int_one_scaled[spec_one - min_s] = int_one\n",
    "    int_two_scaled[spec_two - min_s] = int_two\n",
    "\n",
    "    corr = np.sum(int_one_scaled * int_two_scaled) / np.sqrt(\n",
    "        np.sum(int_one_scaled ** 2) * np.sum(int_two_scaled ** 2)\n",
    "    )\n",
    "\n",
    "    return corr\n",
    "\n",
    "\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def int_list_to_array(numba_list:List)->np.ndarray:\n",
    "    \"\"\"Numba compatbilte function to convert a numba list with integers to a numpy array\n",
    "\n",
    "    Args:\n",
    "        numba_list (List): Input numba-typed List.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Output numpy array.\n",
    "    \"\"\"\n",
    "    array = np.zeros(len(numba_list), dtype=np.int64)\n",
    "\n",
    "    for i in range(len(array)):\n",
    "\n",
    "        array[i] = numba_list[i]\n",
    "\n",
    "    return array\n",
    "\n",
    "M_PROTON = mass_dict['Proton']\n",
    "\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def mz_to_mass(mz:float, charge:int)->float:\n",
    "    \"\"\"Function to calculate the mass from a mz value.\n",
    "\n",
    "    Args:\n",
    "        mz (float): M/z\n",
    "        charge (int): Charge.\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: When a negative charge is used.\n",
    "\n",
    "    Returns:\n",
    "        float: mass\n",
    "    \"\"\"\n",
    "    if charge < 0:\n",
    "        raise NotImplementedError(\"Negative Charges not implemented.\")\n",
    "\n",
    "    mass = mz * charge - charge * M_PROTON\n",
    "\n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "if False:\n",
    "\n",
    "\n",
    "    def test_truncate():\n",
    "        \"\"\"\n",
    "        Generate an intensity profile with local minima\n",
    "        Check wheter the the profile is correctly truncated with respect to the seed\n",
    "\n",
    "        \"\"\"\n",
    "        array = np.arange(0, 20)\n",
    "        intensity_profile = np.ones(20) * 10\n",
    "        \n",
    "        iso_split_level = 1.3\n",
    "\n",
    "        minima_ref = [3, 7, 10, 17]\n",
    "\n",
    "        for minimum in minima_ref:\n",
    "            intensity_profile[minimum] = 1\n",
    "\n",
    "        seedpos = 5\n",
    "        truncated = truncate(array, intensity_profile, seedpos, iso_split_level)\n",
    "        assert np.all(truncated == np.array([3, 4, 5, 6, 7]))\n",
    "\n",
    "        seedpos = 0\n",
    "        truncated = truncate(array, intensity_profile, seedpos, iso_split_level)\n",
    "        assert np.all(truncated == np.array([0, 1, 2, 3]))\n",
    "\n",
    "        seedpos = len(array)\n",
    "        truncated = truncate(array, intensity_profile, seedpos, iso_split_level)\n",
    "        assert np.all(truncated == np.array([17, 18, 19]))\n",
    "\n",
    "    test_truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isotope Patterns\n",
    "\n",
    "The wrapper function `get_isotope_patterns` iterates over all pre_isotope_patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@alphapept.performance.compile_function(compilation_mode=\"numba\")\n",
    "def isolate_isotope_pattern(pre_pattern:np.ndarray, hill_ptrs:np.ndarray, hill_data:np.ndarray, int_data:np.ndarray, scan_idx:np.ndarray, stats:np.ndarray, sortindex_:np.ndarray, iso_mass_range:float, charge_range:List, averagine_aa:Dict, isotopes:Dict, iso_n_seeds:int, cc_cutoff:float, iso_split_level:float)->(np.ndarray, int):\n",
    "    \"\"\"Isolate isotope patterns.\n",
    "\n",
    "    Args:\n",
    "        pre_pattern (np.ndarray): Pre isotope pattern.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        scan_idx (np.ndarray): Array containing the scan index for a centroid.\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        sortindex_ (np.ndarray): Sortindex to access the hills from stats.\n",
    "        iso_mass_range (float): Mass range for checking isotope patterns.\n",
    "        charge_range (List): Charge range.\n",
    "        averagine_aa (Dict): Dict containing averagine masses.\n",
    "        isotopes (Dict): Dict containing isotopes.\n",
    "        iso_n_seeds (int): Number of seeds.\n",
    "        cc_cutoff (float): Cutoff value for what is considered correlating.\n",
    "        iso_split_level (float): Split level when isotopes are split.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with the best pattern.\n",
    "        int: Charge of the best pattern.\n",
    "    \"\"\"\n",
    "    longest_trace = 0\n",
    "    champion_trace = None\n",
    "    champion_charge = 0\n",
    "    champion_intensity = 0\n",
    "\n",
    "    # Sort patterns by mass\n",
    "\n",
    "    sortindex = np.argsort(stats[pre_pattern][:,0]) #intensity\n",
    "    sorted_pattern = pre_pattern[sortindex]\n",
    "    massindex = np.argsort(stats[sorted_pattern][:,2])[::-1][:iso_n_seeds]\n",
    "\n",
    "    # Use all the elements in the pre_pattern as seed\n",
    "\n",
    "    for seed in massindex:  # Loop through all seeds\n",
    "        seed_global = sorted_pattern[seed]\n",
    "\n",
    "        trails = get_trails(seed, sorted_pattern, stats, charge_range, iso_mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)\n",
    "\n",
    "        for index, trail in enumerate(trails):\n",
    "            if len(trail) >= longest_trace:  # Needs to be longer than the current champion\n",
    "                arr = int_list_to_array(trail)\n",
    "                intensity_profile = stats[arr][:,2]\n",
    "                seedpos = np.nonzero(arr==seed_global)[0][0]\n",
    "\n",
    "                # truncate around the seed...\n",
    "                arr = truncate(arr, intensity_profile, seedpos, iso_split_level)\n",
    "                intensity_profile = stats[arr][:,2]\n",
    "\n",
    "                # Remove lower masses:\n",
    "                # Take the index of the maximum and remove all masses on the left side\n",
    "                if charge_range[index] * stats[seed_global, 0] < 1000:\n",
    "                    maxpos = np.argmax(intensity_profile)\n",
    "                    arr = arr[maxpos:]\n",
    "                    intensity_profile = stats[arr][:,2]\n",
    "\n",
    "                if (len(arr) > longest_trace) | ((len(arr) == longest_trace) & (intensity_profile.sum() > champion_intensity)):\n",
    "                    # Averagine check\n",
    "                    cc = check_averagine(stats, arr, charge_range[index], averagine_aa, isotopes)\n",
    "                    if cc > 0.6:\n",
    "                        # Update the champion\n",
    "                        champion_trace = arr\n",
    "                        champion_charge = charge_range[index]\n",
    "                        longest_trace = len(arr)\n",
    "                        champion_intensity = intensity_profile.sum()\n",
    "\n",
    "    return champion_trace, champion_charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "if False:\n",
    "    def test_get_isotope_patterns():\n",
    "\n",
    "        test_centroids = [\n",
    "            [\n",
    "                (300, 50, 1, 1),\n",
    "                (300.501, 40, 1, 1),\n",
    "                (301.003, 30, 1, 1),\n",
    "                (301.504, 20, 1, 1),\n",
    "                (302.006, 10, 1, 1),\n",
    "            ],\n",
    "            [\n",
    "                (300, 50, 2, 2),\n",
    "                (300.501, 40, 2, 2),\n",
    "                (301.003, 30, 2, 2),\n",
    "                (301.504, 20, 2, 2),\n",
    "                (302.006, 10, 2, 2),\n",
    "            ],\n",
    "            [\n",
    "                (300, 50, 3, 3),\n",
    "                (300.501, 40, 3, 3),\n",
    "                (301.003, 30, 3, 3),\n",
    "                (301.504, 20, 3, 3),\n",
    "                (302.006, 10, 3, 3),\n",
    "            ],\n",
    "            [\n",
    "                (300, 50, 4, 4),\n",
    "                (300.501, 40, 4, 4),\n",
    "                (301.003, 30, 4, 4),\n",
    "                (301.504, 20, 4, 4),\n",
    "                (302.006, 10, 4, 4),\n",
    "            ],\n",
    "            [\n",
    "                (300, 50, 5, 5),\n",
    "                (300.501, 40, 5, 5),\n",
    "                (301.003, 30, 5, 5),\n",
    "                (301.504, 20, 5, 5),\n",
    "                (302.006, 10, 5, 5),\n",
    "            ],\n",
    "            [(400, 10, 6, 6), (401, 10, 6, 6), (402, 10, 6, 6)],\n",
    "            [(400, 10, 7, 7), (401, 10, 7, 7), (402, 10, 7, 7)],\n",
    "            [(400, 10, 8, 8), (401, 10, 8, 8), (402, 10, 8, 8)],\n",
    "            [(400, 10, 9, 9), (401, 10, 9, 9), (402, 10, 9, 9)],\n",
    "        ]\n",
    "\n",
    "        centroid_dtype = [(\"mz\", float), (\"int\", float), (\"scan_no\", int), (\"rt\", float)]\n",
    "        test_centroids_tmp = [np.array(_, dtype=centroid_dtype) for _ in test_centroids]\n",
    "\n",
    "        test_centroids = List([_ for _ in test_centroids_tmp])\n",
    "\n",
    "        test_hills = get_hills(test_centroids)\n",
    "        sorted_hills, stats, data, hill_data, hill_ptrs = get_hill_data(test_hills, test_centroids)\n",
    "        pre_patterns = get_edges(stats, data)\n",
    "\n",
    "        isotope_patterns, isotope_charges = get_isotope_patterns(pre_patterns, stats, data, averagine_aa, isotopes)\n",
    "        assert np.all(isotope_patterns[0] == np.array([0, 1, 2, 3, 4]))\n",
    "        assert isotope_charges[0] == 2\n",
    "        assert np.all(isotope_patterns[1] == np.array([5,6,7]))\n",
    "        assert isotope_charges[1] == 1\n",
    "\n",
    "    test_get_isotope_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from numba.typed import List\n",
    "from typing import Callable, Union\n",
    "\n",
    "def get_isotope_patterns(pre_isotope_patterns:list, hill_ptrs:np.ndarray, hill_data:np.ndarray, int_data:np.ndarray, scan_idx:np.ndarray, stats:np.ndarray, sortindex_:np.ndarray,  averagine_aa:Dict, isotopes:Dict, iso_charge_min:int = 1, iso_charge_max:int = 6, iso_mass_range:float = 5, iso_n_seeds:int = 100, cc_cutoff:float=0.6, iso_split_level:float = 1.3, callback:Union[Callable, None]=None) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    \"\"\"Wrapper function to iterate over pre_isotope_patterns.\n",
    "\n",
    "    Args:\n",
    "        pre_isotope_patterns (list): List of pre-isotope patterns.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        scan_idx (np.ndarray): Array containing the scan index for a centroid.\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        sortindex_ (np.ndarray): Sortindex to access the hills from stats.\n",
    "        averagine_aa (Dict): Dict containing averagine masses.\n",
    "        isotopes (Dict): Dict containing isotopes.\n",
    "        iso_charge_min (int, optional): Minimum isotope charge. Defaults to 1.\n",
    "        iso_charge_max (int, optional): Maximum isotope charge. Defaults to 6.\n",
    "        iso_mass_range (float, optional): Mass search range. Defaults to 5.\n",
    "        iso_n_seeds (int, optional): Number of isotope seeds. Defaults to 100.\n",
    "        cc_cutoff (float, optional): Cuttoff for correlation.. Defaults to 0.6.\n",
    "        iso_split_level (float, optional): Isotope split level.. Defaults to 1.3.\n",
    "        callback (Union[Callable, None], optional): Callback function for progress. Defaults to None.\n",
    "    Returns:\n",
    "        list: List of isotope patterns.\n",
    "        np.ndarray: Iso idx.\n",
    "        np.ndarray: Array containing isotope charges.\n",
    "    \"\"\"\n",
    "\n",
    "    isotope_patterns = []\n",
    "    isotope_charges = []\n",
    "\n",
    "    charge_range = List()\n",
    "\n",
    "    for i in range(iso_charge_min, iso_charge_max + 1):\n",
    "        charge_range.append(i)\n",
    "\n",
    "    isotope_patterns = []\n",
    "    isotope_charges = []\n",
    "\n",
    "    for idx, pre_pattern in enumerate(pre_isotope_patterns):\n",
    "        extract = True\n",
    "        while extract:\n",
    "            isotope_pattern, isotope_charge = isolate_isotope_pattern(np.array(pre_pattern), hill_ptrs, hill_data, int_data, scan_idx, stats, sortindex_, iso_mass_range, charge_range, averagine_aa, isotopes, iso_n_seeds, cc_cutoff, iso_split_level)\n",
    "            if isotope_pattern is None:\n",
    "                length = 0\n",
    "            else:\n",
    "                length = len(isotope_pattern)\n",
    "\n",
    "            if length > 1:\n",
    "                isotope_charges.append(isotope_charge)\n",
    "                isotope_patterns.append(isotope_pattern)\n",
    "\n",
    "                pre_pattern = [_ for _ in pre_pattern if _ not in isotope_pattern]\n",
    "\n",
    "                if len(pre_pattern) <= 1:\n",
    "                    extract = False\n",
    "            else:\n",
    "                extract = False\n",
    "\n",
    "\n",
    "        if callback:\n",
    "            callback((idx+1)/len(pre_isotope_patterns))\n",
    "\n",
    "\n",
    "    iso_patterns = np.zeros(sum([len(_) for _ in isotope_patterns]), dtype=np.int64)\n",
    "\n",
    "    iso_idx = np.zeros(len(isotope_patterns)+1, dtype='int')\n",
    "\n",
    "\n",
    "    start = 0\n",
    "    for idx, _ in enumerate(isotope_patterns):\n",
    "        iso_patterns[start:start+len(_)] = _\n",
    "        start += len(_)\n",
    "        iso_idx[idx+1] = start\n",
    "\n",
    "\n",
    "\n",
    "    return iso_patterns, iso_idx, np.array(isotope_charges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@alphapept.performance.performance_function(compilation_mode=\"numba-multithread\")\n",
    "def report_(idx:np.ndarray, isotope_charges:list, isotope_patterns:list, iso_idx:np.ndarray, stats:np.ndarray, sortindex_:np.ndarray, hill_ptrs:np.ndarray, hill_data:np.ndarray, int_data:np.ndarray, rt_:np.ndarray, rt_idx:np.ndarray, results:np.ndarray, lookup_idx:np.ndarray):\n",
    "    \"\"\"Function to extract summary statstics from a list of isotope patterns and charges.\n",
    "    \n",
    "    MS1 feature intensity estimation. For each isotope envelope we interpolate the signal over the retention time\n",
    "    range. All isotope enevelopes are summed up together to estimate the peak sahpe\n",
    "    \n",
    "    Lastly, we report three estimates for the intensity:\n",
    "    \n",
    "    - ms1_int_sum_apex: The intensity at the peak of the summed signal.\n",
    "    - ms1_int_sum_area: The area of the summed signal\n",
    "    - ms1_int_max_apex: The intensity at the peak of the most intense isotope envelope  \n",
    "    - ms1_int_max_area: The area of the the most intense isotope envelope  \n",
    "    \n",
    "    Args:\n",
    "        idx (np.ndarray): Input index. Note that we are using the performance function so this is a range.\n",
    "        isotope_patterns (list): List containing isotope patterns (indices to hills).\n",
    "        isotope_charges (list): List with charges assigned to the isotope patterns.\n",
    "        iso_idx (np.ndarray): Index to isotope pattern.\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        sortindex_ (np.ndarray): Sortindex to access the hills from stats.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "        int_data (np.ndarray): Array containing the intensity to each centroid.\n",
    "        rt_ (np.ndarray): Array with retention time information for each scan.\n",
    "        rt_idx (np.ndarray): Lookup array to match centroid idx to rt.\n",
    "        results (np.ndarray): Recordarray with isotope pattern summary statistics.\n",
    "        lookup_idx (np.ndarray): Lookup array for each centroid.\n",
    "    \"\"\"\n",
    "    pattern = isotope_patterns[iso_idx[idx]:iso_idx[idx+1]]\n",
    "    isotope_data = stats[pattern]\n",
    "\n",
    "    mz = np.min(isotope_data[:, 0])\n",
    "    mz_std = np.mean(isotope_data[:, 1])\n",
    "    charge = isotope_charges[idx]\n",
    "    mass = mz_to_mass(mz, charge)\n",
    "    int_max_idx = np.argmax(isotope_data[:, 2])\n",
    "    mz_most_abundant = isotope_data[:, 0][int_max_idx]\n",
    "\n",
    "    int_max = isotope_data[:,2][int_max_idx]\n",
    "\n",
    "    rt_start = isotope_data[int_max_idx, 4] # This is the start of the most abundant trace\n",
    "    rt_end = isotope_data[int_max_idx, 5]\n",
    "\n",
    "    rt_min_ = min(isotope_data[:, 4])\n",
    "    rt_max_ = max(isotope_data[:, 5])\n",
    "\n",
    "    rt_range = np.linspace(rt_min_, rt_max_, 100) #TODO this is a fixed value - is there an optimum?\n",
    "    \n",
    "    trace_sum = np.zeros_like(rt_range)\n",
    "    \n",
    "    most_intense_pattern = -np.inf\n",
    "\n",
    "    for i, k in enumerate(pattern):\n",
    "        x = sortindex_[k]\n",
    "\n",
    "        start = hill_ptrs[x]\n",
    "        end = hill_ptrs[x + 1]\n",
    "        idx_ = hill_data[start:end]\n",
    "        int_ = int_data[idx_]\n",
    "        rts = rt_[rt_idx[idx_]]\n",
    "        \n",
    "        lookup_idx[idx_, 0] = idx  \n",
    "        lookup_idx[idx_, 1] = i  \n",
    "\n",
    "        interpolation = np.interp(rt_range, rts, int_)\n",
    "\n",
    "        #Filter\n",
    "\n",
    "        interpolation[:(rt_range < rts[0]).sum()] = 0\n",
    "\n",
    "        right_cut = (rt_range > rts[-1]).sum()\n",
    "        if right_cut > 0:\n",
    "            interpolation[-right_cut:]= 0\n",
    "\n",
    "        trace_sum += interpolation\n",
    "        \n",
    "        if int_.sum() > most_intense_pattern:\n",
    "            most_intense_pattern = int_.sum()\n",
    "            ms1_int_max_apex = int_.max()\n",
    "            ms1_int_max_area = np.trapz(int_, rts)\n",
    "        \n",
    "\n",
    "    rt_apex_idx = trace_sum.argmax()\n",
    "    rt_apex = rt_range[rt_apex_idx]\n",
    "\n",
    "    trace = trace_sum\n",
    "    half_max = trace.max()/2\n",
    "\n",
    "    if rt_apex_idx == 0:\n",
    "        left_apex = 0\n",
    "    else:\n",
    "        left_apex = np.abs(trace[:rt_apex_idx]-half_max).argmin()\n",
    "    right_apex = np.abs(trace[rt_apex_idx:]-half_max).argmin()+rt_apex_idx\n",
    "\n",
    "    ms1_int_sum_apex = trace_sum[rt_apex_idx]\n",
    "    \n",
    "    fwhm = rt_range[right_apex] - rt_range[left_apex]\n",
    "\n",
    "    n_isotopes = len(pattern)\n",
    "\n",
    "    rt_cutoff = 0.95 #5%\n",
    "    if rt_apex_idx == 0:\n",
    "        rt_min_idx = 0\n",
    "    else:\n",
    "        rt_min_idx = np.abs(trace[:rt_apex_idx]-trace.max()*(1-rt_cutoff)).argmin()\n",
    "    rt_max_idx = np.abs(trace[rt_apex_idx:]-trace.max()*(1-rt_cutoff)).argmin()+rt_apex_idx\n",
    "    \n",
    "\n",
    "    #plt.xlabel('rt')\n",
    "    #plt.ylabel('int')\n",
    "    #plt.show()\n",
    "    #plt.plot(rt_range, trace_sum)\n",
    "\n",
    "    #plt.plot([rt_range[left_apex], rt_range[right_apex]], [(trace[left_apex] + trace[right_apex])/2]*2, 'k:')\n",
    "\n",
    "    #plt.plot(rt_range[rt_apex_idx], trace[rt_apex_idx], 'k*')\n",
    "    #plt.plot(rt_range[rt_min_idx], trace[rt_min_idx], 'k*')\n",
    "    #plt.plot(rt_range[rt_max_idx], trace[rt_max_idx], 'k*')\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    rt_start = rt_range[rt_min_idx]\n",
    "    rt_end = rt_range[rt_max_idx]\n",
    "\n",
    "    ms1_int_sum_area = np.trapz(trace_sum[rt_min_idx:rt_max_idx], rt_range[rt_min_idx:rt_max_idx])\n",
    "    \n",
    "\n",
    "    results[idx,:] = np.array([mz, mz_std, mz_most_abundant, charge, rt_start, rt_apex, rt_end, fwhm, n_isotopes, mass, ms1_int_sum_apex, ms1_int_sum_area, ms1_int_max_apex, ms1_int_max_area])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "\n",
    "def feature_finder_report(query_data:dict, isotope_patterns:list, isotope_charges:list, iso_idx:np.ndarray, stats:np.ndarray, sortindex_:np.ndarray, hill_ptrs:np.ndarray, hill_data:np.ndarray)->pd.DataFrame:\n",
    "    \"\"\"Creates a report dataframe with summary statistics of the found isotope patterns.\n",
    "\n",
    "    Args:\n",
    "        query_data (dict): Data structure containing the query data.\n",
    "        isotope_patterns (list): List containing isotope patterns (indices to hills).\n",
    "        isotope_charges (list): List with charges assigned to the isotope patterns.\n",
    "        iso_idx (np.ndarray): Index to the isotope pattern.\n",
    "        stats (np.ndarray): Stats array that contains summary statistics of hills.\n",
    "        sortindex_ (np.ndarray): Sortindex to access the hills from stats.\n",
    "        hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\n",
    "        hill_data (np.ndarray): Array containing the indices to hills.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with isotope pattern summary statistics.\n",
    "    \"\"\"\n",
    "    rt_ = np.array(query_data['rt_list_ms1'])\n",
    "    indices_ = np.array(query_data['indices_ms1'])\n",
    "    mass_data = np.array(query_data['mass_list_ms1'])\n",
    "    rt_idx = np.searchsorted(indices_, np.arange(len(mass_data)), side='right') - 1\n",
    "    \n",
    "    lookup_idx= np.zeros((len(mass_data),2), dtype=np.int)-1\n",
    "\n",
    "    int_data = np.array(query_data['int_list_ms1'])\n",
    "\n",
    "    results = np.zeros((len(isotope_charges), 14))\n",
    "\n",
    "    report_(range(len(isotope_charges)), isotope_charges, isotope_patterns, iso_idx, stats, sortindex_, hill_ptrs, hill_data, int_data, rt_, rt_idx, results, lookup_idx)\n",
    "\n",
    "    df = pd.DataFrame(results, columns = ['mz','mz_std','mz_most_abundant','charge','rt_start','rt_apex','rt_end','fwhm','n_isotopes','mass', 'ms1_int_sum_apex', 'ms1_int_sum_area', 'ms1_int_max_apex', 'ms1_int_max_area'])\n",
    "\n",
    "    df.sort_values(['rt_start','mz'])\n",
    "\n",
    "    return df, lookup_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Output\n",
    "\n",
    "For each feature that is found we extract summary statistics and put it in tabular form to be used as as pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "For quality control reasons we also employ a function to plot a feature in its local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Feature Finder\n",
    "\n",
    "To utilize the command-line Feature Finder from Bruker `4DFF-3.13` - `uff-cmdline2.exe`, we call it via a subprocess and wait until completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import subprocess\n",
    "import os\n",
    "import platform\n",
    "\n",
    "\n",
    "def extract_bruker(file:str, base_dir:str = \"ext/bruker/FF\", config:str = \"proteomics_4d.config\"):\n",
    "    \"\"\"Call Bruker Feautre Finder via subprocess.\n",
    "\n",
    "    Args:\n",
    "        file (str): Filename for feature finding.\n",
    "        base_dir (str, optional): Base dir where the feature finder is stored.. Defaults to \"ext/bruker/FF\".\n",
    "        config (str, optional): Config file for feature finder. Defaults to \"proteomics_4d.config\".\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: Unsupported operating system.\n",
    "        FileNotFoundError: Feature finder not found.\n",
    "        FileNotFoundError: Config file not found.\n",
    "        FileNotFoundError: Feature file not found.\n",
    "    \"\"\"    \n",
    "    feature_path = file + '/'+ os.path.split(file)[-1] + '.features'\n",
    "\n",
    "    base_dir = os.path.join(os.path.dirname(__file__), base_dir)\n",
    "\n",
    "    operating_system = platform.system()\n",
    "\n",
    "    if operating_system == 'Linux':\n",
    "        ff_dir = os.path.join(base_dir, 'linux64','uff-cmdline2')\n",
    "        logging.info('Using Linux FF')\n",
    "    elif operating_system == 'Windows':\n",
    "        ff_dir = os.path.join(base_dir, 'win64','uff-cmdline2.exe')\n",
    "        logging.info('Using Windows FF')\n",
    "    else:\n",
    "        raise NotImplementedError(f\"System {operating_system} not supported.\")\n",
    "\n",
    "    if os.path.exists(feature_path):\n",
    "        return feature_path\n",
    "    else:\n",
    "        if not os.path.isfile(ff_dir):\n",
    "            raise FileNotFoundError(f'Bruker feature finder cmd not found here {ff_dir}.')\n",
    "\n",
    "        config_path = base_dir + '/'+ config\n",
    "\n",
    "        if not os.path.isfile(config_path):\n",
    "            raise FileNotFoundError(f'Config file not found here {config_path}.')\n",
    "\n",
    "        if operating_system == 'Windows':\n",
    "            FF_parameters = [ff_dir,'--ff 4d',f'--readconfig \"{config_path}\"', f'--analysisDirectory \"{file}\"']\n",
    "\n",
    "            process = subprocess.Popen(' '.join(FF_parameters), stdout=subprocess.PIPE)\n",
    "            for line in iter(process.stdout.readline, b''):\n",
    "                logtxt = line.decode('utf8')\n",
    "                logging.info(logtxt[48:].rstrip()) #Remove logging info from FF\n",
    "        elif operating_system == 'Linux':\n",
    "            FF_parameters = [\n",
    "                ff_dir,\n",
    "                '--ff',\n",
    "                '4d',\n",
    "                '--readconfig',\n",
    "                config_path,\n",
    "                '--analysisDirectory',\n",
    "                file\n",
    "            ]\n",
    "            process = subprocess.run(FF_parameters, stdout=subprocess.PIPE)\n",
    "\n",
    "        if os.path.exists(feature_path):\n",
    "            return feature_path\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Feature file {feature_path} does not exist.\")\n",
    "\n",
    "\n",
    "import sqlalchemy as db\n",
    "\n",
    "def convert_bruker(feature_path:str)->pd.DataFrame:\n",
    "    \"\"\"Reads feature table and converts to feature table to be used with AlphaPept.\n",
    "\n",
    "    Args:\n",
    "        feature_path (str): Path to the feature file from Bruker FF (.features-file).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing features information.\n",
    "    \"\"\"\n",
    "    engine_featurefile = db.create_engine('sqlite:///{}'.format(feature_path))\n",
    "    feature_table = pd.read_sql_table('LcTimsMsFeature', engine_featurefile)\n",
    "    feature_cluster_mapping = pd.read_sql_table('FeatureClusterMapping', engine_featurefile)\n",
    "    from alphapept.constants import mass_dict\n",
    "\n",
    "    M_PROTON = mass_dict['Proton']\n",
    "    feature_table['Mass'] = feature_table['MZ'].values * feature_table['Charge'].values - feature_table['Charge'].values*M_PROTON\n",
    "    feature_table = feature_table.rename(columns={\"MZ\": \"mz\",\"Mass\": \"mass\", \"RT\": \"rt_apex\", \"RT_lower\":\"rt_start\", \"RT_upper\":\"rt_end\", \"Mobility\": \"mobility\", \"Mobility_lower\": \"mobility_lower\", \"Mobility_upper\": \"mobility_upper\", \"Charge\":\"charge\",\"Intensity\":'ms1_int_sum_apex',\"ClusterCount\":'n_isotopes'})\n",
    "    feature_table['rt_apex'] = feature_table['rt_apex']/60\n",
    "    feature_table['rt_start'] = feature_table['rt_start']/60\n",
    "    feature_table['rt_end'] = feature_table['rt_end']/60\n",
    "    \n",
    "    feature_cluster_mapping = feature_cluster_mapping.rename(columns={\"FeatureId\": \"feature_id\", \"ClusterId\": \"cluster_id\", \"Monoisotopic\": \"monoisotopic\", \"Intensity\": \"ms1_int_sum_apex\"})\n",
    "    \n",
    "    return feature_table, feature_cluster_mapping\n",
    "\n",
    "\n",
    "def map_bruker(feature_path:str, feature_table:pd.DataFrame, query_data:dict)->pd.DataFrame:\n",
    "    \"\"\"Map Ms1 to Ms2 via Table FeaturePrecursorMapping from Bruker FF.\n",
    "\n",
    "    Args:\n",
    "        feature_path (str): Path to the feature file from Bruker FF (.features-file).\n",
    "        feature_table (pd.DataFrame): Pandas DataFrame containing the features.\n",
    "        query_data (dict): Data structure containing the query data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing features information.\n",
    "    \"\"\"\n",
    "    engine_featurefile = db.create_engine('sqlite:///{}'.format(feature_path))\n",
    "\n",
    "    mapping = pd.read_sql_table('FeaturePrecursorMapping', engine_featurefile)\n",
    "    mapping = mapping.set_index('PrecursorId')\n",
    "    feature_table= feature_table.set_index('Id')\n",
    "\n",
    "\n",
    "    query_prec_id = query_data['prec_id']\n",
    "\n",
    "    #Now look up the feature for each precursor\n",
    "\n",
    "    mass_matched = []\n",
    "    mz_matched = []\n",
    "    rt_matched = []\n",
    "    query_idx = []\n",
    "    f_idx = []\n",
    "\n",
    "    for idx, prec_id in tqdm(enumerate(query_prec_id)):\n",
    "        try:\n",
    "            f_id = mapping.loc[prec_id]['FeatureId']\n",
    "            all_matches = feature_table.loc[f_id]\n",
    "            if type(f_id) == np.int64:\n",
    "                match = all_matches\n",
    "                mz_matched.append(match['mz'])\n",
    "                rt_matched.append(match['rt_apex'])\n",
    "                mass_matched.append(match['mass'])\n",
    "                query_idx.append(idx)\n",
    "                f_idx.append(match['FeatureId'])\n",
    "\n",
    "            else:\n",
    "                for k in range(len(all_matches)):\n",
    "                    match = all_matches.iloc[k]\n",
    "                    mz_matched.append(match['mz'])\n",
    "                    rt_matched.append(match['rt_apex'])\n",
    "                    mass_matched.append(match['mass'])\n",
    "                    query_idx.append(idx)\n",
    "                    f_idx.append(match['FeatureId'])\n",
    "\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    features = pd.DataFrame(np.array([mass_matched, mz_matched, rt_matched, query_idx, f_idx]).T, columns = ['mass_matched', 'mz_matched', 'rt_matched', 'query_idx', 'feature_idx'])\n",
    "\n",
    "    features['query_idx'] = features['query_idx'].astype('int')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isotope Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stats(isotope_patterns, iso_idx, stats):\n",
    "    columns = ['mz_average','delta_m','int_sum','int_area','rt_min','rt_max']\n",
    "    \n",
    "    stats_idx = np.zeros(iso_idx[-1], dtype=np.int64)\n",
    "    stats_map = np.zeros(iso_idx[-1], dtype=np.int64)\n",
    "\n",
    "    start_ = 0\n",
    "    end_ = 0 \n",
    "    \n",
    "    for idx in range(len(iso_idx)-1):\n",
    "        k = isotope_patterns[iso_idx[idx]:iso_idx[idx+1]]\n",
    "        end_ += len(k)\n",
    "        stats_idx[start_:end_] = k\n",
    "        stats_map[start_:end_] = idx\n",
    "        start_ = end_\n",
    "        \n",
    "    k = pd.DataFrame(stats[stats_idx], columns=columns)\n",
    "\n",
    "    k['feature_id'] = stats_map\n",
    "        \n",
    "    return k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from alphapept.search import query_data_to_features\n",
    "import alphapept.io\n",
    "import functools\n",
    "\n",
    "\n",
    "def find_features(to_process:tuple, callback:Union[Callable, None] = None, parallel:bool = False)-> Union[str, bool]:\n",
    "    \"\"\"Wrapper for feature finding.\n",
    "\n",
    "    Args:\n",
    "        to_process (tuple): to_process tuple, to be used from a proces spool.\n",
    "        callback (Union[Callable, None], optional): Optional callback function. Defaults to None.\n",
    "        parallel (bool, optional): Flag to use parallel processing. Currently unused. Defaults to False.\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: Error if the file extension is not understood.\n",
    "\n",
    "    Returns:\n",
    "        Union[str, bool]: Returns true if function was sucessfull, otherwise the exception as string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        index, settings = to_process\n",
    "        file_name = settings['experiment']['file_paths'][index]\n",
    "\n",
    "        base, ext = os.path.splitext(file_name)\n",
    "\n",
    "        if ext.lower() == '.raw':\n",
    "            datatype='thermo'\n",
    "        elif ext.lower() == '.d':\n",
    "            datatype='bruker'\n",
    "        elif ext.lower() == '.mzml':\n",
    "            datatype='mzml'\n",
    "        elif ext.lower() == '.wiff':\n",
    "            datatype='sciex'\n",
    "        else:\n",
    "            raise NotImplementedError('File extension {} not understood.'.format(ext))\n",
    "\n",
    "        out_file = f\"{base}.ms_data.hdf\"\n",
    "\n",
    "        skip = True\n",
    "        if os.path.isfile(out_file):\n",
    "            try:\n",
    "                alphapept.io.MS_Data_File(\n",
    "                    out_file\n",
    "                ).read(dataset_name=\"features\")\n",
    "                logging.info(\n",
    "                    'Found *.hdf with features for {}'.format(out_file)\n",
    "                )\n",
    "            except KeyError:\n",
    "\n",
    "                logging.info(\n",
    "                    'No *.hdf file with features found for {}. Adding to feature finding list.'.format(out_file)\n",
    "                )\n",
    "                skip = False\n",
    "\n",
    "        if not skip:\n",
    "            ms_file = alphapept.io.MS_Data_File(out_file, is_read_only=False)\n",
    "            query_data = ms_file.read_DDA_query_data()\n",
    "            \n",
    "            feature_cluster_mapping = pd.DataFrame()\n",
    "\n",
    "            if not settings['workflow'][\"find_features\"]:\n",
    "                features = query_data_to_features(query_data)\n",
    "            else:\n",
    "                if datatype in ['thermo','mzml','sciex']:\n",
    "\n",
    "                    from alphapept.constants import averagine_aa, isotopes\n",
    "\n",
    "                    f_settings = settings['features']\n",
    "                    max_gap = f_settings['max_gap']\n",
    "                    centroid_tol = f_settings['centroid_tol']\n",
    "                    hill_split_level = f_settings['hill_split_level']\n",
    "                    iso_split_level = f_settings['iso_split_level']\n",
    "                    \n",
    "                    \n",
    "                    #Cleanup if \n",
    "                    \n",
    "                    int_data = np.array(query_data['int_list_ms1'])\n",
    "\n",
    "\n",
    "                    window = f_settings['hill_smoothing']\n",
    "                    hill_check_large = f_settings['hill_check_large']\n",
    "\n",
    "                    iso_charge_min = f_settings['iso_charge_min']\n",
    "                    iso_charge_max = f_settings['iso_charge_max']\n",
    "                    iso_n_seeds = f_settings['iso_n_seeds']\n",
    "\n",
    "                    hill_nboot_max = f_settings['hill_nboot_max']\n",
    "                    hill_nboot = f_settings['hill_nboot']\n",
    "\n",
    "                    iso_mass_range = f_settings['iso_mass_range']\n",
    "\n",
    "                    iso_corr_min = f_settings['iso_corr_min']\n",
    "\n",
    "                    logging.info('Feature finding on {}'.format(file_name))\n",
    "\n",
    "                    logging.info(f'Hill extraction with centroid_tol {centroid_tol} and max_gap {max_gap}')\n",
    "\n",
    "                    hill_ptrs, hill_data, path_node_cnt, score_median, score_std = extract_hills(query_data, max_gap, centroid_tol)\n",
    "                    logging.info(f'Number of hills {len(hill_ptrs):,}, len = {np.mean(path_node_cnt):.2f}')\n",
    "\n",
    "                    logging.info(f'Repeating hill extraction with centroid_tol {score_median+score_std*3:.2f}')\n",
    "\n",
    "                    hill_ptrs, hill_data, path_node_cnt, score_median, score_std = extract_hills(query_data, max_gap, score_median+score_std*3)\n",
    "                    logging.info(f'Number of hills {len(hill_ptrs):,}, len = {np.mean(path_node_cnt):.2f}')\n",
    "\n",
    "                    hill_ptrs, hill_data = remove_duplicate_hills(hill_ptrs, hill_data, path_node_cnt)\n",
    "                    logging.info(f'After duplicate removal of hills {len(hill_ptrs):,}')\n",
    "                    \n",
    "                    hill_ptrs = split_hills(hill_ptrs, hill_data, int_data, hill_split_level=hill_split_level, window = window) #hill lenght is inthere already\n",
    "                    logging.info(f'After split hill_ptrs {len(hill_ptrs):,}')\n",
    "\n",
    "                    hill_data, hill_ptrs = filter_hills(hill_data, hill_ptrs, int_data, hill_check_large = hill_check_large, window=window)\n",
    "\n",
    "                    logging.info(f'After filter hill_ptrs {len(hill_ptrs):,}')\n",
    "\n",
    "                    stats, sortindex_, idxs_upper, scan_idx, hill_data, hill_ptrs = get_hill_data(query_data, hill_ptrs, hill_data, hill_nboot_max = hill_nboot_max, hill_nboot = hill_nboot)\n",
    "                    logging.info('Extracting hill stats complete')\n",
    "\n",
    "                    pre_isotope_patterns = get_pre_isotope_patterns(stats, idxs_upper, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, maximum_offset, iso_charge_min=iso_charge_min, iso_charge_max=iso_charge_max, iso_mass_range=iso_mass_range, cc_cutoff=iso_corr_min)\n",
    "                    logging.info('Found {:,} pre isotope patterns.'.format(len(pre_isotope_patterns)))\n",
    "\n",
    "                    isotope_patterns, iso_idx, isotope_charges = get_isotope_patterns(pre_isotope_patterns, hill_ptrs, hill_data, int_data, scan_idx, stats, sortindex_, averagine_aa, isotopes, iso_charge_min = iso_charge_min, iso_charge_max = iso_charge_max, iso_mass_range = iso_mass_range, iso_n_seeds = iso_n_seeds, cc_cutoff = iso_corr_min, iso_split_level=iso_split_level, callback=None)\n",
    "                    logging.info('Extracted {:,} isotope patterns.'.format(len(isotope_charges)))\n",
    "\n",
    "                    feature_table, lookup_idx = feature_finder_report(query_data, isotope_patterns, isotope_charges, iso_idx, stats, sortindex_, hill_ptrs, hill_data)\n",
    "                    \n",
    "                    lookup_idx_df = pd.DataFrame(lookup_idx, columns = ['isotope_pattern', 'isotope_pattern_hill'])\n",
    "                    ms_file.write(lookup_idx_df, dataset_name=\"feature_table_idx\")\n",
    "                    \n",
    "                    feature_cluster_mapping = get_stats(isotope_patterns, iso_idx, stats)\n",
    "                    \n",
    "                    \n",
    "                    logging.info('Report complete.')\n",
    "\n",
    "                elif datatype == 'bruker':\n",
    "                    logging.info('Feature finding on {}'.format(file_name))\n",
    "                    feature_path = extract_bruker(file_name)\n",
    "                    feature_table, feature_cluster_mapping = convert_bruker(feature_path)\n",
    "                    \n",
    "                    logging.info('Bruker featurer finder complete. Extracted {:,} features.'.format(len(feature_table)))\n",
    "\n",
    "                # Calculate additional params\n",
    "                feature_table['rt_length'] = feature_table['rt_end'] - feature_table['rt_start']\n",
    "                feature_table['rt_right'] = feature_table['rt_end'] - feature_table['rt_apex']\n",
    "                feature_table['rt_left'] = feature_table['rt_apex'] - feature_table['rt_start']\n",
    "                feature_table['rt_tail'] = feature_table['rt_right'] / feature_table['rt_left']\n",
    "\n",
    "                logging.info('Matching features to query data.')\n",
    "                \n",
    "                if 'mono_mzs2' not in query_data.keys():\n",
    "                    logging.info('No MS2-data to match.')\n",
    "                    features = pd.DataFrame()\n",
    "                else:\n",
    "                    features = map_ms2(feature_table, query_data, **settings['features'])\n",
    "                    \n",
    "                ms_file.write(feature_cluster_mapping, dataset_name=\"feature_cluster_mapping\")\n",
    "                    \n",
    "                logging.info('Saving feature table.')\n",
    "                ms_file.write(feature_table, dataset_name=\"feature_table\")\n",
    "                \n",
    "                logging.info('Feature table saved to {}'.format(out_file))\n",
    "\n",
    "\n",
    "            logging.info('Saving features.')\n",
    "            ms_file.write(features, dataset_name=\"features\")\n",
    "            logging.info(f'Feature finding of file {file_name} complete.')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f'Feature finding of file {file_name} failed. Exception {e}')\n",
    "        return f\"{e}\" #Can't return exception object, cast as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping\n",
    "\n",
    "Mapping MS1 to MS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def replace_infs(array:np.ndarray)->np.ndarray:\n",
    "    \"\"\"Replace nans and infs with 0\n",
    "\n",
    "    Args:\n",
    "        array (np.ndarray): Input array.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Output array without nans and infs.\n",
    "    \"\"\"\n",
    "    array[array == -np.inf] = 0\n",
    "    array[array == np.inf] = 0\n",
    "    array[np.isnan(array)] = 0\n",
    "\n",
    "    return array\n",
    "\n",
    "def map_ms2(feature_table:pd.DataFrame, query_data:dict, map_mz_range:float = 1, map_rt_range:float = 0.5, map_mob_range:float = 0.3, map_n_neighbors:int=5, search_unidentified:bool = False, **kwargs)->pd.DataFrame:\n",
    "    \"\"\"Map MS1 features to MS2 based on rt and mz.\n",
    "    If ccs is included also add.\n",
    "    Args:\n",
    "        feature_table (pd.DataFrame): Pandas DataFrame with features.\n",
    "        query_data (dict): Data structure containing the query data.\n",
    "        map_mz_range (float, optional): Mapping range for mz (Da). Defaults to 1.\n",
    "        map_rt_range (float, optional): Mapping range for rt (min). Defaults to 0.5.\n",
    "        map_mob_range (float, optional): Mapping range for mobility (%). Defaults to 0.3.\n",
    "        map_n_neighbors (int, optional): Maximum number of neighbors to be extracted. Defaults to 5.\n",
    "        search_unidentified (bool, optional): Flag to perform search on features that have no isotope pattern. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Table with features.\n",
    "    \"\"\"\n",
    "    feature_table['rt'] = feature_table['rt_apex']\n",
    "\n",
    "    range_dict = {}\n",
    "    range_dict['mz'] = ('mono_mzs2', map_mz_range)\n",
    "    range_dict['rt'] = ('rt_list_ms2', map_rt_range)\n",
    "    range_dict['mobility'] = ('mobility', map_mob_range)\n",
    "\n",
    "    query_dict = {}\n",
    "    query_dict['rt'] = 'rt_list_ms2'\n",
    "    query_dict['mass'] = 'prec_mass_list2'\n",
    "    query_dict['mz'] = 'mono_mzs2'\n",
    "    query_dict['charge'] = 'charge2'\n",
    "    query_dict['mobility'] = 'mobility'\n",
    "\n",
    "    if 'mobility' not in feature_table.columns:\n",
    "        del range_dict['mobility']\n",
    "        del query_dict['mobility']\n",
    "        use_mob = False\n",
    "    else:\n",
    "        use_mob = True\n",
    "\n",
    "    tree_points = feature_table[list(range_dict.keys())].values\n",
    "\n",
    "    for i, key in enumerate(range_dict):\n",
    "        tree_points[:,i] = tree_points[:,i]/range_dict[key][1]\n",
    "\n",
    "    matching_tree = KDTree(tree_points, metric=\"euclidean\")\n",
    "    ref_points = np.array([query_data[range_dict[_][0]] / range_dict[_][1] for _ in range_dict]).T\n",
    "    ref_points = replace_infs(ref_points)\n",
    "\n",
    "    dist, idx = matching_tree.query(ref_points, k=map_n_neighbors)\n",
    "    ref_matched = np.zeros(ref_points.shape[0], dtype=np.bool_)\n",
    "\n",
    "    all_df = []\n",
    "    for neighbor in range(map_n_neighbors):\n",
    "\n",
    "        ref_df = pd.DataFrame(np.array([query_data[query_dict[_]] for _ in query_dict]).T, columns = query_dict.keys())\n",
    "\n",
    "        for _ in query_dict:\n",
    "            ref_df[_+'_matched'] = feature_table.iloc[idx[:,neighbor]][_].values\n",
    "            ref_df[_+'_offset'] = ref_df[_+'_matched'] - ref_df[_]\n",
    "\n",
    "        ref_df['query_idx'] = ref_df.index\n",
    "        ref_df['feature_idx'] = idx[:,neighbor]\n",
    "\n",
    "        for field in ['ms1_int_sum_area','ms1_int_sum_apex','ms1_int_max_area','ms1_int_max_apex','rt_start','rt_apex','rt_end','fwhm','mobility_lower','mobility_upper']:\n",
    "            if field in feature_table.keys():\n",
    "                ref_df[field] = feature_table.iloc[idx[:,neighbor]][field].values\n",
    "\n",
    "        rt_check = (ref_df['rt_start'] <= ref_df['rt']) & (ref_df['rt'] <= ref_df['rt_end'])\n",
    "\n",
    "        # check isolation window (win=3)\n",
    "        mass_check = np.abs(ref_df['mz_offset'].values) <= 3\n",
    "\n",
    "        _check = rt_check & mass_check\n",
    "        if use_mob:\n",
    "            mob_check = (ref_df['mobility_lower'] <= ref_df['mobility']) & (ref_df['mobility'] <= ref_df['mobility_upper'])\n",
    "            _check &= mob_check\n",
    "\n",
    "        ref_matched |= _check\n",
    "        ref_df['feature_dist'] = dist[:,neighbor]\n",
    "        ref_df = ref_df[_check]\n",
    "\n",
    "        all_df.append(ref_df)\n",
    "\n",
    "\n",
    "    if search_unidentified:\n",
    "        if use_mob:\n",
    "            unmatched_ref = pd.DataFrame(np.array([query_data['rt_list_ms2'], query_data['prec_mass_list2'], query_data['mono_mzs2'], query_data['charge2'], query_data['mobility']]).T, columns=['rt', 'mass', 'mz', 'charge','mobility'])\n",
    "        else:\n",
    "            unmatched_ref = pd.DataFrame(np.array([query_data['rt_list_ms2'], query_data['prec_mass_list2'], query_data['mono_mzs2'], query_data['charge2']]).T, columns=['rt', 'mass', 'mz', 'charge'])\n",
    "        unmatched_ref = unmatched_ref[~ref_matched]\n",
    "        unmatched_ref['mass_matched'] = unmatched_ref['mass']\n",
    "        unmatched_ref['mass_offset'] = 0\n",
    "        unmatched_ref['rt_matched'] = unmatched_ref['rt']\n",
    "        unmatched_ref['rt_offset']  = 0\n",
    "        unmatched_ref['mz_matched'] = unmatched_ref['mz']\n",
    "        unmatched_ref['mz_offset'] = 0\n",
    "        unmatched_ref['charge_matched'] = unmatched_ref['charge']\n",
    "        unmatched_ref['query_idx'] = unmatched_ref.index\n",
    "        unmatched_ref['feature_idx'] = np.nan\n",
    "\n",
    "        if use_mob:\n",
    "            ref_df['mobility_matched'] = unmatched_ref['mobility']\n",
    "            ref_df['mobility_offset'] = np.nan\n",
    "\n",
    "        for field in ['ms1_int_sum_area','ms1_int_sum_apex','ms1_int_max_area','ms1_int_max_apex','rt_start','rt_apex','rt_end','fwhm']:\n",
    "            if field in feature_table.keys():\n",
    "                unmatched_ref[field] = np.nan\n",
    "        unmatched_ref['feature_dist'] = np.nan\n",
    "\n",
    "        all_df.append(unmatched_ref)\n",
    "\n",
    "    features = pd.concat(all_df)\n",
    "\n",
    "    features = features.sort_values('mass_matched', ascending=True)\n",
    "    features = features.reset_index(drop=True)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
