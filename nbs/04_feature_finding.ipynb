{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp feature_finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Finding\n",
    "\n",
    "> Functions related to feature finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part describes the implementation of the feature finding algorithm. The core of the algorithm is described in the [MaxQuant-Paper](https://www.nature.com/articles/nbt.1511).\n",
    "The supplementary material explains the underlying methodology in great detail and is the foundation of the theoretical background that is described here.\n",
    "A refined version of the algorithm was presented with [Dinosaur](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4933939/), which was also used as a reference for the Python implementation.\n",
    "\n",
    "For the algorithm, we need serval modules:\n",
    "\n",
    "1. Connecting Centroids to Hills\n",
    "\n",
    "2. Refinement of Hills\n",
    "\n",
    "3. Calculating Hill Statistics\n",
    "\n",
    "4. Combining Hills to Isotope Patterns\n",
    "\n",
    "5. Deconvolution of Isotope Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "From the `IO` library, we already have `*.ms_data.hdf` container that contains centroided data. To use it in feature finding, we directly load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Centroids to Hills\n",
    "\n",
    "### Connecting Centroids\n",
    "\n",
    "In this step, we search for centroids that are connected (i.e., within a specific mass difference) over time. The connected centroids are termed hills. As we have a list of centroid lists, we ideally would have a list of tuples with indices to the centroids that describe the connected hills.\n",
    "\n",
    "#### Pairs of Centroids\n",
    "First, we define the function `get_pairs_centroids` that checks two lists of centroids and checks for pairs of centroids. As the masses in the centroid lists are sorted, we can loop through the centroids with two indices and compare the mass difference. If the mass difference is within the defined threshold, `ppm_tol` (default is 7ppm), we have found ourselves a pair. If no match is found, we can advance either index, depending on whether the mass difference is positive or negative. A pair is a `tuple` containing the index to the first centroid list `i` and the second centroid list `j` as well as the difference in ppm `delta`. We add all found pairs to a list termed `pairs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphapept.speed\n",
    "import numpy as np\n",
    "from alphapept.speed import parallel_compiled_func, cupy, jit_fun\n",
    "\n",
    "@parallel_compiled_func\n",
    "def connect_centroids_unidirection(idx, row_borders, connections, scores, centroids, max_gap, ppm_tol):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "\n",
    "    for gap in range(max_gap + 1):\n",
    "        y = x + gap + 1\n",
    "        if y > row_borders.shape[0]:\n",
    "            return\n",
    "\n",
    "        start_index_f = 0\n",
    "        if x > 0:\n",
    "            start_index_f = row_borders[x - 1]\n",
    "\n",
    "        centroids_1 = centroids[start_index_f: row_borders[x]]\n",
    "\n",
    "        start_index_b = row_borders[y - 1]\n",
    "        centroids_2 = centroids[start_index_b: row_borders[y]]\n",
    "\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        while (i < len(centroids_1)) & (j < len(centroids_2)):\n",
    "            mz1, mz2 = centroids_1[i], centroids_2[j]\n",
    "            diff = mz1 - mz2\n",
    "            mz_sum = mz1 + mz2\n",
    "            delta = 2 * 1e6 * abs(diff) / mz_sum\n",
    "\n",
    "            if delta < ppm_tol:\n",
    "\n",
    "                if scores[x, i, gap] > delta:\n",
    "                    scores[x, i, gap] = delta\n",
    "                    connections[x, i, gap] = (connections.shape[1] * y) + j\n",
    "\n",
    "            if diff > 0:\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "@parallel_compiled_func\n",
    "def convert_connections_to_array(idx, from_r, from_c, to_r, to_c, row_borders, out_from_idx, out_to_idx):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "\n",
    "    row = from_r[x]\n",
    "    col = from_c[x]\n",
    "    start_index_f = 0\n",
    "    if row > 0:\n",
    "        start_index_f = row_borders[row - 1]\n",
    "    out_from_idx[x] = start_index_f + col\n",
    "\n",
    "    row = to_r[x]\n",
    "    col = to_c[x]\n",
    "    start_index_f = 0\n",
    "    if row > 0:\n",
    "        start_index_f = row_borders[row - 1]\n",
    "    out_to_idx[x] = start_index_f + col\n",
    "    \n",
    "@parallel_compiled_func\n",
    "def eliminate_overarching_vertex(idx, from_idx, to_idx):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "    if x == 0:\n",
    "        return\n",
    "\n",
    "    if from_idx[x - 1] == from_idx[x]:\n",
    "        to_idx[x] = -1\n",
    "        \n",
    "@parallel_compiled_func\n",
    "def path_finder(idx, from_idx, to_idx, forward, backward):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "    fr = from_idx[x]\n",
    "    to =  to_idx[x]\n",
    "\n",
    "    forward[fr] = to\n",
    "    backward[to] = fr\n",
    "\n",
    "@parallel_compiled_func\n",
    "def find_path_start(idx, forward, backward, path_starts):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "\n",
    "    if forward[x] > -1 and backward[x] == -1:\n",
    "        path_starts[x] = 0\n",
    "\n",
    "@parallel_compiled_func\n",
    "def find_path_length(idx, path_starts, forward, path_cnt):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "\n",
    "    ctr = 1\n",
    "    idx = path_starts[x]\n",
    "    while forward[idx] > -1:\n",
    "        ctr += 1\n",
    "        idx = forward[idx]\n",
    "    path_cnt[x] = ctr\n",
    "\n",
    "@parallel_compiled_func\n",
    "def fill_path_matrix(idx, path_start, forwards, out_hill_data, out_hill_ptr):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "        \n",
    "    path_position = 0\n",
    "    idx = path_start[x]\n",
    "    while idx > -1:\n",
    "        out_hill_data[out_hill_ptr[x] + path_position] = idx\n",
    "        idx = forwards[idx]\n",
    "        path_position += 1\n",
    "\n",
    "@parallel_compiled_func\n",
    "def convert_to_coord_path(idx, out_rows, out_cols, connected_comps, d_width):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "\n",
    "    idx = connected_comps[x]\n",
    "    row = idx // d_width\n",
    "    col = idx - row * d_width\n",
    "\n",
    "    out_rows[x] = row\n",
    "    out_cols[x] = col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting all Centroids\n",
    "\n",
    "As we now have a function that can find pairs of centroids when given two lists of centroids, the next step is to write a function that connects all the centroids within a dataset. For this, we define the function `connect_centroids`. Here, we use the `get_pairs_centroids` function to connect hills in consecutive scans `(n->n+1)` whenever their centroid m/z positions differ less than the defined threshold. If no matching peak is found in the next scan `(n+1)`, additional scans can be considered when looking in the scan after next `(n+2)`. The maximum distance between two hills can be set via the `gap` parameter. \n",
    "\n",
    "For an efficient implementation to find all connected hills over all scans, we create a sparse matrix, named `results` that represent indices to all possible centroids. As the number of centroids varies for each scan, not all positions within the matrix represent a valid centroid. To distinguish these values, we fill the matrix with `-1` upon initialization. We can use the matrix to store the matching centroid as an integer that encodes its position. The x,y index for each cell represents the indices to the centroid list. The idea here is to have an efficient data structure to represent connections between centroids.\n",
    "\n",
    "To calculate indexes back and forth we define the functions `tup_to_ind` and `ind_to_tup`. This could in principle be also done with `numpy.unravel_index`, however, the numba implementation seems to be much faster.\n",
    "\n",
    "Additionally, we define a sparse matrix named `scores` that is intended to store the mass difference. If there are multiple possible connections between two centroids, only the best (i.e., shortest distance) will be stored.\n",
    "\n",
    "The idea behind the matrix-indexing is explained with the following example:\n",
    "\n",
    "##### Example: \n",
    "\n",
    "Imagine you have three scans with the following centroids:\n",
    "\n",
    "* Scan 0: 10, 20, 30\n",
    "* Scan 1: 10.2, 40.1\n",
    "* Scan 2: 40, 50, 60\n",
    "\n",
    "\n",
    "When comparing consecutive scans and defining the maximum delta mass to be 0.5 find the following connections: (Scan No, Centroid No) -> (Scan No, Centroid No). As we cannot easily store tuples in the matrix, we convert tuple containing the position of the connected centroid to an integer.\n",
    "* (0,0) -> (1,0) -> (3): 10 & 10.2 -> delta = 0.2\n",
    "* (1,1) -> (2,0) -> (6): 40.1 & 40 -> delta = 0.1\n",
    "\n",
    "Finally, we store this in the `results` matrix:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "3 & -1  & -1 \\\\ \n",
    "-1 & 6 & -1\\\\ \n",
    "-1 & -1 & -1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "The coressponding `scores` matrix will look as follows:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "0.2 & -1  & -1 \\\\ \n",
    "-1 & 0.1 & -1\\\\ \n",
    "-1 & -1 & -1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "This allows us to not only easily store connections between centroids but also perform a quick lookup for the delta of an existing connection. Note that it also only stores the best connection for each centroid. To extract the connected centroids, we can use `np.where(results >= 0)`. This implementation allows getting millions of connections within seconds. \n",
    "\n",
    "##### Foward and Backward Search\n",
    "\n",
    "One thing that still needs to be considered is that when searching with a large mass tolerance, we could get multiple matches that we do not want. Consider the following example:\n",
    "\n",
    "* Scan 0: 10 (a), \n",
    "* Scan 1: 10 (b) , 10.1 (c)\n",
    "* Scan 2: 10 (d)\n",
    "\n",
    "As we only allow one connection per mass, we would have the following connections:\n",
    "\n",
    "* a -> b\n",
    "* b -> d\n",
    "\n",
    "Now if we have a large delta mass, e.g. of `0.1`, there would be an additional connection:\n",
    "\n",
    "* c-> d\n",
    "\n",
    "\n",
    "![undirected](images/undirected.png)\n",
    "\n",
    "The resulting hill would have a branch to the c centroid, which is obviously intended. To circumvent this, we additionally perform the search backwards. (Note the functions `connect_centroids_forward` and `connect_centroids_backward`).  Now, as we only store, the best connection between two centroids  `d` will only be connected to `b`. To, therefore, remove branches, we remove all connections that are not double.\n",
    "\n",
    "\n",
    "![directed](images/directed.png)\n",
    "\n",
    "We define a wrapper function named `connect_centroids` that allows to search with a gaps ize and returns lists of indices of the connected centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting hills\n",
    "\n",
    "Having all the connected centroids, we would like to find `hills`, referring to centroids that are connected. We can achieve this by creating a graph `G` with the `networkx`-package. Here, each connection is added as an edge using the `add_edge` method to the graph. We can then use the `connected_components` function to extract all the connected components.\n",
    "\n",
    "`NetworkX` is known to be memory [consuming](https://groups.google.com/forum/#!topic/networkx-discuss/Etd4GpkjPdA) and can take hundreds of bytes per edge. For millions of connections, we will, therefore, consume Gigabytes of memory. To keep the graph small, we continuously feed the graph until we reach a certain scan number `buffer size`. Once the limit is reached, we extract all completed hills and remove all nodes that are part of a completed hill or cannot be connected anymore. Once all edges are added, we perform a final extraction step. Finally, everything is put into the `get_hills` function.\n",
    "\n",
    "#### Parallelization Strategies\n",
    "\n",
    "Currently, the parallel connection of hills is not implemented yet. A possible way could be to process individual chunks of edges in parallel and then combine them together, as it is done in `Dinosaur`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def find_centroid_connections(rowwise_peaks, row_borders, centroids, max_gap=2, ppm_tol=8):\n",
    "    max_centroids = int(cupy.max(rowwise_peaks))\n",
    "    spectra_cnt = len(row_borders) - 1\n",
    "\n",
    "    connections = cupy.full((spectra_cnt, max_centroids, max_gap + 1), -1, dtype=np.int32)\n",
    "    score = cupy.full((spectra_cnt, max_centroids, max_gap + 1), np.inf)\n",
    "    \n",
    "    \n",
    "\n",
    "    connect_centroids_unidirection(row_borders,\n",
    "                                   connections,\n",
    "                                   score,\n",
    "                                   centroids,\n",
    "                                   max_gap,\n",
    "                                   ppm_tol)\n",
    "    \n",
    "    score = score[cupy.where(score < np.inf)]\n",
    "    \n",
    "    score_median = cupy.median(score)\n",
    "    score_std = cupy.std(score)\n",
    "    \n",
    "    \n",
    "    del score, max_centroids, spectra_cnt\n",
    "\n",
    "    c_shape = connections.shape\n",
    "    from_r, from_c, from_g = cupy.where(connections >= 0)\n",
    "    to_r = connections[from_r, from_c, from_g] // c_shape[1]\n",
    "    to_c = connections[from_r, from_c, from_g] - to_r * c_shape[1]\n",
    "\n",
    "    del connections, from_g\n",
    "    return from_r, from_c, to_r, to_c, score_median, score_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "def get_hills(centroids, from_idx, to_idx, min_hill_length=3):\n",
    "    forward = cupy.full(centroids.shape[0], -1)\n",
    "    backward = cupy.full(centroids.shape[0], -1)\n",
    "    path_starts = cupy.full(centroids.shape[0], -1)\n",
    "\n",
    "    path_finder(from_idx, to_idx, forward, backward)\n",
    "    find_path_start(forward, backward, path_starts)\n",
    "\n",
    "    # path_starts will now container the first index of all connected centroids\n",
    "    path_starts = cupy.where(path_starts == 0)[0]\n",
    "\n",
    "    path_node_cnt = cupy.full(path_starts.shape[0], -1)\n",
    "    find_path_length(path_starts, forward, path_node_cnt)\n",
    "\n",
    "    relavant_path_node = cupy.where(path_node_cnt >= min_hill_length)[0]\n",
    "    path_starts = cupy.take(path_starts, relavant_path_node)\n",
    "    path_node_cnt = cupy.take( path_node_cnt, relavant_path_node)\n",
    "    del relavant_path_node\n",
    "\n",
    "    # Generate the hill matix indice ptr data\n",
    "    hill_ptrs = cupy.empty((path_starts.shape[0] + 1), dtype=cupy.int32)\n",
    "\n",
    "    hill_ptrs[0] = 0\n",
    "    hill_ptrs[1:] = path_node_cnt.cumsum()\n",
    "    hill_data = cupy.empty((int(hill_ptrs[-1])), np.int32)\n",
    "\n",
    "    fill_path_matrix(path_starts, forward, hill_data, hill_ptrs)\n",
    "\n",
    "    del from_idx, to_idx, path_starts, forward, backward\n",
    "    return hill_ptrs, hill_data, path_node_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def connect_centroids(rowwise_peaks, row_borders, centroids, max_gap=2, ppm_tol=8):\n",
    "\n",
    "    from_r, from_c, to_r, to_c, score_median, score_std = find_centroid_connections(rowwise_peaks,\n",
    "                                                           row_borders,\n",
    "                                                           centroids,\n",
    "                                                           max_gap=max_gap,\n",
    "                                                           ppm_tol=ppm_tol)\n",
    "\n",
    "    from_idx = cupy.zeros(len(from_r), np.int32)\n",
    "    to_idx = cupy.zeros(len(from_r), np.int32)\n",
    "\n",
    "    convert_connections_to_array(from_r,\n",
    "                                 from_c,\n",
    "                                 to_r,\n",
    "                                 to_c,\n",
    "                                 row_borders,\n",
    "                                 from_idx,\n",
    "                                 to_idx)\n",
    "\n",
    "    eliminate_overarching_vertex(from_idx, to_idx)\n",
    "\n",
    "    relavent_idx = cupy.where(to_idx >= 0)\n",
    "    from_idx = cupy.take(from_idx, relavent_idx)[0]\n",
    "    to_idx = cupy.take(to_idx, relavent_idx)[0]\n",
    "\n",
    "    del from_r, from_c, to_r, to_c, relavent_idx\n",
    "    return from_idx, to_idx, score_median, score_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_hills(query_data, max_gap = 2, ppm_tol = 8):\n",
    "\n",
    "    indices = cupy.array(query_data['indices_ms1'])\n",
    "    mass_data = cupy.array(query_data['mass_list_ms1'])\n",
    "\n",
    "    rowwise_peaks = indices[1:] - indices[:-1]\n",
    "    row_borders = indices[1:]\n",
    "\n",
    "    from_idx, to_idx, score_median, score_std = connect_centroids(rowwise_peaks, row_borders, mass_data, max_gap=max_gap, ppm_tol=ppm_tol)\n",
    "    hill_ptrs, hill_data, path_node_cnt = get_hills(mass_data, from_idx, to_idx)\n",
    "\n",
    "    del mass_data\n",
    "    del indices\n",
    "    \n",
    "    if cupy.__name__ != 'numpy':\n",
    "        hill_ptrs = hill_ptrs.get()\n",
    "        hill_data = hill_data.get()\n",
    "        path_node_cnt = path_node_cnt.get()\n",
    "        \n",
    "        score_median = score_median.get()\n",
    "        score_std = score_std.get()\n",
    "\n",
    "    \n",
    "    return hill_ptrs, hill_data, path_node_cnt, score_median, score_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Splitting\n",
    "When having a hill with two or more maxima, we would like to split it at the minimum position. For this, we use a recursive approach. First, the minimum of a hill is detected. A hill is split at this minimum if the smaller of the surrounding maxima is at least the factor `split_level` larger than the minimum. For each split, the process is repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@jit_fun\n",
    "def fast_minima(y):\n",
    "    minima = np.zeros(len(y))\n",
    "    \n",
    "    start = 0\n",
    "    end = len(y)\n",
    "\n",
    "    for i in range(start + 2, end - 2):\n",
    "        if ((y[i - 1] > y[i]) & (y[i + 1] > y[i])) \\\n",
    "            or ((y[i - 1] > y[i]) & (y[i + 1] == y[i]) & (y[i + 2] > y[i])) \\\n",
    "            or ((y[i - 2] > y[i]) & (y[i - 1] == y[i]) & (y[i + 1] > y[i])) \\\n",
    "            or (((y[i - 2] > y[i]) & (y[i - 1] == y[i]) & (y[i + 1] == y[i]) & \\\n",
    "                (y[i + 2] > y[i]))):\n",
    "            minima[i] = 1\n",
    "            \n",
    "    minima = minima.nonzero()[0]  \n",
    "    \n",
    "    return minima\n",
    "\n",
    "@parallel_compiled_func(cpu_only = True)\n",
    "def split(idx, hill_ptrs_range, hill_ptrs, int_data, hill_data, splits, split_level, window):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "        \n",
    "    k = hill_ptrs_range[x]\n",
    "        \n",
    "    start = hill_ptrs[k]\n",
    "    end = hill_ptrs[k + 1]\n",
    "    \n",
    "    int_idx = hill_data[start:end] #index to hill data\n",
    "\n",
    "    int_trace = int_data[int_idx]\n",
    "\n",
    "    for i in range(len(int_idx)):\n",
    "        min_index = max(0, i - window)\n",
    "        max_index = min(len(int_idx), i + window + 1)\n",
    "        int_trace[i] = np.median(int_trace[min_index:max_index])\n",
    "        \n",
    "    for i in range(len(int_idx)):\n",
    "        min_index = max(0, i - window)\n",
    "        max_index = min(len(int_idx), i + window + 1)\n",
    "        int_trace[i] = np.mean(int_trace[min_index:max_index])\n",
    "\n",
    "    #minima = (np.diff(np.sign(np.diff(int_trace))) > 0).nonzero()[0] + 1 #This works also but is slower\n",
    "    \n",
    "    minima = fast_minima(int_trace)\n",
    "    \n",
    "    sorted_minima = np.argsort(int_trace[minima])\n",
    "    \n",
    "    minima = minima[sorted_minima]\n",
    "    \n",
    "    for min_ in minima:\n",
    "\n",
    "        minval = int_trace[min_]\n",
    "\n",
    "        left_max = max(int_trace[:min_])\n",
    "        right_max = max(int_trace[min_:])\n",
    "\n",
    "        min_max = min(left_max, right_max)\n",
    "\n",
    "        if (minval == 0) or ((min_max / minval) > split_level):\n",
    "            splits[k] = start+min_\n",
    "            break # Split only once per iteration\n",
    "            \n",
    "def split_hills(hill_ptrs, hill_data, int_data, split_level, window):\n",
    "    splits = np.zeros(len(int_data), dtype=cupy.int32)\n",
    "    to_check = np.arange(len(hill_ptrs)-1)\n",
    "    \n",
    "    while len(to_check) > 0:\n",
    "        split(to_check, hill_ptrs, int_data, hill_data, splits, split_level, window)\n",
    "        splitpoints = splits.nonzero()[0]\n",
    "\n",
    "        to_check = np.zeros(len(hill_ptrs))\n",
    "        to_check[splitpoints] = 1\n",
    "\n",
    "        to_check = np.insert(to_check, splitpoints+1, np.ones(len(splitpoints))).nonzero()[0] #array, index, what\n",
    "        hill_ptrs = np.insert(hill_ptrs, splitpoints+1, splits[splitpoints]) #array, index, what\n",
    "\n",
    "        splits[:] = 0\n",
    "        \n",
    "    return hill_ptrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Hills\n",
    "\n",
    "To filter hills, we define a minimum length `hill_min_length`. All peaks below the threshold `hill_peak_min_length` are accepted as is. For longer hills, the intensity at the start and the end are compared to the maximum intensity. If the ratio of the maximum raw intensity to the smoothed intensity and the beginning and end are larger than `hill_peak_factor` the hills are accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@parallel_compiled_func(cpu_only=True)\n",
    "def check_large_hills(idx, large_peaks, hill_ptrs, hill_data, int_data, to_remove, large_peak = 40, hill_peak_factor = 2, window=1):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "        \n",
    "    k = large_peaks[x]\n",
    "\n",
    "    start = hill_ptrs[k]\n",
    "    end = hill_ptrs[k + 1]\n",
    "    \n",
    "    int_idx = hill_data[start:end] #index to hill data\n",
    "\n",
    "    int_smooth_ = int_data[int_idx]\n",
    "\n",
    "    for i in range(len(int_idx)):\n",
    "        min_index = max(0, i - window)\n",
    "        max_index = min(len(int_idx), i + window + 1)\n",
    "        int_smooth_[i] = np.median(int_smooth_[min_index:max_index])\n",
    "        \n",
    "    for i in range(len(int_idx)):\n",
    "        min_index = max(0, i - window)\n",
    "        max_index = min(len(int_idx), i + window + 1)\n",
    "        int_smooth_[i] = np.mean(int_smooth_[min_index:max_index])\n",
    "\n",
    "    int_ = int_data[int_idx]\n",
    "\n",
    "    max_ = np.max(int_)\n",
    "\n",
    "    if (max_ / int_smooth_[0] > hill_peak_factor) & (max_ / int_smooth_[-1] > hill_peak_factor):\n",
    "        to_remove[x] = 0 \n",
    "    \n",
    "\n",
    "def filter_hills(hill_data, hill_ptrs, int_data, large_peak =40, window = 1):\n",
    "    \n",
    "    large_peaks = np.where(np.diff(hill_ptrs)>=large_peak)[0]\n",
    "    \n",
    "    to_remove = np.ones(len(large_peaks), dtype=np.int32)\n",
    "    check_large_hills(large_peaks, hill_ptrs, hill_data, int_data, to_remove, window)\n",
    "    \n",
    "    idx_ = np.ones(len(hill_data), dtype = np.int32)\n",
    "    keep = np.ones(len(hill_ptrs)-1, dtype = np.int32)\n",
    "    \n",
    "    to_remove = to_remove.nonzero()[0]\n",
    "\n",
    "    for _ in to_remove:\n",
    "        idx_[hill_ptrs[_]:hill_ptrs[_+1]] = 0\n",
    "        keep[_] = 0\n",
    "\n",
    "    hill_lens = np.diff(hill_ptrs)\n",
    "    keep_ = hill_lens[keep.nonzero()[0]]\n",
    "\n",
    "    hill_data_ = hill_data[idx_.nonzero()[0]]\n",
    "    hill_ptrs_ = np.empty((len(keep_) + 1), dtype=np.int32)\n",
    "    hill_ptrs_[0] = 0\n",
    "    hill_ptrs_[1:] = keep_.cumsum()    \n",
    "    \n",
    "    return hill_data_, hill_ptrs_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mass estimate min the equation above is more complicated than just an average of the mj, a standard deviation based estimate of the error would not be appropriate. Therefore we calculate the error as a bootstrap2 estimate over B=150 bootstrap replications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Hill Statistics\n",
    "\n",
    "Next, we calculate summary statistics for the connected centroids. We can obtain a high precision mass estimate for each hill by taking the average of the the masses and weighting this by their intensiteis:\n",
    "\n",
    "$$\n",
    "\\overline{m} = \\frac{\\sum_{j=1}^nm_jI_j}{\\sum_{j=1}^nI_j}\n",
    "$$\n",
    "\n",
    "To estimate the mass error, we calculate the error as a boostrap estimate:\n",
    " \n",
    "$$\\Delta \\overline{m} = \\sqrt{\\frac{\\sum_{b=1}^{B}(\\overline{m}_b - \\overline{m} )}{(B-1)}}$$\n",
    "\n",
    "The calculation of hill statistics for a single hill is implemented in `get_hill_stats`. To calculate the hill stats for a list of hills, we can call the wrapper `get_hill_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@parallel_compiled_func(cpu_only=True) #Only CPU optimized at this point\n",
    "def hill_stats(idx, hill_range, hill_ptrs, hill_data, int_data, mass_data, rt_, rt_idx, stats, hill_nboot_max, hill_nboot):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "        \n",
    "    np.random.seed(42)\n",
    "        \n",
    "    start = hill_ptrs[x]\n",
    "    end = hill_ptrs[x + 1]\n",
    "\n",
    "    idx_ = hill_data[start:end]\n",
    "\n",
    "    int_ = int_data[idx_]\n",
    "    mz_ = mass_data[idx_]\n",
    "\n",
    "    int_sum = np.sum(int_) \n",
    "    int_area = np.trapz(rt_[rt_idx[idx_]], int_) #Area\n",
    "\n",
    "    rt_min = rt_[rt_idx[idx_]].min()\n",
    "    rt_max = rt_[rt_idx[idx_]].max()\n",
    "    \n",
    "    if len(idx_) > hill_nboot_max:\n",
    "        bootsize = hill_nboot_max\n",
    "    else:\n",
    "        bootsize = len(idx_)\n",
    "\n",
    "    averages = np.zeros(hill_nboot)\n",
    "    average = 0\n",
    "\n",
    "    for i in range(hill_nboot):\n",
    "        boot = np.random.choice(len(int_), bootsize, replace=True)\n",
    "        boot_mz = np.sum((mz_[boot] * int_[boot])) / np.sum(int_[boot])\n",
    "        averages[i] = boot_mz\n",
    "        average += boot_mz\n",
    "\n",
    "    average_mz = average/hill_nboot\n",
    "\n",
    "    delta = 0\n",
    "    for i in range(hill_nboot):\n",
    "        delta += (average_mz - averages[i]) ** 2 #maybe easier?\n",
    "    delta_m = np.sqrt(delta / (hill_nboot - 1))\n",
    "    \n",
    "    stats[x,0] = average_mz\n",
    "    stats[x,1] = delta_m\n",
    "    stats[x,2] = int_sum\n",
    "    stats[x,3] = int_area\n",
    "    stats[x,4] = rt_min\n",
    "    stats[x,5] = rt_max\n",
    "    \n",
    "def get_hill_data(query_data, hill_ptrs, hill_data, hill_nboot_max = 300, hill_nboot = 150):\n",
    "\n",
    "    indices_ = np.array(query_data['indices_ms1'])\n",
    "    rt_ = np.array(query_data['rt_list_ms1'])\n",
    "    mass_data = np.array(query_data['mass_list_ms1'])\n",
    "    scan_idx = np.searchsorted(indices_, np.arange(len(mass_data)), side='right') - 1\n",
    "    int_data = np.array(query_data['int_list_ms1'])\n",
    "    \n",
    "    stats = np.zeros((len(hill_ptrs)-1, 6)) #mz, delta, rt_min, rt_max, sum_max\n",
    "    hill_stats(np.arange(len(hill_ptrs)-1), hill_ptrs, hill_data, int_data, mass_data, rt_, scan_idx, stats, hill_nboot_max, hill_nboot)\n",
    "\n",
    "    # sort the stats\n",
    "    sortindex = np.argsort(stats[:,4]) #Sorted by rt_min\n",
    "    stats = stats[sortindex,:]\n",
    "    idxs_upper = stats[:,4].searchsorted(stats[:,5], side=\"right\")\n",
    "    sortindex_ = np.arange(len(sortindex))[sortindex]\n",
    "    \n",
    "    return stats, sortindex_, idxs_upper, scan_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Hills to Isotope Patterns\n",
    "\n",
    "After obtaining summary statistics of hills, the next step is to check whether they belong together to form an isotope pattern. For this, we check wheter it is possible that they are neighbors in an isotope pattern, e.g. one having a 12C atom that has been replaced by a 13C version. The detailed criterion for the check is implemented in `check_isotope_pattern` and is as follows:\n",
    "\n",
    "\n",
    "$$\\left | \\Delta m-\\frac{\\Delta M}{z} \\right |\\leq \\sqrt{\\left ( \\frac{\\Delta S}{z}  \\right )^{2}+\\Delta {m_{1}}^{2} +\\Delta {m_{2}}^{2}}$$\n",
    "\n",
    "The left side contains $\\Delta m$, being the delta of the precise mass estimates from the summary statistics and $\\Delta M = 1.00286864$, which is the mass difference ebtween the 13C peak and the monoisotopic peak in an averagine molecule of 1500 Da mass divided by the charge $z$.\n",
    "\n",
    "The right side contains $\\Delta S = 0.0109135$, which is the maximum shift that a sulphur atom can cause ($\\Delta S = 2m(^{13}C) - 2m(^{12}C) - m(^{34}S) + m(^{32}S)$) and $\\Delta {m_{1}}$ and $\\Delta {m_{2}}$, which are the bootstrapped mass standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from alphapept.constants import mass_dict\n",
    "from numba import njit\n",
    "\n",
    "DELTA_M = mass_dict['delta_M']\n",
    "DELTA_S = mass_dict['delta_S']\n",
    "maximum_offset = DELTA_M + DELTA_S\n",
    "\n",
    "@njit\n",
    "def check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge, mass_range = 5):\n",
    "    \"\"\"\n",
    "    Check if two masses could belong to the same isotope pattern\n",
    "    \"\"\"\n",
    "    delta_mass1 = delta_mass1 * mass_range\n",
    "    delta_mass2 = delta_mass2 * mass_range\n",
    "\n",
    "    delta_mass = np.abs(mass1 - mass2)\n",
    "\n",
    "    left_side = np.abs(delta_mass - DELTA_M / charge)\n",
    "    right_side = np.sqrt((DELTA_S / charge) ** 2 + delta_mass1 ** 2 + delta_mass2 ** 2)\n",
    "\n",
    "    return left_side <= right_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "if False:\n",
    "    def test_get_hill_data():\n",
    "        test_centroids = [\n",
    "            [(600, 1, 1, 1)],\n",
    "            [(300, 1, 2, 2), (600, 0, 1, 2)],\n",
    "            [(300, 10, 3, 3), (600, 20, 3, 3)],\n",
    "            [(300, 20, 4, 4), (600, 40, 4, 4)],\n",
    "            [(300, 40, 5, 5), (600, 80, 5, 5)],\n",
    "            [(300, 20, 6, 6), (600, 40, 6, 6)],\n",
    "            [(300, 10, 7, 7), (600, 20, 7, 7)],\n",
    "            [(300, 1, 8, 8), (600, 0, 8, 8)],\n",
    "        ]\n",
    "\n",
    "        centroid_dtype = [(\"mz\", float), (\"int\", float), (\"scan_no\", int), (\"rt\", float)]\n",
    "        test_centroids_tmp = [np.array(_, dtype=centroid_dtype) for _ in test_centroids]\n",
    "\n",
    "        test_centroids = List([_ for _ in test_centroids_tmp])\n",
    "\n",
    "        test_hills = get_hills(test_centroids)\n",
    "        sorted_hills, sorted_stats, sorted_data = get_hill_data(test_hills, test_centroids)\n",
    "\n",
    "        # Check masses\n",
    "        assert sorted_stats[0][\"mz_avg\"] == 600\n",
    "        assert sorted_stats[1][\"mz_avg\"] == 300\n",
    "\n",
    "        # Check intensities\n",
    "        assert sorted_stats[0][\"int_sum\"] == 201\n",
    "        assert sorted_stats[1][\"int_sum\"] == 102\n",
    "\n",
    "        # Check start\n",
    "        assert sorted_stats[0][\"rt_min\"] == 1\n",
    "        assert sorted_stats[1][\"rt_min\"] == 2\n",
    "\n",
    "    test_get_hill_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "charge = 1\n",
    "\n",
    "mass1, delta_mass1 = 100, 0.1\n",
    "mass2, delta_mass2 = 101.1, 0.05\n",
    "\n",
    "print(check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge))\n",
    "\n",
    "mass2, delta_mass2 = 102.1, 0.05\n",
    "\n",
    "print(check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Correlation of two hills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional criterion that is being checked is that the intensity profiles have sufficient overalp in retention time. This is validated by ensuring that two hills have a cosine correlation of at least 0.6.\n",
    "\n",
    "$$\\frac{\\sum_{s=s_{min}}^{s_{max}}I_sJ_s}{\\sum_{s=s_{min}}^{s_{max}}I_s^{2} \\sum_{s=s_{min}}^{s_{max}}J_s^{2}} \\geq 0.6$$\n",
    "\n",
    "The intensities of two hills are only compared if both have an intensity value in a particular scan. Otherwise, the intensity is set to zero. Additionally, an overlap of at least three elements is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@njit\n",
    "def correlate(scans_, scans_2, int_, int_2):\n",
    "    \n",
    "    min_one, max_one = scans_[0], scans_[-1]\n",
    "    min_two, max_two = scans_2[0], scans_2[-1]\n",
    "\n",
    "    if min_one + 3 > max_two:  # at least an overlap of 3 elements\n",
    "        corr = 0\n",
    "    elif min_two + 3 > max_one:\n",
    "        corr = 0\n",
    "    else:\n",
    "        min_s = min(min_one, min_two)\n",
    "        max_s = max(max_one, max_two)\n",
    "\n",
    "        int_one_scaled = np.zeros(int(max_s - min_s + 1))\n",
    "        int_two_scaled = np.zeros(int(max_s - min_s + 1))\n",
    "\n",
    "        int_one_scaled[scans_ - min_s] = int_\n",
    "        int_two_scaled[scans_2 - min_s] = int_2\n",
    "\n",
    "        corr = np.sum(int_one_scaled * int_two_scaled) / np.sqrt(\n",
    "            np.sum(int_one_scaled ** 2) * np.sum(int_two_scaled ** 2)\n",
    "        )\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting pre-Isotope Patterns\n",
    "\n",
    "Now having two criteria to check whether hills could, in principle, belong together, we define the wrapper functions `extract_edge` and `get_edges` to extract the connected hills. To minimize the number of comparisons we need to perform, we only compare the hills that overlap in time (i.e., the start of one hill `rt_min` needs to be before the end of the other hill `rt_max`) and are less than the sum of $\\Delta M$ and $\\Delta S$ apart. \n",
    "\n",
    "To extract all hills that belong together, we again rely on the `NetworkX`-package to extract the connected components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def extract_edge(stats, idxs_upper, runner, max_index, maximum_offset,  min_charge = 1, max_charge = 6, mass_range=5):\n",
    "    edges = []\n",
    "\n",
    "    mass1 = stats[runner, 0]\n",
    "    delta_mass1 = stats[runner, 1]\n",
    "\n",
    "    for j in range(runner+1, idxs_upper[runner]):\n",
    "        mass2 = stats[j, 0]\n",
    "        if np.abs(mass2 - mass1) <= maximum_offset:\n",
    "            delta_mass2 = stats[j, 1]\n",
    "            for charge in range(min_charge, max_charge + 1):\n",
    "                if check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge, mass_range):    \n",
    "                    edges.append((runner, j))\n",
    "                    break\n",
    "\n",
    "    return edges\n",
    "\n",
    "@parallel_compiled_func(cpu_only=True) #Only CPU optimized at this point\n",
    "def edge_correlation(idx, to_keep, sortindex_, pre_edges, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "        \n",
    "    edge = pre_edges[x,:]\n",
    "    \n",
    "    y = sortindex_[edge[0]]\n",
    "    start = hill_ptrs[y]\n",
    "    end = hill_ptrs[y + 1]\n",
    "    idx_ = hill_data[start:end]\n",
    "    int_ = int_data[idx_]\n",
    "    scans_ = scan_idx[idx_]\n",
    "\n",
    "    con = sortindex_[edge[1]]\n",
    "    start = hill_ptrs[con]\n",
    "    end = hill_ptrs[con + 1]\n",
    "    idx_2 = hill_data[start:end]\n",
    "    int_2 = int_data[idx_2]\n",
    "    scans_2 = scan_idx[idx_2]\n",
    "\n",
    "    if correlate(scans_, scans_2, int_, int_2) > cc_cutoff:\n",
    "        to_keep[x] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import networkx as nx\n",
    "\n",
    "def get_pre_isotope_patterns(stats, idxs_upper, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, maximum_offset, min_charge=1, max_charge=6, mass_range=5, cc_cutoff=0.6):\n",
    "    pre_edges = []\n",
    "\n",
    "    # Step 1\n",
    "    for runner in range(len(stats)):\n",
    "        pre_edges.extend(extract_edge(stats, idxs_upper, runner, idxs_upper[runner], maximum_offset, min_charge, max_charge, mass_range))\n",
    "\n",
    "    to_keep = np.zeros(len(pre_edges), dtype='int')\n",
    "    pre_edges = np.array(pre_edges)\n",
    "    edge_correlation(to_keep, sortindex_, pre_edges, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)\n",
    "    edges = pre_edges[to_keep.nonzero()]\n",
    "\n",
    "    G2 = nx.Graph()\n",
    "    for i in range(len(edges)):\n",
    "        G2.add_edge(edges[i][0], edges[i][1])\n",
    "\n",
    "    pre_isotope_patterns = [\n",
    "        sorted(list(c))\n",
    "        for c in sorted(nx.connected_components(G2), key=len, reverse=True)\n",
    "    ]\n",
    "    \n",
    "    return pre_isotope_patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Isotope Patterns\n",
    "\n",
    "The extracted pre-isotope patterns may not be consistent because their pair-wise mass differences may not correspond to the same charge. To extract isotope patterns from pre-isotope patterns, we need to ensure that they are consistent for a single charge. \n",
    "\n",
    "To do this, we start with the 100 most intense peaks from a pre-isotope pattern to be used as a seed. For each seed and charge we then try to extract the longest consistent isotope pattern. To check wheter a hill is consistent with the seed we employ a modified checking criterion (`check_isotope_pattern_directed`) to be as follows:\n",
    "\n",
    "$$\\left | m-m_j-\\frac{j\\Delta M}{z} \\right |\\leq \\sqrt{\\left ( \\frac{\\Delta S}{z}  \\right )^{2}+\\Delta {m}^{2} +\\Delta {m_{j}}^{2}}$$\n",
    "\n",
    "Here $m$ is the mass of a seed peak, and $m_{j}$ refers to a peak relative to the seed. $j$ refers to the peaks to the left or right (negative or positive index) within the pattern. $j$ needs to run over consecutive values so that gaps are not allowed. Besides this consistency check, two hills are also checked to have a cosine correlation of at least 0.6.\n",
    "\n",
    "Programmatically, this is implemented in `grow_trail` and `grow`. These function uses a recursive approach that adds matching hills to the seed on the left and right side until no more hills can be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@njit\n",
    "def check_isotope_pattern_directed(mass1, mass2, delta_mass1, delta_mass2, charge, index, mass_range):\n",
    "    \"\"\"\n",
    "    Check if two masses could belong to the same isotope pattern\n",
    "\n",
    "    \"\"\"\n",
    "    delta_mass1 = delta_mass1 * mass_range\n",
    "    delta_mass2 = delta_mass2 * mass_range\n",
    "\n",
    "    left_side = np.abs(mass1 - mass2 - index * DELTA_M / charge)\n",
    "    right_side = np.sqrt((DELTA_S / charge) ** 2 + delta_mass1 ** 2 + delta_mass2 ** 2)\n",
    "\n",
    "    return left_side <= right_side\n",
    "\n",
    "\n",
    "@njit\n",
    "def grow(trail, seed, direction, relative_pos, index, stats, pattern, charge, mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff):\n",
    "    \"\"\"\n",
    "    Grows isotope pattern based on a seed and direction\n",
    "\n",
    "    \"\"\"\n",
    "    x = pattern[seed]  # This is the seed\n",
    "    mass1 = stats[x,0]\n",
    "    delta_mass1 = stats[x,1]\n",
    "    \n",
    "    k = sortindex_[x]\n",
    "    start = hill_ptrs[k]\n",
    "    end = hill_ptrs[k + 1]\n",
    "    idx_ = hill_data[start:end]\n",
    "    int_ = int_data[idx_]\n",
    "    scans_ = scan_idx[idx_]\n",
    "\n",
    "    growing = True\n",
    "\n",
    "    while growing:\n",
    "        if direction == 1:\n",
    "            if seed + relative_pos == len(pattern):\n",
    "                growing = False\n",
    "                break\n",
    "        else:\n",
    "            if seed + relative_pos < 0:\n",
    "                growing = False\n",
    "                break\n",
    "\n",
    "        y = pattern[seed + relative_pos]  # This is a reference peak\n",
    "        \n",
    "        l = sortindex_[y]\n",
    "        \n",
    "        mass2 = stats[y,0]\n",
    "        delta_mass2 = stats[y,1]\n",
    "        \n",
    "        start = hill_ptrs[l]\n",
    "        end = hill_ptrs[l + 1]\n",
    "        idx_ = hill_data[start:end]\n",
    "        int_2 = int_data[idx_]\n",
    "        scans_2 = scan_idx[idx_]\n",
    "        \n",
    "        if correlate(scans_, scans_2, int_, int_2) > cc_cutoff:\n",
    "            if check_isotope_pattern_directed(mass1, mass2, delta_mass1, delta_mass2, charge, -direction * index, mass_range):\n",
    "                if direction == 1:\n",
    "                    trail.append(y)\n",
    "                else:\n",
    "                    trail.insert(0, y)\n",
    "                index += (\n",
    "                    1\n",
    "                )  # Greedy matching: Only one edge for a specific distance, will not affect the following matches\n",
    "\n",
    "        delta_mass = np.abs(mass1 - mass2)\n",
    "\n",
    "        if (delta_mass > (DELTA_M+DELTA_S) * index):  # the pattern is sorted so there is a maximum to look back\n",
    "            break\n",
    "\n",
    "        relative_pos += direction\n",
    "\n",
    "    return trail\n",
    "\n",
    "@njit\n",
    "def grow_trail(seed, pattern, stats, charge, mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff):\n",
    "    \"\"\"\n",
    "    Wrapper to grow an isotope pattern to the left and right side\n",
    "    \"\"\"\n",
    "    x = pattern[seed]\n",
    "    trail = List()\n",
    "    trail.append(x)\n",
    "    trail = grow(trail, seed, -1, -1, 1, stats, pattern, charge, mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)\n",
    "    trail = grow(trail, seed, 1, 1, 1, stats, pattern, charge, mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)\n",
    "\n",
    "    return trail\n",
    "\n",
    "\n",
    "@njit\n",
    "def get_trails(seed, pattern, stats, charge_range, mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff):\n",
    "    \"\"\"\n",
    "    Wrapper to extract trails for a given charge range\n",
    "    \"\"\"\n",
    "    trails = []\n",
    "    for charge in charge_range:\n",
    "        trail = grow_trail(seed, pattern, stats, charge, mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)\n",
    "\n",
    "        trails.append(trail)\n",
    "\n",
    "    return trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def plot_pattern(pattern, sorted_hills, centroids, hill_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to plot a pattern\n",
    "    \"\"\"\n",
    "    f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,10))\n",
    "    centroid_dtype = [(\"mz\", float), (\"int\", float), (\"scan_no\", int), (\"rt\", float)]\n",
    "\n",
    "    mzs = []\n",
    "    rts = []\n",
    "    ints = []\n",
    "    for entry in pattern:\n",
    "        hill = sorted_hills[entry]\n",
    "        hill_data = np.array([centroids[_[0]][_[1]] for _ in hill], dtype=centroid_dtype)\n",
    "\n",
    "        int_profile = hill_data[\"int\"]\n",
    "        ax1.plot(hill_data[\"rt\"], hill_data[\"int\"])\n",
    "        ax2.scatter(hill_data[\"rt\"], hill_data[\"mz\"], s = hill_data[\"int\"]/5e5 )\n",
    "\n",
    "\n",
    "    ax1.set_title('Pattern')\n",
    "    ax1.set_xlabel('RT (min)')\n",
    "    ax1.set_ylabel('Intensity')\n",
    "\n",
    "    ax2.set_xlabel('RT (min)')\n",
    "    ax2.set_ylabel('m/z')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@njit\n",
    "def get_minpos(y, split=5):\n",
    "    \"\"\"\n",
    "    Function to get a list of minima in a trace.\n",
    "    A minimum is returned if the ratio of lower of the surrounding maxima to the minimum is larger than the splitting factor.\n",
    "    \"\"\"\n",
    "    minima = get_local_minima(y)\n",
    "    minima_list = List()\n",
    "\n",
    "    for minpos in minima:\n",
    "\n",
    "        minval = y[minpos]\n",
    "        left_side = y[:minpos]\n",
    "        right_side = y[minpos:]\n",
    "\n",
    "        left_max = np.max(left_side)\n",
    "        right_max = np.max(right_side)\n",
    "\n",
    "        minimum_max = np.min(np.array((left_max, right_max)))\n",
    "\n",
    "        if minimum_max / minval > split:\n",
    "            minima_list.append(minpos)\n",
    "\n",
    "    return minima_list\n",
    "\n",
    "@njit\n",
    "def get_local_minima(y):\n",
    "    \"\"\"\n",
    "    Function to return all local minima of a array\n",
    "    \"\"\"\n",
    "    minima = List()\n",
    "    for i in range(1, len(y) - 1):\n",
    "        if is_local_minima(y, i):\n",
    "            minima.append(i)\n",
    "    return minima\n",
    "\n",
    "\n",
    "@njit\n",
    "def is_local_minima(y, i):\n",
    "    return (y[i - 1] > y[i]) & (y[i + 1] > y[i])\n",
    "\n",
    "\n",
    "#export\n",
    "@njit\n",
    "def truncate(array, intensity_profile, seedpos):\n",
    "    \"\"\"\n",
    "    Function to truncate an intensity profile around its seedposition\n",
    "    \"\"\"\n",
    "    minima = int_list_to_array(get_minpos(intensity_profile))\n",
    "\n",
    "    if len(minima) > 0:\n",
    "        left_minima = minima[minima < seedpos]\n",
    "        right_minima = minima[minima > seedpos]\n",
    "\n",
    "        # If the minimum is smaller than the seed\n",
    "        if len(left_minima) > 0:\n",
    "            minpos = left_minima[-1]\n",
    "        else:\n",
    "            minpos = 0\n",
    "\n",
    "        if len(right_minima) > 0:\n",
    "            maxpos = right_minima[0]\n",
    "        else:\n",
    "            maxpos = len(array)\n",
    "\n",
    "        array = array[minpos:maxpos+1]\n",
    "\n",
    "    return array, intensity_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolating Isotope_patterns\n",
    "\n",
    "The extraction of the longest consistent isotope pattern is implemented in `isolate_isotope_pattern`. Here, three additional checks for an isotope pattern are implemented. \n",
    "\n",
    "The first one is `truncate`. Here, one checks the seed position, whether it has a minimum to its left or right side. If a minimum is found, the isotope pattern is cut off at this position.\n",
    "\n",
    "The second one is a mass filter. If the seed has a mass of smaller than 1000, the intensity maximum is detected, and all smaller masses are discarded. This reflects the averagine distribution for small masses where no minimum on the left side can be found.\n",
    "\n",
    "The third one is `check_averagine` that relies on `pattern_to_mz` and `cosine_averagine`. It is used to ensure that the extracted isotope pattern has a cosine correlation of the averagine isotope pattern of the same mass of at least 0.6.\n",
    "\n",
    "After the longest consistent isotope pattern is found, the hills are removed from the pre-isotope pattern, and the process is repeated until no more isotope patterns can be extracted from the pre-isotope patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from alphapept.chem import mass_to_dist\n",
    "from alphapept.constants import averagine_aa, isotopes\n",
    "\n",
    "@njit\n",
    "def check_averagine(stats, pattern, charge, averagine_aa, isotopes):\n",
    "\n",
    "    masses, intensity = pattern_to_mz(stats, pattern, charge)\n",
    "\n",
    "    spec_one = np.floor(masses).astype(np.int64)\n",
    "    int_one = intensity\n",
    "\n",
    "    spec_two, int_two = mass_to_dist(np.min(masses), averagine_aa, isotopes) # maybe change to no rounded version\n",
    "\n",
    "    spec_two = np.floor(spec_two).astype(np.int64)\n",
    "\n",
    "    return cosine_averagine(int_one, int_two, spec_one, spec_two)\n",
    "\n",
    "@njit\n",
    "def pattern_to_mz(stats, pattern, charge):\n",
    "    \"\"\"\n",
    "    Function to calculate masses and intensities from pattern for a given charge\n",
    "    \"\"\"\n",
    "    mzs = np.zeros(len(pattern))\n",
    "    ints = np.zeros(len(pattern))\n",
    "\n",
    "    for i in range(len(pattern)):\n",
    "        entry = pattern[i]\n",
    "        mzs[i] = mz_to_mass(stats[entry,0], charge)\n",
    "        ints[i] = stats[entry,2]\n",
    "\n",
    "    sortindex = np.argsort(mzs)\n",
    "\n",
    "    masses = mzs[sortindex]\n",
    "    intensity = ints[sortindex]\n",
    "\n",
    "    return masses, intensity\n",
    "\n",
    "@njit\n",
    "def cosine_averagine(int_one, int_two, spec_one, spec_two):\n",
    "\n",
    "    min_one, max_one = spec_one[0], spec_one[-1]\n",
    "    min_two, max_two = spec_two[0], spec_two[-1]\n",
    "\n",
    "    min_s = np.min(np.array([min_one, min_two]))\n",
    "    max_s = np.max(np.array([max_one, max_two]))\n",
    "\n",
    "    int_one_scaled = np.zeros(int(max_s - min_s + 1))\n",
    "    int_two_scaled = np.zeros(int(max_s - min_s + 1))\n",
    "\n",
    "    int_one_scaled[spec_one - min_s] = int_one\n",
    "    int_two_scaled[spec_two - min_s] = int_two\n",
    "\n",
    "    corr = np.sum(int_one_scaled * int_two_scaled) / np.sqrt(\n",
    "        np.sum(int_one_scaled ** 2) * np.sum(int_two_scaled ** 2)\n",
    "    )\n",
    "\n",
    "    return corr\n",
    "\n",
    "\n",
    "\n",
    "@njit\n",
    "def int_list_to_array(numba_list):\n",
    "    \"\"\"\n",
    "    Numba compatbilte function to convert a numba list with integers to a numpy array\n",
    "    \"\"\"\n",
    "    array = np.zeros(len(numba_list), dtype=np.int64)\n",
    "\n",
    "    for i in range(len(array)):\n",
    "\n",
    "        array[i] = numba_list[i]\n",
    "\n",
    "    return array\n",
    "\n",
    "M_PROTON = mass_dict['Proton']\n",
    "\n",
    "@njit\n",
    "def mz_to_mass(mz, charge):\n",
    "    \"\"\"\n",
    "    Function to calculate the mass from a mz value.\n",
    "    \"\"\"\n",
    "    if charge < 0:\n",
    "        raise NotImplementedError(\"Negative Charges not implemented.\")\n",
    "\n",
    "    mass = mz * charge - charge * M_PROTON\n",
    "\n",
    "    return mass\n",
    "\n",
    "\n",
    "@njit\n",
    "def get_minpos(y, split=5):\n",
    "    \"\"\"\n",
    "    Function to get a list of minima in a trace.\n",
    "    A minimum is returned if the ratio of lower of the surrounding maxima to the minimum is larger than the splitting factor.\n",
    "    \"\"\"\n",
    "    minima = get_local_minima(y)\n",
    "    minima_list = List()\n",
    "\n",
    "    for minpos in minima:\n",
    "\n",
    "        minval = y[minpos]\n",
    "        left_side = y[:minpos]\n",
    "        right_side = y[minpos:]\n",
    "\n",
    "        left_max = np.max(left_side)\n",
    "        right_max = np.max(right_side)\n",
    "\n",
    "        minimum_max = np.min(np.array((left_max, right_max)))\n",
    "\n",
    "        if minimum_max / minval > split:\n",
    "            minima_list.append(minpos)\n",
    "\n",
    "    return minima_list\n",
    "\n",
    "@njit\n",
    "def get_local_minima(y):\n",
    "    \"\"\"\n",
    "    Function to return all local minima of a array\n",
    "    \"\"\"\n",
    "    minima = List()\n",
    "    for i in range(1, len(y) - 1):\n",
    "        if is_local_minima(y, i):\n",
    "            minima.append(i)\n",
    "    return minima\n",
    "\n",
    "\n",
    "@njit\n",
    "def is_local_minima(y, i):\n",
    "    return (y[i - 1] > y[i]) & (y[i + 1] > y[i])\n",
    "\n",
    "\n",
    "#export\n",
    "@njit\n",
    "def truncate(array, intensity_profile, seedpos):\n",
    "    \"\"\"\n",
    "    Function to truncate an intensity profile around its seedposition\n",
    "    \"\"\"\n",
    "    minima = int_list_to_array(get_minpos(intensity_profile))\n",
    "\n",
    "    if len(minima) > 0:\n",
    "        left_minima = minima[minima < seedpos]\n",
    "        right_minima = minima[minima > seedpos]\n",
    "\n",
    "        # If the minimum is smaller than the seed\n",
    "        if len(left_minima) > 0:\n",
    "            minpos = left_minima[-1]\n",
    "        else:\n",
    "            minpos = 0\n",
    "\n",
    "        if len(right_minima) > 0:\n",
    "            maxpos = right_minima[0]\n",
    "        else:\n",
    "            maxpos = len(array)\n",
    "\n",
    "        array = array[minpos:maxpos+1]\n",
    "        intensity_profile = intensity_profile[minpos:maxpos+1]\n",
    "\n",
    "    return array, intensity_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "if False:\n",
    "    def test_get_minpos():\n",
    "        \"\"\"\n",
    "        Generate an intensity profile with local minima\n",
    "        Check that the minima are found\n",
    "\n",
    "        \"\"\"\n",
    "        intensity_profile = np.ones(20) * 10\n",
    "\n",
    "        minima_ref = [3, 7, 10, 17]\n",
    "\n",
    "        for minimum in minima_ref:\n",
    "            intensity_profile[minimum] = 1\n",
    "\n",
    "        minima = get_minpos(intensity_profile)\n",
    "\n",
    "        minima_list = [_ for _ in minima]\n",
    "\n",
    "        assert minima_list == minima_ref\n",
    "\n",
    "    test_get_minpos() \n",
    "\n",
    "    def test_truncate():\n",
    "        \"\"\"\n",
    "        Generate an intensity profile with local minima\n",
    "        Check wheter the the profile is correctly truncated with respect to the seed\n",
    "\n",
    "        \"\"\"\n",
    "        array = np.arange(0, 20)\n",
    "        intensity_profile = np.ones(20) * 10\n",
    "\n",
    "        minima_ref = [3, 7, 10, 17]\n",
    "\n",
    "        for minimum in minima_ref:\n",
    "            intensity_profile[minimum] = 1\n",
    "\n",
    "        seedpos = 5\n",
    "        truncated = truncate(array, intensity_profile, seedpos)\n",
    "        assert np.all(truncated == np.array([3, 4, 5, 6, 7]))\n",
    "\n",
    "        seedpos = 0\n",
    "        truncated = truncate(array, intensity_profile, seedpos)\n",
    "        assert np.all(truncated == np.array([0, 1, 2, 3]))\n",
    "\n",
    "        seedpos = len(array)\n",
    "        truncated = truncate(array, intensity_profile, seedpos)\n",
    "        assert np.all(truncated == np.array([17, 18, 19]))\n",
    "\n",
    "    test_truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isotope Patterns\n",
    "\n",
    "The wrapper function `get_isotope_patterns` iterates over all pre_isotope_patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def isolate_isotope_pattern(pre_pattern, hill_ptrs, hill_data, int_data, scan_idx, stats, sortindex_, mass_range, charge_range, averagine_aa, isotopes, seed_masses, cc_cutoff):\n",
    "    \"\"\"\n",
    "    Isolate isotope patterns\n",
    "    \"\"\"\n",
    "\n",
    "    longest_trace = 0\n",
    "    champion_trace = None\n",
    "    champion_charge = 0\n",
    "\n",
    "    # Sort patterns by mass\n",
    "    \n",
    "    sortindex = np.argsort(stats[pre_pattern][:,0]) #intensity\n",
    "    sorted_pattern = pre_pattern[sortindex]\n",
    "    massindex = np.argsort(stats[sorted_pattern][:,2])[::-1][:seed_masses]\n",
    "\n",
    "    # Use all the elements in the pre_pattern as seed\n",
    "\n",
    "    for seed in massindex:  # Loop through all seeds\n",
    "        seed_global = sorted_pattern[seed]\n",
    "\n",
    "        trails = get_trails(seed, sorted_pattern, stats, charge_range, mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)\n",
    "\n",
    "        for index, trail in enumerate(trails):\n",
    "            if len(trail) > longest_trace:  # Needs to be longer than the current champion\n",
    "\n",
    "                arr = int_list_to_array(trail)\n",
    "                intensity_profile = stats[arr][:,2]\n",
    "                seedpos = np.nonzero(arr==seed_global)[0][0]\n",
    "\n",
    "                # truncate around the seed...\n",
    "                arr, intensity_profile = truncate(arr, intensity_profile, seedpos)\n",
    "\n",
    "                # Remove lower masses:\n",
    "                # Take the index of the maximum and remove all masses on the left side\n",
    "                if charge_range[index] * stats[seed_global, 0] < 1000:\n",
    "                    maxpos = np.argmax(intensity_profile)\n",
    "                    arr = arr[maxpos:]\n",
    "\n",
    "                if len(arr) > longest_trace:\n",
    "                    # Averagine check\n",
    "                    cc = check_averagine(stats, arr, charge_range[index], averagine_aa, isotopes)\n",
    "                    if cc > 0.6:\n",
    "                        # Update the champion\n",
    "                        champion_trace = arr\n",
    "                        champion_charge = charge_range[index]\n",
    "                        longest_trace = len(arr)\n",
    "                        \n",
    "    return champion_trace, champion_charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def test_get_isotope_patterns():\n",
    "\n",
    "        test_centroids = [\n",
    "            [\n",
    "                (300, 50, 1, 1),\n",
    "                (300.501, 40, 1, 1),\n",
    "                (301.003, 30, 1, 1),\n",
    "                (301.504, 20, 1, 1),\n",
    "                (302.006, 10, 1, 1),\n",
    "            ],\n",
    "            [\n",
    "                (300, 50, 2, 2),\n",
    "                (300.501, 40, 2, 2),\n",
    "                (301.003, 30, 2, 2),\n",
    "                (301.504, 20, 2, 2),\n",
    "                (302.006, 10, 2, 2),\n",
    "            ],\n",
    "            [\n",
    "                (300, 50, 3, 3),\n",
    "                (300.501, 40, 3, 3),\n",
    "                (301.003, 30, 3, 3),\n",
    "                (301.504, 20, 3, 3),\n",
    "                (302.006, 10, 3, 3),\n",
    "            ],\n",
    "            [\n",
    "                (300, 50, 4, 4),\n",
    "                (300.501, 40, 4, 4),\n",
    "                (301.003, 30, 4, 4),\n",
    "                (301.504, 20, 4, 4),\n",
    "                (302.006, 10, 4, 4),\n",
    "            ],\n",
    "            [\n",
    "                (300, 50, 5, 5),\n",
    "                (300.501, 40, 5, 5),\n",
    "                (301.003, 30, 5, 5),\n",
    "                (301.504, 20, 5, 5),\n",
    "                (302.006, 10, 5, 5),\n",
    "            ],\n",
    "            [(400, 10, 6, 6), (401, 10, 6, 6), (402, 10, 6, 6)],\n",
    "            [(400, 10, 7, 7), (401, 10, 7, 7), (402, 10, 7, 7)],\n",
    "            [(400, 10, 8, 8), (401, 10, 8, 8), (402, 10, 8, 8)],\n",
    "            [(400, 10, 9, 9), (401, 10, 9, 9), (402, 10, 9, 9)],\n",
    "        ]\n",
    "\n",
    "        centroid_dtype = [(\"mz\", float), (\"int\", float), (\"scan_no\", int), (\"rt\", float)]\n",
    "        test_centroids_tmp = [np.array(_, dtype=centroid_dtype) for _ in test_centroids]\n",
    "\n",
    "        test_centroids = List([_ for _ in test_centroids_tmp])\n",
    "\n",
    "        test_hills = get_hills(test_centroids)\n",
    "        sorted_hills, stats, data = get_hill_data(test_hills, test_centroids)\n",
    "        pre_patterns = get_edges(stats, data)\n",
    "\n",
    "        isotope_patterns, isotope_charges = get_isotope_patterns(pre_patterns, stats, data, averagine_aa, isotopes)\n",
    "        assert np.all(isotope_patterns[0] == np.array([0, 1, 2, 3, 4]))\n",
    "        assert isotope_charges[0] == 2\n",
    "        assert np.all(isotope_patterns[1] == np.array([5,6,7]))\n",
    "        assert isotope_charges[1] == 1\n",
    "\n",
    "    test_get_isotope_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from numba.typed import List\n",
    "\n",
    "def get_isotope_patterns(pre_isotope_patterns, hill_ptrs, hill_data, int_data, scan_idx, stats, sortindex_,  averagine_aa, isotopes, min_charge = 1, max_charge = 6, mass_range = 5, seed_masses = 100, cc_cutoff=0.6, callback=None):\n",
    "    \"\"\"\n",
    "    Wrapper function to iterate over pre_isotope_patterns\n",
    "    \"\"\"\n",
    "\n",
    "    isotope_patterns = []\n",
    "    isotope_charges = []\n",
    "\n",
    "    charge_range = List()\n",
    "\n",
    "    for i in range(min_charge, max_charge + 1):\n",
    "        charge_range.append(i)\n",
    "\n",
    "    isotope_patterns = []\n",
    "    isotope_charges = []\n",
    "\n",
    "    for idx, pre_pattern in enumerate(pre_isotope_patterns):\n",
    "        extract = True\n",
    "        while extract:\n",
    "            isotope_pattern, isotope_charge = isolate_isotope_pattern(np.array(pre_pattern), hill_ptrs, hill_data, int_data, scan_idx, stats, sortindex_, mass_range, charge_range, averagine_aa, isotopes, seed_masses, cc_cutoff)\n",
    "            if isotope_pattern is None:\n",
    "                length = 0\n",
    "            else:\n",
    "                length = len(isotope_pattern)\n",
    "\n",
    "            if length > 1:\n",
    "                isotope_charges.append(isotope_charge)\n",
    "                isotope_patterns.append(isotope_pattern)\n",
    "\n",
    "                pre_pattern = [_ for _ in pre_pattern if _ not in isotope_pattern]\n",
    "\n",
    "                if len(pre_pattern) <= 1:\n",
    "                    extract = False\n",
    "            else:\n",
    "                extract = False\n",
    "\n",
    "\n",
    "        if callback:\n",
    "            callback((idx+1)/len(pre_isotope_patterns))\n",
    "            \n",
    "            \n",
    "    iso_patterns = np.zeros(sum([len(_) for _ in isotope_patterns]), dtype=np.int64)\n",
    "    \n",
    "    iso_idx = np.zeros(len(isotope_patterns)+1, dtype='int')\n",
    "    \n",
    "    \n",
    "    start = 0\n",
    "    for idx, _ in enumerate(isotope_patterns):\n",
    "        iso_patterns[start:start+len(_)] = _\n",
    "        start += len(_)\n",
    "        iso_idx[idx+1] = start\n",
    "    \n",
    "        \n",
    "\n",
    "    return iso_patterns, iso_idx, np.array(isotope_charges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@parallel_compiled_func(cpu_only=True)\n",
    "def report_(idx, isotope_charges, isotope_patterns, iso_idx, stats, sortindex_, hill_ptrs, hill_data, int_data, rt_, rt_idx, results):\n",
    "    if idx == -1:\n",
    "        x = alphapept.speed.cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "        \n",
    "    i = x\n",
    "        \n",
    "    pattern = isotope_patterns[iso_idx[i]:iso_idx[i+1]]\n",
    "    isotope_data = stats[pattern]\n",
    "\n",
    "    mz = np.min(isotope_data[:, 0])\n",
    "    mz_std = np.mean(isotope_data[:, 1])\n",
    "    charge = isotope_charges[i]\n",
    "    mass = mz_to_mass(mz, charge)\n",
    "    int_max_idx = np.argmax(isotope_data[:, 2])\n",
    "    mz_most_abundant = isotope_data[:, 0][int_max_idx]\n",
    "    \n",
    "    int_max = isotope_data[:,2][int_max_idx]\n",
    "\n",
    "    rt_start = isotope_data[int_max_idx, 4] # This is the start of the most abundant trace\n",
    "    rt_end = isotope_data[int_max_idx, 5]\n",
    "\n",
    "    int_sum = np.sum(isotope_data[:, 2])\n",
    "    \n",
    "    # better measurement of the peak with interpolation\n",
    "    \n",
    "    rt_min_ = min(isotope_data[:, 4])\n",
    "    rt_max_ = max(isotope_data[:, 5])\n",
    "    \n",
    "    rt_range = np.linspace(rt_min_, rt_max_, 100)\n",
    "    trace_sum = np.zeros_like(rt_range)\n",
    "    \n",
    "    for k in pattern:\n",
    "        x = sortindex_[k]\n",
    "        \n",
    "        start = hill_ptrs[x]\n",
    "        end = hill_ptrs[x + 1]\n",
    "        idx_ = hill_data[start:end]\n",
    "        int_ = int_data[idx_]        \n",
    "        rts = rt_[rt_idx[idx_]]\n",
    "\n",
    "        interpolation = np.interp(rt_range, rts, int_)\n",
    "        \n",
    "        #Filter \n",
    "        \n",
    "        interpolation[:(rt_range < rts[0]).sum()] = 0\n",
    "        \n",
    "        right_cut = (rt_range > rts[-1]).sum()\n",
    "        if right_cut > 0:\n",
    "            interpolation[-right_cut:]= 0 \n",
    "        \n",
    "        trace_sum += interpolation\n",
    "        \n",
    "\n",
    "        #plt.plot(rt_range, interpolation)\n",
    "\n",
    "            \n",
    "    rt_apex_idx = trace_sum.argmax()\n",
    "    rt_apex = rt_range[rt_apex_idx]\n",
    "    \n",
    "    trace = trace_sum\n",
    "    half_max = trace.max()/2\n",
    "    \n",
    "    if rt_apex_idx == 0:\n",
    "        left_apex = 0\n",
    "    else:\n",
    "        left_apex = np.abs(trace[:rt_apex_idx]-half_max).argmin()\n",
    "    right_apex = np.abs(trace[rt_apex_idx:]-half_max).argmin()+rt_apex_idx\n",
    "    \n",
    "    int_apex = trace_sum[rt_apex_idx]\n",
    "\n",
    "    fwhm = rt_range[right_apex] - rt_range[left_apex]\n",
    "    \n",
    "    n_isotopes = len(pattern)\n",
    "\n",
    "    rt_cutoff = 0.95 #5%\n",
    "    if rt_apex_idx == 0:\n",
    "        rt_min_idx = 0\n",
    "    else:\n",
    "        rt_min_idx = np.abs(trace[:rt_apex_idx]-trace.max()*(1-rt_cutoff)).argmin()\n",
    "    rt_max_idx = np.abs(trace[rt_apex_idx:]-trace.max()*(1-rt_cutoff)).argmin()+rt_apex_idx\n",
    "\n",
    "    #plt.xlabel('rt')\n",
    "    #plt.ylabel('int')\n",
    "    #plt.show()\n",
    "    #plt.plot(rt_range, trace_sum)\n",
    "\n",
    "    #plt.plot([rt_range[left_apex], rt_range[right_apex]], [(trace[left_apex] + trace[right_apex])/2]*2, 'k:')\n",
    "\n",
    "    #plt.plot(rt_range[rt_apex_idx], trace[rt_apex_idx], 'k*')\n",
    "    #plt.plot(rt_range[rt_min_idx], trace[rt_min_idx], 'k*')\n",
    "    #plt.plot(rt_range[rt_max_idx], trace[rt_max_idx], 'k*')\n",
    "\n",
    "    #plt.show()\n",
    "            \n",
    "    rt_start = rt_range[rt_min_idx]\n",
    "    rt_end = rt_range[rt_max_idx]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    int_area = np.trapz(trace_sum[rt_min_idx:rt_max_idx], rt_range[rt_min_idx:rt_max_idx])\n",
    "    int_sum = trace_sum.sum()\n",
    "        \n",
    "    results[i,:] = np.array([mz, mz_std, mz_most_abundant, charge, rt_start, rt_apex, rt_end, fwhm, n_isotopes, mass, int_apex, int_area, int_sum])\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "\n",
    "def feature_finder_report(query_data, isotope_patterns, isotope_charges, iso_idx, stats, sortindex_, hill_ptrs, hill_data,):\n",
    "    rt_ = np.array(query_data['rt_list_ms1'])\n",
    "    indices_ = np.array(query_data['indices_ms1'])\n",
    "    mass_data = np.array(query_data['mass_list_ms1'])\n",
    "    rt_idx = np.searchsorted(indices_, np.arange(len(mass_data)), side='right') - 1\n",
    "    \n",
    "    int_data = np.array(query_data['int_list_ms1'])\n",
    "\n",
    "    results = np.zeros((len(isotope_charges), 13))\n",
    "\n",
    "    report_(isotope_charges, isotope_patterns, iso_idx, stats, sortindex_, hill_ptrs, hill_data, int_data, rt_, rt_idx, results)\n",
    "\n",
    "    df = pd.DataFrame(results, columns = ['mz','mz_std','mz_most_abundant','charge','rt_start','rt_apex','rt_end','fwhm','n_isotopes','mass','int_apex','int_area', 'int_sum'])\n",
    "\n",
    "    df.sort_values(['rt_start','mz'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Output\n",
    "\n",
    "For each feature that is found we extract summary statistics and put it in tabular form to be used as as pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "For quality control reasons we also employ a function to plot a feature in its local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_isotope_pattern(index, df, sorted_stats, centroids, scan_range=100, mz_range=2, plot_hills = False):\n",
    "    \"\"\"\n",
    "    Plot an isotope pattern in its local environment\n",
    "    \"\"\"\n",
    "\n",
    "    markersize = 10\n",
    "    plot_offset_mz = 1\n",
    "    plot_offset_rt = 2\n",
    "\n",
    "    feature =  df.loc[index]\n",
    "\n",
    "    scan = rt_dict[feature['rt_apex']]\n",
    "\n",
    "    start_scan = scan-scan_range\n",
    "    end_scan = scan+scan_range\n",
    "\n",
    "    mz_min = feature['mz']-mz_range-plot_offset_mz\n",
    "    mz_max = feature['mz']+mz_range+plot_offset_mz\n",
    "\n",
    "    sub_data = np.hstack(centroids[start_scan:end_scan])\n",
    "\n",
    "    selection = sub_data[(sub_data['mz']>mz_min) & (sub_data['mz']<mz_max)]\n",
    "\n",
    "    min_rt = selection['rt'].min() - plot_offset_rt\n",
    "    max_rt = selection['rt'].max() + plot_offset_rt\n",
    "\n",
    "    hill_selection = sorted_stats[(sorted_stats['mz_avg']>mz_min) & (sorted_stats['mz_avg']<mz_max) & (sorted_stats['rt_max']<max_rt) & (sorted_stats['rt_min']>min_rt)]\n",
    "\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.scatter(selection['rt'], selection['mz'], c= np.log(selection['int']), marker='s', s=markersize, alpha=0.9)\n",
    "    plt.colorbar()\n",
    "    plt.grid(False)\n",
    "    plt.xlabel('RT (min)')\n",
    "    plt.ylabel('m/z')\n",
    "\n",
    "    box_height = mz_range/50\n",
    "\n",
    "    if plot_hills:\n",
    "        for hill in hill_selection:\n",
    "            bbox = [hill['rt_min'], hill['mz_avg']-box_height, hill['rt_max'], hill['mz_avg']+box_height]\n",
    "\n",
    "            rect = plt.Rectangle((bbox[0], bbox[1]),\n",
    "                                      bbox[2] - bbox[0],\n",
    "                                      bbox[3] - bbox[1], fill=False,\n",
    "                                      edgecolor='w', linewidth=1, alpha = 0.3)\n",
    "            plt.gca().add_patch(rect)\n",
    "\n",
    "\n",
    "    feature_selection = df[(df['mz']>mz_min) & (df['mz']<mz_max) & (df['rt_end']<max_rt) & (df['rt_start']>min_rt)]\n",
    "\n",
    "    for f_idx in feature_selection.index:\n",
    "        for c_idx in range(len(sorted_stats[isotope_patterns[f_idx]])-1):\n",
    "\n",
    "            start = sorted_stats[isotope_patterns[f_idx]][c_idx]\n",
    "            end = sorted_stats[isotope_patterns[f_idx]][c_idx+1]\n",
    "\n",
    "            start_mass = start['mz_avg']\n",
    "            start_rt = (start['rt_min']+start['rt_max'])/2\n",
    "\n",
    "            end_mass = end['mz_avg']\n",
    "            end_rt = (end['rt_min']+end['rt_max'])/2\n",
    "\n",
    "            plt.plot([start_rt, end_rt], [start_mass, end_mass], '+', color='y')\n",
    "            plt.plot([start_rt, end_rt], [start_mass, end_mass], ':', color='y')\n",
    "\n",
    "        if plot_hills:\n",
    "            for hill_idx in isotope_patterns[f_idx]:\n",
    "\n",
    "                hill = sorted_stats[hill_idx]\n",
    "                bbox = [hill['rt_min'], hill['mz_avg']-box_height, hill['rt_max'], hill['mz_avg']+box_height]\n",
    "\n",
    "                rect = plt.Rectangle((bbox[0], bbox[1]),\n",
    "                                          bbox[2] - bbox[0],\n",
    "                                          bbox[3] - bbox[1], fill=False,\n",
    "                                          edgecolor='g', linewidth=1, alpha = 0.8)\n",
    "                plt.gca().add_patch(rect)\n",
    "\n",
    "\n",
    "    plt.xlim([min_rt+plot_offset_rt, max_rt-plot_offset_rt])\n",
    "    plt.ylim([mz_min+plot_offset_mz, mz_max-plot_offset_mz])\n",
    "    plt.title('Pattern')\n",
    "    plt.show()\n",
    "\n",
    "    plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Feature Finder\n",
    "\n",
    "To utilize the command-line Feature Finder from Bruker `4DFF-3.13` - `uff-cmdline2.exe`, we call it via a subprocess and wait until completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import subprocess\n",
    "import os\n",
    "import platform \n",
    "\n",
    "\n",
    "def extract_bruker(file, base_dir = \"ext/bruker/FF\", config = \"proteomics_4d.config\"):\n",
    "    \"\"\"\n",
    "    Call Bruker Feautre Finder via subprocess\n",
    "    \"\"\"\n",
    "\n",
    "    feature_path = file + '/'+ os.path.split(file)[-1] + '.features'\n",
    "\n",
    "    base_dir = os.path.join(os.path.dirname(__file__), base_dir)\n",
    "    \n",
    "    if os.path.exists(feature_path):\n",
    "        return feature_path\n",
    "    else:\n",
    "        \n",
    "        operating_system = platform.system()\n",
    "        \n",
    "        if operating_system == 'Linux':\n",
    "            ff_dir = os.path.join(base_dir, 'linux64','uff-cmdline2')\n",
    "            logging.info('Using Linux FF')\n",
    "        elif operating_system == 'Windows':\n",
    "            ff_dir = os.path.join(base_dir, 'win64','uff-cmdline2.exe')\n",
    "            logging.info('Using Windows FF')\n",
    "        else:\n",
    "            raise NotImplementedError(f\"System {operating_system} not supported.\")\n",
    "        \n",
    "        if not os.path.isfile(ff_dir):\n",
    "            raise FileNotFoundError(f'Bruker feature finder cmd not found here {ff_dir}.')\n",
    "\n",
    "        config_path = base_dir + '/'+ config\n",
    "\n",
    "        if not os.path.isfile(config_path):\n",
    "            raise FileNotFoundError(f'Config file not found here {config_path}.')\n",
    "\n",
    "        if operating_system == 'Windows':\n",
    "            FF_parameters = [ff_dir,'--ff 4d',f'--readconfig \"{config_path}\"', f'--analysisDirectory \"{file}\"']\n",
    "\n",
    "            process = subprocess.Popen(' '.join(FF_parameters), stdout=subprocess.PIPE)\n",
    "            for line in iter(process.stdout.readline, b''):\n",
    "                logtxt = line.decode('utf8')\n",
    "                logging.info(logtxt[48:].rstrip()) #Remove logging info from FF\n",
    "        elif operating_system == 'Linux':\n",
    "            FF_parameters = [\n",
    "                ff_dir,\n",
    "                '--ff',\n",
    "                '4d',\n",
    "                '--readconfig',\n",
    "                config_path,\n",
    "                '--analysisDirectory',\n",
    "                file\n",
    "            ]\n",
    "            process = subprocess.run(FF_parameters, stdout=subprocess.PIPE)\n",
    "\n",
    "        if os.path.exists(feature_path):\n",
    "            return feature_path\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Feature file {feature_path} does not exist.\")\n",
    "\n",
    "\n",
    "import sqlalchemy as db\n",
    "\n",
    "def convert_bruker(feature_path):\n",
    "    \"\"\"\n",
    "    Reads feature table and converts to feature table to be used with AlphaPept\n",
    "\n",
    "    \"\"\"\n",
    "    engine_featurefile = db.create_engine('sqlite:///{}'.format(feature_path))\n",
    "    feature_table = pd.read_sql_table('LcTimsMsFeature', engine_featurefile)\n",
    "\n",
    "    from alphapept.constants import mass_dict\n",
    "\n",
    "    M_PROTON = mass_dict['Proton']\n",
    "    feature_table['Mass'] = feature_table['MZ'].values * feature_table['Charge'].values - feature_table['Charge'].values*M_PROTON\n",
    "    feature_table = feature_table.rename(columns={\"MZ\": \"mz\",\"Mass\": \"mass\", \"RT\": \"rt_apex\", \"RT_lower\":\"rt_start\", \"RT_upper\":\"rt_end\", \"Mobility\": \"mobility\", \"Mobility_lower\": \"mobility_lower\", \"Mobility_upper\": \"mobility_upper\", \"Charge\":\"charge\",\"Intensity\":'int_sum'})\n",
    "    feature_table['rt_apex'] = feature_table['rt_apex']/60\n",
    "    feature_table['rt_start'] = feature_table['rt_start']/60\n",
    "    feature_table['rt_end'] = feature_table['rt_end']/60\n",
    "\n",
    "    return feature_table\n",
    "\n",
    "\n",
    "def map_bruker(feature_path, feature_table, query_data):\n",
    "    \"\"\"\n",
    "    Map Ms1 to Ms2 via Table FeaturePrecursorMapping from Bruker FF\n",
    "    \"\"\"\n",
    "    engine_featurefile = db.create_engine('sqlite:///{}'.format(feature_path))\n",
    "\n",
    "    mapping = pd.read_sql_table('FeaturePrecursorMapping', engine_featurefile)\n",
    "    mapping = mapping.set_index('PrecursorId')\n",
    "    feature_table= feature_table.set_index('Id')\n",
    "\n",
    "\n",
    "    query_prec_id = query_data['prec_id']\n",
    "\n",
    "    #Now look up the feature for each precursor\n",
    "\n",
    "    mass_matched = []\n",
    "    mz_matched = []\n",
    "    rt_matched = []\n",
    "    query_idx = []\n",
    "    f_idx = []\n",
    "\n",
    "    for idx, prec_id in tqdm(enumerate(query_prec_id)):\n",
    "        try:\n",
    "            f_id = mapping.loc[prec_id]['FeatureId']\n",
    "            all_matches = feature_table.loc[f_id]\n",
    "            if type(f_id) == np.int64:\n",
    "                match = all_matches\n",
    "                mz_matched.append(match['mz'])\n",
    "                rt_matched.append(match['rt_apex'])\n",
    "                mass_matched.append(match['mass'])\n",
    "                query_idx.append(idx)\n",
    "                f_idx.append(match['FeatureId'])\n",
    "\n",
    "            else:\n",
    "                for k in range(len(all_matches)):\n",
    "                    match = all_matches.iloc[k]\n",
    "                    mz_matched.append(match['mz'])\n",
    "                    rt_matched.append(match['rt_apex'])\n",
    "                    mass_matched.append(match['mass'])\n",
    "                    query_idx.append(idx)\n",
    "                    f_idx.append(match['FeatureId'])\n",
    "\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    features = pd.DataFrame(np.array([mass_matched, mz_matched, rt_matched, query_idx, f_idx]).T, columns = ['mass_matched', 'mz_matched', 'rt_matched', 'query_idx', 'feature_idx'])\n",
    "\n",
    "    features['query_idx'] = features['query_idx'].astype('int')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from alphapept.search import query_data_to_features\n",
    "import alphapept.io\n",
    "import functools\n",
    "\n",
    "\n",
    "def find_features(to_process, callback = None, parallel = False):\n",
    "    \"\"\"\n",
    "    Wrapper for feature finding\n",
    "    \"\"\"\n",
    "    try:\n",
    "        index, settings = to_process\n",
    "        file_name = settings['experiment']['file_paths'][index]\n",
    "\n",
    "        base, ext = os.path.splitext(file_name)\n",
    "        \n",
    "        if ext.lower() == '.raw':\n",
    "            datatype='thermo'\n",
    "        elif ext.lower() == '.d':\n",
    "            datatype='bruker'\n",
    "        else:\n",
    "            raise NotImplementedError('File extension {} not understood.'.format(ext))\n",
    "\n",
    "        out_file = f\"{base}.ms_data.hdf\"\n",
    "         \n",
    "        skip = True\n",
    "        if os.path.isfile(out_file):\n",
    "            try:\n",
    "                alphapept.io.MS_Data_File(\n",
    "                    out_file\n",
    "                ).read(dataset_name=\"features\")\n",
    "                logging.info(\n",
    "                    'Found *.hdf with features for {}'.format(out_file)\n",
    "                )\n",
    "            except KeyError:\n",
    "\n",
    "                logging.info(\n",
    "                    'No *.hdf file with features found for {}. Adding to feature finding list.'.format(out_file)\n",
    "                )\n",
    "                skip = False\n",
    "                \n",
    "        if not skip:\n",
    "            ms_file = alphapept.io.MS_Data_File(out_file, is_read_only=False)\n",
    "            query_data = ms_file.read_DDA_query_data()\n",
    "\n",
    "            if not settings['workflow'][\"find_features\"]:\n",
    "                features = query_data_to_features(query_data)\n",
    "            else:\n",
    "                if datatype == 'thermo':\n",
    "                    \n",
    "                    from alphapept.constants import averagine_aa, isotopes\n",
    "                    \n",
    "                    f_settings = settings['features']\n",
    "                    max_gap = f_settings['max_gap']\n",
    "                    ppm_tol = f_settings['ppm_tol']\n",
    "                    split_level = f_settings['split_level']\n",
    "                    window = f_settings['smoothing_window']\n",
    "                    large_peak = f_settings['large_peak']\n",
    "                    \n",
    "                    min_charge = f_settings['min_charge']\n",
    "                    max_charge = f_settings['max_charge']\n",
    "                    seed_masses = f_settings['seed_masses']\n",
    "                    \n",
    "                    hill_nboot_max = f_settings['hill_nboot_max']\n",
    "                    hill_nboot = f_settings['hill_nboot']\n",
    "                    \n",
    "                    mass_range = f_settings['mass_range']\n",
    "                    \n",
    "                    min_correlation = f_settings['min_correlation']\n",
    "                    \n",
    "                    logging.info('Feature finding on {}'.format(file_name))\n",
    "                    hill_ptrs, hill_data, path_node_cnt, score_median, score_std = extract_hills(query_data, max_gap = max_gap, ppm_tol = ppm_tol)\n",
    "                    logging.info(f'Number of hills {len(hill_ptrs):,}, len = {np.mean(path_node_cnt):.2f}')\n",
    "                    \n",
    "                    logging.info(f'Repeating hill extraction with ppm_tol {score_median+score_std*3:.2f}')\n",
    "                    \n",
    "                    hill_ptrs, hill_data, path_node_cnt, score_median, score_std = extract_hills(query_data, max_gap = max_gap, ppm_tol = score_median+score_std*3)\n",
    "                    logging.info(f'Number of hills {len(hill_ptrs):,}, len = {np.mean(path_node_cnt):.2f}')\n",
    "\n",
    "                    int_data = np.array(query_data['int_list_ms1'])\n",
    "\n",
    "                    hill_ptrs = split_hills(hill_ptrs, hill_data, int_data, split_level=split_level, window = window) #hill lenght is inthere already\n",
    "                    logging.info(f'After split hill_ptrs {len(hill_ptrs):,}')\n",
    "\n",
    "                    hill_data, hill_ptrs = filter_hills(hill_data, hill_ptrs, int_data, large_peak =large_peak, window=window)\n",
    "\n",
    "                    logging.info(f'After filter hill_ptrs {len(hill_ptrs):,}')\n",
    "\n",
    "                    stats, sortindex_, idxs_upper, scan_idx = get_hill_data(query_data, hill_ptrs, hill_data, hill_nboot_max = hill_nboot_max, hill_nboot = hill_nboot)\n",
    "                    logging.info('Extracting hill stats complete')\n",
    "\n",
    "                    pre_isotope_patterns = get_pre_isotope_patterns(stats, idxs_upper, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, maximum_offset, min_charge=min_charge, max_charge=max_charge, mass_range=mass_range, cc_cutoff=min_correlation)\n",
    "                    logging.info('Found {:,} pre isotope patterns.'.format(len(pre_isotope_patterns)))\n",
    "\n",
    "                    isotope_patterns, iso_idx, isotope_charges = get_isotope_patterns(pre_isotope_patterns, hill_ptrs, hill_data, int_data, scan_idx, stats, sortindex_, averagine_aa, isotopes, min_charge = min_charge, max_charge = max_charge, mass_range = mass_range, seed_masses = seed_masses, cc_cutoff = min_correlation, callback=None)\n",
    "                    logging.info('Extracted {:,} isotope patterns.'.format(len(isotope_charges)))\n",
    "\n",
    "                    feature_table = feature_finder_report(query_data, isotope_patterns, isotope_charges, iso_idx, stats, sortindex_, hill_ptrs, hill_data)\n",
    "                    \n",
    "                    logging.info('Report complete.')\n",
    "\n",
    "                elif datatype == 'bruker':\n",
    "                    logging.info('Feature finding on {}'.format(file_name))\n",
    "                    feature_path = extract_bruker(file_name)\n",
    "                    feature_table = convert_bruker(feature_path)\n",
    "                    logging.info('Bruker featurer finder complete. Extracted {:,} features.'.format(len(feature_table)))\n",
    "\n",
    "                logging.info('Matching features to query data.')\n",
    "                features = map_ms2(feature_table, query_data, **settings['features'])\n",
    "\n",
    "                logging.info('Saving feature table.')\n",
    "                ms_file.write(feature_table, dataset_name=\"feature_table\")\n",
    "                logging.info('Feature table saved to {}'.format(out_file))\n",
    "\n",
    "\n",
    "            logging.info('Saving features.')\n",
    "            ms_file.write(features, dataset_name=\"features\")\n",
    "            logging.info(f'Feature finding of file {file_name} complete.')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f'Feature finding of file {file_name} failed. Exception {e}')\n",
    "        return f\"{e}\" #Can't return exception object, cast as string  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping\n",
    "\n",
    "Mapping MS1 to MS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def map_ms2(feature_table, query_data, ppm_range = 20, rt_range = 0.5, mob_range = 0.3, n_neighbors=5, search_unidentified = False, **kwargs):\n",
    "    \"\"\"\n",
    "    Map MS1 features to MS2 based on rt and mz\n",
    "    if ccs is included also add\n",
    "    \"\"\"\n",
    "\n",
    "    if 'mobility' in feature_table.columns:\n",
    "        use_mob = True\n",
    "    else:\n",
    "        use_mob = False\n",
    "\n",
    "    if use_mob:\n",
    "\n",
    "        tree_points = feature_table[['mz','rt_apex','mobility']].values\n",
    "\n",
    "        tree_points[:,0] = np.log(tree_points[:,0])*1e6/ppm_range #m/z -> log transform, this is in ppm then\n",
    "        tree_points[:,1] = tree_points[:,1]/rt_range # -> this is in minutes\n",
    "        tree_points[:,2] = tree_points[:,2]/mob_range\n",
    "\n",
    "        matching_tree = KDTree(tree_points, metric=\"minkowski\")\n",
    "\n",
    "        query_mz = np.log(query_data['mono_mzs2'])*1e6/ppm_range\n",
    "        query_rt = query_data['rt_list_ms2'] / rt_range\n",
    "        query_mob = query_data['mobility'] / mob_range\n",
    "\n",
    "        ref_points = np.array([query_mz, query_rt, query_mob]).T\n",
    "\n",
    "        ref_points[ref_points == -np.inf] = 0\n",
    "        ref_points[ref_points == np.inf] = 0\n",
    "        ref_points[np.isnan(ref_points)] = 0\n",
    "\n",
    "        dist, idx = matching_tree.query(ref_points, k=n_neighbors)\n",
    "\n",
    "    else:\n",
    "        tree_points = feature_table[['mz','rt_apex']].values\n",
    "        tree_points[:,0] = np.log(tree_points[:,0])*1e6/ppm_range #m/z -> log transform, this is in ppm then\n",
    "        tree_points[:,1] = tree_points[:,1]/rt_range # -> this is in minutes\n",
    "\n",
    "        matching_tree = KDTree(tree_points, metric=\"minkowski\")\n",
    "\n",
    "        query_mz = np.log(query_data['mono_mzs2'])*1e6/ppm_range\n",
    "        query_rt = query_data['rt_list_ms2'] / rt_range\n",
    "\n",
    "        ref_points = np.array([query_mz, query_rt]).T\n",
    "\n",
    "        ref_points[ref_points == -np.inf] = 0\n",
    "        ref_points[ref_points == np.inf] = 0\n",
    "        ref_points[np.isnan(ref_points)] = 0\n",
    "\n",
    "        dist, idx = matching_tree.query(ref_points, k=n_neighbors)\n",
    "\n",
    "\n",
    "    ref_matched = np.zeros(ref_points.shape[0], dtype=np.bool_)\n",
    "\n",
    "    all_df = []\n",
    "    for neighbor in range(n_neighbors):\n",
    "        if use_mob:\n",
    "            ref_df = pd.DataFrame(np.array([query_data['rt_list_ms2'], query_data['prec_mass_list2'], query_data['mono_mzs2'], query_data['charge2'], query_data['mobility']]).T, columns=['rt', 'mass', 'mz', 'charge','mobility'])\n",
    "        else:\n",
    "            ref_df = pd.DataFrame(np.array([query_data['rt_list_ms2'], query_data['prec_mass_list2'], query_data['mono_mzs2'], query_data['charge2']]).T, columns=['rt', 'mass', 'mz', 'charge'])\n",
    "\n",
    "        ref_df['mass_matched'] = feature_table.iloc[idx[:,neighbor]]['mass'].values\n",
    "        ref_df['mass_offset'] = ref_df['mass_matched'] - ref_df['mass']\n",
    "\n",
    "        ref_df['rt_matched'] = feature_table.iloc[idx[:,neighbor]]['rt_apex'].values\n",
    "        ref_df['rt_offset'] = ref_df['rt_matched'] - ref_df['rt']\n",
    "\n",
    "        ref_df['mz_matched'] = feature_table.iloc[idx[:,neighbor]]['mz'].values\n",
    "        ref_df['mz_offset'] = ref_df['mz_matched'] - ref_df['mz']\n",
    "\n",
    "        if use_mob:\n",
    "            ref_df['mobility_matched'] = feature_table.iloc[idx[:,neighbor]]['mobility'].values\n",
    "            ref_df['mobility_offset'] = ref_df['mobility_matched'] - ref_df['mobility']\n",
    "\n",
    "        ref_df['charge_matched'] = feature_table.iloc[idx[:,neighbor]]['charge'].values\n",
    "\n",
    "        ref_df['query_idx'] = ref_df.index\n",
    "        ref_df['feature_idx'] = idx[:,neighbor]\n",
    "\n",
    "        for field in ['int_sum','int_apex','rt_start','rt_apex','rt_end','fwhm','mobility_lower','mobility_upper']:\n",
    "            if field in feature_table.keys():\n",
    "                ref_df[field] = feature_table.iloc[idx[:,neighbor]][field].values\n",
    "        \n",
    "        rt_check = (ref_df['rt_start'] <= ref_df['rt']) & (ref_df['rt'] <= ref_df['rt_end'])\n",
    "\n",
    "        # check isolation window (win=3)\n",
    "        mass_check = np.abs(ref_df['mz_offset'].values) <= 3\n",
    "\n",
    "        _check = rt_check & mass_check\n",
    "        if use_mob:\n",
    "            mob_check = (ref_df['mobility_lower'] <= ref_df['mobility']) & (ref_df['mobility'] <= ref_df['mobility_upper'])\n",
    "            _check &= mob_check\n",
    "\n",
    "        ref_matched |= _check\n",
    "        ref_df['dist'] = dist[:,neighbor]\n",
    "        ref_df = ref_df[_check]\n",
    "    \n",
    "        #ref_df['dist'] = dist[:,neighbor]\n",
    "        #ref_matched |= (ref_df['dist']<1)\n",
    "        #ref_df = ref_df[ref_df['dist']<1]\n",
    "\n",
    "        all_df.append(ref_df)\n",
    "\n",
    "    \n",
    "    if search_unidentified:\n",
    "        if use_mob:\n",
    "            unmatched_ref = pd.DataFrame(np.array([query_data['rt_list_ms2'], query_data['prec_mass_list2'], query_data['mono_mzs2'], query_data['charge2'], query_data['mobility']]).T, columns=['rt', 'mass', 'mz', 'charge','mobility'])\n",
    "        else:\n",
    "            unmatched_ref = pd.DataFrame(np.array([query_data['rt_list_ms2'], query_data['prec_mass_list2'], query_data['mono_mzs2'], query_data['charge2']]).T, columns=['rt', 'mass', 'mz', 'charge'])\n",
    "        unmatched_ref = unmatched_ref[~ref_matched]\n",
    "        unmatched_ref['mass_matched'] = unmatched_ref['mass']\n",
    "        unmatched_ref['mass_offset'] = 0\n",
    "        unmatched_ref['rt_matched'] = unmatched_ref['rt']\n",
    "        unmatched_ref['rt_offset']  = 0\n",
    "        unmatched_ref['mz_matched'] = unmatched_ref['mz']\n",
    "        unmatched_ref['mz_offset'] = 0\n",
    "        unmatched_ref['charge_matched'] = unmatched_ref['charge']\n",
    "        unmatched_ref['query_idx'] = unmatched_ref.index\n",
    "        unmatched_ref['feature_idx'] = np.nan\n",
    "\n",
    "        if use_mob:\n",
    "            ref_df['mobility_matched'] = unmatched_ref['mobility']\n",
    "            ref_df['mobility_offset'] = np.nan\n",
    "\n",
    "        for field in ['int_sum','int_apex','rt_start','rt_apex','rt_end','fwhm']:\n",
    "            if field in feature_table.keys():\n",
    "                unmatched_ref[field] = np.nan\n",
    "        unmatched_ref['dist'] = np.nan\n",
    "\n",
    "        all_df.append(unmatched_ref)\n",
    "\n",
    "    features = pd.concat(all_df)\n",
    "\n",
    "    features = features.sort_values('mass_matched', ascending=True)\n",
    "    features = features.reset_index(drop=True)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted 12_speed.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphapept]",
   "language": "python",
   "name": "conda-env-alphapept-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
