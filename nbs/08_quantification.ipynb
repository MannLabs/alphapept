{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantification\n",
    "\n",
    "> Functions related to quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains everything to perform quantification\n",
    "\n",
    "Current ToDo here:\n",
    "\n",
    "- Most of the functions are not very well described yet\n",
    "- Introductory text to give an overview / relevant papers would be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFQ\n",
    "\n",
    "Algorithms related to label-free quantifications are motivated by the [MaxLFQ paper](https://doi.org/10.1074/mcp.m113.031591).\n",
    "\n",
    "\n",
    "TODO: Include weighing?\n",
    "TODO: Check boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Silico Test data\n",
    "\n",
    "To test the performance of different approaches we resort to simulating peptide intensities and adjusting them with a simulated normalization. We can then use different approaches to recover the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def gaussian(mu, sigma, grid):\n",
    "    norm = 0.3989422804014327 / sigma\n",
    "    return norm * np.exp(-0.5 * ((grid - mu) / sigma) ** 2)\n",
    "\n",
    "\n",
    "def return_elution_profile(timepoint, sigma, n_runs):\n",
    "    \"\"\"\n",
    "    Simulation of a Gaussian Elution Profile\n",
    "    \"\"\"\n",
    "    return gaussian(timepoint, sigma, np.arange(0, n_runs))\n",
    "\n",
    "\n",
    "def simulate_sample_profiles(n_peptides, n_runs, n_samples, threshold=0.2, use_noise=True):\n",
    "    \"\"\"\n",
    "    Generate random profiles to serve as test data\n",
    "\n",
    "    \"\"\"\n",
    "    abundances = np.random.rand(n_peptides)*10e7\n",
    "    true_normalization = np.random.normal(loc=1, scale=0.1, size=(n_runs, n_samples))\n",
    "    \n",
    "    true_normalization[true_normalization<0] = 0\n",
    "    \n",
    "    true_normalization = true_normalization/np.max(true_normalization)\n",
    "\n",
    "    maxvals = np.max(true_normalization, axis=1)\n",
    "\n",
    "    elution_timepoints = random.choices(list(range(n_runs)), k=n_peptides)\n",
    "\n",
    "    profiles = np.empty((n_runs, n_samples, n_peptides))\n",
    "    profiles[:] = np.nan\n",
    "\n",
    "    for i in range(n_peptides):\n",
    "\n",
    "        elution_timepoint = elution_timepoints[i]\n",
    "        abundance = abundances[i]\n",
    "\n",
    "        profile = return_elution_profile(elution_timepoint, 1, n_runs)\n",
    "        profile = profile/np.max(profile)\n",
    "        profile = profile * abundance\n",
    "        elution_profiles = np.tile(profile, (n_samples, 1)).T\n",
    "\n",
    "        # Some gaussian noise\n",
    "        if use_noise:\n",
    "            noise = np.random.normal(1, 0.2, elution_profiles.shape)\n",
    "            noisy_profile = noise * elution_profiles\n",
    "        else:\n",
    "            noisy_profile = elution_profiles\n",
    "            \n",
    "        #print(noisy_profile)\n",
    "\n",
    "        normalized_profile = noisy_profile * true_normalization\n",
    "\n",
    "        normalized_profile[normalized_profile < threshold] = 0\n",
    "        normalized_profile[normalized_profile == 0] = np.nan\n",
    "        \n",
    "\n",
    "        profiles[:,:,i] = normalized_profile\n",
    "\n",
    "    return profiles, true_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit  \n",
    "def get_peptide_error(profile, normalization): \n",
    "    \n",
    "    pep_ints = np.zeros(profile.shape[1])\n",
    "\n",
    "    normalized_profile = profile*normalization\n",
    "\n",
    "    for i in range(len(pep_ints)):\n",
    "        pep_ints[i] = np.nansum(normalized_profile[:,i])\n",
    "\n",
    "    pep_ints = pep_ints[pep_ints>0]\n",
    "\n",
    "    # Loop through all combinations\n",
    "    n = len(pep_ints)\n",
    "\n",
    "    error = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            error += np.abs(np.log(pep_ints[i]/pep_ints[j]))**2\n",
    "            \n",
    "    return error\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_total_error_parallel(normalization, profiles):\n",
    "    \n",
    "    normalization = normalization.reshape(profiles.shape[:2])\n",
    "    \n",
    "    total_error = 0\n",
    "    \n",
    "    for index in prange(profiles.shape[2]):\n",
    "        total_error += get_peptide_error(profiles[:,:, index], normalization)\n",
    "    \n",
    "    return total_error\n",
    "\n",
    "\n",
    "def get_total_error(normalization, profiles):\n",
    "    \n",
    "    normalization = normalization.reshape(profiles.shape[:2])\n",
    "    \n",
    "    total_error = 0\n",
    "    \n",
    "    for index in range(profiles.shape[2]):\n",
    "        total_error += get_peptide_error(profiles[:,:, index], normalization)\n",
    "    \n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking different optimiziers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Time Elapsed (min)</th>\n",
       "      <th>Error / Baseline Error</th>\n",
       "      <th>Error / Ground Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L-BFGS-B</td>\n",
       "      <td>0.032783</td>\n",
       "      <td>0.675963</td>\n",
       "      <td>0.495459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TNC</td>\n",
       "      <td>0.031129</td>\n",
       "      <td>0.724456</td>\n",
       "      <td>0.531003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLSQP</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.675963</td>\n",
       "      <td>0.495459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trf</td>\n",
       "      <td>0.169703</td>\n",
       "      <td>0.676007</td>\n",
       "      <td>0.495491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Method  Time Elapsed (min)  Error / Baseline Error  Error / Ground Truth\n",
       "0  L-BFGS-B            0.032783                0.675963              0.495459\n",
       "1       TNC            0.031129                0.724456              0.531003\n",
       "2     SLSQP            0.003027                0.675963              0.495459\n",
       "3       trf            0.169703                0.676007              0.495491"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from time import time\n",
    "from scipy.optimize import least_squares\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "n_peptides = 100\n",
    "n_runs = 10\n",
    "n_samples = 3\n",
    "\n",
    "profiles, true_normalization = simulate_sample_profiles(n_peptides, n_runs, n_samples)\n",
    "\n",
    "methods = ['L-BFGS-B', 'TNC', 'SLSQP','trf']\n",
    "\n",
    "results = []\n",
    "\n",
    "for method in methods:\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    if method in ['trf']:\n",
    "        x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "        bounds = (x0*0.1, x0)\n",
    "        res = least_squares(get_total_error, args = [profiles], bounds = bounds, x0 = x0, verbose=0, method = method)\n",
    "\n",
    "    else:\n",
    "        x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "        bounds = [(0.1, 1) for _ in x0]\n",
    "        res = minimize(get_total_error, args = profiles , x0 = x0, bounds=bounds, method=method)\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "    \n",
    "    end = time()\n",
    "    \n",
    "    time_elapsed_min = (end-start)/60\n",
    "\n",
    "    optimality = get_total_error(solution, profiles) /get_total_error(x0, profiles)\n",
    "    optimality_ = get_total_error(solution, profiles) / get_total_error(true_normalization, profiles)\n",
    "    \n",
    "    results.append((method, time_elapsed_min, optimality, optimality_))\n",
    "    \n",
    "pd.DataFrame(results, columns=['Method', 'Time Elapsed (min)','Error / Baseline Error','Error / Ground Truth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "minimum_occurence = 10\n",
    "\n",
    "def normalize_experiment_SLSQP(profiles):\n",
    "    \"\"\"\n",
    "    Calculate normalization with SLSQP approach\n",
    "    \"\"\"\n",
    "    x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "    bounds = [(0.1, 1) for _ in x0]\n",
    "    res = minimize(get_total_error, args = profiles , x0 = x0, bounds=bounds, method='SLSQP', options={'disp': False} )\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "    \n",
    "    return solution\n",
    "\n",
    "def delayed_normalization(df, field='int_sum', minimum_occurence=None):\n",
    "    \"\"\"\n",
    "    Returns normalization for given peptide intensities \n",
    "    \"\"\"\n",
    "    experiments = np.sort(df['experiment'].unique()).tolist()\n",
    "    fractions = np.sort(df['fraction'].unique()).tolist()\n",
    "\n",
    "    n_fractions = len(fractions)\n",
    "    n_experiments = len(experiments)\n",
    "\n",
    "    df_max = df.groupby(['precursor','fraction','experiment'])[field].max() #Maximum per fraction\n",
    "\n",
    "    prec_count = df_max.index.get_level_values('precursor').value_counts()\n",
    "    \n",
    "    if not minimum_occurence:\n",
    "        minimum_occurence = np.percentile(prec_count[prec_count>1], 75) #Take the 25% best datapoints\n",
    "    \n",
    "    shared_precs = prec_count[prec_count >= minimum_occurence]\n",
    "    precs = prec_count[prec_count > minimum_occurence].index.tolist()\n",
    "\n",
    "    n_profiles = len(precs)\n",
    "\n",
    "    selected_precs = df_max.loc[precs]\n",
    "    selected_precs = selected_precs.reset_index()\n",
    "\n",
    "    profiles = np.empty((n_fractions, n_experiments, n_profiles))\n",
    "    profiles[:] = np.nan\n",
    "\n",
    "    #get dictionaries\n",
    "    fraction_dict = {_:i for i,_ in enumerate(fractions)}\n",
    "    experiment_dict = {_:i for i,_ in enumerate(experiments)}\n",
    "    precursor_dict = {_:i for i,_ in enumerate(precs)}\n",
    "\n",
    "    prec_id = [precursor_dict[_] for _ in selected_precs['precursor']]\n",
    "    frac_id = [fraction_dict[_] for _ in selected_precs['fraction']]\n",
    "    ex_id = [experiment_dict[_] for _ in selected_precs['experiment']]\n",
    "\n",
    "    profiles[frac_id,ex_id, prec_id] = selected_precs[field]\n",
    "    \n",
    "    normalization = normalize_experiment_SLSQP(profiles)\n",
    "    \n",
    "    df[field+'_dn'] = df[field]*normalization[[fraction_dict[_] for _ in df['fraction']], [experiment_dict[_] for _ in df['experiment']]]\n",
    "    \n",
    "    return df, normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing protein intensity profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "def generate_dummy_data(n_sequences, n_samples, noise=True, remove = True, peptide_ratio = True, abundance=True, signal_level=100, noise_divider=10, keep=0.8):\n",
    "\n",
    "    species = ['P'+str(_) for _ in range(1,n_sequences+1)]\n",
    "    sample = [string.ascii_uppercase[_] for _ in range(n_samples)]\n",
    "    \n",
    "    if peptide_ratio:\n",
    "        peptide_ratio = np.random.rand(n_sequences)\n",
    "        peptide_ratio = peptide_ratio/np.sum(peptide_ratio)\n",
    "    else:\n",
    "        peptide_ratio = np.ones(n_sequences)\n",
    "\n",
    "    if abundance:\n",
    "        abundance_profile = np.random.rand(n_samples,1)\n",
    "    else:\n",
    "        abundance_profile = np.ones((n_samples,1))\n",
    "\n",
    "    original_signal = np.ones((n_samples, n_sequences))\n",
    "\n",
    "    noise_sim = (np.random.rand(n_samples, n_sequences)-0.5)/noise_divider\n",
    "\n",
    "    if noise:\n",
    "        noisy_signal = original_signal+noise_sim\n",
    "        noisy_signal = noisy_signal*signal_level*peptide_ratio*abundance_profile\n",
    "    else:\n",
    "        noisy_signal = original_signal*signal_level*peptide_ratio*abundance_profile\n",
    "\n",
    "    if remove:\n",
    "        #Remove points\n",
    "        keep_probability = keep #keep 60% of the points \n",
    "        to_remove = np.random.rand(n_samples, n_sequences)\n",
    "        to_remove = to_remove>=keep_probability\n",
    "\n",
    "        dummy_data = noisy_signal.copy()\n",
    "\n",
    "        dummy_data[to_remove] = 0\n",
    "\n",
    "    else:\n",
    "        dummy_data = noisy_signal\n",
    "\n",
    "            \n",
    "    dummy_data = pd.DataFrame(dummy_data, index = sample, columns = species).T\n",
    "    \n",
    "    ground_truth = abundance_profile.flatten()\n",
    "    ground_truth = ground_truth/np.max(ground_truth)\n",
    "        \n",
    "    return dummy_data, sample, ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine pair-wise intenisty ratios\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def get_protein_ratios(signal, column_combinations, minimum_ratios = 2):\n",
    "    n_samples = signal.shape[1]\n",
    "    ratios = np.empty((n_samples, n_samples))\n",
    "    ratios[:] = np.nan\n",
    "\n",
    "    for element in column_combinations:\n",
    "        i = element[0]\n",
    "        j = element[1]\n",
    "\n",
    "        ratio = signal[:,j] / signal[:,i]\n",
    "        \n",
    "        non_nan = np.sum(~np.isnan(ratio))\n",
    "\n",
    "        if non_nan >= minimum_ratios:\n",
    "            ratio_median = np.nanmedian(ratio)\n",
    "        else:\n",
    "            ratio_median = np.nan\n",
    "\n",
    "        ratios[j,i] = ratio_median\n",
    "    \n",
    "    return ratios "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def triangle_error(normalization, ratios):\n",
    "    int_matrix = np.repeat(normalization, len(normalization)).reshape((len(normalization), len(normalization))).transpose()\n",
    "    x = (np.log(ratios) - np.log(int_matrix.T) + np.log(int_matrix))**2\n",
    "    \n",
    "    return np.nansum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "## L-BFGS-B\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# LFBGSB\n",
    "\n",
    "def solve_profile_LFBGSB(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = [(x0[0]*0+0.01, x0[0]-0.01) for _ in x0]\n",
    "    res_wrapped = minimize(triangle_error, args = ratios , x0 = x0, bounds=bounds, method = 'L-BFGS-B')\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success\n",
    "\n",
    "\n",
    "def solve_profile_SLSQP(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = [(x0[0]*0+0.01, x0[0]-0.01) for _ in x0]\n",
    "    res_wrapped = minimize(triangle_error, args = ratios , x0 = x0, bounds=bounds, method = 'SLSQP', options={'maxiter':10000})\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success\n",
    "\n",
    "\n",
    "# TRF\n",
    "def solve_profile_trf(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = (x0*0+0.01, x0-0.01)\n",
    "    res_wrapped = least_squares(triangle_error, args = [ratios] , x0 = x0, bounds=bounds, verbose=0, method = 'trf')\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking different Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\alphap\\lib\\site-packages\\scipy\\optimize\\slsqp.py:63: RuntimeWarning: invalid value encountered in subtract\n",
      "  jac[i] = (func(*((x0+dx,)+args)) - f0)/epsilon\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time Elapsed (s)</th>\n",
       "      <th>Mean absolute percentage error</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L-FBGS-B</th>\n",
       "      <td>0.040468</td>\n",
       "      <td>0.858015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLSQP</th>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.019723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trf</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Time Elapsed (s)  Mean absolute percentage error  Errors  Success\n",
       "Method                                                                     \n",
       "L-FBGS-B          0.040468                        0.858015     0.0     0.75\n",
       "SLSQP             0.002057                        0.019723     0.0     1.00\n",
       "trf               0.000000                             NaN     1.0     1.00"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from numba.typed import List\n",
    "\n",
    "n_sequences = 10\n",
    "n_samples = 6\n",
    "\n",
    "column_combinations = List()\n",
    "[column_combinations.append(_) for _ in combinations(range(n_samples), 2)]\n",
    "\n",
    "methods = {'L-FBGS-B':solve_profile_LFBGSB, 'SLSQP':solve_profile_SLSQP, 'trf':solve_profile_trf}\n",
    "results = []\n",
    "\n",
    "for run in range(20):\n",
    "    signal, sample, ground_truth = generate_dummy_data(n_sequences, n_samples)\n",
    "    ratios = get_protein_ratios(signal.values, column_combinations)\n",
    "\n",
    "    for method in methods.keys():\n",
    "        error = False\n",
    "        start = time()\n",
    "        try:\n",
    "            solution, success = methods[method](ratios)\n",
    "            mape = np.mean(np.abs((solution-ground_truth)/ground_truth))\n",
    "        except:\n",
    "            error = True\n",
    "            mape = np.nan\n",
    "            \n",
    "        \n",
    "        end = time()\n",
    "\n",
    "        time_elapsed_s = (end-start)\n",
    "\n",
    "        results.append((method, time_elapsed_s, mape, error, success))\n",
    "    \n",
    "result_df = pd.DataFrame(results, columns=['Method', 'Time Elapsed (s)', 'Mean absolute percentage error', 'Errors', 'Success'])\n",
    "\n",
    "result_df.groupby('Method').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SLSQP seems to be a robust and fast choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba.typed import List\n",
    "from itertools import combinations\n",
    "\n",
    "def get_protein_table(df, field = 'int_sum', callback = None):\n",
    "    unique_proteins = df['protein'].unique()\n",
    "    experiments = df['experiment'].unique().tolist()\n",
    "    experiments.sort()\n",
    "    \n",
    "    column_combinations = List()\n",
    "    [column_combinations.append(_) for _ in combinations(range(len(experiments)), 2)]\n",
    "        \n",
    "    columnes_ext = [_+'_LFQ' for _ in experiments]\n",
    "    protein_table = pd.DataFrame(index=unique_proteins, columns=columnes_ext + experiments)\n",
    "\n",
    "    if field+'_dn' in df.columns:\n",
    "        field_ = field+'_dn'\n",
    "    else:\n",
    "        field_ = field\n",
    "\n",
    "    for idx, protein in enumerate(unique_proteins):\n",
    "        subset = df[df['protein'] == protein].copy()\n",
    "        per_protein = subset.groupby(['experiment','precursor'])[field_].sum().unstack().T\n",
    "\n",
    "        for _ in experiments:\n",
    "            if _ not in per_protein.columns:\n",
    "                per_protein[_] = np.nan\n",
    "\n",
    "        per_protein = per_protein[experiments] \n",
    "\n",
    "        ratios = get_protein_ratios(per_protein.values, column_combinations)\n",
    "        solution, success = solve_profile_SLSQP(ratios)\n",
    "            \n",
    "        experiment_ids = per_protein.columns.tolist()\n",
    "        \n",
    "        pre_lfq = per_protein.sum().values\n",
    "\n",
    "        if not success or np.sum(~np.isnan(ratios)) == 0: # or np.sum(solution) == len(pre_lfq):\n",
    "            profile = pre_lfq\n",
    "        else:\n",
    "            total_int = subset[field_].sum()\n",
    "            total_int_ = np.sum(solution)\n",
    "            profile = total_int/total_int_*solution\n",
    "\n",
    "        protein_table.loc[protein, [_+'_LFQ' for _ in experiment_ids]] = profile\n",
    "        protein_table.loc[protein, experiment_ids] = pre_lfq\n",
    "        \n",
    "        if callback:\n",
    "            callback((idx+1)/len(unique_proteins))\n",
    "        \n",
    "    protein_table[protein_table == 0] = np.nan\n",
    "    protein_table = protein_table.astype('float')\n",
    "\n",
    "    return protein_table\n",
    "\n",
    "def protein_profile(df, experiments, field_, protein):\n",
    "    \"\"\"\n",
    "    Calculate the protein profile for a a df based on a dateframe\n",
    "    \n",
    "    \"\"\"\n",
    "    column_combinations = List()\n",
    "    [column_combinations.append(_) for _ in combinations(range(len(experiments)), 2)]\n",
    "    \n",
    "    subset = df[df['protein'] == protein].copy()\n",
    "    per_protein = subset.groupby(['experiment','precursor'])[field_].sum().unstack().T\n",
    "    \n",
    "    for _ in experiments:\n",
    "        if _ not in per_protein.columns:\n",
    "            per_protein[_] = np.nan\n",
    "            \n",
    "    per_protein = per_protein[experiments] \n",
    "\n",
    "    ratios = get_protein_ratios(per_protein.values, column_combinations)\n",
    "    solution, success = solve_profile_SLSQP(ratios)\n",
    "    \n",
    "    experiment_ids = per_protein.columns.tolist()\n",
    "    pre_lfq = per_protein.sum().values\n",
    "\n",
    "    if not success or np.sum(~np.isnan(ratios)) == 0: # or np.sum(solution) == len(pre_lfq):\n",
    "        profile = pre_lfq\n",
    "    else:\n",
    "        total_int = subset[field_].sum()\n",
    "        total_int_ = np.sum(solution)\n",
    "        profile = total_int/total_int_*solution\n",
    "    \n",
    "    return profile, pre_lfq, experiment_ids, protein\n",
    "\n",
    "\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from functools import partial \n",
    "\n",
    "\n",
    "def protein_profile_parallel(settings, df, callback=None):\n",
    "\n",
    "    n_processes = settings['general']['n_processes']\n",
    "    field = settings['quantification']['mode']\n",
    "    \n",
    "    unique_proteins = df['protein'].unique().tolist()\n",
    "    experiments = df['experiment'].unique().tolist()\n",
    "    files = df['filename'].unique().tolist()\n",
    "    \n",
    "    experiments.sort()\n",
    "          \n",
    "    columnes_ext = [_+'_LFQ' for _ in experiments]\n",
    "    protein_table = pd.DataFrame(index=unique_proteins, columns=columnes_ext + experiments)\n",
    "\n",
    "    if field+'_dn' in df.columns:\n",
    "        field_ = field+'_dn'\n",
    "    else:\n",
    "        field_ = field\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    if len(files) > 1:\n",
    "        with Pool(n_processes) as p:\n",
    "            max_ = len(unique_proteins)\n",
    "            for i, _ in enumerate(p.imap_unordered(partial(protein_profile, df, experiments, field_), unique_proteins)):\n",
    "                results.append(_)\n",
    "                if callback:\n",
    "                    callback((i+1)/max_)\n",
    "\n",
    "        for result in results:\n",
    "\n",
    "            profile, pre_lfq, experiment_ids, protein = result\n",
    "            protein_table.loc[protein, [_+'_LFQ' for _ in experiment_ids]] = profile\n",
    "            protein_table.loc[protein, experiment_ids] = pre_lfq\n",
    "\n",
    "        protein_table[protein_table == 0] = np.nan\n",
    "        protein_table = protein_table.astype('float')\n",
    "    else:\n",
    "        protein_table = df.groupby(['protein'])[field_].sum().to_frame().reset_index()\n",
    "        protein_table = protein_table.set_index('protein')\n",
    "        protein_table.index.name = None\n",
    "        protein_table.columns=[experiments[0]] \n",
    "        \n",
    "        if callback:\n",
    "            callback(1)\n",
    "        \n",
    "    return protein_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted # Search Modes.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_settings.ipynb.\n",
      "Converted 12_runner.ipynb.\n",
      "Converted 13_parallel.ipynb.\n",
      "Converted comparison_lfq.ipynb.\n",
      "Converted FF_parallel.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted Large fasta Search.ipynb.\n",
      "Converted OTF, revised.ipynb.\n",
      "Converted parallel_revision.ipynb.\n",
      "Converted Sample Data Generation.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted Untitled1.ipynb.\n",
      "Converted Untitled2.ipynb.\n",
      "Converted Untitled3.ipynb.\n",
      "Converted Untitled4.ipynb.\n",
      "Converted utilities.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
