{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantification\n",
    "\n",
    "> Functions related to quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label-free quantification\n",
    "\n",
    "Algorithms related to label-free quantifications are motivated by the [MaxLFQ paper](https://doi.org/10.1074/mcp.m113.031591)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed Normalization\n",
    "\n",
    "Delayed normalization describes the process of normalizing the differences that occur from prefractionation.\n",
    "\n",
    "ToDo: Describe the mathematical background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Silico Test data\n",
    "\n",
    "To test the delayed normalization approach we create an in silico test dataset with a known ground truth.\n",
    "We employ different solvers to recover the normalization parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "def gaussian(mu, sigma, grid):\n",
    "    \"\"\"\n",
    "    Gaussian function\n",
    "    \"\"\"\n",
    "    norm = 0.3989422804014327 / sigma\n",
    "    return norm * np.exp(-0.5 * ((grid - mu) / sigma) ** 2)\n",
    "\n",
    "\n",
    "def return_elution_profile(timepoint, sigma, n_runs):\n",
    "    \"\"\"\n",
    "    Returns a gaussian elution profile for a given timepoint\n",
    "    \"\"\"\n",
    "    return gaussian(timepoint, sigma, np.arange(0, n_runs))\n",
    "\n",
    "\n",
    "def simulate_sample_profiles(n_peptides, n_runs, n_samples, threshold=0.2, use_noise=True):\n",
    "    \"\"\"\n",
    "    Generate random profiles to serve as test_data\n",
    "    \"\"\"\n",
    "    abundances = np.random.rand(n_peptides)*10e7\n",
    "\n",
    "    true_normalization = np.random.normal(loc=1, scale=0.1, size=(n_runs, n_samples))\n",
    "\n",
    "    true_normalization[true_normalization<0] = 0\n",
    "\n",
    "    true_normalization = true_normalization/np.max(true_normalization)\n",
    "\n",
    "    maxvals = np.max(true_normalization, axis=1)\n",
    "\n",
    "    elution_timepoints = random.choices(list(range(n_runs)), k=n_peptides)\n",
    "\n",
    "    profiles = np.empty((n_runs, n_samples, n_peptides))\n",
    "    profiles[:] = np.nan\n",
    "\n",
    "    for i in range(n_peptides):\n",
    "\n",
    "        elution_timepoint = elution_timepoints[i]\n",
    "        abundance = abundances[i]\n",
    "\n",
    "        profile = return_elution_profile(elution_timepoint, 1, n_runs)\n",
    "        profile = profile/np.max(profile)\n",
    "        profile = profile * abundance\n",
    "        elution_profiles = np.tile(profile, (n_samples, 1)).T\n",
    "\n",
    "        # Add Gaussian Noise\n",
    "        if use_noise:\n",
    "            noise = np.random.normal(1, 0.2, elution_profiles.shape)\n",
    "            noisy_profile = noise * elution_profiles\n",
    "        else:\n",
    "            noisy_profile = elution_profiles\n",
    "\n",
    "        normalized_profile = noisy_profile * true_normalization\n",
    "\n",
    "        normalized_profile[normalized_profile < threshold] = 0\n",
    "        normalized_profile[normalized_profile == 0] = np.nan\n",
    "\n",
    "\n",
    "        profiles[:,:,i] = normalized_profile\n",
    "\n",
    "    return profiles, true_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit\n",
    "def get_peptide_error(profile, normalization):\n",
    "\n",
    "    pep_ints = np.zeros(profile.shape[1])\n",
    "\n",
    "    normalized_profile = profile*normalization\n",
    "\n",
    "    for i in range(len(pep_ints)):\n",
    "        pep_ints[i] = np.nansum(normalized_profile[:,i])\n",
    "\n",
    "    pep_ints = pep_ints[pep_ints>0]\n",
    "\n",
    "    # Loop through all combinations\n",
    "    n = len(pep_ints)\n",
    "\n",
    "    error = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            error += np.abs(np.log(pep_ints[i]/pep_ints[j]))**2\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_total_error_parallel(normalization, profiles):\n",
    "\n",
    "    normalization = normalization.reshape(profiles.shape[:2])\n",
    "\n",
    "    total_error = 0\n",
    "\n",
    "    for index in prange(profiles.shape[2]):\n",
    "        total_error += get_peptide_error(profiles[:,:, index], normalization)\n",
    "\n",
    "    return total_error\n",
    "\n",
    "\n",
    "def get_total_error(normalization, profiles):\n",
    "\n",
    "    normalization = normalization.reshape(profiles.shape[:2])\n",
    "\n",
    "    total_error = 0\n",
    "\n",
    "    for index in range(profiles.shape[2]):\n",
    "        total_error += get_peptide_error(profiles[:,:, index], normalization)\n",
    "\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking different optimiziers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Time Elapsed (min)</th>\n",
       "      <th>Error / Baseline Error</th>\n",
       "      <th>Error / Ground Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L-BFGS-B</td>\n",
       "      <td>0.039515</td>\n",
       "      <td>0.639518</td>\n",
       "      <td>0.415446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TNC</td>\n",
       "      <td>0.029501</td>\n",
       "      <td>0.706563</td>\n",
       "      <td>0.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLSQP</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.639518</td>\n",
       "      <td>0.415446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trf</td>\n",
       "      <td>0.254894</td>\n",
       "      <td>0.641983</td>\n",
       "      <td>0.417048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Method  Time Elapsed (min)  Error / Baseline Error  Error / Ground Truth\n",
       "0  L-BFGS-B            0.039515                0.639518              0.415446\n",
       "1       TNC            0.029501                0.706563              0.459000\n",
       "2     SLSQP            0.003713                0.639518              0.415446\n",
       "3       trf            0.254894                0.641983              0.417048"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from time import time\n",
    "from scipy.optimize import least_squares\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "n_peptides = 100\n",
    "n_runs = 10\n",
    "n_samples = 3\n",
    "\n",
    "profiles, true_normalization = simulate_sample_profiles(n_peptides, n_runs, n_samples)\n",
    "\n",
    "methods = ['L-BFGS-B', 'TNC', 'SLSQP','trf']\n",
    "\n",
    "results = []\n",
    "\n",
    "for method in methods:\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    if method in ['trf']:\n",
    "        x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "        bounds = (x0*0.1, x0)\n",
    "        res = least_squares(get_total_error, args = [profiles], bounds = bounds, x0 = x0*0.5, verbose=0, method = method)\n",
    "\n",
    "    else:\n",
    "        x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "        bounds = [(0.1, 1) for _ in x0]\n",
    "        res = minimize(get_total_error, args = profiles , x0 = x0*0.5, bounds=bounds, method=method)\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "    \n",
    "    end = time()\n",
    "    \n",
    "    time_elapsed_min = (end-start)/60\n",
    "\n",
    "    optimality = get_total_error(solution, profiles) /get_total_error(x0, profiles)\n",
    "    optimality_ = get_total_error(solution, profiles) / get_total_error(true_normalization, profiles)\n",
    "    \n",
    "    results.append((method, time_elapsed_min, optimality, optimality_))\n",
    "    \n",
    "pd.DataFrame(results, columns=['Method', 'Time Elapsed (min)','Error / Baseline Error','Error / Ground Truth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def normalize_experiment_SLSQP(profiles):\n",
    "    \"\"\"\n",
    "    Calculate normalization with SLSQP approach\n",
    "    \"\"\"\n",
    "    x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "    bounds = [(0.1, 1) for _ in x0]\n",
    "    res = minimize(get_total_error, args = profiles , x0 = x0*0.5, bounds=bounds, method='SLSQP', options={'disp': False} )\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "\n",
    "    return solution\n",
    "\n",
    "def normalize_experiment_BFGS(profiles):\n",
    "    \"\"\"\n",
    "    Calculate normalization with BFGS approach\n",
    "    \"\"\"\n",
    "    x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "    bounds = [(0.1, 1) for _ in x0]\n",
    "    res = minimize(get_total_error, args = profiles , x0 = x0*0.5, bounds=bounds, method='L-BFGS-B', options={'disp': False} )\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "\n",
    "    return solution\n",
    "\n",
    "def delayed_normalization(df, field='int_sum', minimum_occurence=None):\n",
    "    \"\"\"\n",
    "    Returns normalization for given peptide intensities\n",
    "    \"\"\"\n",
    "    files = np.sort(df['shortname'].unique()).tolist()\n",
    "    n_files = len(files)\n",
    "\n",
    "    if 'fraction' in df.keys():\n",
    "        fractions = np.sort(df['fraction'].unique()).tolist()\n",
    "        n_fractions = len(fractions)\n",
    "\n",
    "        df_max = df.groupby(['precursor','fraction','shortname'])[field].max() #Maximum per fraction\n",
    "\n",
    "        prec_count = df_max.index.get_level_values('precursor').value_counts()\n",
    "\n",
    "        if not minimum_occurence:\n",
    "            minimum_occurence = np.percentile(prec_count[prec_count>1].values, 75) #Take the 25% best datapoints\n",
    "            logging.info('Setting minimum occurence to {}'.format(minimum_occurence))\n",
    "\n",
    "        shared_precs = prec_count[prec_count >= minimum_occurence]\n",
    "        precs = prec_count[prec_count > minimum_occurence].index.tolist()\n",
    "\n",
    "        n_profiles = len(precs)\n",
    "\n",
    "        selected_precs = df_max.loc[precs]\n",
    "        selected_precs = selected_precs.reset_index()\n",
    "\n",
    "        profiles = np.empty((n_fractions, n_files, n_profiles))\n",
    "        profiles[:] = np.nan\n",
    "\n",
    "        #get dictionaries\n",
    "        fraction_dict = {_:i for i,_ in enumerate(fractions)}\n",
    "        filename_dict = {_:i for i,_ in enumerate(files)}\n",
    "        precursor_dict = {_:i for i,_ in enumerate(precs)}\n",
    "\n",
    "        prec_id = [precursor_dict[_] for _ in selected_precs['precursor']]\n",
    "        frac_id = [fraction_dict[_] for _ in selected_precs['fraction']]\n",
    "        file_id = [filename_dict[_] for _ in selected_precs['shortname']]\n",
    "\n",
    "        profiles[frac_id,file_id, prec_id] = selected_precs[field]\n",
    "\n",
    "        try:\n",
    "            normalization = normalize_experiment_SLSQP(profiles)\n",
    "        except ValueError: # SLSQP error in scipy https://github.com/scipy/scipy/issues/11403\n",
    "            logging.info('Normalization with SLSQP failed. Trying BFGS')\n",
    "            normalization = normalize_experiment_BFGS(profiles)\n",
    "\n",
    "\n",
    "        #intensity normalization: total intensity to remain unchanged\n",
    "\n",
    "        df[field+'_dn'] = df[field]*normalization[[fraction_dict[_] for _ in df['fraction']], [filename_dict[_] for _ in df['shortname']]]\n",
    "        df[field+'_dn'] *= df[field].sum()/df[field+'_dn'].sum()\n",
    "\n",
    "    else:\n",
    "        logging.info('No fractions present. Skipping delayed normalization.')\n",
    "        normalization = None\n",
    "\n",
    "    return df, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.496190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.958734</td>\n",
       "      <td>0.485063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.496190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  1.000000  0.496190\n",
       "1  0.958734  0.485063\n",
       "2  1.000000  0.496190"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precursor</th>\n",
       "      <th>fraction</th>\n",
       "      <th>shortname</th>\n",
       "      <th>int_sum</th>\n",
       "      <th>int_sum_dn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prec_1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.915112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prec_1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.169798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prec_1</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.915112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prec_1</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.908139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prec_1</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.183699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Prec_1</td>\n",
       "      <td>3</td>\n",
       "      <td>B</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.908139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  precursor  fraction shortname  int_sum  int_sum_dn\n",
       "0    Prec_1         1         A      0.6    0.915112\n",
       "1    Prec_1         2         A      0.8    1.169798\n",
       "2    Prec_1         3         A      0.6    0.915112\n",
       "3    Prec_1         1         B      1.2    0.908139\n",
       "4    Prec_1         2         B      1.6    1.183699\n",
       "5    Prec_1         3         B      1.2    0.908139"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data = {}\n",
    "\n",
    "sample_data['precursor'] = ['Prec_1'] * 6 + ['Prec_2'] * 6 + ['Prec_3'] * 6\n",
    "sample_data['fraction'] = [1,2,3]*6\n",
    "sample_data['shortname'] = ['A','A','A', 'B','B','B'] * 3\n",
    "sample_data['int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2] * 3\n",
    "\n",
    "test_df = pd.DataFrame(sample_data)\n",
    "test_df, normalization = delayed_normalization(test_df, field='int_sum', minimum_occurence=2)\n",
    "\n",
    "display(pd.DataFrame(normalization))\n",
    "display(test_df.head(6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing protein intensity profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "def generate_dummy_data(n_sequences, n_samples, noise=True, remove = True, peptide_ratio = True, abundance=True, signal_level=100, noise_divider=10, keep=0.8):\n",
    "\n",
    "    species = ['P'+str(_) for _ in range(1,n_sequences+1)]\n",
    "    sample = [string.ascii_uppercase[_] for _ in range(n_samples)]\n",
    "    \n",
    "    if peptide_ratio:\n",
    "        peptide_ratio = np.random.rand(n_sequences)\n",
    "        peptide_ratio = peptide_ratio/np.sum(peptide_ratio)\n",
    "    else:\n",
    "        peptide_ratio = np.ones(n_sequences)\n",
    "\n",
    "    if abundance:\n",
    "        abundance_profile = np.random.rand(n_samples,1)\n",
    "    else:\n",
    "        abundance_profile = np.ones((n_samples,1))\n",
    "\n",
    "    original_signal = np.ones((n_samples, n_sequences))\n",
    "\n",
    "    noise_sim = (np.random.rand(n_samples, n_sequences)-0.5)/noise_divider\n",
    "\n",
    "    if noise:\n",
    "        noisy_signal = original_signal+noise_sim\n",
    "        noisy_signal = noisy_signal*signal_level*peptide_ratio*abundance_profile\n",
    "    else:\n",
    "        noisy_signal = original_signal*signal_level*peptide_ratio*abundance_profile\n",
    "\n",
    "    if remove:\n",
    "        #Remove points\n",
    "        keep_probability = keep #keep 60% of the points \n",
    "        to_remove = np.random.rand(n_samples, n_sequences)\n",
    "        to_remove = to_remove>=keep_probability\n",
    "\n",
    "        dummy_data = noisy_signal.copy()\n",
    "\n",
    "        dummy_data[to_remove] = 0\n",
    "\n",
    "    else:\n",
    "        dummy_data = noisy_signal\n",
    "\n",
    "            \n",
    "    dummy_data = pd.DataFrame(dummy_data, index = sample, columns = species).T\n",
    "    \n",
    "    ground_truth = abundance_profile.flatten()\n",
    "    ground_truth = ground_truth/np.max(ground_truth)\n",
    "        \n",
    "    return dummy_data, sample, ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine pair-wise intenisty ratios\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def get_protein_ratios(signal, column_combinations, minimum_ratios = 1):\n",
    "    n_samples = signal.shape[1]\n",
    "    ratios = np.empty((n_samples, n_samples))\n",
    "    ratios[:] = np.nan\n",
    "\n",
    "    for element in column_combinations:\n",
    "        i = element[0]\n",
    "        j = element[1]\n",
    "\n",
    "        ratio = signal[:,j] / signal[:,i]\n",
    "\n",
    "        non_nan = np.sum(~np.isnan(ratio))\n",
    "\n",
    "        if non_nan >= minimum_ratios:\n",
    "            ratio_median = np.nanmedian(ratio)\n",
    "        else:\n",
    "            ratio_median = np.nan\n",
    "\n",
    "        ratios[j,i] = ratio_median\n",
    "\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def triangle_error(normalization, ratios):\n",
    "    int_matrix = np.repeat(normalization, len(normalization)).reshape((len(normalization), len(normalization))).transpose()\n",
    "    x = (np.log(ratios) - np.log(int_matrix.T) + np.log(int_matrix))**2\n",
    "\n",
    "    return np.nansum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "## L-BFGS-B\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize, least_squares\n",
    "\n",
    "# LFBGSB\n",
    "\n",
    "def solve_profile_LFBGSB(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = [(x0[0]*0+0.01, x0[0]) for _ in x0]\n",
    "    res_wrapped = minimize(triangle_error, args = ratios , x0 = x0*0.5, bounds=bounds, method = 'L-BFGS-B')\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success\n",
    "\n",
    "\n",
    "def solve_profile_SLSQP(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = [(x0[0]*0+0.01, x0[0]) for _ in x0]\n",
    "    res_wrapped = minimize(triangle_error, args = ratios , x0 = x0*0.5, bounds=bounds, method = 'SLSQP', options={'maxiter':10000})\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success\n",
    "\n",
    "\n",
    "# TRF\n",
    "def solve_profile_trf(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = (x0*0+0.01, x0)\n",
    "    res_wrapped = least_squares(triangle_error, args = [ratios] , x0 = x0*0.5, bounds=bounds, verbose=0, method = 'trf')\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking different Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\alphapept\\lib\\site-packages\\scipy\\optimize\\slsqp.py:63: RuntimeWarning: invalid value encountered in subtract\n",
      "  jac[i] = (func(*((x0+dx,)+args)) - f0)/epsilon\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time Elapsed (s)</th>\n",
       "      <th>Mean absolute percentage error</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L-FBGS-B</th>\n",
       "      <td>0.044621</td>\n",
       "      <td>0.422225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLSQP</th>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.422228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trf</th>\n",
       "      <td>0.031017</td>\n",
       "      <td>0.020241</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Time Elapsed (s)  Mean absolute percentage error  Errors  Success\n",
       "Method                                                                     \n",
       "L-FBGS-B          0.044621                        0.422225     0.0      0.8\n",
       "SLSQP             0.001576                        0.422228     0.0      0.8\n",
       "trf               0.031017                        0.020241     0.2      0.8"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from numba.typed import List\n",
    "\n",
    "n_sequences = 10\n",
    "n_samples = 6\n",
    "\n",
    "column_combinations = List()\n",
    "[column_combinations.append(_) for _ in combinations(range(n_samples), 2)]\n",
    "\n",
    "methods = {'L-FBGS-B':solve_profile_LFBGSB, 'SLSQP':solve_profile_SLSQP, 'trf':solve_profile_trf}\n",
    "results = []\n",
    "\n",
    "for run in range(20):\n",
    "    signal, sample, ground_truth = generate_dummy_data(n_sequences, n_samples)\n",
    "    ratios = get_protein_ratios(signal.values, column_combinations)\n",
    "\n",
    "    for method in methods.keys():\n",
    "        error = False\n",
    "        start = time()\n",
    "        try:\n",
    "            solution, success = methods[method](ratios)\n",
    "            mape = np.mean(np.abs((solution-ground_truth)/ground_truth))\n",
    "        except:\n",
    "            error = True\n",
    "            mape = np.nan\n",
    "            \n",
    "        \n",
    "        end = time()\n",
    "\n",
    "        time_elapsed_s = (end-start)\n",
    "\n",
    "        results.append((method, time_elapsed_s, mape, error, success))\n",
    "    \n",
    "result_df = pd.DataFrame(results, columns=['Method', 'Time Elapsed (s)', 'Mean absolute percentage error', 'Errors', 'Success'])\n",
    "\n",
    "result_df.groupby('Method').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SLSQP seems to be a robust and fast choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\alphapept\\lib\\site-packages\\scipy\\optimize\\slsqp.py:63: RuntimeWarning: invalid value encountered in subtract\n",
      "  jac[i] = (func(*((x0+dx,)+args)) - f0)/epsilon\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from numba.typed import List\n",
    "\n",
    "n_sequences = 10\n",
    "n_samples = 6\n",
    "\n",
    "column_combinations = List()\n",
    "[column_combinations.append(_) for _ in combinations(range(n_samples), 2)]\n",
    "\n",
    "methods = {'L-FBGS-B':solve_profile_LFBGSB, 'SLSQP':solve_profile_SLSQP, 'trf':solve_profile_trf}\n",
    "results = []\n",
    "\n",
    "for run in range(20):\n",
    "    signal, sample, ground_truth = generate_dummy_data(n_sequences, n_samples)\n",
    "    ratios = get_protein_ratios(signal.values, column_combinations)\n",
    "\n",
    "    for method in methods.keys():\n",
    "        error = False\n",
    "        start = time()\n",
    "        try:\n",
    "            solution, success = methods[method](ratios)\n",
    "            mape = np.mean(np.abs((solution-ground_truth)/ground_truth))\n",
    "        except:\n",
    "            error = True\n",
    "            mape = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba.typed import List\n",
    "from itertools import combinations\n",
    "\n",
    "def get_protein_table(df, field = 'int_sum', minimum_ratios = 1, callback = None):\n",
    "    unique_proteins = df['protein'].unique()\n",
    "    files = df['shortname'].unique().tolist()\n",
    "    files.sort()\n",
    "    \n",
    "    if len(files) == 1:\n",
    "        raise ValueError('Only one file present.')\n",
    "    \n",
    "    column_combinations = List()\n",
    "    [column_combinations.append(_) for _ in combinations(range(len(files)), 2)]\n",
    "        \n",
    "    columnes_ext = [_+'_LFQ' for _ in files]\n",
    "    protein_table = pd.DataFrame(index=unique_proteins, columns=columnes_ext + files)\n",
    "\n",
    "    if field+'_dn' in df.columns:\n",
    "        field_ = field+'_dn'\n",
    "    else:\n",
    "        field_ = field\n",
    "    \n",
    "    if df[field_].min() < 0:\n",
    "        raise ValueError('Negative intensity values present.')\n",
    "        \n",
    "    for idx, protein in enumerate(unique_proteins):\n",
    "        subset = df[df['protein'] == protein].copy()\n",
    "        per_protein = subset.groupby(['shortname','precursor'])[field_].sum().unstack().T\n",
    "\n",
    "        for _ in files:\n",
    "            if _ not in per_protein.columns:\n",
    "                per_protein[_] = np.nan\n",
    "\n",
    "        per_protein = per_protein[files] \n",
    "\n",
    "        ratios = get_protein_ratios(per_protein.values, column_combinations, minimum_ratios)\n",
    "        try:\n",
    "            solution, success = solve_profile_SLSQP(ratios)\n",
    "        except ValueError:\n",
    "            logging.info('Normalization with SLSQP failed. Trying BFGS')\n",
    "            solution, success = solve_profile_LFBGSB(ratios)\n",
    "            \n",
    "        file_ids = per_protein.columns.tolist()\n",
    "        \n",
    "        pre_lfq = per_protein.sum().values\n",
    "\n",
    "        if not success or np.sum(~np.isnan(ratios)) == 0: # or np.sum(solution) == len(pre_lfq):\n",
    "            profile = pre_lfq\n",
    "        else:\n",
    "            invalid = ((np.nansum(ratios, axis=1) == 0) & (np.nansum(ratios, axis=0) == 0))\n",
    "            total_int = subset[field_].sum() * solution \n",
    "            total_int[invalid] = 0\n",
    "            profile = total_int * subset[field_].sum().sum() / np.sum(total_int) #Normalize inensity again\n",
    "            \n",
    "\n",
    "        protein_table.loc[protein, [_+'_LFQ' for _ in file_ids]] = profile\n",
    "        protein_table.loc[protein, file_ids] = pre_lfq\n",
    "        \n",
    "        if callback:\n",
    "            callback((idx+1)/len(unique_proteins))\n",
    "        \n",
    "    protein_table[protein_table == 0] = np.nan\n",
    "    protein_table = protein_table.astype('float64')\n",
    "\n",
    "    return protein_table\n",
    "\n",
    "def protein_profile(df, files, field_, protein, minimum_ratios=1):\n",
    "    \"\"\"\n",
    "    Calculate the protein profile for a a df based on a dateframe\n",
    "    \n",
    "    \"\"\"\n",
    "    column_combinations = List()\n",
    "    [column_combinations.append(_) for _ in combinations(range(len(files)), 2)]\n",
    "    \n",
    "    subset = df[df['protein'] == protein].copy()\n",
    "    per_protein = subset.groupby(['shortname','precursor'])[field_].sum().unstack().T\n",
    "    \n",
    "    for _ in files:\n",
    "        if _ not in per_protein.columns:\n",
    "            per_protein[_] = np.nan\n",
    "            \n",
    "    per_protein = per_protein[files] \n",
    "\n",
    "    ratios = get_protein_ratios(per_protein.values, column_combinations, minimum_ratios)\n",
    "    try:\n",
    "        solution, success = solve_profile_SLSQP(ratios)\n",
    "    except ValueError:\n",
    "        logging.info('Normalization with SLSQP failed. Trying BFGS')\n",
    "        solution, success = solve_profile_LFBGSB(ratios)\n",
    "    \n",
    "    file_ids = per_protein.columns.tolist()\n",
    "    pre_lfq = per_protein.sum().values\n",
    "\n",
    "    if not success or np.sum(~np.isnan(ratios)) == 0: # or np.sum(solution) == len(pre_lfq):\n",
    "        profile = pre_lfq\n",
    "    else:\n",
    "        invalid = ((np.nansum(ratios, axis=1) == 0) & (np.nansum(ratios, axis=0) == 0))\n",
    "        total_int = subset[field_].sum() * solution \n",
    "        total_int[invalid] = 0\n",
    "        profile = total_int * subset[field_].sum().sum() / np.sum(total_int) #Normalize inensity again\n",
    "    \n",
    "    return profile, pre_lfq, file_ids, protein\n",
    "\n",
    "\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def protein_profile_parallel(settings, df, callback=None):\n",
    "\n",
    "    n_processes = settings['general']['n_processes']\n",
    "    field = settings['quantification']['mode']\n",
    "    \n",
    "    unique_proteins = df['protein'].unique().tolist()\n",
    "\n",
    "    files = df['shortname'].unique().tolist()\n",
    "    \n",
    "    files.sort()\n",
    "                  \n",
    "    columnes_ext = [_+'_LFQ' for _ in files]\n",
    "    protein_table = pd.DataFrame(index=unique_proteins, columns=columnes_ext + files)\n",
    "\n",
    "    if field+'_dn' in df.columns:\n",
    "        field_ = field+'_dn'\n",
    "    else:\n",
    "        field_ = field\n",
    "\n",
    "    if df[field_].min() < 0:\n",
    "        raise ValueError('Negative intensity values present.')\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    if len(files) > 1:\n",
    "        with Pool(n_processes) as p:\n",
    "            max_ = len(unique_proteins)\n",
    "            for i, _ in enumerate(p.imap_unordered(partial(protein_profile, df, files, field_), unique_proteins)):\n",
    "                results.append(_)\n",
    "                if callback:\n",
    "                    callback((i+1)/max_)\n",
    "\n",
    "        for result in results:\n",
    "\n",
    "            profile, pre_lfq, file_ids, protein = result\n",
    "            protein_table.loc[protein, [_+'_LFQ' for _ in file_ids]] = profile\n",
    "            protein_table.loc[protein, file_ids] = pre_lfq\n",
    "\n",
    "        protein_table[protein_table == 0] = np.nan\n",
    "        protein_table = protein_table.astype('float')\n",
    "    else:\n",
    "        protein_table = df.groupby(['protein'])[field_].sum().to_frame().reset_index()\n",
    "        protein_table = protein_table.set_index('protein')\n",
    "        protein_table.index.name = None\n",
    "        protein_table.columns=[files[0]] \n",
    "        \n",
    "        if callback:\n",
    "            callback(1)\n",
    "        \n",
    "    return protein_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precursor</th>\n",
       "      <th>shortname</th>\n",
       "      <th>protein</th>\n",
       "      <th>int_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prec_1</td>\n",
       "      <td>A</td>\n",
       "      <td>X</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prec_1</td>\n",
       "      <td>B</td>\n",
       "      <td>X</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prec_2</td>\n",
       "      <td>A</td>\n",
       "      <td>X</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prec_2</td>\n",
       "      <td>B</td>\n",
       "      <td>X</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prec_3</td>\n",
       "      <td>A</td>\n",
       "      <td>X</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Prec_3</td>\n",
       "      <td>B</td>\n",
       "      <td>X</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  precursor shortname protein  int_sum\n",
       "0    Prec_1         A       X      0.6\n",
       "1    Prec_1         B       X      0.8\n",
       "2    Prec_2         A       X      0.6\n",
       "3    Prec_2         B       X      1.2\n",
       "4    Prec_3         A       X      1.6\n",
       "5    Prec_3         B       X      1.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.57134929 3.42865071]\n",
      "[2.8 3.2]\n"
     ]
    }
   ],
   "source": [
    "sample_data = {}\n",
    "\n",
    "sample_data['precursor'] = ['Prec_1'] * 2 + ['Prec_2'] * 2 + ['Prec_3'] * 2\n",
    "sample_data['shortname'] = ['A','B'] * 3\n",
    "sample_data['protein'] = ['X'] * 6\n",
    "sample_data['int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2]\n",
    "\n",
    "test_df = pd.DataFrame(sample_data)\n",
    "\n",
    "display(test_df.head(6))\n",
    "\n",
    "protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "\n",
    "profile, pre_lfq, file_ids, protein = protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "\n",
    "print(profile)\n",
    "print(pre_lfq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_protein_profile():\n",
    "    \n",
    "    sample_data = {}\n",
    "\n",
    "    sample_data['precursor'] = ['Prec_1'] * 6 + ['Prec_2'] * 6 + ['Prec_3'] * 6\n",
    "    sample_data['fraction'] = [1,2,3]*6\n",
    "    sample_data['shortname'] = ['A','A','A', 'B','B','B'] * 3\n",
    "    sample_data['protein'] = ['X'] * 18\n",
    "    sample_data['int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2] * 3\n",
    "\n",
    "    test_df = pd.DataFrame(sample_data)\n",
    "\n",
    "    protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "\n",
    "    profile, pre_lfq, file_ids, protein = protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "    \n",
    "    # total intensity should be preserved\n",
    "    assert np.allclose(profile.sum(), pre_lfq.sum())\n",
    "    \n",
    "    sample_data = {}\n",
    "\n",
    "    sample_data['precursor'] = ['Prec_1'] * 2 + ['Prec_2'] * 2 + ['Prec_3'] * 2\n",
    "    sample_data['shortname'] = ['A','B'] * 3\n",
    "    sample_data['protein'] = ['X'] * 6\n",
    "    sample_data['int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2]\n",
    "\n",
    "    test_df = pd.DataFrame(sample_data)\n",
    "\n",
    "    protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "\n",
    "    profile, pre_lfq, file_ids, protein = protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "\n",
    "    assert np.allclose(profile.sum(), pre_lfq.sum())\n",
    "    \n",
    "test_protein_profile()\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
