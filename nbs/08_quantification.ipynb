{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantification\n",
    "\n",
    "> Functions related to quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains everything to perform quantification\n",
    "\n",
    "Current ToDo here:\n",
    "\n",
    "- Most of the functions are not very well described yet\n",
    "- Introductory text to give an overview / relevant papers would be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFQ\n",
    "\n",
    "Algorithms related to label-free quantifications are motivated by the [MaxLFQ paper](https://doi.org/10.1074/mcp.m113.031591).\n",
    "\n",
    "\n",
    "TODO: Include weighing?\n",
    "TODO: Check boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Silico Test data\n",
    "\n",
    "To test the performance of different approaches we resort to simulating peptide intensities and adjusting them with a simulated normalization. We can then use different approaches to recover the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def gaussian(mu, sigma, grid):\n",
    "    norm = 0.3989422804014327 / sigma\n",
    "    return norm * np.exp(-0.5 * ((grid - mu) / sigma) ** 2)\n",
    "\n",
    "\n",
    "def return_elution_profile(timepoint, sigma, n_runs):\n",
    "    \"\"\"\n",
    "    Simulation of a Gaussian Elution Profile\n",
    "    \"\"\"\n",
    "    return gaussian(timepoint, sigma, np.arange(0, n_runs))\n",
    "\n",
    "\n",
    "def simulate_sample_profiles(n_peptides, n_runs, n_samples, threshold=0.2, use_noise=True):\n",
    "    \"\"\"\n",
    "    Generate random profiles to serve as test data\n",
    "\n",
    "    \"\"\"\n",
    "    abundances = np.random.rand(n_peptides)*10e7\n",
    "    true_normalization = np.random.normal(loc=1, scale=0.1, size=(n_runs, n_samples))\n",
    "    \n",
    "    true_normalization[true_normalization<0] = 0\n",
    "    \n",
    "    true_normalization = true_normalization/np.max(true_normalization)\n",
    "\n",
    "    maxvals = np.max(true_normalization, axis=1)\n",
    "\n",
    "    elution_timepoints = random.choices(list(range(n_runs)), k=n_peptides)\n",
    "\n",
    "    profiles = np.empty((n_runs, n_samples, n_peptides))\n",
    "    profiles[:] = np.nan\n",
    "\n",
    "    for i in range(n_peptides):\n",
    "\n",
    "        elution_timepoint = elution_timepoints[i]\n",
    "        abundance = abundances[i]\n",
    "\n",
    "        profile = return_elution_profile(elution_timepoint, 1, n_runs)\n",
    "        profile = profile/np.max(profile)\n",
    "        profile = profile * abundance\n",
    "        elution_profiles = np.tile(profile, (n_samples, 1)).T\n",
    "\n",
    "        # Some gaussian noise\n",
    "        if use_noise:\n",
    "            noise = np.random.normal(1, 0.2, elution_profiles.shape)\n",
    "            noisy_profile = noise * elution_profiles\n",
    "        else:\n",
    "            noisy_profile = elution_profiles\n",
    "            \n",
    "        #print(noisy_profile)\n",
    "\n",
    "        normalized_profile = noisy_profile * true_normalization\n",
    "\n",
    "        normalized_profile[normalized_profile < threshold] = 0\n",
    "        normalized_profile[normalized_profile == 0] = np.nan\n",
    "        \n",
    "\n",
    "        profiles[:,:,i] = normalized_profile\n",
    "\n",
    "    return profiles, true_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit  \n",
    "def get_peptide_error(profile, normalization): \n",
    "    \n",
    "    pep_ints = np.zeros(profile.shape[1])\n",
    "\n",
    "    normalized_profile = profile*normalization\n",
    "\n",
    "    for i in range(len(pep_ints)):\n",
    "        pep_ints[i] = np.nansum(normalized_profile[:,i])\n",
    "\n",
    "    pep_ints = pep_ints[pep_ints>0]\n",
    "\n",
    "    # Loop through all combinations\n",
    "    n = len(pep_ints)\n",
    "\n",
    "    error = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            error += np.abs(np.log(pep_ints[i]/pep_ints[j]))**2\n",
    "            \n",
    "    return error\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_total_error_parallel(normalization, profiles):\n",
    "    \n",
    "    normalization = normalization.reshape(profiles.shape[:2])\n",
    "    \n",
    "    total_error = 0\n",
    "    \n",
    "    for index in prange(profiles.shape[2]):\n",
    "        total_error += get_peptide_error(profiles[:,:, index], normalization)\n",
    "    \n",
    "    return total_error\n",
    "\n",
    "\n",
    "def get_total_error(normalization, profiles):\n",
    "    \n",
    "    normalization = normalization.reshape(profiles.shape[:2])\n",
    "    \n",
    "    total_error = 0\n",
    "    \n",
    "    for index in range(profiles.shape[2]):\n",
    "        total_error += get_peptide_error(profiles[:,:, index], normalization)\n",
    "    \n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking different optimiziers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Time Elapsed (min)</th>\n",
       "      <th>Error / Baseline Error</th>\n",
       "      <th>Error / Ground Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L-BFGS-B</td>\n",
       "      <td>0.055321</td>\n",
       "      <td>0.723333</td>\n",
       "      <td>0.505968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TNC</td>\n",
       "      <td>0.029185</td>\n",
       "      <td>0.813514</td>\n",
       "      <td>0.569050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLSQP</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.723332</td>\n",
       "      <td>0.505968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trf</td>\n",
       "      <td>0.258366</td>\n",
       "      <td>0.726323</td>\n",
       "      <td>0.508060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Method  Time Elapsed (min)  Error / Baseline Error  Error / Ground Truth\n",
       "0  L-BFGS-B            0.055321                0.723333              0.505968\n",
       "1       TNC            0.029185                0.813514              0.569050\n",
       "2     SLSQP            0.004755                0.723332              0.505968\n",
       "3       trf            0.258366                0.726323              0.508060"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from time import time\n",
    "from scipy.optimize import least_squares\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "n_peptides = 100\n",
    "n_runs = 10\n",
    "n_samples = 3\n",
    "\n",
    "profiles, true_normalization = simulate_sample_profiles(n_peptides, n_runs, n_samples)\n",
    "\n",
    "methods = ['L-BFGS-B', 'TNC', 'SLSQP','trf']\n",
    "\n",
    "results = []\n",
    "\n",
    "for method in methods:\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    if method in ['trf']:\n",
    "        x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "        bounds = (x0*0.1, x0)\n",
    "        res = least_squares(get_total_error, args = [profiles], bounds = bounds, x0 = x0, verbose=0, method = method)\n",
    "\n",
    "    else:\n",
    "        x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "        bounds = [(0.1, 1) for _ in x0]\n",
    "        res = minimize(get_total_error, args = profiles , x0 = x0, bounds=bounds, method=method)\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "    \n",
    "    end = time()\n",
    "    \n",
    "    time_elapsed_min = (end-start)/60\n",
    "\n",
    "    optimality = get_total_error(solution, profiles) /get_total_error(x0, profiles)\n",
    "    optimality_ = get_total_error(solution, profiles) / get_total_error(true_normalization, profiles)\n",
    "    \n",
    "    results.append((method, time_elapsed_min, optimality, optimality_))\n",
    "    \n",
    "pd.DataFrame(results, columns=['Method', 'Time Elapsed (min)','Error / Baseline Error','Error / Ground Truth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "minimum_occurence = 10\n",
    "\n",
    "def normalize_experiment_SLSQP(profiles):\n",
    "    \"\"\"\n",
    "    Calculate normalization with SLSQP approach\n",
    "    \"\"\"\n",
    "    x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "    bounds = [(0.1, 1) for _ in x0]\n",
    "    res = minimize(get_total_error, args = profiles , x0 = x0, bounds=bounds, method='SLSQP', options={'disp': False} )\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "    \n",
    "    return solution\n",
    "\n",
    "def delayed_normalization(df, field='int_sum', minimum_occurence=None):\n",
    "    \"\"\"\n",
    "    Returns normalization for given peptide intensities \n",
    "    \"\"\"\n",
    "    experiments = np.sort(df['experiment'].unique()).tolist()\n",
    "    fractions = np.sort(df['fraction'].unique()).tolist()\n",
    "\n",
    "    n_fractions = len(fractions)\n",
    "    n_experiments = len(experiments)\n",
    "\n",
    "    df_max = df.groupby(['precursor','fraction','experiment'])[field].max() #Maximum per fraction\n",
    "\n",
    "    prec_count = df_max.index.get_level_values('precursor').value_counts()\n",
    "    \n",
    "    if not minimum_occurence:\n",
    "        minimum_occurence = np.percentile(prec_count[prec_count>1], 75) #Take the 25% best datapoints\n",
    "    \n",
    "    shared_precs = prec_count[prec_count >= minimum_occurence]\n",
    "    precs = prec_count[prec_count > minimum_occurence].index.tolist()\n",
    "\n",
    "    n_profiles = len(precs)\n",
    "\n",
    "    selected_precs = df_max.loc[precs]\n",
    "    selected_precs = selected_precs.reset_index()\n",
    "\n",
    "    profiles = np.empty((n_fractions, n_experiments, n_profiles))\n",
    "    profiles[:] = np.nan\n",
    "\n",
    "    #get dictionaries\n",
    "    fraction_dict = {_:i for i,_ in enumerate(fractions)}\n",
    "    experiment_dict = {_:i for i,_ in enumerate(experiments)}\n",
    "    precursor_dict = {_:i for i,_ in enumerate(precs)}\n",
    "\n",
    "    prec_id = [precursor_dict[_] for _ in selected_precs['precursor']]\n",
    "    frac_id = [fraction_dict[_] for _ in selected_precs['fraction']]\n",
    "    ex_id = [experiment_dict[_] for _ in selected_precs['experiment']]\n",
    "\n",
    "    profiles[frac_id,ex_id, prec_id] = selected_precs[field]\n",
    "    \n",
    "    normalization = normalize_experiment_SLSQP(profiles)\n",
    "    \n",
    "    df[field+'_dn'] = df[field]*normalization[[fraction_dict[_] for _ in df['fraction']], [experiment_dict[_] for _ in df['experiment']]]\n",
    "    \n",
    "    return df, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_settings.ipynb.\n",
      "Converted 12_runner.ipynb.\n",
      "Converted FF_parallel.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
