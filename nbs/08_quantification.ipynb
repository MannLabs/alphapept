{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantification\n",
    "\n",
    "> Functions related to quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains everything to perform quantification\n",
    "\n",
    "Current ToDo here:\n",
    "\n",
    "- Most of the functions are not very well described yet\n",
    "- Introductory text to give an overview / relevant papers would be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFQ\n",
    "\n",
    "Algorithms related to label-free quantifications are motivated by the [MaxLFQ paper](https://doi.org/10.1074/mcp.m113.031591).\n",
    "\n",
    "\n",
    "TODO: Include weighing?\n",
    "TODO: Check boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Silico Test data\n",
    "\n",
    "To test the performance of different approaches we resort to simulating peptide intensities and adjusting them with a simulated normalization. We can then use different approaches to recover the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def gaussian(mu, sigma, grid):\n",
    "    norm = 0.3989422804014327 / sigma\n",
    "    return norm * np.exp(-0.5 * ((grid - mu) / sigma) ** 2)\n",
    "\n",
    "\n",
    "def return_elution_profile(timepoint, sigma, n_runs):\n",
    "    \"\"\"\n",
    "    Simulation of a Gaussian Elution Profile\n",
    "    \"\"\"\n",
    "    return gaussian(timepoint, sigma, np.arange(0, n_runs))\n",
    "\n",
    "\n",
    "def simulate_sample_profiles(n_peptides, n_runs, n_samples, threshold=0.2, use_noise=True):\n",
    "    \"\"\"\n",
    "    Generate random profiles to serve as test data\n",
    "\n",
    "    \"\"\"\n",
    "    abundances = np.random.rand(n_peptides)*10e7\n",
    "    true_normalization = np.random.normal(loc=1, scale=0.1, size=(n_runs, n_samples))\n",
    "    \n",
    "    true_normalization[true_normalization<0] = 0\n",
    "    \n",
    "    true_normalization = true_normalization/np.max(true_normalization)\n",
    "\n",
    "    maxvals = np.max(true_normalization, axis=1)\n",
    "\n",
    "    elution_timepoints = random.choices(list(range(n_runs)), k=n_peptides)\n",
    "\n",
    "    profiles = np.empty((n_runs, n_samples, n_peptides))\n",
    "    profiles[:] = np.nan\n",
    "\n",
    "    for i in range(n_peptides):\n",
    "\n",
    "        elution_timepoint = elution_timepoints[i]\n",
    "        abundance = abundances[i]\n",
    "\n",
    "        profile = return_elution_profile(elution_timepoint, 1, n_runs)\n",
    "        profile = profile/np.max(profile)\n",
    "        profile = profile * abundance\n",
    "        elution_profiles = np.tile(profile, (n_samples, 1)).T\n",
    "\n",
    "        # Some gaussian noise\n",
    "        if use_noise:\n",
    "            noise = np.random.normal(1, 0.2, elution_profiles.shape)\n",
    "            noisy_profile = noise * elution_profiles\n",
    "        else:\n",
    "            noisy_profile = elution_profiles\n",
    "            \n",
    "        #print(noisy_profile)\n",
    "\n",
    "        normalized_profile = noisy_profile * true_normalization\n",
    "\n",
    "        normalized_profile[normalized_profile < threshold] = 0\n",
    "        normalized_profile[normalized_profile == 0] = np.nan\n",
    "        \n",
    "\n",
    "        profiles[:,:,i] = normalized_profile\n",
    "\n",
    "    return profiles, true_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit  \n",
    "def get_peptide_error(profile, normalization): \n",
    "    \n",
    "    pep_ints = np.zeros(profile.shape[1])\n",
    "\n",
    "    normalized_profile = profile*normalization\n",
    "\n",
    "    for i in range(len(pep_ints)):\n",
    "        pep_ints[i] = np.nansum(normalized_profile[:,i])\n",
    "\n",
    "    pep_ints = pep_ints[pep_ints>0]\n",
    "\n",
    "    # Loop through all combinations\n",
    "    n = len(pep_ints)\n",
    "\n",
    "    error = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            error += np.abs(np.log(pep_ints[i]/pep_ints[j]))**2\n",
    "            \n",
    "    return error\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_total_error_parallel(normalization, profiles):\n",
    "    \n",
    "    normalization = normalization.reshape(profiles.shape[:2])\n",
    "    \n",
    "    total_error = 0\n",
    "    \n",
    "    for index in prange(profiles.shape[2]):\n",
    "        total_error += get_peptide_error(profiles[:,:, index], normalization)\n",
    "    \n",
    "    return total_error\n",
    "\n",
    "\n",
    "def get_total_error(normalization, profiles):\n",
    "    \n",
    "    normalization = normalization.reshape(profiles.shape[:2])\n",
    "    \n",
    "    total_error = 0\n",
    "    \n",
    "    for index in range(profiles.shape[2]):\n",
    "        total_error += get_peptide_error(profiles[:,:, index], normalization)\n",
    "    \n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking different optimiziers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Time Elapsed (min)</th>\n",
       "      <th>Error / Baseline Error</th>\n",
       "      <th>Error / Ground Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L-BFGS-B</td>\n",
       "      <td>0.039854</td>\n",
       "      <td>0.635079</td>\n",
       "      <td>0.379579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TNC</td>\n",
       "      <td>0.026556</td>\n",
       "      <td>0.702032</td>\n",
       "      <td>0.419596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLSQP</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.635079</td>\n",
       "      <td>0.379579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trf</td>\n",
       "      <td>0.246381</td>\n",
       "      <td>0.635249</td>\n",
       "      <td>0.379681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Method  Time Elapsed (min)  Error / Baseline Error  Error / Ground Truth\n",
       "0  L-BFGS-B            0.039854                0.635079              0.379579\n",
       "1       TNC            0.026556                0.702032              0.419596\n",
       "2     SLSQP            0.003122                0.635079              0.379579\n",
       "3       trf            0.246381                0.635249              0.379681"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from time import time\n",
    "from scipy.optimize import least_squares\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "n_peptides = 100\n",
    "n_runs = 10\n",
    "n_samples = 3\n",
    "\n",
    "profiles, true_normalization = simulate_sample_profiles(n_peptides, n_runs, n_samples)\n",
    "\n",
    "methods = ['L-BFGS-B', 'TNC', 'SLSQP','trf']\n",
    "\n",
    "results = []\n",
    "\n",
    "for method in methods:\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    if method in ['trf']:\n",
    "        x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "        bounds = (x0*0.1, x0)\n",
    "        res = least_squares(get_total_error, args = [profiles], bounds = bounds, x0 = x0, verbose=0, method = method)\n",
    "\n",
    "    else:\n",
    "        x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "        bounds = [(0.1, 1) for _ in x0]\n",
    "        res = minimize(get_total_error, args = profiles , x0 = x0, bounds=bounds, method=method)\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "    \n",
    "    end = time()\n",
    "    \n",
    "    time_elapsed_min = (end-start)/60\n",
    "\n",
    "    optimality = get_total_error(solution, profiles) /get_total_error(x0, profiles)\n",
    "    optimality_ = get_total_error(solution, profiles) / get_total_error(true_normalization, profiles)\n",
    "    \n",
    "    results.append((method, time_elapsed_min, optimality, optimality_))\n",
    "    \n",
    "pd.DataFrame(results, columns=['Method', 'Time Elapsed (min)','Error / Baseline Error','Error / Ground Truth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "minimum_occurence = 10\n",
    "\n",
    "def normalize_experiment_SLSQP(profiles):\n",
    "    \"\"\"\n",
    "    Calculate normalization with SLSQP approach\n",
    "    \"\"\"\n",
    "    x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "    bounds = [(0.1, 1) for _ in x0]\n",
    "    res = minimize(get_total_error, args = profiles , x0 = x0, bounds=bounds, method='SLSQP', options={'disp': False} )\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "    \n",
    "    return solution\n",
    "\n",
    "def delayed_normalization(df, field='int_sum', minimum_occurence=None):\n",
    "    \"\"\"\n",
    "    Returns normalization for given peptide intensities \n",
    "    \"\"\"\n",
    "    experiments = np.sort(df['experiment'].unique()).tolist()\n",
    "    fractions = np.sort(df['fraction'].unique()).tolist()\n",
    "\n",
    "    n_fractions = len(fractions)\n",
    "    n_experiments = len(experiments)\n",
    "\n",
    "    df_max = df.groupby(['precursor','fraction','experiment'])[field].max() #Maximum per fraction\n",
    "\n",
    "    prec_count = df_max.index.get_level_values('precursor').value_counts()\n",
    "    \n",
    "    if not minimum_occurence:\n",
    "        minimum_occurence = np.percentile(prec_count[prec_count>1], 75) #Take the 25% best datapoints\n",
    "    \n",
    "    shared_precs = prec_count[prec_count >= minimum_occurence]\n",
    "    precs = prec_count[prec_count > minimum_occurence].index.tolist()\n",
    "\n",
    "    n_profiles = len(precs)\n",
    "\n",
    "    selected_precs = df_max.loc[precs]\n",
    "    selected_precs = selected_precs.reset_index()\n",
    "\n",
    "    profiles = np.empty((n_fractions, n_experiments, n_profiles))\n",
    "    profiles[:] = np.nan\n",
    "\n",
    "    #get dictionaries\n",
    "    fraction_dict = {_:i for i,_ in enumerate(fractions)}\n",
    "    experiment_dict = {_:i for i,_ in enumerate(experiments)}\n",
    "    precursor_dict = {_:i for i,_ in enumerate(precs)}\n",
    "\n",
    "    prec_id = [precursor_dict[_] for _ in selected_precs['precursor']]\n",
    "    frac_id = [fraction_dict[_] for _ in selected_precs['fraction']]\n",
    "    ex_id = [experiment_dict[_] for _ in selected_precs['experiment']]\n",
    "\n",
    "    profiles[frac_id,ex_id, prec_id] = selected_precs[field]\n",
    "    \n",
    "    normalization = normalize_experiment_SLSQP(profiles)\n",
    "    \n",
    "    df[field+'_dn'] = df[field]*normalization[[fraction_dict[_] for _ in df['fraction']], [experiment_dict[_] for _ in df['experiment']]]\n",
    "    \n",
    "    return df, normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing protein intensity profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "def generate_dummy_data(n_sequences, n_samples, noise=True, remove = True, peptide_ratio = True, abundance=True, signal_level=100, noise_divider=10, keep=0.8):\n",
    "\n",
    "    species = ['P'+str(_) for _ in range(1,n_sequences+1)]\n",
    "    sample = [string.ascii_uppercase[_] for _ in range(n_samples)]\n",
    "    \n",
    "    if peptide_ratio:\n",
    "        peptide_ratio = np.random.rand(n_sequences)\n",
    "        peptide_ratio = peptide_ratio/np.sum(peptide_ratio)\n",
    "    else:\n",
    "        peptide_ratio = np.ones(n_sequences)\n",
    "\n",
    "    if abundance:\n",
    "        abundance_profile = np.random.rand(n_samples,1)\n",
    "    else:\n",
    "        abundance_profile = np.ones((n_samples,1))\n",
    "\n",
    "    original_signal = np.ones((n_samples, n_sequences))\n",
    "\n",
    "    noise_sim = (np.random.rand(n_samples, n_sequences)-0.5)/noise_divider\n",
    "\n",
    "    if noise:\n",
    "        noisy_signal = original_signal+noise_sim\n",
    "        noisy_signal = noisy_signal*signal_level*peptide_ratio*abundance_profile\n",
    "    else:\n",
    "        noisy_signal = original_signal*signal_level*peptide_ratio*abundance_profile\n",
    "\n",
    "    if remove:\n",
    "        #Remove points\n",
    "        keep_probability = keep #keep 60% of the points \n",
    "        to_remove = np.random.rand(n_samples, n_sequences)\n",
    "        to_remove = to_remove>=keep_probability\n",
    "\n",
    "        dummy_data = noisy_signal.copy()\n",
    "\n",
    "        dummy_data[to_remove] = 0\n",
    "\n",
    "    else:\n",
    "        dummy_data = noisy_signal\n",
    "\n",
    "            \n",
    "    dummy_data = pd.DataFrame(dummy_data, index = sample, columns = species).T\n",
    "    \n",
    "    ground_truth = abundance_profile.flatten()\n",
    "    ground_truth = ground_truth/np.max(ground_truth)\n",
    "        \n",
    "    return dummy_data, sample, ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine pair-wise intenisty ratios\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def get_protein_ratios(signal, column_combinations, minimum_ratios = 2):\n",
    "    n_samples = signal.shape[1]\n",
    "    ratios = np.empty((n_samples, n_samples))\n",
    "    ratios[:] = np.nan\n",
    "\n",
    "    for element in column_combinations:\n",
    "        i = element[0]\n",
    "        j = element[1]\n",
    "\n",
    "        ratio = signal[:,j] / signal[:,i]\n",
    "        \n",
    "        non_nan = np.sum(~np.isnan(ratio))\n",
    "\n",
    "        if non_nan >= minimum_ratios:\n",
    "            ratio_median = np.nanmedian(ratio)\n",
    "        else:\n",
    "            ratio_median = np.nan\n",
    "\n",
    "        ratios[j,i] = ratio_median\n",
    "    \n",
    "    return ratios "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def triangle_error(normalization, ratios):\n",
    "    int_matrix = np.repeat(normalization, len(normalization)).reshape((len(normalization), len(normalization))).transpose()\n",
    "    x = (np.log(ratios) - np.log(int_matrix.T) + np.log(int_matrix))**2\n",
    "    \n",
    "    return np.nansum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "## L-BFGS-B\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# LFBGSB\n",
    "\n",
    "def solve_profile_LFBGSB(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = [(x0[0]*0+0.01, x0[0]-0.01) for _ in x0]\n",
    "    res_wrapped = minimize(triangle_error, args = ratios , x0 = x0, bounds=bounds, method = 'L-BFGS-B')\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success\n",
    "\n",
    "\n",
    "def solve_profile_SLSQP(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = [(x0[0]*0+0.01, x0[0]-0.01) for _ in x0]\n",
    "    res_wrapped = minimize(triangle_error, args = ratios , x0 = x0, bounds=bounds, method = 'SLSQP', options={'maxiter':10000})\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success\n",
    "\n",
    "\n",
    "# TRF\n",
    "def solve_profile_trf(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = (x0*0+0.01, x0-0.01)\n",
    "    res_wrapped = least_squares(triangle_error, args = [ratios] , x0 = x0, bounds=bounds, verbose=0, method = 'trf')\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking different Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\alphap\\lib\\site-packages\\scipy\\optimize\\slsqp.py:63: RuntimeWarning: invalid value encountered in subtract\n",
      "  jac[i] = (func(*((x0+dx,)+args)) - f0)/epsilon\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time Elapsed (s)</th>\n",
       "      <th>Mean absolute percentage error</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L-FBGS-B</th>\n",
       "      <td>0.039508</td>\n",
       "      <td>0.587252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLSQP</th>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.024196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trf</th>\n",
       "      <td>0.000784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Time Elapsed (s)  Mean absolute percentage error  Errors  Success\n",
       "Method                                                                     \n",
       "L-FBGS-B          0.039508                        0.587252     0.0      0.8\n",
       "SLSQP             0.001107                        0.024196     0.0      1.0\n",
       "trf               0.000784                             NaN     1.0      1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from numba.typed import List\n",
    "\n",
    "n_sequences = 10\n",
    "n_samples = 6\n",
    "\n",
    "column_combinations = List()\n",
    "[column_combinations.append(_) for _ in combinations(range(n_samples), 2)]\n",
    "\n",
    "methods = {'L-FBGS-B':solve_profile_LFBGSB, 'SLSQP':solve_profile_SLSQP, 'trf':solve_profile_trf}\n",
    "results = []\n",
    "\n",
    "for run in range(20):\n",
    "    signal, sample, ground_truth = generate_dummy_data(n_sequences, n_samples)\n",
    "    ratios = get_protein_ratios(signal.values, column_combinations)\n",
    "\n",
    "    for method in methods.keys():\n",
    "        error = False\n",
    "        start = time()\n",
    "        try:\n",
    "            solution, success = methods[method](ratios)\n",
    "            mape = np.mean(np.abs((solution-ground_truth)/ground_truth))\n",
    "        except:\n",
    "            error = True\n",
    "            mape = np.nan\n",
    "            \n",
    "        \n",
    "        end = time()\n",
    "\n",
    "        time_elapsed_s = (end-start)\n",
    "\n",
    "        results.append((method, time_elapsed_s, mape, error, success))\n",
    "    \n",
    "result_df = pd.DataFrame(results, columns=['Method', 'Time Elapsed (s)', 'Mean absolute percentage error', 'Errors', 'Success'])\n",
    "\n",
    "result_df.groupby('Method').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SLSQP seems to be a robust and fast choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba.typed import List\n",
    "from itertools import combinations\n",
    "\n",
    "def get_protein_table(df, field = 'int_sum', callback = None):\n",
    "    unique_proteins = df['protein'].unique()\n",
    "    experiments = df['experiment'].unique().tolist()\n",
    "    experiments.sort()\n",
    "    \n",
    "    column_combinations = List()\n",
    "    [column_combinations.append(_) for _ in combinations(range(len(experiments)), 2)]\n",
    "        \n",
    "    columnes_ext = [_+'_LFQ' for _ in experiments]\n",
    "    protein_table = pd.DataFrame(index=unique_proteins, columns=columnes_ext + experiments)\n",
    "\n",
    "    if field+'_dn' in df.columns:\n",
    "        field_ = field+'_dn'\n",
    "    else:\n",
    "        field_ = field\n",
    "\n",
    "    for idx, protein in enumerate(unique_proteins):\n",
    "        subset = df[df['protein'] == protein].copy()\n",
    "        per_protein = subset.groupby(['experiment','precursor'])[field_].sum().unstack().T\n",
    "\n",
    "        for _ in experiments:\n",
    "            if _ not in per_protein.columns:\n",
    "                per_protein[_] = np.nan\n",
    "\n",
    "        per_protein = per_protein[experiments] \n",
    "\n",
    "        ratios = get_protein_ratios(per_protein.values, column_combinations)\n",
    "        solution, success = solve_profile_SLSQP(ratios)\n",
    "            \n",
    "        experiment_ids = per_protein.columns.tolist()\n",
    "        \n",
    "        pre_lfq = per_protein.sum().values\n",
    "\n",
    "        if not success or np.sum(~np.isnan(ratios)) == 0: # or np.sum(solution) == len(pre_lfq):\n",
    "            profile = pre_lfq\n",
    "        else:\n",
    "            total_int = subset[field_].sum()\n",
    "            total_int_ = np.sum(solution)\n",
    "            profile = total_int/total_int_*solution\n",
    "\n",
    "        protein_table.loc[protein, [_+'_LFQ' for _ in experiment_ids]] = profile\n",
    "        protein_table.loc[protein, experiment_ids] = pre_lfq\n",
    "        \n",
    "        if callback:\n",
    "            callback((idx+1)/len(unique_proteins))\n",
    "        \n",
    "    protein_table[protein_table == 0] = np.nan\n",
    "    protein_table = protein_table.astype('float')\n",
    "\n",
    "    return protein_table\n",
    "\n",
    "def protein_profile(df, experiments, field_, protein):\n",
    "    \"\"\"\n",
    "    Calculate the protein profile for a a df based on a dateframe\n",
    "    \n",
    "    \"\"\"\n",
    "    column_combinations = List()\n",
    "    [column_combinations.append(_) for _ in combinations(range(len(experiments)), 2)]\n",
    "    \n",
    "    subset = df[df['protein'] == protein].copy()\n",
    "    per_protein = subset.groupby(['experiment','precursor'])[field_].sum().unstack().T\n",
    "    \n",
    "    for _ in experiments:\n",
    "        if _ not in per_protein.columns:\n",
    "            per_protein[_] = np.nan\n",
    "            \n",
    "    per_protein = per_protein[experiments] \n",
    "\n",
    "    ratios = get_protein_ratios(per_protein.values, column_combinations)\n",
    "    solution, success = solve_profile_SLSQP(ratios)\n",
    "    \n",
    "    experiment_ids = per_protein.columns.tolist()\n",
    "    pre_lfq = per_protein.sum().values\n",
    "\n",
    "    if not success or np.sum(~np.isnan(ratios)) == 0: # or np.sum(solution) == len(pre_lfq):\n",
    "        profile = pre_lfq\n",
    "    else:\n",
    "        total_int = subset[field_].sum()\n",
    "        total_int_ = np.sum(solution)\n",
    "        profile = total_int/total_int_*solution\n",
    "    \n",
    "    return profile, pre_lfq, experiment_ids, protein\n",
    "\n",
    "\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from functools import partial \n",
    "\n",
    "\n",
    "def protein_profile_parallel(settings, df, callback=None):\n",
    "\n",
    "    n_processes = settings['general']['n_processes']\n",
    "    field = settings['quantification']['mode']\n",
    "    \n",
    "    unique_proteins = df['protein'].unique().tolist()\n",
    "    experiments = df['experiment'].unique().tolist()\n",
    "    experiments.sort()\n",
    "          \n",
    "    columnes_ext = [_+'_LFQ' for _ in experiments]\n",
    "    protein_table = pd.DataFrame(index=unique_proteins, columns=columnes_ext + experiments)\n",
    "\n",
    "    if field+'_dn' in df.columns:\n",
    "        field_ = field+'_dn'\n",
    "    else:\n",
    "        field_ = field\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with Pool(n_processes) as p:\n",
    "        max_ = len(unique_proteins)\n",
    "        for i, _ in enumerate(p.imap_unordered(partial(protein_profile, df, experiments, field_), unique_proteins)):\n",
    "            results.append(_)\n",
    "            if callback:\n",
    "                callback((i+1)/max_)\n",
    "                \n",
    "    for result in results:\n",
    "        \n",
    "        profile, pre_lfq, experiment_ids, protein = result\n",
    "        protein_table.loc[protein, [_+'_LFQ' for _ in experiment_ids]] = profile\n",
    "        protein_table.loc[protein, experiment_ids] = pre_lfq\n",
    "                \n",
    "    protein_table[protein_table == 0] = np.nan\n",
    "    protein_table = protein_table.astype('float')\n",
    "   \n",
    "    return protein_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    \n",
    "    protein_table = protein_table.apply(np.log10)\n",
    "\n",
    "    protein_table['human'] = protein_table.index.str.contains('HUMAN')\n",
    "    protein_table['eco'] = protein_table.index.str.contains('ECO')\n",
    "\n",
    "    protein_table['H_LFQ'] = protein_table[['H1_LFQ','H2_LFQ','H3_LFQ']].mean(axis=1)\n",
    "    protein_table['L_LFQ'] = protein_table[['L1_LFQ','L2_LFQ','L3_LFQ']].mean(axis=1)\n",
    "\n",
    "    protein_table['LFQ_ratio'] = protein_table['H_LFQ'] / protein_table['L_LFQ']\n",
    "    protein_table['LFQ_sum'] = protein_table['H_LFQ'] + protein_table['L_LFQ']\n",
    "\n",
    "    protein_table['H'] = protein_table[['H1','H2','H3']].mean(axis=1)\n",
    "    protein_table['L'] = protein_table[['L1','L2','L3']].mean(axis=1)\n",
    "\n",
    "    protein_table['_ratio'] = protein_table['H'] / protein_table['L']\n",
    "    protein_table['_sum'] = protein_table['H'] + protein_table['L']\n",
    "\n",
    "\n",
    "    protein_table['H_count'] = protein_table[['H1_LFQ','H2_LFQ','H3_LFQ']].count(axis=1)\n",
    "    protein_table['L_count'] = protein_table[['L1_LFQ','L2_LFQ','L3_LFQ']].count(axis=1)\n",
    "    \n",
    "    sub = protein_table.query('H_count >= 2 and L_count >=2')\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    human = sub[sub['human']]\n",
    "    eco = sub[protein_table['eco']]\n",
    "    plt.plot(human['_ratio'], human['_sum'],'.', alpha=0.5)\n",
    "    plt.plot(eco['_ratio'], eco['_sum'],'.', alpha=0.5)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    plt.plot(human['LFQ_ratio'], human['LFQ_sum'],'.', alpha=0.5)\n",
    "    plt.plot(eco['LFQ_ratio'], eco['LFQ_sum'],'.', alpha=0.5)\n",
    "    \n",
    "    bins = np.linspace(0.9,1.1,100)\n",
    "\n",
    "    plt.hist(eco['LFQ_ratio'],bins=bins, alpha=0.5)\n",
    "    plt.hist(human['LFQ_ratio'],bins=bins, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted # Search Modes.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_settings.ipynb.\n",
      "Converted 12_runner.ipynb.\n",
      "Converted 13_parallel.ipynb.\n",
      "Converted FF_parallel.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted Large fasta Search.ipynb.\n",
      "Converted OTF, revised.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\projects\\\\alphapept\\\\alphapept\\\\None.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-09bc8eae79de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#hide\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnbdev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnotebook2script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\alphap\\lib\\site-packages\\nbdev\\export.py\u001b[0m in \u001b[0;36mnotebook2script\u001b[1;34m(fname, silent, to_dict)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_notebook2script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0madd_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\alphap\\lib\\site-packages\\nbdev\\export.py\u001b[0m in \u001b[0;36m_notebook2script\u001b[1;34m(fname, silent, to_dict)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mextra\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_add2all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mf\"'{f}'\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' +$'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMULTILINE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\alphap\\lib\\site-packages\\nbdev\\export.py\u001b[0m in \u001b[0;36m_add2all\u001b[1;34m(fname, names, line_width)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_add2all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m     \u001b[0mtw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_indent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsequent_indent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbreak_long_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[0mre_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_re__all__def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\projects\\\\alphapept\\\\alphapept\\\\None.py'"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
