{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantification\n",
    "\n",
    "> Functions related to quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label-free quantification\n",
    "\n",
    "Algorithms related to label-free quantifications are motivated by the [MaxLFQ paper](https://doi.org/10.1074/mcp.m113.031591). The main goal is to derive relative protein intensities that can be used for downstream analyses. In a first step, constant normalization coefficients are derived for each run. In a second step, pseudointensities are derived for each protein, such that differing conditions can be compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed Normalization\n",
    "\n",
    "Delayed normalization describes the process of normalizing the differences that occur from prefractionation as well as from sample handling. For each sample, a constant scaling factor is derived by minimizing the term\n",
    "$$H(\\vec{N}) = \\sum_{P \\in peptides} \\sum_{A,B \\in sample pairs} |\\frac{I(N_A, P, A)}{I(N_B, P, B)}|, $$\n",
    "with peptide intensities $I$, which are determined by the peptide $P$ the sample $A$ or $B$ and the normalization factors $N_A$, $N_B$. In principle H(N) quantifies the variation of peptides over the samples. Minimizing this variation gives appropriate scaling factors under the assumption that most peptides do not change between the samples. Peptide intensities for fractionated samples are described as the sum of the intensities over the fractions, with fraction-specific normalization factors. Therefore, calculation of the summed intensities is *delayed* until the normalization is finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Silico Test data\n",
    "\n",
    "To test the delayed normalization approach we create an in silico test dataset with a known ground truth. We therefore know, which systematic changes are between the samples and we employ different solvers to recover the normalization parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "def gaussian(mu: float, sigma: float, grid : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculates normally distributed probability densities along an input array.\n",
    "\n",
    "    Args:\n",
    "        mu (float): mean of ND.\n",
    "        sigma (float): standard deviation of ND.\n",
    "        grid (np.ndarray): input array np.int[:]. For each element of the array, the  probability density is calculated.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: probability density array, np.float[:].\n",
    "    \"\"\"\n",
    "    norm = 0.3989422804014327 / sigma\n",
    "    return norm * np.exp(-0.5 * ((grid - mu) / sigma) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_gaussian():\n",
    "    \n",
    "    assert np.allclose(gaussian(10, 3, np.arange(5)), np.array([0.00051409, 0.00147728, 0.00379866, 0.00874063, 0.01799699]))\n",
    "    assert np.allclose(gaussian(1, 3, np.arange(6)), np.array([0.12579441, 0.13298076, 0.12579441, 0.10648267, 0.08065691, 0.05467002]))\n",
    "\n",
    "test_gaussian() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def return_elution_profile(timepoint: float, sigma : float, n_runs : int) -> np.ndarray:\n",
    "    \"\"\"Simulates a gaussian elution profile.\n",
    "\n",
    "    Args:\n",
    "        timepoint (float): coordinate of the peak apex.\n",
    "        sigma (float): standard deviation of the gaussian.\n",
    "        n_runs (int): number of points along which the density is calculated.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: probability density array, np.float[:].\n",
    "    \"\"\"\n",
    "    return gaussian(timepoint, sigma, np.arange(0, n_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_return_elution_profile():\n",
    "    assert np.allclose(return_elution_profile(10, 2, 6), np.array([7.43359757e-07, 7.99187055e-06, 6.69151129e-05, 4.36341348e-04,\n",
    "           2.21592421e-03, 8.76415025e-03]))\n",
    "\n",
    "    assert np.allclose(return_elution_profile(1, 5, 3), np.array([0.07820854, 0.07978846, 0.07820854]))\n",
    "\n",
    "test_return_elution_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def simulate_sample_profiles(n_peptides: int, n_runs: int, n_samples: int, threshold:float=0.2, use_noise:bool=True) -> [np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generates random profiles to serve as test_data.\n",
    "\n",
    "    Args:\n",
    "        n_peptides (int): number of peptides to be simulated.\n",
    "        n_runs (int): number of runs to be simulated.\n",
    "        n_samples (int): number of samples to be simulated.\n",
    "        threshold (float, optional): threshold below which a simulated intensity will be discarded. Defaults to 0.2.\n",
    "        use_noise (bool, optional): add simulated noise to the profile values. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: profiles: np.float[:,:,:] array containing the simulated profiles, true_normalization: np.float[:,:,:] array containing the ground truth.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    abundances = np.random.rand(n_peptides)*10e7\n",
    "\n",
    "    true_normalization = np.random.normal(loc=1, scale=0.1, size=(n_runs, n_samples))\n",
    "    true_normalization[true_normalization<0] = 0\n",
    "    true_normalization = true_normalization/np.max(true_normalization)\n",
    "    maxvals = np.max(true_normalization, axis=1)\n",
    "    elution_timepoints = random.choices(list(range(n_runs)), k=n_peptides)\n",
    "\n",
    "    profiles = np.empty((n_runs, n_samples, n_peptides))\n",
    "    profiles[:] = np.nan\n",
    "\n",
    "    for i in range(n_peptides):\n",
    "\n",
    "        elution_timepoint = elution_timepoints[i]\n",
    "        abundance = abundances[i]\n",
    "\n",
    "        profile = return_elution_profile(elution_timepoint, 1, n_runs)\n",
    "        profile = profile/np.max(profile)\n",
    "        profile = profile * abundance\n",
    "        elution_profiles = np.tile(profile, (n_samples, 1)).T\n",
    "\n",
    "        # Add Gaussian Noise\n",
    "        if use_noise:\n",
    "            noise = np.random.normal(1, 0.2, elution_profiles.shape)\n",
    "            noisy_profile = noise * elution_profiles\n",
    "        else:\n",
    "            noisy_profile = elution_profiles\n",
    "\n",
    "        normalized_profile = noisy_profile * true_normalization\n",
    "\n",
    "        normalized_profile[normalized_profile < threshold] = 0\n",
    "        normalized_profile[normalized_profile == 0] = np.nan\n",
    "\n",
    "\n",
    "        profiles[:,:,i] = normalized_profile\n",
    "\n",
    "    return profiles, true_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_simulate_sample_profiles():\n",
    "    #The function to be tested is a random generator so we only test the output format here\n",
    "    \n",
    "    n_peptides = 2\n",
    "    n_runs = 5\n",
    "    n_samples = 10\n",
    "    \n",
    "    profiles, true_normalization = simulate_sample_profiles(n_peptides, n_runs, n_samples)\n",
    "    \n",
    "    assert profiles.shape == (n_runs, n_samples, n_peptides)\n",
    "    assert true_normalization.shape == (n_runs, n_samples)\n",
    "    \n",
    "    assert np.all(profiles > 0)\n",
    "    assert np.all(true_normalization > 0)\n",
    "    \n",
    "test_simulate_sample_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit\n",
    "def get_peptide_error(profile: np.ndarray, normalization: np.ndarray) -> float:\n",
    "    \"\"\"Distance function for least squares optimization. Calculates the peptide ratios between samples. Smaller ratios mean better normalization.\n",
    "\n",
    "    Args:\n",
    "        profile (np.ndarray): peptide intensity values.\n",
    "        normalization (np.ndarray): per sample normalization factors.\n",
    "\n",
    "    Returns:\n",
    "        float: summed squared error.\n",
    "    \"\"\"\n",
    "    pep_ints = np.zeros(profile.shape[1])\n",
    "\n",
    "    normalized_profile = profile*normalization\n",
    "\n",
    "    for i in range(len(pep_ints)):\n",
    "        pep_ints[i] = np.nansum(normalized_profile[:,i])\n",
    "\n",
    "    pep_ints = pep_ints[pep_ints>0]\n",
    "\n",
    "    # Loop through all combinations\n",
    "    n = len(pep_ints)\n",
    "\n",
    "    error = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            error += np.abs(np.log(pep_ints[i]/pep_ints[j]))**2\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_peptide_error():\n",
    "    profile = np.ones((10, 10))\n",
    "\n",
    "    normalization = np.ones((10))\n",
    "    assert get_peptide_error(profile, normalization) == 0\n",
    "\n",
    "    normalization = np.arange((10))\n",
    "    assert np.allclose(get_peptide_error(profile, normalization), 37.24832444019646)\n",
    "    \n",
    "test_get_peptide_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_total_error(normalization: np.ndarray, profiles: np.ndarray) -> float:\n",
    "    \"\"\"Computes the summed peptide errors over the whole dataset.\n",
    "\n",
    "    Args:\n",
    "        normalization (np.ndarray): per sample normalization factors.\n",
    "        profiles (np.ndarray): peptide intensity profiles over the dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: summed peptide error.\n",
    "    \"\"\"\n",
    "    normalization = normalization.reshape(profiles.shape[:2])\n",
    "\n",
    "    total_error = 0\n",
    "\n",
    "    for index in range(profiles.shape[2]):\n",
    "        total_error += get_peptide_error(profiles[:,:, index], normalization)\n",
    "\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_get_total_error():\n",
    "    profiles = np.ones((10, 10, 4))\n",
    "\n",
    "    normalization = np.ones((10, 10))\n",
    "    assert get_total_error(normalization, profiles) == 0\n",
    "\n",
    "    normalization = np.array([np.arange(10) for i in range(10)])\n",
    "    assert np.allclose(get_total_error(normalization, profiles), 4*37.24832444019646)\n",
    "\n",
    "test_get_total_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking different optimiziers\n",
    "The normalization step is in principle a quadratic minimization of the normalization factors. Such minimization problems can be solved in various ways and a variety of approaches are realized in python community packages. We compare different solvers using our benchmarking set and uncover substantial differences in precision and runtime. We observe that the *Sequential Least Squares Quadratic Programming* (SLSQP) approach is a robust solution in our benchmarking, which gives substantial speed improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from time import time\n",
    "from scipy.optimize import least_squares\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "n_peptides = 100\n",
    "n_runs = 10\n",
    "n_samples = 3\n",
    "\n",
    "profiles, true_normalization = simulate_sample_profiles(n_peptides, n_runs, n_samples)\n",
    "\n",
    "methods = ['L-BFGS-B', 'TNC', 'SLSQP','trf']\n",
    "\n",
    "results = []\n",
    "\n",
    "for method in methods:\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "        if method in ['trf']:\n",
    "            x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "            bounds = (x0*0.1, x0)\n",
    "            res = least_squares(get_total_error, args = [profiles], bounds = bounds, x0 = x0*0.5, verbose=0, method = method)\n",
    "\n",
    "        else:\n",
    "            x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "            bounds = [(0.1, 1) for _ in x0]\n",
    "            res = minimize(get_total_error, args = profiles , x0 = x0*0.5, bounds=bounds, method=method)\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "    \n",
    "    end = time()\n",
    "    \n",
    "    time_elapsed_min = (end-start)/60\n",
    "\n",
    "    optimality = get_total_error(solution, profiles) /get_total_error(x0, profiles)\n",
    "    optimality_ = get_total_error(solution, profiles) / get_total_error(true_normalization, profiles)\n",
    "    \n",
    "    results.append((method, time_elapsed_min, optimality, optimality_))\n",
    "    \n",
    "pd.DataFrame(results, columns=['Method', 'Time Elapsed (min)','Error / Baseline Error','Error / Ground Truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def normalize_experiment_SLSQP(profiles: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculates normalization with SLSQP approach.\n",
    "\n",
    "    Args:\n",
    "        profiles (np.ndarray): peptide intensities.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: normalization factors.\n",
    "    \"\"\"\n",
    "    x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "    bounds = [(0.1, 1) for _ in x0]\n",
    "    res = minimize(get_total_error, args = profiles , x0 = x0*0.5, bounds=bounds, method='SLSQP', options={'disp': False, 'maxiter':100*profiles.shape[1]})\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_normalize_experiment_SLSQP():\n",
    "    n_peptides = 15\n",
    "    n_runs = 5\n",
    "    n_samples = 20\n",
    "\n",
    "    profiles, true_normalization = simulate_sample_profiles(n_peptides, n_runs, n_samples)\n",
    "\n",
    "    x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        solution = normalize_experiment_SLSQP(profiles)\n",
    "    optimality = get_total_error(solution, profiles) / get_total_error(x0, profiles)\n",
    "\n",
    "    assert optimality < 1\n",
    "\n",
    "test_normalize_experiment_SLSQP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize_experiment_BFGS(profiles: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculates normalization with BFGS approach.\n",
    "\n",
    "    Args:\n",
    "        profiles (np.ndarray): peptide intensities.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: normalization factors.\n",
    "    \"\"\"\n",
    "    x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "    bounds = [(0.1, 1) for _ in x0]\n",
    "    res = minimize(get_total_error, args = profiles , x0 = x0*0.5, bounds=bounds, method='L-BFGS-B', options={'disp': False, 'maxiter':100*profiles.shape[1]} )\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_normalize_experiment_BFGS():\n",
    "    n_peptides = 15\n",
    "    n_runs = 5\n",
    "    n_samples = 20\n",
    "\n",
    "    profiles, true_normalization = simulate_sample_profiles(n_peptides, n_runs, n_samples)\n",
    "\n",
    "    x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        solution = normalize_experiment_BFGS(profiles)\n",
    "    optimality = get_total_error(solution, profiles) / get_total_error(x0, profiles)\n",
    "\n",
    "    assert optimality < 1\n",
    "    \n",
    "test_normalize_experiment_BFGS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def delayed_normalization(df: pd.DataFrame, field: str='ms1_int_sum', minimum_occurence:bool=None) -> [pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"Returns normalization factors for given peptide intensities.\n",
    "    If the solver does not converge, the unnormalized data will be used.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): alphapept quantified features table.\n",
    "        field (str, optional): The column in df containing the quantitative peptide information (i.e. precursor intensities).\n",
    "        minimum_occurence (bool, optional): minimum number of replicates the peptide must be observed in. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        [pd.DataFrame, np.ndarray]: pd.DataFrame: alphapept quantified features table extended with the normalized intensities, np.ndarray: normalized intensities\n",
    "    \"\"\"\n",
    "    files = np.sort(df['filename'].unique()).tolist()\n",
    "    n_files = len(files)\n",
    "\n",
    "    if 'fraction' not in df.keys():\n",
    "        df['fraction'] = [1 for x in range(len(df.index))]\n",
    "\n",
    "    fractions = np.sort(df['fraction'].unique()).tolist()\n",
    "\n",
    "    n_fractions = len(fractions)\n",
    "\n",
    "    df_max = df.groupby(['precursor','fraction','filename'])[field].max() #Maximum per fraction\n",
    "\n",
    "    prec_count = df_max.index.get_level_values('precursor').value_counts()\n",
    "\n",
    "    if not minimum_occurence:\n",
    "        minimum_occurence = np.percentile(prec_count[prec_count>1].values, 75) #Take the 25% best datapoints\n",
    "        logging.info('Setting minimum occurence to {}'.format(minimum_occurence))\n",
    "\n",
    "    shared_precs = prec_count[prec_count >= minimum_occurence]\n",
    "\n",
    "\n",
    "    precs = shared_precs.index.tolist()\n",
    "    n_profiles = len(precs)\n",
    "\n",
    "    selected_precs = df_max.loc[precs]\n",
    "    selected_precs = selected_precs.reset_index()\n",
    "\n",
    "    profiles = np.empty((n_fractions, n_files, n_profiles))\n",
    "    profiles[:] = np.nan\n",
    "\n",
    "    #get dictionaries\n",
    "    fraction_dict = {_:i for i,_ in enumerate(fractions)}\n",
    "    filename_dict = {_:i for i,_ in enumerate(files)}\n",
    "    precursor_dict = {_:i for i,_ in enumerate(precs)}\n",
    "\n",
    "    prec_id = [precursor_dict[_] for _ in selected_precs['precursor']]\n",
    "    frac_id = [fraction_dict[_] for _ in selected_precs['fraction']]\n",
    "    file_id = [filename_dict[_] for _ in selected_precs['filename']]\n",
    "\n",
    "    profiles[frac_id,file_id, prec_id] = selected_precs[field]\n",
    "    \n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "        try:\n",
    "            normalization = normalize_experiment_SLSQP(profiles)\n",
    "            norm1d = np.ravel(normalization)\n",
    "            if sum((norm1d!=1))==0:\n",
    "                raise ValueError(\"optimization with SLSQP terminated at initial values. Trying BFGS\")\n",
    "        except ValueError: # SLSQP error in scipy https://github.com/scipy/scipy/issues/11403\n",
    "            logging.info('Normalization with SLSQP failed. Trying BFGS')\n",
    "            normalization = normalize_experiment_BFGS(profiles)\n",
    "            norm1d = np.ravel(normalization)\n",
    "            if sum((norm1d!=1))==0:\n",
    "                logging.warn('No normalization factors could be determined. Continuing with non-normalized data.')\n",
    "\n",
    "\n",
    "    #intensity normalization: total intensity to remain unchanged\n",
    "\n",
    "    df[field+'_dn'] = df[field]*normalization[[fraction_dict[_] for _ in df['fraction']], [filename_dict[_] for _ in df['filename']]]\n",
    "    df[field+'_dn'] *= df[field].sum()/df[field+'_dn'].sum()\n",
    "\n",
    "    return df, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = {}\n",
    "\n",
    "sample_data['precursor'] = ['Prec_1'] * 6 + ['Prec_2'] * 6 + ['Prec_3'] * 6\n",
    "sample_data['fraction'] = [1,1,2]*6\n",
    "sample_data['filename'] = ['A','A','A', 'B','B','B'] * 3\n",
    "sample_data['ms1_int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2] * 3\n",
    "\n",
    "test_df = pd.DataFrame(sample_data)\n",
    "test_df, normalization = delayed_normalization(test_df, field='ms1_int_sum', minimum_occurence=0)\n",
    "\n",
    "display(pd.DataFrame(normalization))\n",
    "display(test_df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_delayed_normalization():\n",
    "    sample_data = {}\n",
    "\n",
    "    sample_data['precursor'] = ['Prec_1'] * 6 + ['Prec_2'] * 6 + ['Prec_3'] * 6\n",
    "    sample_data['fraction'] = [1,1,2]*6\n",
    "    sample_data['filename'] = ['A','A','A', 'B','B','B'] * 3\n",
    "    sample_data['ms1_int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2] * 3\n",
    "\n",
    "    test_df = pd.DataFrame(sample_data)\n",
    "    test_df, normalization = delayed_normalization(test_df, field='ms1_int_sum', minimum_occurence=0)\n",
    "    \n",
    "    assert normalization.shape == (2,2)\n",
    "    \n",
    "test_delayed_normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing protein intensity profiles\n",
    "Protein intensity profiles are constructed for each protein individually. All possible protein fold changes between the samples are derived from the median peptide fold changes. Subsequently, pseudointensities are chosen such that the fold changes between the pseudointensities ideally reconstruct the actually observed fold changes. Similar to the delayed normalization, this is formulated as a quadratic minimization, which we solve with the SLSQP solver.\n",
    "\n",
    "Codewise, we start with simulating in-silico test data to serve as a ground-truth for assessing solvers for the optimization problem. For the algorithmic optimization, we define the function `get_protein_ratios` that allows to quickly calculate the protein ratios. Next, we define an error function `triangle_error` that we use for the optimization problem. Lastly, we have several wrapper functions to access the functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-silico test data\n",
    "\n",
    "Create a simulated input dataset of peptide intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import string\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_dummy_data(n_sequences: int, n_samples: int, noise:bool=True, remove:bool= True, peptide_ratio:bool= True, abundance:bool=True, signal_level:int=100, noise_divider:int=10, keep:float=0.8) -> [pd.DataFrame, list, np.ndarray]:\n",
    "    \"\"\"Simulate an input dataset of peptide intensities.\n",
    "\n",
    "    Args:\n",
    "        n_sequences (int): number of peptides to simulate.\n",
    "        n_samples (int): number of samples to simulate.\n",
    "        noise (bool, optional): add random signal to distort the simulated intensity levels. Defaults to True.\n",
    "        remove (bool, optional): remove intensities (i.e. add missing values). Defaults to True.\n",
    "        peptide_ratio (bool, optional): simulate different peptide intensities. Defaults to True.\n",
    "        abundance (bool, optional): simulate different abundances for each sample (i.e. systematic shifts). Defaults to True.\n",
    "        signal_level (int, optional): signal level for simulated intensity. Defaults to 100.\n",
    "        noise_divider (int, optional): the factor through which the noise is divided (higher factor -> higher signal to noise). Defaults to 10.\n",
    "        keep (float, optional): aimed-at fraction of non-missing values, applies if 'remove' is set. Defaults to 0.8.\n",
    "\n",
    "    Returns:\n",
    "        [pd.DataFrame, list, np.ndarray]: pd.DataFrame: simulated dataset with peptide intensities, list: sample names: np.ndarray: shift factors of each sample\n",
    "    \"\"\"\n",
    "    species = ['P'+str(_) for _ in range(1,n_sequences+1)]\n",
    "    sample = [string.ascii_uppercase[_%26]+str(_//26) for _ in range(n_samples)]\n",
    "    \n",
    "    if peptide_ratio:\n",
    "        peptide_ratio = np.random.rand(n_sequences)\n",
    "        peptide_ratio = peptide_ratio/np.sum(peptide_ratio)\n",
    "    else:\n",
    "        peptide_ratio = np.ones(n_sequences)\n",
    "\n",
    "    if abundance:\n",
    "        abundance_profile = np.random.rand(n_samples,1)\n",
    "    else:\n",
    "        abundance_profile = np.ones((n_samples,1))\n",
    "\n",
    "    original_signal = np.ones((n_samples, n_sequences))\n",
    "\n",
    "    noise_sim = (np.random.rand(n_samples, n_sequences)-0.5)/noise_divider\n",
    "\n",
    "    if noise:\n",
    "        noisy_signal = original_signal+noise_sim\n",
    "        noisy_signal = noisy_signal*signal_level*peptide_ratio*abundance_profile\n",
    "    else:\n",
    "        noisy_signal = original_signal*signal_level*peptide_ratio*abundance_profile\n",
    "\n",
    "    if remove:\n",
    "        #Remove points\n",
    "        keep_probability = keep #keep 60% of the points \n",
    "        to_remove = np.random.rand(n_samples, n_sequences)\n",
    "        to_remove = to_remove>=keep_probability\n",
    "\n",
    "        dummy_data = noisy_signal.copy()\n",
    "\n",
    "        dummy_data[to_remove] = 0\n",
    "\n",
    "    else:\n",
    "        dummy_data = noisy_signal\n",
    "\n",
    "            \n",
    "    dummy_data = pd.DataFrame(dummy_data, index = sample, columns = species).T\n",
    "    \n",
    "    ground_truth = abundance_profile.flatten()\n",
    "    ground_truth = ground_truth/np.max(ground_truth)\n",
    "        \n",
    "    return dummy_data, sample, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_generate_dummy_data():\n",
    "    #The function to be tested is a random-generateor, so we test the output format here.\n",
    "    n_samples = 10\n",
    "    n_sequences = 5\n",
    "\n",
    "    dummy_data, sample, ground_truth = generate_dummy_data(n_samples, n_sequences)\n",
    "\n",
    "    assert dummy_data.shape == (n_samples, n_sequences)\n",
    "    assert len(sample) == n_sequences\n",
    "    assert len(ground_truth) == n_sequences\n",
    "    \n",
    "test_generate_dummy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine pair-wise intenisty ratios\n",
    "The pair-wise protein ratios are determined from the median peptide ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def get_protein_ratios(signal: np.ndarray, column_combinations: list, minimum_ratios:int = 1) -> np.ndarray:\n",
    "    \"\"\"Calculates the protein ratios between samples for one protein.\n",
    "\n",
    "    Args:\n",
    "        signal (np.ndarray): np.array[:,:] containing peptide intensities for each sample.\n",
    "        column_combinations (list): list of all index combinations to compare (usually all sample combinations).\n",
    "        minimum_ratios (int, optional): minimum number of peptide ratios necessary to calculate a protein ratio. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: np.array[:,:] matrix comparing the ratios for all column combinations.\n",
    "    \"\"\"\n",
    "    n_samples = signal.shape[1]\n",
    "    ratios = np.empty((n_samples, n_samples))\n",
    "    ratios[:] = np.nan\n",
    "\n",
    "    for element in column_combinations:\n",
    "        i = element[0]\n",
    "        j = element[1]\n",
    "\n",
    "        ratio = signal[:,j] / signal[:,i]\n",
    "\n",
    "        non_nan = np.sum(~np.isnan(ratio))\n",
    "\n",
    "        if non_nan >= minimum_ratios:\n",
    "            ratio_median = np.nanmedian(ratio)\n",
    "        else:\n",
    "            ratio_median = np.nan\n",
    "\n",
    "        ratios[j,i] = ratio_median\n",
    "\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from itertools import combinations\n",
    "from numba.typed import List\n",
    "\n",
    "def test_get_protein_ratios():\n",
    "    n_samples = 5\n",
    "    n_peptides = 2\n",
    "\n",
    "    signal = np.ones((n_samples, n_peptides))\n",
    "    column_combinations = List([_ for _ in combinations(range(n_samples), 2)])\n",
    "\n",
    "    ratios = get_protein_ratios(signal, column_combinations)\n",
    "\n",
    "    assert ratios[1,0] == 1\n",
    "\n",
    "    signal[:,1]*=2\n",
    "\n",
    "    ratios = get_protein_ratios(signal, column_combinations)\n",
    "\n",
    "    assert ratios[1,0] == 2\n",
    "    \n",
    "#test_get_protein_ratios()  #TODO: this test seems to break the CI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Function\n",
    "The error function evaluates the difference between the actual observed fold change and the fold change that is derived from the pseudointensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def triangle_error(normalization: np.ndarray, ratios:np.ndarray) -> float:\n",
    "    \"\"\"Calculates the difference between calculated ratios and expected ratios.\n",
    "\n",
    "    Args:\n",
    "        normalization (np.ndarray): Used normalization.\n",
    "        ratios (np.ndarray): Peptide ratios.\n",
    "\n",
    "    Returns:\n",
    "        float: summed quadratic difference.\n",
    "    \"\"\"\n",
    "    int_matrix = np.repeat(normalization, len(normalization)).reshape((len(normalization), len(normalization))).transpose()\n",
    "    x = (np.log(ratios) - np.log(int_matrix.T) + np.log(int_matrix))**2\n",
    "\n",
    "    return np.nansum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_triangle_error():\n",
    "    n_samples = 5\n",
    "    n_peptides = 4\n",
    "\n",
    "    signal = np.ones((n_samples, n_peptides))\n",
    "    column_combinations = List([_ for _ in combinations(range(n_samples), 2)])\n",
    "\n",
    "    ratios = get_protein_ratios(signal, column_combinations)\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "\n",
    "    assert triangle_error(x0, ratios) == 0\n",
    "\n",
    "    signal[:,1]*=2\n",
    "    ratios = get_protein_ratios(signal, column_combinations)\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "\n",
    "    assert np.allclose(triangle_error(x0, ratios), 1.441359041754604)\n",
    "    \n",
    "#test_triangle_error()  #TODO: this test seems to break the CI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver implementation\n",
    "As with the delayed normalization we implement multiple solvers from scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "## L-BFGS-B\n",
    "from scipy.optimize import minimize, least_squares\n",
    "\n",
    "def solve_profile(ratios: np.ndarray, method: str) -> [np.ndarray, bool]:\n",
    "    \"\"\"Calculates protein pseudointensities with a specified solver.\n",
    "    Args:\n",
    "        ratios (np.ndarray): np.array[:,:] matrix containing all estimated protein ratios between samples.\n",
    "        method (str): string specifying which solver to use.\n",
    "    Raises:\n",
    "        NotImplementedError: if the solver is not implemented.\n",
    "    Returns:\n",
    "        [np.ndarray, bool]: np.ndarray: the protein pseudointensities, bool: wether the solver was successful.\n",
    "    \"\"\"\n",
    "    if method not in ['L-BFGS-B', 'SLSQP', 'Powell', 'trust-constr','trf']:\n",
    "        raise NotImplementedError(method)\n",
    "\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = [(min(np.nanmin(ratios), 1/np.nanmax(ratios)), 1) for _ in x0]\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "        if method == 'trf':\n",
    "            bounds = (x0*0+0.01, x0)\n",
    "            res_wrapped = least_squares(triangle_error, args = [ratios] , x0 = x0, bounds=bounds, verbose=0, method = 'trf')\n",
    "            solution = res_wrapped.x\n",
    "        else:\n",
    "            \n",
    "            ncor = max((20, int(2*np.ceil(np.sqrt(ratios.shape[0])))))\n",
    "            \n",
    "            res_wrapped = minimize(triangle_error, args = ratios , x0 = x0, bounds=bounds, method = method, options={'disp': 1, 'maxiter':int(1e6),'maxfun':int(ratios.shape[0]*2e4), 'eps': 1e-06, 'ncor':ncor}, )\n",
    "            solution = res_wrapped.x\n",
    "\n",
    "    solution = solution/np.max(solution)\n",
    "\n",
    "    return solution, res_wrapped.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_solve_profile():\n",
    "    # The tested function is a wrapper for scipy, so we only test execution here\n",
    "    n_samples = 5\n",
    "    n_peptides = 2\n",
    "\n",
    "    signal = np.ones((n_samples, n_peptides))\n",
    "    column_combinations = List([_ for _ in combinations(range(n_samples), 2)])\n",
    "\n",
    "    ratios = get_protein_ratios(signal, column_combinations)\n",
    "\n",
    "    for method in ['L-BFGS-B', 'SLSQP', 'Powell', 'trust-constr','trf']:\n",
    "        solve_profile(ratios, method)\n",
    "        \n",
    "#test_solve_profile()  #TODO: this test seems to break the CI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving single profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba.typed import List\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def protein_profile(files: list, minimum_ratios: int, chunk:tuple) -> (np.ndarray, np.ndarray, str):\n",
    "    \"\"\"Function to extract optimal protein ratios for a given input of peptides.\n",
    "    \n",
    "    Note for the chunk argument: This construction is needed to call this function from a parallel pool.\n",
    "    \n",
    "    Args:\n",
    "        files (list): A list of files for which the profile shall be extracted.\n",
    "        minimum_ratios (int): A minimum number of peptide ratios to be considered for optimization.\n",
    "        chunk: (tuple[pd.DataFrame, str]): A pandas dataframe with the peptide information and a string to identify the protein.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: optimized profile\n",
    "        np.ndarray: profile w/o optimization\n",
    "        str: protein identifier\n",
    "    \"\"\"\n",
    "    grouped, protein = chunk\n",
    "\n",
    "    files_ = grouped.index.get_level_values('filename').unique().tolist()\n",
    "\n",
    "    selection = grouped.unstack().T.copy()\n",
    "    selection = selection.replace(0, np.nan)\n",
    "    \n",
    "    if len(files_) > 1:\n",
    "        success = False\n",
    "        column_combinations = List()\n",
    "        [column_combinations.append(_) for _ in combinations(range(len(files_)), 2)]\n",
    "\n",
    "        ratios = get_protein_ratios(selection.values, column_combinations, minimum_ratios)\n",
    "        \n",
    "        retry = False\n",
    "        try:\n",
    "            solution, success = solve_profile(ratios, 'L-BFGS-B')\n",
    "        except ValueError:\n",
    "            retry = True\n",
    "\n",
    "        if retry or not success:\n",
    "            logging.info('Normalization with L-BFGS-B failed. Trying Powell')\n",
    "            solution, success = solve_profile(ratios, 'Powell')\n",
    "\n",
    "        pre_lfq = selection.sum().values\n",
    "\n",
    "        if not success or np.sum(~np.isnan(ratios)) == 0: # or np.sum(solution) == len(pre_lfq):\n",
    "            profile = np.zeros(len(files_))\n",
    "            if np.sum(np.isnan(ratios)) != ratios.size:\n",
    "                logging.info(f'Solver failed for protein {protein} despite available ratios:\\n {ratios}')\n",
    "\n",
    "        else:\n",
    "            invalid = ((np.nansum(ratios, axis=1) == 0) & (np.nansum(ratios, axis=0) == 0))\n",
    "            peptide_int_sum = pre_lfq.sum() * solution \n",
    "            peptide_int_sum[invalid] = 0\n",
    "            profile = peptide_int_sum * pre_lfq.sum() / np.sum(peptide_int_sum) #Normalize inensity again\n",
    "    else:\n",
    "        success = True\n",
    "        pre_lfq = profile = selection.sum(axis=0).values\n",
    "\n",
    "    #Rewrite ratios\n",
    "    profile_dict = dict(zip(files_, profile))\n",
    "    pre_dict = dict(zip(files_, pre_lfq))\n",
    "    profile = np.array([0 if file not in profile_dict else profile_dict[file] for file in files])\n",
    "    pre_lfq = np.array([0 if file not in pre_dict else pre_dict[file] for file in files])\n",
    "\n",
    "    return profile, pre_lfq, protein, success \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_data = {}\n",
    "\n",
    "sample_data['precursor'] = ['Prec_1'] * 3 + ['Prec_2'] * 3 + ['Prec_3'] * 3\n",
    "sample_data['filename'] = ['A','B','C'] * 3\n",
    "sample_data['protein_group'] = ['X'] * 9\n",
    "sample_data['ms1_int_sum'] = [0.6, 0.8, 1.0, 0.6, 1.2, 1.4, 1.6, 1.2, 1.8]\n",
    "\n",
    "test_df = pd.DataFrame(sample_data)\n",
    "\n",
    "display(test_df.head(6))\n",
    "\n",
    "grouped = test_df.groupby(['protein_group','filename','precursor']).sum().loc['X']\n",
    "files = ['A','B','C']\n",
    "minimum_ratios = 1\n",
    "chunk = (grouped, 'X')\n",
    "\n",
    "if False: #TODO: this test seems to break the CI\n",
    "    profile, pre_lfq, protein, success = protein_profile(files, minimum_ratios, chunk)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Protein ratio')\n",
    "    plt.plot(pre_lfq, 'o', label='before optimization')\n",
    "    plt.plot(profile, 'o', label='after optimization')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_protein_profile():\n",
    "\n",
    "    sample_data = {}\n",
    "    sample_data['precursor'] = ['Prec_1'] * 6 + ['Prec_2'] * 6 + ['Prec_3'] * 6\n",
    "    sample_data['fraction'] = [1,2,3]*6\n",
    "    sample_data['filename'] = ['A','A','A', 'B','B','B'] * 3\n",
    "    sample_data['protein_group'] = ['X'] * 18\n",
    "    sample_data['ms1_int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2] * 3\n",
    "    test_df = pd.DataFrame(sample_data)\n",
    "\n",
    "    grouped = test_df.groupby(['protein_group','filename','precursor']).sum().loc['X']\n",
    "    files = ['A','B']\n",
    "    minimum_ratios = 1\n",
    "    chunk = (grouped, 'X')\n",
    "\n",
    "    profile, pre_lfq, protein, success = protein_profile(files, minimum_ratios, chunk)\n",
    "    assert np.allclose(profile.sum(), pre_lfq.sum())\n",
    "\n",
    "    sample_data = {}\n",
    "    sample_data['precursor'] = ['Prec_1'] * 2 + ['Prec_2'] * 2 + ['Prec_3'] * 2\n",
    "    sample_data['filename'] = ['A','B'] * 3\n",
    "    sample_data['protein_group'] = ['X'] * 6\n",
    "    sample_data['ms1_int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2]\n",
    "\n",
    "    test_df = pd.DataFrame(sample_data)\n",
    "\n",
    "    grouped = test_df.groupby(['protein_group','filename','precursor']).sum().loc['X']\n",
    "    files = ['A','B']\n",
    "    minimum_ratios = 1\n",
    "    chunk = (grouped, 'X')\n",
    "\n",
    "    profile, pre_lfq, protein, success = protein_profile(files, minimum_ratios, chunk)\n",
    "\n",
    "    assert np.allclose(profile.sum(), pre_lfq.sum())\n",
    "\n",
    "test_protein_profile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper functions\n",
    "\n",
    "To be compatible with interface, we have three wrapper functions:\n",
    "\n",
    "* protein_profile_parallel: A wrapper that executes protein_profile in parallel\n",
    "* protein_profile_parallel_ap: A wrapper function to calculate protein ratios based on AlphaPept tabular data\n",
    "* protein_profile_prallalel_mq: A wrapper function to calculate protein ratios based on MaxQuant tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "import os\n",
    "import alphapept.performance\n",
    "from functools import partial\n",
    "\n",
    "# This function invokes a parallel pool and has therfore no dedicated test in the notebook\n",
    "def protein_profile_parallel(df: pd.DataFrame, minimum_ratios: int, field: str, callback=None) -> pd.DataFrame:\n",
    "    \"\"\"Derives LFQ intensities from the feature table.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Feature table by alphapept.\n",
    "        minimum_ratios (int): Minimum number of peptide ratios necessary to derive a protein ratio.\n",
    "        field (str): The field containing the quantitative peptide information (i.e. precursor intensities). \n",
    "        callback ([type], optional): Callback function. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: table containing the LFQ intensities of each protein in each sample.\n",
    "    \"\"\"\n",
    "    unique_proteins = df['protein_group'].unique().tolist()\n",
    "    \n",
    "    files = df['filename'].unique().tolist()\n",
    "    files.sort()\n",
    "    \n",
    "    columnes_ext = [_+'_LFQ' for _ in files]\n",
    "    protein_table = pd.DataFrame(index=unique_proteins, columns=columnes_ext + files)\n",
    "\n",
    "    #Take the best precursor for protein quantification. .max()\n",
    "    grouped = df[[field, 'filename','precursor','protein_group']].groupby(['protein_group','filename','precursor']).max()\n",
    "    \n",
    "    files = df['filename'].unique().tolist()\n",
    "    files.sort()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if len(files) > 1:\n",
    "        logging.info('Preparing protein table for parallel processing.')\n",
    "        split_df = []\n",
    "        \n",
    "        for idx, protein in enumerate(unique_proteins):\n",
    "            split_df.append((grouped.loc[protein], protein))\n",
    "            if callback:\n",
    "                callback((idx+1)/len(unique_proteins)*1/5)\n",
    "        \n",
    "        results = []\n",
    "\n",
    "        logging.info(f'Starting protein extraction for {len(split_df)} proteins.')\n",
    "        n_processes = alphapept.performance.set_worker_count(\n",
    "            worker_count=0,\n",
    "            set_global=False\n",
    "        )\n",
    "        with alphapept.performance.AlphaPool(n_processes) as p:\n",
    "            max_ = len(split_df)\n",
    "            for i, _ in enumerate(p.imap_unordered(partial(protein_profile, files, minimum_ratios), split_df)):\n",
    "                results.append(_)\n",
    "                \n",
    "                if not _[-1]:\n",
    "                    logging.info(f'LFQ profile extraction for protein {_[-2]} did not converge. Setting to zero.')\n",
    "                if callback:\n",
    "                    callback((i+1)/max_*4/5+1/5)\n",
    "\n",
    "        for result in results:\n",
    "            profile, pre_lfq, protein, success = result\n",
    "            protein_table.loc[protein, [_+'_LFQ' for _ in files]] = profile\n",
    "            protein_table.loc[protein, files] = pre_lfq\n",
    "\n",
    "        protein_table[protein_table == 0] = np.nan\n",
    "        protein_table = protein_table.astype('float')\n",
    "    else:\n",
    "        protein_table = df.groupby(['protein_group'])[field].sum().to_frame().reset_index()\n",
    "        protein_table = protein_table.set_index('protein_group')\n",
    "        protein_table.index.name = None\n",
    "        protein_table.columns=[files[0]] \n",
    "        \n",
    "        if callback:\n",
    "            callback(1)\n",
    "        \n",
    "    return protein_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# This function invokes a parallel pool and has therfore no dedicated test in the notebook\n",
    "def protein_profile_parallel_ap(settings: dict, df : pd.DataFrame, callback=None) -> pd.DataFrame:\n",
    "    \"\"\"Derives protein LFQ intensities from the alphapept quantified feature table\n",
    "\n",
    "    Args:\n",
    "        settings (dict): alphapept settings dictionary.\n",
    "        df (pd.DataFrame): alphapept feature table.\n",
    "        callback ([type], optional): [description]. Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: raised in case of observed negative intensities.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: table containing the LFQ intensities of each protein in each sample.\n",
    "    \"\"\"\n",
    "    minimum_ratios = settings['quantification']['lfq_ratio_min']\n",
    "    field = settings['quantification']['mode']\n",
    "    \n",
    "    if field+'_dn' in df.columns:\n",
    "        field_ = field+'_dn'\n",
    "    else:\n",
    "        field_ = field\n",
    "\n",
    "    if df[field_].min() < 0:\n",
    "        raise ValueError('Negative intensity values present.')\n",
    "        \n",
    "    protein_table = protein_profile_parallel(df, minimum_ratios, field_, callback)\n",
    "    \n",
    "    return protein_table\n",
    "    \n",
    "# This function invokes a parallel pool and has therfore no dedicated test in the notebook\n",
    "def protein_profile_parallel_mq(evidence_path : str, protein_groups_path: str, minimum_ratios: int = 1, minimum_occurence:bool=None, delayed:bool = True, callback=None) -> pd.DataFrame:\n",
    "    \"\"\"Derives protein LFQ intensities from Maxquant quantified features.\n",
    "\n",
    "    Args:\n",
    "        evidence_path (str): path to the Maxquant standard output table evidence.txt.\n",
    "        protein_groups_path (str): path to the Maxquant standard output table proteinGroups.txt.\n",
    "        minimum_ratios (int): minimum ratios (LFQ parameter)\n",
    "        minimum_occurence (int): minimum occurence (LFQ parameter)\n",
    "        delayed (bool): toggle for delayed normalization (on/off)\n",
    "        callback ([type], optional): [description]. Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: if Maxquant files cannot be found.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: table containing the LFQ intensities of each protein in each sample.\n",
    "    \"\"\"\n",
    "    logging.info('Loading files')\n",
    "    \n",
    "    for file in [evidence_path, protein_groups_path]:\n",
    "        if not os.path.isfile(file):\n",
    "            raise FileNotFoundError(f'File {file} not found.')\n",
    "            \n",
    "    evd = pd.read_csv(evidence_path, sep='\\t') \n",
    "    ref = pd.read_csv(protein_groups_path, sep='\\t')\n",
    "            \n",
    "    experiments = evd['Raw file'].unique().tolist()\n",
    "    logging.info(f'A total of {len(experiments):,} files.')\n",
    "\n",
    "    protein_df = []\n",
    "    \n",
    "    max_ = len(ref)\n",
    "    for i in range(max_):\n",
    "        investigate = ref.iloc[i]\n",
    "        evd_ids = [int(_) for _ in investigate['Evidence IDs'].split(';')]\n",
    "        subset = evd.loc[evd_ids].copy()\n",
    "\n",
    "        subset['protein_group'] =  investigate['Protein IDs']\n",
    "        subset['filename'] = subset['Raw file']\n",
    "        subset['precursor']  = ['_'.join(_) for _ in zip(subset['Sequence'].values, subset['Charge'].values.astype('str'))]\n",
    "\n",
    "        protein_df.append(subset)\n",
    "        \n",
    "        if callback:\n",
    "            callback((i+1)/len(ref))\n",
    "                     \n",
    "    logging.info(f'A total of {max_:,} proteins.')\n",
    "\n",
    "    df = pd.concat(protein_df)\n",
    "    \n",
    "    field = 'Intensity'\n",
    "    \n",
    "    if delayed:\n",
    "        df, normed = delayed_normalization(df, field ='Intensity', minimum_occurence = minimum_occurence)\n",
    "        field = 'Intensity_dn'\n",
    "        \n",
    "    protein_table = protein_profile_parallel(df, minimum_ratios=minimum_ratios, field=field, callback=callback)\n",
    "                     \n",
    "    return protein_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
