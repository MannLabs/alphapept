{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantification\n",
    "\n",
    "> Functions related to quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label-free quantification\n",
    "\n",
    "Algorithms related to label-free quantifications are motivated by the [MaxLFQ paper](https://doi.org/10.1074/mcp.m113.031591). The main goal is to derive relative protein intensities that can be used for downstream analyses. In a first step, constant normalization coefficients are derived for each run. In a second step, pseudointensities are derived for each protein, such that differing conditions can be compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed Normalization\n",
    "\n",
    "Delayed normalization describes the process of normalizing the differences that occur from prefractionation as well as from sample handling. For each sample, a constant scaling factor is derived by minimizing the term\n",
    "$$H(\\vec{N}) = \\sum_{P \\in peptides} \\sum_{A,B \\in sample pairs} |\\frac{I(N_A, P, A)}{I(N_B, P, B)}|, $$\n",
    "with peptide intensities $I$, which are determined by the peptide $P$ the sample $A$ or $B$ and the normalization factors $N_A$, $N_B$. In principle H(N) quantifies the variation of peptides over the samples. Minimizing this variation gives appropriate scaling factors under the assumption that most peptides do not change between the samples. Peptide intensities for fractionated samples are described as the sum of the intensities over the fractions, with fraction-specific normalization factors. Therefore, calculation of the summed intensities is *delayed* until the normalization is finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Silico Test data\n",
    "\n",
    "To test the delayed normalization approach we create an in silico test dataset with a known ground truth. We therefore know, which systematic changes are between the samples and we employ different solvers to recover the normalization parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "def gaussian(mu, sigma, grid):\n",
    "    \"\"\"\n",
    "    Gaussian function\n",
    "    \"\"\"\n",
    "    norm = 0.3989422804014327 / sigma\n",
    "    return norm * np.exp(-0.5 * ((grid - mu) / sigma) ** 2)\n",
    "\n",
    "\n",
    "def return_elution_profile(timepoint, sigma, n_runs):\n",
    "    \"\"\"\n",
    "    Returns a gaussian elution profile for a given timepoint\n",
    "    \"\"\"\n",
    "    return gaussian(timepoint, sigma, np.arange(0, n_runs))\n",
    "\n",
    "\n",
    "def simulate_sample_profiles(n_peptides, n_runs, n_samples, threshold=0.2, use_noise=True):\n",
    "    \"\"\"\n",
    "    Generate random profiles to serve as test_data\n",
    "    \"\"\"\n",
    "    abundances = np.random.rand(n_peptides)*10e7\n",
    "\n",
    "    true_normalization = np.random.normal(loc=1, scale=0.1, size=(n_runs, n_samples))\n",
    "\n",
    "    true_normalization[true_normalization<0] = 0\n",
    "\n",
    "    true_normalization = true_normalization/np.max(true_normalization)\n",
    "\n",
    "    maxvals = np.max(true_normalization, axis=1)\n",
    "\n",
    "    elution_timepoints = random.choices(list(range(n_runs)), k=n_peptides)\n",
    "\n",
    "    profiles = np.empty((n_runs, n_samples, n_peptides))\n",
    "    profiles[:] = np.nan\n",
    "\n",
    "    for i in range(n_peptides):\n",
    "\n",
    "        elution_timepoint = elution_timepoints[i]\n",
    "        abundance = abundances[i]\n",
    "\n",
    "        profile = return_elution_profile(elution_timepoint, 1, n_runs)\n",
    "        profile = profile/np.max(profile)\n",
    "        profile = profile * abundance\n",
    "        elution_profiles = np.tile(profile, (n_samples, 1)).T\n",
    "\n",
    "        # Add Gaussian Noise\n",
    "        if use_noise:\n",
    "            noise = np.random.normal(1, 0.2, elution_profiles.shape)\n",
    "            noisy_profile = noise * elution_profiles\n",
    "        else:\n",
    "            noisy_profile = elution_profiles\n",
    "\n",
    "        normalized_profile = noisy_profile * true_normalization\n",
    "\n",
    "        normalized_profile[normalized_profile < threshold] = 0\n",
    "        normalized_profile[normalized_profile == 0] = np.nan\n",
    "\n",
    "\n",
    "        profiles[:,:,i] = normalized_profile\n",
    "\n",
    "    return profiles, true_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit\n",
    "def get_peptide_error(profile, normalization):\n",
    "\n",
    "    pep_ints = np.zeros(profile.shape[1])\n",
    "\n",
    "    normalized_profile = profile*normalization\n",
    "\n",
    "    for i in range(len(pep_ints)):\n",
    "        pep_ints[i] = np.nansum(normalized_profile[:,i])\n",
    "\n",
    "    pep_ints = pep_ints[pep_ints>0]\n",
    "\n",
    "    # Loop through all combinations\n",
    "    n = len(pep_ints)\n",
    "\n",
    "    error = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            error += np.abs(np.log(pep_ints[i]/pep_ints[j]))**2\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_total_error_parallel(normalization, profiles):\n",
    "\n",
    "    normalization = normalization.reshape(profiles.shape[:2])\n",
    "\n",
    "    total_error = 0\n",
    "\n",
    "    for index in prange(profiles.shape[2]):\n",
    "        total_error += get_peptide_error(profiles[:,:, index], normalization)\n",
    "\n",
    "    return total_error\n",
    "\n",
    "\n",
    "def get_total_error(normalization, profiles):\n",
    "\n",
    "    normalization = normalization.reshape(profiles.shape[:2])\n",
    "\n",
    "    total_error = 0\n",
    "\n",
    "    for index in range(profiles.shape[2]):\n",
    "        total_error += get_peptide_error(profiles[:,:, index], normalization)\n",
    "\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def normalize_experiment_SLSQP(profiles):\n",
    "    \"\"\"\n",
    "    Calculate normalization with SLSQP approach\n",
    "    \"\"\"\n",
    "    x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "    bounds = [(0.1, 1) for _ in x0]\n",
    "    res = minimize(get_total_error, args = profiles , x0 = x0*0.5, bounds=bounds, method='SLSQP', options={'disp': False} )\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "\n",
    "    return solution\n",
    "\n",
    "def normalize_experiment_BFGS(profiles):\n",
    "    \"\"\"\n",
    "    Calculate normalization with BFGS approach\n",
    "    \"\"\"\n",
    "    x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "    bounds = [(0.1, 1) for _ in x0]\n",
    "    res = minimize(get_total_error, args = profiles , x0 = x0*0.5, bounds=bounds, method='L-BFGS-B', options={'disp': False} )\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "\n",
    "    return solution\n",
    "\n",
    "def delayed_normalization(df, field='int_sum', minimum_occurence=None):\n",
    "    \"\"\"\n",
    "    Returns normalization for given peptide intensities\n",
    "    \"\"\"\n",
    "    files = np.sort(df['shortname'].unique()).tolist()\n",
    "    n_files = len(files)\n",
    "\n",
    "    if 'fraction' not in df.keys():\n",
    "        df['fraction'] = [1 for x in range(len(df.index))]\n",
    "    \n",
    "    fractions = np.sort(df['fraction'].unique()).tolist()\n",
    "\n",
    "    n_fractions = len(fractions)\n",
    "\n",
    "    df_max = df.groupby(['precursor','fraction','shortname'])[field].max() #Maximum per fraction\n",
    "\n",
    "    prec_count = df_max.index.get_level_values('precursor').value_counts()\n",
    "\n",
    "    if not minimum_occurence:\n",
    "        minimum_occurence = np.percentile(prec_count[prec_count>1].values, 75) #Take the 25% best datapoints\n",
    "        logging.info('Setting minimum occurence to {}'.format(minimum_occurence))\n",
    "\n",
    "    shared_precs = prec_count[prec_count >= minimum_occurence]\n",
    "    precs = prec_count[prec_count > minimum_occurence].index.tolist()\n",
    "\n",
    "    n_profiles = len(precs)\n",
    "\n",
    "    selected_precs = df_max.loc[precs]\n",
    "    selected_precs = selected_precs.reset_index()\n",
    "\n",
    "    profiles = np.empty((n_fractions, n_files, n_profiles))\n",
    "    profiles[:] = np.nan\n",
    "\n",
    "    #get dictionaries\n",
    "    fraction_dict = {_:i for i,_ in enumerate(fractions)}\n",
    "    filename_dict = {_:i for i,_ in enumerate(files)}\n",
    "    precursor_dict = {_:i for i,_ in enumerate(precs)}\n",
    "\n",
    "    prec_id = [precursor_dict[_] for _ in selected_precs['precursor']]\n",
    "    frac_id = [fraction_dict[_] for _ in selected_precs['fraction']]\n",
    "    file_id = [filename_dict[_] for _ in selected_precs['shortname']]\n",
    "\n",
    "    profiles[frac_id,file_id, prec_id] = selected_precs[field]\n",
    "\n",
    "    try:\n",
    "        normalization = normalize_experiment_SLSQP(profiles)\n",
    "    except ValueError: # SLSQP error in scipy https://github.com/scipy/scipy/issues/11403\n",
    "        logging.info('Normalization with SLSQP failed. Trying BFGS')\n",
    "        normalization = normalize_experiment_BFGS(profiles)\n",
    "\n",
    "\n",
    "    #intensity normalization: total intensity to remain unchanged\n",
    "\n",
    "    df[field+'_dn'] = df[field]*normalization[[fraction_dict[_] for _ in df['fraction']], [filename_dict[_] for _ in df['shortname']]]\n",
    "    df[field+'_dn'] *= df[field].sum()/df[field+'_dn'].sum()\n",
    "\n",
    "    # else:\n",
    "    #     logging.info('No fractions present. Skipping delayed normalization.')\n",
    "    #     normalization = None\n",
    "\n",
    "    return df, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/optimize.py:282: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  warnings.warn(\"Values in x were outside bounds during a \"\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.957777</td>\n      <td>0.480980</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.000000</td>\n      <td>0.497222</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "          0         1\n0  0.957777  0.480980\n1  1.000000  0.497222"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precursor</th>\n      <th>fraction</th>\n      <th>shortname</th>\n      <th>int_sum</th>\n      <th>int_sum_dn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Prec_1</td>\n      <td>1</td>\n      <td>A</td>\n      <td>0.6</td>\n      <td>0.887676</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Prec_1</td>\n      <td>1</td>\n      <td>A</td>\n      <td>0.8</td>\n      <td>1.183568</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Prec_1</td>\n      <td>2</td>\n      <td>A</td>\n      <td>0.6</td>\n      <td>0.926809</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Prec_1</td>\n      <td>1</td>\n      <td>B</td>\n      <td>1.2</td>\n      <td>0.891552</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Prec_1</td>\n      <td>1</td>\n      <td>B</td>\n      <td>1.6</td>\n      <td>1.188736</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Prec_1</td>\n      <td>2</td>\n      <td>B</td>\n      <td>1.2</td>\n      <td>0.921659</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "  precursor  fraction shortname  int_sum  int_sum_dn\n0    Prec_1         1         A      0.6    0.887676\n1    Prec_1         1         A      0.8    1.183568\n2    Prec_1         2         A      0.6    0.926809\n3    Prec_1         1         B      1.2    0.891552\n4    Prec_1         1         B      1.6    1.188736\n5    Prec_1         2         B      1.2    0.921659"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data = {}\n",
    "\n",
    "sample_data['precursor'] = ['Prec_1'] * 6 + ['Prec_2'] * 6 + ['Prec_3'] * 6\n",
    "sample_data['fraction'] = [1,1,2]*6\n",
    "sample_data['shortname'] = ['A','A','A', 'B','B','B'] * 3\n",
    "sample_data['int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2] * 3\n",
    "\n",
    "test_df = pd.DataFrame(sample_data)\n",
    "test_df, normalization = delayed_normalization(test_df, field='int_sum', minimum_occurence=2)\n",
    "\n",
    "display(pd.DataFrame(normalization))\n",
    "display(test_df.head(6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking different optimiziers\n",
    "The normalization step is in principle a quadratic minimization of the normalization factors. Such minimization problems can be solved in various ways and a variety of approaches are realized in python community packages. We compare different solvers using our benchmarking set and uncover substantial differences in precision and runtime. We observe that the *Sequential Least Squares Quadratic Programming* (SLSQP) approach is a robust solution in our benchmarking, which gives substantial speed improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\alphapept\\lib\\site-packages\\scipy\\optimize\\optimize.py:282: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  warnings.warn(\"Values in x were outside bounds during a \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Time Elapsed (min)</th>\n",
       "      <th>Error / Baseline Error</th>\n",
       "      <th>Error / Ground Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L-BFGS-B</td>\n",
       "      <td>0.038645</td>\n",
       "      <td>0.613492</td>\n",
       "      <td>0.336389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TNC</td>\n",
       "      <td>0.019305</td>\n",
       "      <td>0.638834</td>\n",
       "      <td>0.350285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLSQP</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.613492</td>\n",
       "      <td>0.336389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trf</td>\n",
       "      <td>0.184128</td>\n",
       "      <td>0.615414</td>\n",
       "      <td>0.337443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Method  Time Elapsed (min)  Error / Baseline Error  Error / Ground Truth\n",
       "0  L-BFGS-B            0.038645                0.613492              0.336389\n",
       "1       TNC            0.019305                0.638834              0.350285\n",
       "2     SLSQP            0.003049                0.613492              0.336389\n",
       "3       trf            0.184128                0.615414              0.337443"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from time import time\n",
    "from scipy.optimize import least_squares\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "n_peptides = 100\n",
    "n_runs = 10\n",
    "n_samples = 3\n",
    "\n",
    "profiles, true_normalization = simulate_sample_profiles(n_peptides, n_runs, n_samples)\n",
    "\n",
    "methods = ['L-BFGS-B', 'TNC', 'SLSQP','trf']\n",
    "\n",
    "results = []\n",
    "\n",
    "for method in methods:\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    if method in ['trf']:\n",
    "        x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "        bounds = (x0*0.1, x0)\n",
    "        res = least_squares(get_total_error, args = [profiles], bounds = bounds, x0 = x0*0.5, verbose=0, method = method)\n",
    "\n",
    "    else:\n",
    "        x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n",
    "        bounds = [(0.1, 1) for _ in x0]\n",
    "        res = minimize(get_total_error, args = profiles , x0 = x0*0.5, bounds=bounds, method=method)\n",
    "\n",
    "    solution = res.x/np.max(res.x)\n",
    "    solution = solution.reshape(profiles.shape[:2])\n",
    "    \n",
    "    end = time()\n",
    "    \n",
    "    time_elapsed_min = (end-start)/60\n",
    "\n",
    "    optimality = get_total_error(solution, profiles) /get_total_error(x0, profiles)\n",
    "    optimality_ = get_total_error(solution, profiles) / get_total_error(true_normalization, profiles)\n",
    "    \n",
    "    results.append((method, time_elapsed_min, optimality, optimality_))\n",
    "    \n",
    "pd.DataFrame(results, columns=['Method', 'Time Elapsed (min)','Error / Baseline Error','Error / Ground Truth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing protein intensity profiles\n",
    "Protein intensity profiles are constructed for each protein individually. All possible protein fold changes between the samples are derived from the median peptide fold changes. Subsequently, pseudointensities are chosen such that the fold changes between the pseudointensities ideally reconstruct the actually observed fold changes. Similar to the delayed normalization, this is formulated as a quadratic minimization, which we solve with the SLSQP solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "def generate_dummy_data(n_sequences, n_samples, noise=True, remove = True, peptide_ratio = True, abundance=True, signal_level=100, noise_divider=10, keep=0.8):\n",
    "\n",
    "    species = ['P'+str(_) for _ in range(1,n_sequences+1)]\n",
    "    sample = [string.ascii_uppercase[_%26]+str(_//26) for _ in range(n_samples)]\n",
    "    \n",
    "    if peptide_ratio:\n",
    "        peptide_ratio = np.random.rand(n_sequences)\n",
    "        peptide_ratio = peptide_ratio/np.sum(peptide_ratio)\n",
    "    else:\n",
    "        peptide_ratio = np.ones(n_sequences)\n",
    "\n",
    "    if abundance:\n",
    "        abundance_profile = np.random.rand(n_samples,1)\n",
    "    else:\n",
    "        abundance_profile = np.ones((n_samples,1))\n",
    "\n",
    "    original_signal = np.ones((n_samples, n_sequences))\n",
    "\n",
    "    noise_sim = (np.random.rand(n_samples, n_sequences)-0.5)/noise_divider\n",
    "\n",
    "    if noise:\n",
    "        noisy_signal = original_signal+noise_sim\n",
    "        noisy_signal = noisy_signal*signal_level*peptide_ratio*abundance_profile\n",
    "    else:\n",
    "        noisy_signal = original_signal*signal_level*peptide_ratio*abundance_profile\n",
    "\n",
    "    if remove:\n",
    "        #Remove points\n",
    "        keep_probability = keep #keep 60% of the points \n",
    "        to_remove = np.random.rand(n_samples, n_sequences)\n",
    "        to_remove = to_remove>=keep_probability\n",
    "\n",
    "        dummy_data = noisy_signal.copy()\n",
    "\n",
    "        dummy_data[to_remove] = 0\n",
    "\n",
    "    else:\n",
    "        dummy_data = noisy_signal\n",
    "\n",
    "            \n",
    "    dummy_data = pd.DataFrame(dummy_data, index = sample, columns = species).T\n",
    "    \n",
    "    ground_truth = abundance_profile.flatten()\n",
    "    ground_truth = ground_truth/np.max(ground_truth)\n",
    "        \n",
    "    return dummy_data, sample, ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine pair-wise intenisty ratios\n",
    "The pair-wise protein ratios are determined from the median peptide ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def get_protein_ratios(signal, column_combinations, minimum_ratios = 1):\n",
    "    n_samples = signal.shape[1]\n",
    "    ratios = np.empty((n_samples, n_samples))\n",
    "    ratios[:] = np.nan\n",
    "\n",
    "    for element in column_combinations:\n",
    "        i = element[0]\n",
    "        j = element[1]\n",
    "\n",
    "        ratio = signal[:,j] / signal[:,i]\n",
    "\n",
    "        non_nan = np.sum(~np.isnan(ratio))\n",
    "\n",
    "        if non_nan >= minimum_ratios:\n",
    "            ratio_median = np.nanmedian(ratio)\n",
    "        else:\n",
    "            ratio_median = np.nan\n",
    "\n",
    "        ratios[j,i] = ratio_median\n",
    "\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Function\n",
    "The error function evaluates the difference between the actually observed fold change and the fold change that is derived from the pseudointensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def triangle_error(normalization, ratios):\n",
    "    int_matrix = np.repeat(normalization, len(normalization)).reshape((len(normalization), len(normalization))).transpose()\n",
    "    x = (np.log(ratios) - np.log(int_matrix.T) + np.log(int_matrix))**2\n",
    "\n",
    "    return np.nansum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways to solve\n",
    "Different quadratic solving strategies are tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "## L-BFGS-B\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize, least_squares\n",
    "\n",
    "# LFBGSB\n",
    "\n",
    "def solve_profile_LFBGSB(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = [(min(np.nanmin(ratios), 1/np.nanmax(ratios)), 1) for _ in x0]\n",
    "    res_wrapped = minimize(triangle_error, args = ratios , x0 = x0, bounds=bounds, method = 'L-BFGS-B')\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success\n",
    "\n",
    "\n",
    "def solve_profile_SLSQP(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = [(min(np.nanmin(ratios), 1/np.nanmax(ratios)), 1) for _ in x0]\n",
    "    res_wrapped = minimize(triangle_error, args = ratios , x0 = x0, bounds=bounds, method = 'SLSQP', options={'maxiter':10000})\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success\n",
    "\n",
    "\n",
    "# TRF\n",
    "def solve_profile_trf(ratios):\n",
    "    x0 = np.ones(ratios.shape[1])\n",
    "    bounds = (x0*0+0.01, x0)\n",
    "    res_wrapped = least_squares(triangle_error, args = [ratios] , x0 = x0*0.5, bounds=bounds, verbose=0, method = 'trf')\n",
    "    solution = res_wrapped.x\n",
    "    solution = solution/np.max(solution)\n",
    "    return solution, res_wrapped.success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking different Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\alphapept\\lib\\site-packages\\scipy\\optimize\\optimize.py:282: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  warnings.warn(\"Values in x were outside bounds during a \"\n",
      "C:\\ProgramData\\Anaconda3\\envs\\alphapept\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:557: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time Elapsed (s)</th>\n",
       "      <th>Mean absolute percentage error</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L-FBGS-B</th>\n",
       "      <td>0.037563</td>\n",
       "      <td>1.092636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLSQP</th>\n",
       "      <td>0.003630</td>\n",
       "      <td>1.092644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trf</th>\n",
       "      <td>0.029791</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Time Elapsed (s)  Mean absolute percentage error  Errors  Success\n",
       "Method                                                                     \n",
       "L-FBGS-B          0.037563                        1.092636     0.0     0.80\n",
       "SLSQP             0.003630                        1.092644     0.0     0.80\n",
       "trf               0.029791                        0.015071     0.2     0.75"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from numba.typed import List\n",
    "\n",
    "n_sequences = 10\n",
    "n_samples = 6\n",
    "\n",
    "column_combinations = List()\n",
    "[column_combinations.append(_) for _ in combinations(range(n_samples), 2)]\n",
    "\n",
    "methods = {'L-FBGS-B':solve_profile_LFBGSB, 'SLSQP':solve_profile_SLSQP, 'trf':solve_profile_trf}\n",
    "results = []\n",
    "\n",
    "for run in range(20):\n",
    "    signal, sample, ground_truth = generate_dummy_data(n_sequences, n_samples)\n",
    "    ratios = get_protein_ratios(signal.values, column_combinations)\n",
    "\n",
    "    for method in methods.keys():\n",
    "        error = False\n",
    "        start = time()\n",
    "        try:\n",
    "            solution, success = methods[method](ratios)\n",
    "            mape = np.mean(np.abs((solution-ground_truth)/ground_truth))\n",
    "        except:\n",
    "            error = True\n",
    "            mape = np.nan\n",
    "            \n",
    "        \n",
    "        end = time()\n",
    "\n",
    "        time_elapsed_s = (end-start)\n",
    "\n",
    "        results.append((method, time_elapsed_s, mape, error, success))\n",
    "    \n",
    "result_df = pd.DataFrame(results, columns=['Method', 'Time Elapsed (s)', 'Mean absolute percentage error', 'Errors', 'Success'])\n",
    "\n",
    "result_df.groupby('Method').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SLSQP seems to be a robust and fast choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\alphapept\\lib\\site-packages\\scipy\\optimize\\optimize.py:282: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  warnings.warn(\"Values in x were outside bounds during a \"\n",
      "C:\\ProgramData\\Anaconda3\\envs\\alphapept\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:557: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from numba.typed import List\n",
    "\n",
    "n_sequences = 10\n",
    "n_samples = 6\n",
    "\n",
    "column_combinations = List()\n",
    "[column_combinations.append(_) for _ in combinations(range(n_samples), 2)]\n",
    "\n",
    "methods = {'L-FBGS-B':solve_profile_LFBGSB, 'SLSQP':solve_profile_SLSQP, 'trf':solve_profile_trf}\n",
    "results = []\n",
    "\n",
    "for run in range(20):\n",
    "    signal, sample, ground_truth = generate_dummy_data(n_sequences, n_samples)\n",
    "    ratios = get_protein_ratios(signal.values, column_combinations)\n",
    "\n",
    "    for method in methods.keys():\n",
    "        error = False\n",
    "        start = time()\n",
    "        try:\n",
    "            solution, success = methods[method](ratios)\n",
    "            mape = np.mean(np.abs((solution-ground_truth)/ground_truth))\n",
    "        except:\n",
    "            error = True\n",
    "            mape = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba.typed import List\n",
    "from itertools import combinations\n",
    "\n",
    "def get_protein_table(df, field = 'int_sum', minimum_ratios = 1, callback = None):\n",
    "    unique_proteins = df['protein'].unique()\n",
    "    files = df['shortname'].unique().tolist()\n",
    "    files.sort()\n",
    "    \n",
    "    if len(files) == 1:\n",
    "        raise ValueError('Only one file present.')\n",
    "    \n",
    "    column_combinations = List()\n",
    "    [column_combinations.append(_) for _ in combinations(range(len(files)), 2)]\n",
    "        \n",
    "    columnes_ext = [_+'_LFQ' for _ in files]\n",
    "    protein_table = pd.DataFrame(index=unique_proteins, columns=columnes_ext + files)\n",
    "\n",
    "    if field+'_dn' in df.columns:\n",
    "        field_ = field+'_dn'\n",
    "    else:\n",
    "        field_ = field\n",
    "    \n",
    "    if df[field_].min() < 0:\n",
    "        raise ValueError('Negative intensity values present.')\n",
    "        \n",
    "    for idx, protein in enumerate(unique_proteins):\n",
    "        subset = df[df['protein'] == protein].copy()\n",
    "        per_protein = subset.groupby(['shortname','precursor'])[field_].sum().unstack().T\n",
    "\n",
    "        for _ in files:\n",
    "            if _ not in per_protein.columns:\n",
    "                per_protein[_] = np.nan\n",
    "\n",
    "        per_protein = per_protein[files]\n",
    "        per_protein = per_protein.replace(0, np.nan)\n",
    "\n",
    "        ratios = get_protein_ratios(per_protein.values, column_combinations, minimum_ratios)\n",
    "        try:\n",
    "            solution, success = solve_profile_SLSQP(ratios)\n",
    "        except ValueError:\n",
    "            logging.info('Normalization with SLSQP failed. Trying BFGS')\n",
    "            solution, success = solve_profile_LFBGSB(ratios)\n",
    "            \n",
    "        file_ids = per_protein.columns.tolist()\n",
    "        \n",
    "        pre_lfq = per_protein.sum().values\n",
    "\n",
    "        if not success or np.sum(~np.isnan(ratios)) == 0: # or np.sum(solution) == len(pre_lfq):\n",
    "            profile = np.zeros_like(pre_lfq)\n",
    "            if np.sum(np.isnan(ratios)) != ratios.size:\n",
    "                logging.info(f'Solver failed for protein {protein} despite available ratios:\\n {ratios}')\n",
    "        else:\n",
    "            invalid = ((np.nansum(ratios, axis=1) == 0) & (np.nansum(ratios, axis=0) == 0))\n",
    "            total_int = subset[field_].sum() * solution \n",
    "            total_int[invalid] = 0\n",
    "            profile = total_int * subset[field_].sum().sum() / np.sum(total_int) #Normalize inensity again\n",
    "            \n",
    "\n",
    "        protein_table.loc[protein, [_+'_LFQ' for _ in file_ids]] = profile\n",
    "        protein_table.loc[protein, file_ids] = pre_lfq\n",
    "        \n",
    "        if callback:\n",
    "            callback((idx+1)/len(unique_proteins))\n",
    "        \n",
    "    protein_table[protein_table == 0] = np.nan\n",
    "    protein_table = protein_table.astype('float64')\n",
    "\n",
    "    return protein_table\n",
    "\n",
    "def protein_profile(files, minimum_ratios, chunk):\n",
    "    \n",
    "    grouped, protein = chunk\n",
    "\n",
    "    column_combinations = List()\n",
    "    [column_combinations.append(_) for _ in combinations(range(len(files)), 2)]\n",
    "    \n",
    "    selection = grouped.unstack().T.copy()\n",
    "    selection = selection.replace(0, np.nan)\n",
    "\n",
    "    if not selection.shape[1] == len(files):\n",
    "        selection[[_ for _ in files if _ not in selection.columns]] = np.nan\n",
    "\n",
    "    selection = selection[files]\n",
    "\n",
    "    ratios = get_protein_ratios(selection.values, column_combinations, minimum_ratios)\n",
    "    \n",
    "    try:\n",
    "        solution, success = solve_profile_SLSQP(ratios)\n",
    "    except ValueError:\n",
    "        logging.info('Normalization with SLSQP failed. Trying BFGS')\n",
    "        solution, success = solve_profile_LFBGSB(ratios)\n",
    "        \n",
    "    pre_lfq = selection.sum().values\n",
    "\n",
    "    if not success or np.sum(~np.isnan(ratios)) == 0: # or np.sum(solution) == len(pre_lfq):\n",
    "        profile = np.zeros_like(pre_lfq)\n",
    "        if np.sum(np.isnan(ratios)) != ratios.size:\n",
    "            logging.info(f'Solver failed for protein {protein} despite available ratios:\\n {ratios}')\n",
    "\n",
    "    else:\n",
    "        invalid = ((np.nansum(ratios, axis=1) == 0) & (np.nansum(ratios, axis=0) == 0))\n",
    "        total_int = pre_lfq.sum() * solution \n",
    "        total_int[invalid] = 0\n",
    "        profile = total_int * pre_lfq.sum() / np.sum(total_int) #Normalize inensity again\n",
    "        \n",
    "    \n",
    "    return profile, pre_lfq, protein  \n",
    "\n",
    "\n",
    "import os\n",
    "import alphapept.speed\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def protein_profile_parallel(df, minimum_ratios, field, callback=None):\n",
    "    \n",
    "    unique_proteins = df['protein'].unique().tolist()\n",
    "    \n",
    "    files = df['shortname'].unique().tolist()\n",
    "    files.sort()\n",
    "    \n",
    "    columnes_ext = [_+'_LFQ' for _ in files]\n",
    "    protein_table = pd.DataFrame(index=unique_proteins, columns=columnes_ext + files)\n",
    "\n",
    "    grouped = df[[field, 'shortname','precursor','protein']].groupby(['protein','shortname','precursor']).sum()\n",
    "\n",
    "    column_combinations = List()\n",
    "    [column_combinations.append(_) for _ in combinations(range(len(files)), 2)]\n",
    "    \n",
    "    files = df['shortname'].unique().tolist()\n",
    "    files.sort()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if len(files) > 1:\n",
    "        logging.info('Preparing protein table for parallel processing.')\n",
    "        split_df = []\n",
    "        \n",
    "        for idx, protein in enumerate(unique_proteins):\n",
    "            split_df.append((grouped.loc[protein], protein))\n",
    "            if callback:\n",
    "                callback((idx+1)/len(unique_proteins)*1/5)\n",
    "        \n",
    "        results = []\n",
    "\n",
    "        logging.info(f'Starting protein extraction for {len(split_df)} proteins.')\n",
    "        with alphapept.speed.AlphaPool(-1) as p:\n",
    "            max_ = len(split_df)\n",
    "            for i, _ in enumerate(p.imap_unordered(partial(protein_profile, files, minimum_ratios), split_df)):\n",
    "                results.append(_)\n",
    "                if callback:\n",
    "                    callback((i+1)/max_*4/5+1/5)\n",
    "\n",
    "        for result in results:\n",
    "            profile, pre_lfq, protein = result\n",
    "            protein_table.loc[protein, [_+'_LFQ' for _ in files]] = profile\n",
    "            protein_table.loc[protein, files] = pre_lfq\n",
    "\n",
    "        protein_table[protein_table == 0] = np.nan\n",
    "        protein_table = protein_table.astype('float')\n",
    "    else:\n",
    "        protein_table = df.groupby(['protein'])[field].sum().to_frame().reset_index()\n",
    "        protein_table = protein_table.set_index('protein')\n",
    "        protein_table.index.name = None\n",
    "        protein_table.columns=[files[0]] \n",
    "        \n",
    "        if callback:\n",
    "            callback(1)\n",
    "        \n",
    "    return protein_table\n",
    "\n",
    "\n",
    "def protein_profile_parallel_ap(settings, df, callback=None):\n",
    "    \n",
    "    minimum_ratios = settings['quantification']['lfq_minimum_ratio']\n",
    "    field = settings['quantification']['mode']\n",
    "    \n",
    "    if field+'_dn' in df.columns:\n",
    "        field_ = field+'_dn'\n",
    "    else:\n",
    "        field_ = field\n",
    "\n",
    "    if df[field_].min() < 0:\n",
    "        raise ValueError('Negative intensity values present.')\n",
    "        \n",
    "    protein_table = protein_profile_parallel(df, minimum_ratios, field_, callback)\n",
    "    \n",
    "    return protein_table\n",
    "    \n",
    "\n",
    "def protein_profile_parallel_mq(evidence_path, protein_groups_path, callback=None):\n",
    "    logging.info('Loading files')\n",
    "    \n",
    "    for file in [evidence_path, protein_groups_path]:\n",
    "        if not os.path.isfile(file):\n",
    "            raise FileNotFoundError(f'File {file} not found.')\n",
    "            \n",
    "    evd = pd.read_csv(evidence_path, sep='\\t') \n",
    "    ref = pd.read_csv(protein_groups_path, sep='\\t')\n",
    "            \n",
    "    experiments = evd['Raw file'].unique().tolist()\n",
    "    logging.info(f'A total of {len(experiments):,} files.')\n",
    "\n",
    "    protein_df = []\n",
    "    \n",
    "    max_ = len(ref)\n",
    "    for i in range(max_):\n",
    "        investigate = ref.iloc[i]\n",
    "        evd_ids = [int(_) for _ in investigate['Evidence IDs'].split(';')]\n",
    "        subset = evd.loc[evd_ids].copy()\n",
    "\n",
    "        subset['protein'] =  investigate['Protein IDs']\n",
    "        subset['shortname'] = subset['Raw file']\n",
    "        subset['precursor']  = ['_'.join(_) for _ in zip(subset['Sequence'].values, subset['Charge'].values.astype('str'))]\n",
    "\n",
    "        protein_df.append(subset)\n",
    "        \n",
    "        if callback:\n",
    "            callback((i+1)/len(ref))\n",
    "                     \n",
    "    logging.info(f'A total of {max_:,} proteins.')\n",
    "\n",
    "    df = pd.concat(protein_df)\n",
    "    df, normed = delayed_normalization(df, field ='Intensity')\n",
    "    protein_table = protein_profile_parallel(df, minimum_ratios=1, field='Intensity', callback=callback)\n",
    "                     \n",
    "    return protein_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    sample_data = {}\n",
    "\n",
    "    sample_data['precursor'] = ['Prec_1'] * 2 + ['Prec_2'] * 2 + ['Prec_3'] * 2\n",
    "    sample_data['shortname'] = ['A','B'] * 3\n",
    "    sample_data['protein'] = ['X'] * 6\n",
    "    sample_data['int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2]\n",
    "\n",
    "    test_df = pd.DataFrame(sample_data)\n",
    "\n",
    "    display(test_df.head(6))\n",
    "\n",
    "    protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "\n",
    "    profile, pre_lfq, file_ids, protein = protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "\n",
    "    print(profile)\n",
    "    print(pre_lfq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def test_protein_profile():\n",
    "\n",
    "        sample_data = {}\n",
    "\n",
    "        sample_data['precursor'] = ['Prec_1'] * 6 + ['Prec_2'] * 6 + ['Prec_3'] * 6\n",
    "        sample_data['fraction'] = [1,2,3]*6\n",
    "        sample_data['shortname'] = ['A','A','A', 'B','B','B'] * 3\n",
    "        sample_data['protein'] = ['X'] * 18\n",
    "        sample_data['int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2] * 3\n",
    "\n",
    "        test_df = pd.DataFrame(sample_data)\n",
    "\n",
    "        protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "\n",
    "        profile, pre_lfq, file_ids, protein = protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "\n",
    "        # total intensity should be preserved\n",
    "        assert np.allclose(profile.sum(), pre_lfq.sum())\n",
    "\n",
    "        sample_data = {}\n",
    "\n",
    "        sample_data['precursor'] = ['Prec_1'] * 2 + ['Prec_2'] * 2 + ['Prec_3'] * 2\n",
    "        sample_data['shortname'] = ['A','B'] * 3\n",
    "        sample_data['protein'] = ['X'] * 6\n",
    "        sample_data['int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2]\n",
    "\n",
    "        test_df = pd.DataFrame(sample_data)\n",
    "\n",
    "        protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "\n",
    "        profile, pre_lfq, file_ids, protein = protein_profile(test_df, ['A','B'], 'int_sum', 'X')\n",
    "\n",
    "        assert np.allclose(profile.sum(), pre_lfq.sum())\n",
    "\n",
    "    test_protein_profile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted 12_speed.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
