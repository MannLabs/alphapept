{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score\n",
    "\n",
    "> Functions related to the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all functions related to the scoring of peptide-spectrum-matches (PSMS).\n",
    "\n",
    "In brief, this notebook includes the following:\n",
    "\n",
    "- Functions for target-decoy based FDR estimation\n",
    "- X!tandem based scoring of PSMs\n",
    "- Machine learning based scoring of PSMs\n",
    "- Protein grouping by the razor approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "The filtering functions are essential base functions for scoring in AlphaPept. They make sure that only the 'best precursor per spectum' and the 'best spectrum per precursor' is used.\n",
    "\n",
    "Recall from the search that when having feautres, `raw_idx` refers to the actual index from the raw data. Otherwise it is`query_data`.\n",
    "\n",
    "For filtering, we have several functions. When applying for a score, we first use `filter_score` and then `filter_precursor`.\n",
    "`filter_score` is keeping the best score per experimental spectrum. First we rank by score for each `query_idx`. As we have multiple hits for each experimental spectrum from the search we only want to keep the best one.\n",
    "\n",
    "When performing feature finding, we assign multiple possible features to each experimental spectrum. The idea here is that a spectrum could originate from various precursors. To disentangle these psms we can use the following modes:\n",
    "\n",
    "* `single`: This mode will only keep one feature per experimental spectrum (the one with the highest score and the closest distance). Each feature can only occur once.\n",
    "* `multiple`: Allow multiple features per experimental spectrum. Each feature can only occur once.\n",
    "\n",
    "`filter_precursor` is intended for the case that a precursor (charge + sequence) occurs more than once. Only the one with the highest score will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import alphapept.io\n",
    "\n",
    "def filter_score(df: pd.DataFrame, mode: str='multiple') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter psms feature table by keeping only the best scoring psm per experimental spectrum.\n",
    "    \n",
    "    TODO: psms could still have the same score when having modifications at multiple positions that are not distinguishable.\n",
    "    Only keep one.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        mode (str, optional): string specifying which mode to use for psms filtering. The two options are 'single' and 'multiple'. 'single' will only keep one feature per experimental spectrum. 'multiple' will allow multiple features per experimental spectrum. In either option, each feature can only occur once. Defaults to 'multiple'.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: table containing the filtered psms results.\n",
    "    \"\"\"\n",
    "        \n",
    "    df[\"score_rank\"] = df.groupby(\"query_idx\")[\"score\"].rank(\"dense\", ascending=False).astype(\"int\")\n",
    "    df = df[df[\"score_rank\"] == 1]\n",
    "\n",
    "    # in case two hits have the same score and therfore the same score_rank only accept the first one\n",
    "    df = df.drop_duplicates(\"query_idx\")\n",
    "\n",
    "    if 'feature_dist' in df.columns:\n",
    "        df[\"feature_rank\"] = df.groupby(\"feature_idx\")[\"feature_dist\"].rank(\"dense\", ascending=True).astype(\"int\")\n",
    "        df[\"raw_rank\"] = df.groupby(\"raw_idx\")[\"score\"].rank(\"dense\", ascending=False).astype(\"int\")\n",
    "        \n",
    "        if mode == 'single':\n",
    "            df_filtered = df[(df[\"feature_rank\"] == 1) & (df[\"raw_rank\"] == 1) ]\n",
    "            df_filtered = df_filtered.drop_duplicates(\"raw_idx\")\n",
    "\n",
    "        elif mode == 'multiple':\n",
    "            df_filtered = df[(df[\"feature_rank\"] == 1)]\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Mode {} not implemented yet'.format(mode))\n",
    "\n",
    "    else:\n",
    "        df_filtered = df\n",
    "\n",
    "    # TOD: this needs to be sorted out, for modifications -> What if we have MoxM -> oxMM, this will screw up with the filter sequence part\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_filter_score():\n",
    "    ## DataFrame with unique assignments\n",
    "    df = pd.DataFrame({'query_idx':[1,1,2,2,3,3], 'score':[1,2,3,4,5,6],'feature_idx':[1,1,1,3,4,5],'raw_idx':[1,1,2,2,3,3]})\n",
    "    assert len(filter_score(df)) == 3\n",
    "\n",
    "    ## Spectra competing for a feature, only keep one\n",
    "    df = pd.DataFrame({'query_idx':[1,2], 'score':[1,2],'feature_idx':[1,1],'raw_idx':[1,2], 'feature_dist':[1,2]})\n",
    "    assert len(filter_score(df) == 1)\n",
    "\n",
    "    ## Same spectra multiple features\n",
    "    df = pd.DataFrame({'query_idx':[1,2], 'score':[1,2],'feature_idx':[1,2],'raw_idx':[1,1], 'feature_dist':[1,2]})\n",
    "    assert len(filter_score(df, mode='single')) == 1\n",
    "    assert len(filter_score(df, mode='multiple')) == 2\n",
    "\n",
    "test_filter_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def filter_precursor(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter psms feature table by precursor.\n",
    "    Allow each precursor only once.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "               \n",
    "    Returns:\n",
    "        pd.DataFrame: table containing the filtered psms results.\n",
    "\n",
    "    \"\"\"\n",
    "    df[\"precursor_rank\"] = (\n",
    "        df.groupby(\"precursor\")[\"score\"].rank(\"dense\", ascending=False).astype(\"int\")\n",
    "    )\n",
    "    \n",
    "    df_filtered = df[df[\"precursor_rank\"] == 1]\n",
    "    \n",
    "    if 'ms1_int_sum' in df_filtered.columns:\n",
    "        #if ms1_int_sum from feature finding is present: Remove duplicates in case there are any\n",
    "        df_filtered = df_filtered.sort_values('ms1_int_sum')[::-1]\n",
    "        df_filtered = df_filtered.drop_duplicates([\"precursor\", \"precursor_rank\"])\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def test_filter_precursor():\n",
    "    df = pd.DataFrame({'precursor':['A','A','A'],'score':[1,2,3]})\n",
    "    assert len(filter_precursor(df)) == 1\n",
    "    df = pd.DataFrame({'precursor':['A','A','B'],'score':[1,2,3]})\n",
    "    assert len(filter_precursor(df)) == 2\n",
    "    df = pd.DataFrame({'precursor':['A','B','C'],'score':[1,2,3]})\n",
    "    assert len(filter_precursor(df)) == 3\n",
    "\n",
    "test_filter_precursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_q_values` is used to calculate q-values from FDR values. The direct relationship is illustrated further down in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from numba import njit\n",
    "@njit\n",
    "def get_q_values(fdr_values: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate q-values from fdr_values.\n",
    "    \n",
    "    Args:\n",
    "        fdr_values (np.ndarray): np.ndarray of fdr values.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: np.ndarray of q-values.\n",
    "    \"\"\"\n",
    "    q_values = np.zeros_like(fdr_values)\n",
    "    min_q_value = np.max(fdr_values)\n",
    "    for i in range(len(fdr_values) - 1, -1, -1):\n",
    "        fdr = fdr_values[i]\n",
    "        if fdr < min_q_value:\n",
    "            min_q_value = fdr\n",
    "        q_values[i] = min_q_value\n",
    "\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_get_q_values():\n",
    "    assert np.allclose(get_q_values(np.array([1,2,3,4])), np.array([1,2,3,4]))\n",
    "    assert np.allclose(get_q_values(np.array([3,3,3,3])), np.array([3,3,3,3]))\n",
    "    assert np.allclose(get_q_values(np.array([4,3,2,1])), np.array([1,1,1,1]))\n",
    "    \n",
    "test_get_q_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR\n",
    "\n",
    "The employed FDR strategy is based on a classical target-decoy competition approach. The procedure works as follows:\n",
    "1. Consider only the best scoring target or decoy PSM per spectrum. \n",
    "2. Sort all PSMs by decreasing scores.\n",
    "3. Estimate the FDR as #decoys / #targets, where #targets (#decoys) is the number of positive target (decoy) PSMs at a given score threshold t (i.e. PSMs with scores higher than t).\n",
    "4. Convert the estimated FDR to q-values by selecting the minimum FDR at which the identification could be made, i.e. the lowest score threshold t that could be set to include an identification without increasing the number of false positives.\n",
    "5. Report the set of target PSMs with q-values smaller or equal to the selected `fdr_level`.\n",
    "\n",
    "Informative literature describing and discussing different FDR estimation approaches for shotgun proteomics can be found here (the implemented strategy in alphapept is referred to as T-TDC in this article):\n",
    "> Keich, Uri et al. \"Improved False Discovery Rate Estimation Procedure for Shotgun Proteomics.\" Journal of proteome research vol. 14,8 (2015): 3148-61. <https://pubs.acs.org/doi/10.1021/acs.jproteome.5b00081>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Note that the test function for cut_fdr is further down in the notebook to also test protein-level FDR.\n",
    "def cut_fdr(df: pd.DataFrame, fdr_level:float=0.01, plot:bool=True, cut:bool=True) -> (float, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Cuts a dataframe with a given fdr level\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01.\n",
    "        plot (bool, optional): flag to enable plot. Defaults to 'True'.\n",
    "        cut (bool, optional): flag to cut above fdr threshold. Defaults to 'True'.\n",
    "\n",
    "    Returns:\n",
    "        float: numerical value of the applied score cutoff\n",
    "        pd.DataFrame: df with psms within fdr\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"target\"] = ~df[\"decoy\"]\n",
    "\n",
    "    df = df.sort_values(by=[\"score\",\"decoy\"], ascending=False)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df[\"target_cum\"] = np.cumsum(df[\"target\"])\n",
    "    df[\"decoys_cum\"] = np.cumsum(df[\"decoy\"])\n",
    "\n",
    "    df[\"fdr\"] = df[\"decoys_cum\"] / df[\"target_cum\"]\n",
    "    df[\"q_value\"] = get_q_values(df[\"fdr\"].values)\n",
    "\n",
    "    last_q_value = df[\"q_value\"].iloc[-1]\n",
    "    first_q_value = df[\"q_value\"].iloc[0]\n",
    "\n",
    "    if last_q_value <= fdr_level:\n",
    "        logging.info('Last q_value {:.3f} of dataset is smaller than fdr_level {:.3f}'.format(last_q_value, fdr_level))\n",
    "        cutoff_index = len(df)-1\n",
    "\n",
    "    elif first_q_value >= fdr_level:\n",
    "        logging.info('First q_value {:.3f} of dataset is larger than fdr_level {:.3f}'.format(last_q_value, fdr_level))\n",
    "        cutoff_index = 0\n",
    "\n",
    "    else:\n",
    "        cutoff_index = df[df[\"q_value\"].gt(fdr_level)].index[0] - 1\n",
    "\n",
    "    cutoff_value = df.loc[cutoff_index][\"score\"]\n",
    "    \n",
    "    if cut:\n",
    "        cutoff = df[df[\"score\"] >= cutoff_value]\n",
    "    else:\n",
    "        cutoff= df\n",
    "\n",
    "    targets = df.loc[cutoff_index, \"target_cum\"]\n",
    "    decoy = df.loc[cutoff_index, \"decoys_cum\"]\n",
    "\n",
    "    fdr = df.loc[cutoff_index, \"fdr\"]\n",
    "\n",
    "    \n",
    "    logging.info(f\"{targets:,} target ({decoy:,} decoy) of {len(df):,} PSMs. FDR {fdr:.6f} for a cutoff of {cutoff_value:.2f} (set FDR was {fdr_level}).\")\n",
    "\n",
    "    if plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(df[\"score\"], df[\"fdr\"])\n",
    "        plt.axhline(0.01, color=\"k\", linestyle=\"--\")\n",
    "\n",
    "        plt.axvline(cutoff_value, color=\"r\", linestyle=\"--\")\n",
    "        plt.title(\"fdr vs Cutoff value\")\n",
    "        plt.xlabel(\"Score\")\n",
    "        plt.ylabel(\"fdr\")\n",
    "        # plt.savefig('fdr.png')\n",
    "        plt.show()\n",
    "\n",
    "        bins = np.linspace(np.min(df[\"score\"]), np.max(df[\"score\"]), 100)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(df[df[\"decoy\"]][\"score\"].values, label=\"decoy\", bins=bins, alpha=0.5)\n",
    "        plt.hist(df[~df[\"decoy\"]][\"score\"].values, label=\"target\", bins=bins, alpha=0.5)\n",
    "        plt.xlabel(\"Score\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Score vs Class\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    cutoff = cutoff.reset_index(drop=True)\n",
    "    return cutoff_value, cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the test function for cut_fdr is further down in the notebook to also test protein-level FDR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Helper functions to create in-silico data.\n",
    "def simulateTargetDecoyScores(n, target_mu=4.0, stdev=1.0, pi0=0.5):\n",
    "    decoys = np.random.normal(loc=0.0, scale=stdev, size=n)\n",
    "    false_targets = np.random.normal(loc=0.0, scale=stdev, size=int(np.round(n*pi0)))\n",
    "    true_targets = np.random.normal(loc=target_mu, scale=stdev, size=int(np.round(n*(1-pi0))))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'TD':np.append(np.append(np.repeat('TT',len(true_targets)),np.repeat('FT',len(false_targets))),np.repeat('D',len(decoys))),\n",
    "        'decoy':np.append(np.repeat(False,len(true_targets)+len(false_targets)), np.repeat(True,len(decoys))),\n",
    "        'score':np.append(np.append(true_targets,false_targets),decoys),\n",
    "        'sequence':np.append(np.arange(0,n),np.arange(0,n)),\n",
    "        'protein':np.append(np.arange(0,n),np.arange(0,n))})\n",
    "    \n",
    "    return df\n",
    "\n",
    "def simulateProteinLevelTargetDecoyScores(n, target_mu=4.0, stdev=1.0, pi0=0.5, plot=True):\n",
    "    idx = np.arange(0,n)\n",
    "\n",
    "    protein_size = np.random.poisson(lam=3.0, size=n) + 1\n",
    "    \n",
    "    if plot:\n",
    "        plt.hist(protein_size)\n",
    "        plt.title(\"Number of peptides per protein\")\n",
    "        plt.xlabel(\"Number of peptides per protein\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.show()\n",
    "\n",
    "    TT_protein_size = protein_size[idx[0:int(np.round(1-(n*pi0)))]]\n",
    "    FT_protein_size = protein_size[idx[int(np.round(1-(n*pi0))):n]]\n",
    "    D_protein_size = protein_size\n",
    "\n",
    "    true_targets = np.random.normal(loc=target_mu, scale=stdev, size=sum(TT_protein_size))\n",
    "    false_targets = np.random.normal(loc=0.0, scale=stdev, size=sum(FT_protein_size))\n",
    "    decoys = np.random.normal(loc=0.0, scale=stdev, size=sum(D_protein_size))\n",
    "\n",
    "    D_proteins = np.repeat(idx,D_protein_size)\n",
    "    TT_proteins = np.repeat(idx[0:int(np.round(1-(n*pi0)))],TT_protein_size)\n",
    "    FT_proteins = np.repeat(idx[int(np.round(1-(n*pi0))):n],FT_protein_size)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'TD':np.append(np.append(np.repeat('TT',len(TT_proteins)),np.repeat('FT',len(FT_proteins))),np.repeat('D',len(D_proteins))),\n",
    "        'decoy':np.append(np.repeat(False,len(TT_proteins)+len(FT_proteins)), np.repeat(True,len(D_proteins))),\n",
    "        'score':np.append(np.append(true_targets,false_targets),decoys),\n",
    "        'sequence':np.append(np.arange(0,sum(protein_size)),np.arange(0,sum(protein_size))),\n",
    "        'protein':np.append(np.append(TT_proteins,FT_proteins),D_proteins)})\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_score_hist(df, analyte_level='sequence'):\n",
    "    \n",
    "    if analyte_level=='protein':\n",
    "        df = df.sort_values(by=['protein','score'], ascending=False)\n",
    "        df = df.drop_duplicates(subset='protein', keep=\"first\")\n",
    "    \n",
    "    decoys=df[df.decoy].score.values\n",
    "    false_targets= df[df.TD == 'FT'].score.values\n",
    "    true_targets= df[df.TD == 'TT'].score.values\n",
    "    \n",
    "    minS = int(np.round(np.min(np.append(decoys, np.append(false_targets, true_targets)))))\n",
    "    maxS = int(np.round(np.max(np.append(decoys, np.append(false_targets, true_targets)))))\n",
    "    \n",
    "    plt.hist(false_targets, rwidth=.4, bins=range(minS,maxS), range=[minS,maxS], align='mid', label='false targets')\n",
    "    plt.hist(true_targets, rwidth=.4, bins=range(minS,maxS), range=[minS,maxS], align='mid', label='true targets')\n",
    "    plt.hist(decoys, rwidth=.4, bins=range(minS,maxS), range=[minS,maxS], align='left', label='decoys')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"score histogram\")\n",
    "    plt.xlabel(\"score\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.xlim(-5,10)\n",
    "    plt.show()\n",
    "\n",
    "def score_TDcompetition(df):\n",
    "    td_dataframe = pd.DataFrame({'T':df[~df.decoy].score.values, 'D':df[df.decoy].score.values, 'label':df[~df.decoy].TD.values})\n",
    "    td_dataframe['win'] = td_dataframe.apply(lambda x: 'T' if x['T'] > x['D'] else 'D', axis = 1)\n",
    "\n",
    "    target_in = np.where(td_dataframe.win=='T')\n",
    "    decoy_in = np.where(td_dataframe.win=='D')\n",
    "\n",
    "    T_df = df[(~df.decoy) & (np.isin(df.sequence, target_in))]\n",
    "    D_df = df[(df.decoy) & (np.isin(df.sequence, decoy_in))]\n",
    "    \n",
    "    df = T_df.append(D_df)\n",
    "\n",
    "    df = pd.concat([T_df, D_df], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_simulated_stat_rates(df, TDcompetition = False, analyte_level='sequence', df_ini = None):\n",
    "    alpha = np.arange(0.002,1,0.002)\n",
    "    stat_rates = pd.DataFrame(columns=['alpha','TP','FP','TN','FN','TPR','FPR','FDR','FNR','ACC'])\n",
    "\n",
    "    if analyte_level=='protein':\n",
    "        df = df.drop_duplicates(subset='protein', keep=\"first\")\n",
    "        \n",
    "    for idx in range(len(alpha)):\n",
    "        sig = df[df.q_value <= alpha[idx]]\n",
    "        not_sig = df[df.q_value > alpha[idx]]\n",
    "\n",
    "        TP = len(sig[sig.TD == 'TT'][analyte_level].unique())\n",
    "        FP = len(sig[sig.TD == 'FT'][analyte_level].unique())\n",
    "        TN = len(not_sig[not_sig.TD == 'FT'][analyte_level].unique())\n",
    "        FN = len(not_sig[not_sig.TD == 'TT'][analyte_level].unique())\n",
    "        \n",
    "        if TDcompetition:\n",
    "            TN = TN + (len(df_ini[df_ini.TD == 'FT'][analyte_level].unique()) - len(df[df.TD == 'FT'][analyte_level].unique()))\n",
    "            FN = FN + (len(df_ini[df_ini.TD == 'TT'][analyte_level].unique()) - len(df[df.TD == 'TT'][analyte_level].unique()))\n",
    "\n",
    "\n",
    "        TPR = TP/(TP+FN) \n",
    "        FPR = FP/(FP+TN) \n",
    "        if (FP+TP)==0:\n",
    "            FDR = 0\n",
    "        else:\n",
    "            FDR = FP/(FP+TP)\n",
    "        FNR = FN/(FN+TP)\n",
    "\n",
    "        ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "        stat_rates.loc[idx] = [alpha[idx], TP, FP, TN, FN, TPR, FPR, FDR, FNR, ACC] \n",
    "    \n",
    "    border = 0.1\n",
    "    \n",
    "    plt.plot([-1,2], [-1,2], linestyle=\"--\", color='red')\n",
    "    plt.scatter(stat_rates.alpha, stat_rates.FDR)\n",
    "    plt.ylim(0-border,1+border)\n",
    "    plt.xlim(0-border,1+border)\n",
    "    plt.title(\"decoy vs. true FDR\")\n",
    "    plt.xlabel(\"decoy FDR\")\n",
    "    plt.ylabel(\"true FDR\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot([-1,1], [-1,1], linestyle=\"--\", color='red')\n",
    "    plt.scatter(stat_rates.alpha, stat_rates.FDR)\n",
    "    plt.ylim(0-(0.01),0.1+(0.01))\n",
    "    plt.xlim(0-(0.01),0.1+(0.01))\n",
    "    plt.title(\"decoy vs. true FDR (zoom)\")\n",
    "    plt.xlabel(\"decoy FDR\")\n",
    "    plt.ylabel(\"true FDR\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot([-1,2], [1,1], linestyle=\"--\", color='red')\n",
    "    plt.scatter(stat_rates.FPR, stat_rates.TPR)\n",
    "    plt.ylim(0-border,1+border)\n",
    "    plt.xlim(0-border,1+border)\n",
    "    plt.title(\"ROC curve\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot([-1,2], [1,1], linestyle=\"--\", color='red')\n",
    "    plt.scatter(stat_rates.FPR, stat_rates.TPR)\n",
    "    plt.ylim(0-border,1+border)\n",
    "    plt.xlim(0-0.01,0.1+0.01)\n",
    "    plt.title(\"ROC curve (zoom)\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.show()\n",
    "    \n",
    "    return stat_rates\n",
    "\n",
    "def plot_qvalue_vs_fdr(df):\n",
    "    plt.plot(df.fdr, df.target_cum, label='FDR')\n",
    "    plt.plot(df.q_value, df.target_cum, label='q-value')\n",
    "    plt.xlim(0-0.0001,0.005)\n",
    "    plt.ylim(0-100,7000)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"Difference between q-value and FDR\")\n",
    "    plt.xlabel(\"q-value / FDR\")\n",
    "    plt.ylabel(\"Cummulative number of accepted targets\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "TD = simulateTargetDecoyScores(n=50000, pi0=0.8, target_mu=3.5)\n",
    "TDC = score_TDcompetition(TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation of random scores for 50'000 measurements (corresponding to spectra). Simulated are decoys, true targets and false targets. We assume a false traget raio (pi0) of 0.8 and a mean score difference of 3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated score distribution for a separate target and decoy database search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "plot_score_hist(TD, analyte_level='sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated score distribution for a corresponding concatinated target-decoy database search with target-decoy-competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "plot_score_hist(TDC, analyte_level='sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of the `cut_fdr` function to the simulated target-decoy competition dataset saved in `TDC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cval, cut_TDC = cut_fdr(TDC, fdr_level=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the FDR estimated by the target-decoy approach versus the true FDR confirms accurate FDR estimation by our approach. The true FDR is capped by the selected fraction of false targets (pi0 = 0.8) and by the effect of target decoy competition. Similarly, the true positive rate (TPR) is limited by the effect of target decoy competition and can only reach 1 in cases where not a single decoy scores higher than a true target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cval_, cut_TDC_ = cut_fdr(TDC, fdr_level=100, plot=False)\n",
    "stat = get_simulated_stat_rates(cut_TDC_, TDcompetition = True, analyte_level='sequence', df_ini = TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure illustrates the difference between `fdr` and `q_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "plot_qvalue_vs_fdr(cut_TDC_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be aware that the shown simulations are not an accurate model for PSMS scoring and they were designed only for illustrative purposes and to test the implemeted functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global FDR\n",
    "\n",
    "The `cut_global_fdr` function has two specific applications:\n",
    "1. **Estimate q-values on the peptide and protein level** <br/>\n",
    "The concept here is based on selecting the best scoring precursor per peptide (or protein) to then estimate the FDR by target-decoy competition using the `cut_fdr` function.\n",
    "2. **Estimate q-values across an entire dataset on either precursor, peptide or protein level** <br/>\n",
    "The concept here is based on selecting the best scoring precursor, peptide or protein signal across an entire dataset to then estimate the FDR by target-decoy competition using the `cut_fdr` function.\n",
    "\n",
    "This strategy was extensively tested and discussed in the following publications:\n",
    "\n",
    "* Nesvizhskii, Alexey I. \"A survey of computational methods and error rate estimation procedures for peptide and protein identification in shotgun proteomics.\" Journal of proteomics vol. 73,11 (2010): 2092-123. <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2956504/> \n",
    "\n",
    "* Savitski, Mikhail M et al. \"A Scalable Approach for Protein False Discovery Rate Estimation in Large Proteomic Data Sets.\" Molecular & cellular proteomics : MCP vol. 14,9 (2015): 2394-404. <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4563723/>\n",
    "\n",
    "* The, Matthew et al. \"Fast and Accurate Protein False Discovery Rates on Large-Scale Proteomics Data Sets with Percolator 3.0.\" Journal of the American Society for Mass Spectrometry vol. 27,11 (2016): 1719-1727. <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5059416/>\n",
    "\n",
    "* Gupta, Nitin, and Pavel A Pevzner. \"False discovery rates of protein identifications: a strike against the two-peptide rule.\" Journal of proteome research vol. 8,9 (2009): 4173-81. \n",
    "<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3398614/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def cut_global_fdr(data: pd.DataFrame, analyte_level: str='sequence', fdr_level: float=0.01, plot: bool=True, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to estimate and filter by global peptide or protein fdr\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): psms table of search results from alphapept.\n",
    "        analyte_level (str, optional): string specifying the analyte level to apply the fdr threshold. Options include: 'precursor', 'sequence', 'protein_group' and 'protein'. Defaults to 'sequence'.\n",
    "        fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01.\n",
    "        plot (bool, optional): flag to enable plot. Defaults to 'True'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with filtered results\n",
    "\n",
    "    \"\"\"\n",
    "    logging.info('Global FDR on {}'.format(analyte_level))\n",
    "    data_sub = data[[analyte_level,'score','decoy']]\n",
    "    data_sub_unique = data_sub.groupby([analyte_level,'decoy'], as_index=False).agg({\"score\": \"max\"})\n",
    "\n",
    "    analyte_levels = ['precursor', 'sequence', 'protein_group','protein']\n",
    "\n",
    "    if analyte_level in analyte_levels:\n",
    "        agg_score = data_sub_unique.groupby([analyte_level,'decoy'])['score'].max().reset_index()\n",
    "    else:\n",
    "        raise Exception('analyte_level should be either sequence or protein. The selected analyte_level was: {}'.format(analyte_level))\n",
    "\n",
    "    agg_cval, agg_cutoff = cut_fdr(agg_score, fdr_level=fdr_level, plot=plot)\n",
    "    \n",
    "    logging.info(f'Global FDR cutoff at {agg_cval:.3f}.')\n",
    "\n",
    "    agg_report = data.reset_index().merge(\n",
    "                        agg_cutoff,\n",
    "                        how = 'inner',\n",
    "                        on = [analyte_level,'decoy'],\n",
    "                        suffixes=('', '_'+analyte_level),\n",
    "                        validate=\"many_to_one\").set_index('index') #retain the original index\n",
    "    return agg_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the sequence level simulations we can simulatae score distributions for peptides beloning to proteins. In our simulation we assumed a poisson distribution for the number of peptides for each protein centered at 4 peptides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "TD_prot = simulateProteinLevelTargetDecoyScores(n=8000, pi0=0.8, target_mu=3.5)\n",
    "\n",
    "TDC_prot = score_TDcompetition(TD_prot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of the `cut_global_fdr` function to the simulated protein-level target-decoy competition dataset saved in `TDC_prot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_TDC_prot = cut_global_fdr(TDC_prot, fdr_level=0.01, analyte_level='protein')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the protein-level FDR estimated by the target-decoy approach versus the true FDR confirms accurate FDR estimation by our approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cut_TDC_prot_ = cut_global_fdr(TDC_prot, fdr_level=100, analyte_level='protein', plot=False)\n",
    "stat_prot = get_simulated_stat_rates(cut_TDC_prot_, TDcompetition = True, analyte_level='protein', df_ini = TD_prot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the peptide-level statistics after protein-level FDR filtering shows a conservative pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "stat_prot = get_simulated_stat_rates(cut_TDC_prot_, TDcompetition = True, analyte_level='sequence', df_ini = TD_prot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be aware that the shown simulations are not an accurate model for PSMS scoring and they were designed only for illustrative purposes and to test the implemeted functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X!tandem scoring\n",
    "\n",
    "* `get_x_tandem_score` performs scoring of PSMs according to the X!tandem strategy:\n",
    "\n",
    "* `score_x_tandem` first calls `get_x_tandem_score` and and subsequently applies the `cut_fdr` function to filter PSMs at the specified `fdr_level`.\n",
    "\n",
    "> X!Tandem, Craig,R. and Beavis,R.C. (2003) Rapid Commun. Mass Spectrom., 17, 2310-2316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "def get_x_tandem_score(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to calculate the x tandem score\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: np.ndarray with x_tandem scores\n",
    "\n",
    "    \"\"\"\n",
    "    b = df['hits_b'].astype('int').apply(lambda x: np.math.factorial(x)).values\n",
    "    y = df['hits_y'].astype('int').apply(lambda x: np.math.factorial(x)).values\n",
    "    x_tandem = np.log(b.astype('float')*y.astype('float')*df['fragments_matched_int_sum'].values)\n",
    "\n",
    "    x_tandem[x_tandem==-np.inf] = 0\n",
    "\n",
    "    return x_tandem\n",
    "\n",
    "def score_x_tandem(df: pd.DataFrame, fdr_level: float = 0.01, plot: bool = True, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the psms table by using the x_tandem score and filtering the results for fdr_level.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: psms table with an extra 'score' column for x_tandem, filtered for no feature or precursor to be assigned multiple times.\n",
    "    \"\"\"\n",
    "    logging.info('Scoring using X-Tandem')\n",
    "\n",
    "    df['score'] = get_x_tandem_score(df)\n",
    "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "\n",
    "    df = filter_score(df)\n",
    "    df = filter_precursor(df)\n",
    "    cval, cutoff = cut_fdr(df, fdr_level, plot)\n",
    "\n",
    "    return cutoff\n",
    "\n",
    "\n",
    "def get_generic_score(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to calculate the a generic score\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: np.ndarray with x_tandem scores\n",
    "\n",
    "    \"\"\"\n",
    "    score = df['hits'] * df['hits']/(2*df['n_frags_db']) + df['hits'] * df['fragments_matched_int_ratio'] + df['hits_y']\n",
    "    \n",
    "    return score.values\n",
    "\n",
    "\n",
    "def score_generic(df: pd.DataFrame, fdr_level: float = 0.01, plot: bool = True, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the psms table by using a generic score and filtering the results for fdr_level.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: psms table with an extra 'score' column for the generic score, filtered for no feature or precursor to be assigned multiple times.\n",
    "    \"\"\"\n",
    "    logging.info('Scoring using generic score')\n",
    "\n",
    "    df['score'] = get_generic_score(df)\n",
    "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "\n",
    "    df = filter_score(df)\n",
    "    df = filter_precursor(df)\n",
    "    cval, cutoff = cut_fdr(df, fdr_level, plot)\n",
    "\n",
    "    return cutoff\n",
    "\n",
    "  \n",
    "\n",
    "def filter_with_x_tandem(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the psms table by using the x_tandem score, no fdr filter.\n",
    "    TODO: Remove redundancy with score functions, see issue: #275\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: psms table with an extra 'score' column for x_tandem, filtered for no feature or precursor to be assigned multiple times.\n",
    "    \"\"\"\n",
    "    logging.info('Filter df with x_tandem score')\n",
    "\n",
    "    df['score'] = get_x_tandem_score(df)\n",
    "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "\n",
    "    df = filter_score(df)\n",
    "    df = filter_precursor(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_with_score(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Filters the psms table by using the score column, no fdr filter.\n",
    "    TODO: Remove redundancy with score functions, see issue: #275\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: psms table filtered for no feature or precursor to be assigned multiple times.\n",
    "    \"\"\"\n",
    "    logging.info('Filter df with custom score')\n",
    "\n",
    "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "\n",
    "    df = filter_score(df)\n",
    "    df = filter_precursor(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def test_get_x_tandem_score():\n",
    "    hits_y = np.array([1,2,3,0])\n",
    "    hits_b = np.array([0,1,2,1])\n",
    "    fragments_matched_int_sum = np.array([1000,1000,1000,1000])\n",
    "    df = pd.DataFrame({'hits_y':hits_y,'hits_b':hits_b,'fragments_matched_int_sum':fragments_matched_int_sum})\n",
    "    np.testing.assert_almost_equal(get_x_tandem_score(df), np.array([6.90775528, 7.60090246, 9.39266193, 6.90775528]))\n",
    "\n",
    "test_get_x_tandem_score()\n",
    "\n",
    "def test_score_x_tandem():\n",
    "    hits_y = np.array([1,2,3,0])\n",
    "    hits_b = np.array([0,1,2,1])\n",
    "    fragments_matched_int_sum = np.array([1000,1000,1000,1000])\n",
    "    sequence = np.array(['A','A','B','C_decoy'])\n",
    "    precursor = np.array(['A1','A1','B','C_decoy'])\n",
    "    query_idx = np.array([1,2,3,4])\n",
    "    df = pd.DataFrame({'hits_y':hits_y,'hits_b':hits_b,'fragments_matched_int_sum':fragments_matched_int_sum,\n",
    "                      'sequence':sequence,'precursor':precursor,'query_idx':query_idx})\n",
    "    res = score_x_tandem(df, fdr_level=1, plot=False)\n",
    "    assert all(res.precursor == ['B','A1','C_decoy'])\n",
    "    assert all(res.q_value == [0,0,0.5])\n",
    "\n",
    "test_score_x_tandem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score and filter PSMs by any specified score\n",
    "\n",
    "`score_psms` uses the specified `score` and applies the `cut_fdr` function to filter PSMs at the specified `fdr_level`. `filter_score` and `filter_precursor` are applied to only report the best PSM per acquired spectrum and the best signal per precursor (i.e. sequence + charge combination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def score_psms(df: pd.DataFrame, score: str='hits_y', fdr_level: float=0.01, plot: bool=True, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Uses the specified score in df to filter psms and to apply the fdr_level threshold.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        score (str, optional): string specifying the column in df to use as score. Defaults to 'hits_y'.\n",
    "        fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01.\n",
    "        plot (bool, optional): flag to enable plot. Defaults to 'True'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: filtered df with psms within fdr\n",
    "\n",
    "    \"\"\"\n",
    "    if score in df.columns:\n",
    "        df['score'] = df[score]\n",
    "    else:\n",
    "        raise ValueError(\"The specified 'score' {} is not available in 'df'.\".format(score))\n",
    "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "\n",
    "    df = filter_score(df)\n",
    "    df = filter_precursor(df)\n",
    "    cval, cutoff = cut_fdr(df, fdr_level, plot)\n",
    "\n",
    "    return cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def test_score_psms():\n",
    "    hits_y = np.array([1,2,3,0])\n",
    "    hits_b = np.array([0,1,2,1])\n",
    "    fragments_matched_int_sum = np.array([1000,1000,1000,1000])\n",
    "    sequence = np.array(['A','A','B','C_decoy'])\n",
    "    precursor = np.array(['A1','A1','B','C_decoy'])\n",
    "    query_idx = np.array([1,2,3,4])\n",
    "    df = pd.DataFrame({'hits_y':hits_y,'hits_b':hits_b,'fragments_matched_int_sum':fragments_matched_int_sum,\n",
    "                      'sequence':sequence,'precursor':precursor,'query_idx':query_idx})\n",
    "    \n",
    "    res = score_psms(df, fdr_level=1, plot=False)\n",
    "    assert all(res.precursor == ['B','A1','C_decoy'])\n",
    "    assert all(res.q_value == [0,0,0.5])\n",
    "    \n",
    "    res = score_psms(df, score='hits_b', fdr_level=1, plot=False)\n",
    "    assert all(res.precursor == ['B','C_decoy','A1'])\n",
    "    assert all(res.q_value == [0,0.5,0.5])\n",
    "\n",
    "test_score_x_tandem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning based scoring of PSMs\n",
    "\n",
    "* `get_ML_features` extracts additional scoring metrics for the machine learning, including the number of amino acids per precursor, the number of missed cleavages and the logarithmic number of times the same peptide occurs in the set of PSMs\n",
    "\n",
    "* `train_RF` trains a random forest classifier for scoring all PSMs. For this, we use the scikit-learn library.\n",
    "    * First, a machine learning pipeline is created including the sklearn `StandardScaler` and `RandomForestClassifier`. The `StandardScaler` is used to standardize all features by removing the mean and scaling to unit variance. For details on the `RandomForestClassifier` see: <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>.\n",
    "    * Next, a grid search is initialized for testing the hyperparameter space (`max_depth` and `max_leaf_nodes`) of the random forest classifier by a 5-fold cross-validation using `GridSearchCV`.\n",
    "    * To train the classifier, we first select a suitable set of PSMSs. This is achieved by an initial scoring and FDR estimation of the input PSMs based on the `ini_score`. Only targets below the `train_fdr_level` cutoff are considered for training the classifier. To ensure a balanced dataset for training (i.e. same number of targets and decoys), the number of PSMs per category is selected to be the minimum of either the number of high scoring targets below the `train_fdr_level` cutoff or the overall number of decoys among the PSMs. `min_train` specifies the minimum number of targets and decoys that should be available. \n",
    "    * Once a balanced set of PSMs is established, the PSMs are split into a training and test set accoring to the `test_size` argument using `train_test_split` from sklearn while ensuring the PSMs are split in a stratified fashion (i.e. equal number of targets and decoys in both the training and test sets).\n",
    "    * The grid search and training of the random forest classifier is performed on the training set of PSMs. The `GridSearchCV` returns the classifier which performed best across all cross-validation hold-out sets according to the `scoring` function (classification *'accuracy'* is set as default score). The grid search is parallelize dinto `n_jobs`.\n",
    "    * Next, the trained classifier is applied to the testing set of PSMs and the test score is reported. \n",
    "    * If `plot` is enabled, a figure illustrating the weights of each feature is produced.\n",
    "    * Finally the function returns the trained random forest classifier for subsequent application to the entire set of PSMs or for transfering to a different dataset. \n",
    "\n",
    "* `score_ML` applies a classifier trained by `train_RF` to a complete set of PSMs. It calls the `cut_fdr` function and filters for the specified `fdr_level`. `filter_score` and `filter_precursor` are applied to only report the best PSM per acquired spectrum and the best signal per precursor (i.e. sequence + charge combination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from alphapept.fasta import count_missed_cleavages, count_internal_cleavages\n",
    "\n",
    "\n",
    "def get_ML_features(df: pd.DataFrame, protease: str='trypsin', **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Uses the specified score in df to filter psms and to apply the fdr_level threshold.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        protease (str, optional): string specifying the protease that was used for proteolytic digestion. Defaults to 'trypsin'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df including additional scores for subsequent ML. \n",
    "\n",
    "    \"\"\"\n",
    "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "\n",
    "    df['delta_m_ppm_abs'] = np.abs(df['delta_m_ppm'])\n",
    "    df['sequence_naked'] = df['sequence'].apply(lambda x: ''.join([_ for _ in x if _.isupper()]))\n",
    "    df['n_AA']= df['sequence_naked'].str.len()\n",
    "    df['fragments_matched_n_ratio'] = df['hits']/(2*df['n_AA'])\n",
    "\n",
    "    df['n_missed'] = df['sequence_naked'].apply(lambda x: count_missed_cleavages(x, protease))\n",
    "    df['n_internal'] = df['sequence_naked'].apply(lambda x: count_internal_cleavages(x, protease))\n",
    "    \n",
    "    df['x_tandem'] = get_x_tandem_score(df)\n",
    "    df['generic_score'] = get_generic_score(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_RF(df: pd.DataFrame,\n",
    "             exclude_features: list = ['precursor_idx', 'fragment_ion_idx', 'fasta_index', 'feature_rank', 'raw_rank',\n",
    "                                       'score_rank', 'db_idx', 'feature_idx', 'precursor', 'query_idx', 'raw_idx',\n",
    "                                       'sequence', 'decoy', 'sequence_naked', 'target'],\n",
    "             train_fdr_level: float = 0.1,\n",
    "             ini_score: str = 'hits',\n",
    "             min_train: int = 1000,\n",
    "             test_size: float = 0.8,\n",
    "             max_depth: list = [5, 25, 50],\n",
    "             max_leaf_nodes: list = [150, 200, 250],\n",
    "             n_jobs: int = -1,\n",
    "             scoring: str = 'accuracy',\n",
    "             plot: bool = False,\n",
    "             random_state: int = 42,\n",
    "             **kwargs) -> (GridSearchCV, list):\n",
    "    \"\"\"\n",
    "    Function to train a random forest classifier to separate targets from decoys via semi-supervised learning.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        exclude_features (list, optional): list with column names of features to exclude for ML. Defaults to ['precursor_idx','fragment_ion_idx','fasta_index','feature_rank','raw_rank','score_rank','db_idx', 'feature_idx', 'precursor', 'query_idx', 'raw_idx','sequence','decoy','sequence_naked','target'].\n",
    "        train_fdr_level (float, optional): Only targets below the train_fdr_level cutoff are considered for training the classifier. Defaults to 0.1.\n",
    "        ini_score (str, optional): Initial score to select psms set for semi-supervised learning. Defaults to 'x_tandem'.\n",
    "        min_train (int, optional): Minimum number of psms in the training set. Defaults to 1000.\n",
    "        test_size (float, optional): Fraction of psms used for testing. Defaults to 0.8.\n",
    "        max_depth (list, optional): List of clf__max_depth parameters to test in the grid search. Defaults to [5,25,50].\n",
    "        max_leaf_nodes (list, optional): List of clf__max_leaf_nodes parameters to test in the grid search. Defaults to [150,200,250].\n",
    "        n_jobs (int, optional): Number of jobs to use for parallelizing the gridsearch. Defaults to -1, which in GridSearchCV corresponds to 'use all available cores'.\n",
    "        scoring (str, optional): Scoring method for the gridsearch. Defaults to 'accuracy'.\n",
    "        plot (bool, optional): flag to enable plot. Defaults to 'False'.\n",
    "        random_state (int, optional): Random state for initializing the RandomForestClassifier. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        [GridSearchCV, list]: GridSearchCV: GridSearchCV object with trained RandomForestClassifier. list: list of features used for training the classifier.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    features = [col for col in df.columns if col not in exclude_features]\n",
    "\n",
    "    # Setup ML pipeline\n",
    "    scaler = StandardScaler()\n",
    "    rfc = RandomForestClassifier(random_state=random_state)  # class_weight={False:1,True:5},\n",
    "    ## Initiate scaling + classification pipeline\n",
    "    pipeline = Pipeline([('scaler', scaler), ('clf', rfc)])\n",
    "    parameters = {'clf__max_depth': max_depth, 'clf__max_leaf_nodes': max_leaf_nodes}\n",
    "    ## Setup grid search framework for parameter selection and internal cross validation\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters, cv=5, scoring=scoring,\n",
    "                      verbose=0, return_train_score=True, n_jobs=_get_limited_n_jobs(n_jobs))\n",
    "\n",
    "    # Prepare target and decoy df\n",
    "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "    df['target'] = ~df['decoy']\n",
    "    df['score'] = df[ini_score]\n",
    "    dfT = df[~df.decoy]\n",
    "    dfD = df[df.decoy]\n",
    "\n",
    "    # Select high scoring targets (<= train_fdr_level)\n",
    "    df_prescore = filter_score(df)\n",
    "    df_prescore = filter_precursor(df_prescore)\n",
    "    scored = cut_fdr(df_prescore, fdr_level=train_fdr_level, plot=False)[1]\n",
    "    highT = scored[scored.decoy == False]\n",
    "    dfT_high = dfT[dfT['query_idx'].isin(highT.query_idx)]\n",
    "    dfT_high = dfT_high[dfT_high['db_idx'].isin(highT.db_idx)]\n",
    "\n",
    "    # Determine the number of psms for semi-supervised learning\n",
    "    n_train = int(dfT_high.shape[0])\n",
    "    if dfD.shape[0] < n_train:\n",
    "        n_train = int(dfD.shape[0])\n",
    "        logging.info(\"The total number of available decoys is lower than the initial set of high scoring targets.\")\n",
    "    if n_train < min_train:\n",
    "        raise ValueError(\"There are fewer high scoring targets or decoys than required by 'min_train'.\")\n",
    "\n",
    "    # Subset the targets and decoys datasets to result in a balanced dataset\n",
    "    df_training = pd.concat(\n",
    "        [dfT_high.sample(n=n_train, random_state=random_state), dfD.sample(n=n_train, random_state=random_state)])\n",
    "\n",
    "    # Select training and test sets\n",
    "    X = df_training[features]\n",
    "    y = df_training['target'].astype(int)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=test_size,\n",
    "                                                        random_state=random_state, stratify=y.values)\n",
    "\n",
    "    # Train the classifier on the training set via 5-fold cross-validation and subsequently test on the test set\n",
    "    n_targets = np.sum(y_train)\n",
    "    n_decoys = X_train.shape[0] - n_targets\n",
    "\n",
    "    logging.info(f'Training & cross-validation on {n_targets:,} targets and {n_decoys:,} decoys')\n",
    "    cv.fit(X_train, y_train)\n",
    "\n",
    "    logging.info(f'The best parameters selected by 5-fold cross-validation were {cv.best_params_}')\n",
    "    logging.info(f'The train {scoring} was {cv.score(X_train, y_train):.3f}')\n",
    "    logging.info(f'Testing on {np.sum(y_test):,} targets and {X_test.shape[0] - np.sum(y_test):,} decoys')\n",
    "    logging.info(f'The test {scoring} was {cv.score(X_test, y_test):.3f}')\n",
    "\n",
    "    feature_importances = cv.best_estimator_.named_steps['clf'].feature_importances_\n",
    "    indices = np.argsort(feature_importances)[::-1][:40]\n",
    "\n",
    "    top_features = X.columns[indices][:40]\n",
    "    top_score = feature_importances[indices][:40]\n",
    "\n",
    "    logging.info(f\"ML Top features\")\n",
    "\n",
    "    for i in range(len(top_features)):\n",
    "        logging.info(f\"{i + 1}\\t{top_features[i].ljust(30)} {top_score[i]:.4f}\")\n",
    "\n",
    "    # Inspect feature importances\n",
    "    if plot:\n",
    "        import seaborn as sns\n",
    "        g = sns.barplot(y=X.columns[indices][:40],\n",
    "                        x=feature_importances[indices][:40],\n",
    "                        orient='h', palette='RdBu')\n",
    "        g.set_xlabel(\"Relative importance\", fontsize=12)\n",
    "        g.set_ylabel(\"Features\", fontsize=12)\n",
    "        g.tick_params(labelsize=9)\n",
    "        g.set_title(\"Feature importance\")\n",
    "        plt.show()\n",
    "\n",
    "    return cv, features\n",
    "\n",
    "\n",
    "def _get_limited_n_jobs(n_jobs_max: int) -> int:\n",
    "    \"\"\"\n",
    "    Limit number of jobs for train_RF() if required due to type of installation or if it exceeds the maximum number allowed.\n",
    "\n",
    "    Args:\n",
    "        n_jobs_max (int): Number of jobs to use at most.\n",
    "\n",
    "    Returns:\n",
    "        int: Actual number of jobs to use.\n",
    "    \"\"\"\n",
    "\n",
    "    n_jobs = n_jobs_max\n",
    "\n",
    "    if getattr(sys, 'frozen', False):\n",
    "        logging.info('Using frozen pyinstaller version. Setting n_jobs to 1 for train_RF().')\n",
    "        return 1\n",
    "\n",
    "    # to circumvent https://github.com/pycaret/pycaret/issues/38 :\n",
    "    n_jobs_limit = 60\n",
    "    if n_jobs > n_jobs_limit or (n_jobs == -1 and psutil.cpu_count() > n_jobs_limit):\n",
    "        n_jobs = n_jobs_limit\n",
    "        logging.info(f'Capping n_jobs at {n_jobs_limit} for train_RF().')\n",
    "\n",
    "    return n_jobs\n",
    "\n",
    "\n",
    "def score_ML(df: pd.DataFrame,\n",
    "             trained_classifier: GridSearchCV,\n",
    "             features: list = None,\n",
    "             fdr_level: float = 0.01,\n",
    "             plot: bool = True,\n",
    "             **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies a trained ML classifier to df and uses the ML score to filter psms and to apply the fdr_level threshold.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        trained_classifier (GridSearchCV): GridSearchCV object returned by train_RF.\n",
    "        features (list): list with features returned by train_RF. Defaults to None.\n",
    "        fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01.\n",
    "        plot (bool, optional): flag to enable plot. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: filtered df with psms within fdr\n",
    "    \"\"\"\n",
    "    logging.info('Scoring using Machine Learning')\n",
    "    # Apply the classifier to the entire dataset\n",
    "    df_new = df.copy()\n",
    "    df_new['score'] = trained_classifier.predict_proba(df_new[features])[:,1]\n",
    "    df_new = filter_score(df_new)\n",
    "    df_new = filter_precursor(df_new)\n",
    "    cval, cutoff = cut_fdr(df_new, fdr_level, plot)\n",
    "\n",
    "    return cutoff\n",
    "\n",
    "\n",
    "def filter_with_ML(df: pd.DataFrame,\n",
    "             trained_classifier: GridSearchCV,\n",
    "             features: list = None,\n",
    "             **kwargs) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Filters the psms table by using the x_tandem score, no fdr filter.\n",
    "    TODO: Remove redundancy with score functions, see issue: #275\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): psms table of search results from alphapept.\n",
    "        trained_classifier (GridSearchCV): GridSearchCV object returned by train_RF.\n",
    "        features (list): list with features returned by train_RF. Defaults to 'None'.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: psms table with an extra 'score' column from the trained_classifier by ML, filtered for no feature or precursor to be assigned multiple times.\n",
    "    \"\"\"\n",
    "    logging.info('Filter df with x_tandem score')\n",
    "    # Apply the classifier to the entire dataset\n",
    "    df_new = df.copy()\n",
    "    df_new['score'] = trained_classifier.predict_proba(df_new[features])[:,1]\n",
    "    df_new = filter_score(df_new)\n",
    "    df_new = filter_precursor(df_new)\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein grouping\n",
    "\n",
    "What is a protein group? A introduction and explanation can be found here [1]:\n",
    "\n",
    "```\n",
    "The proteome is characterized by a relatively high sequence redundancy. This results from different evolutionary processes and the presence of isoforms. In bottom-up proteomics, this situation leads to the problem that often a peptide cannot be uniquely associated with one protein of origin, which makes it impossible to unambiguously claim the presence of one protein over another. MaxQuant resolves this issue by collapsing all proteins that cannot be distinguished based on the identified peptides into protein groups.\n",
    "The rule is that if all peptides of a given protein are a subset of the peptides used for identification of another protein, these proteins will be merged in a protein group. A more complex situation arises when two protein groups are identified with distinct peptides, except for one that is shared between the two. In this case, the two groups cannot be combined, as they contain group-unique peptides and will thus be reported separately in the MaxQuant output table. Depending on the user-defined setting, the shared peptide will not be used for quantification (unique peptides only), or it will be used for the quantification of the protein group with a larger number of associated peptides (unique + razor peptides).\n",
    "\n",
    "```\n",
    "\n",
    "In AlphaPept we employ the following strategy:\n",
    "First, we check whether a peptide is proteotypic, meaning that the peptide can only belong to one protein. For peptides that are shared between multiple proteins, we employ a razor approach. \n",
    "\n",
    "We create a network and add all connections between the peptides and proteins. Then, we extract all connected components, referring to all peptides and proteins that are connected. For a cluster of connected components, we then iterate over all proteins and count the number of peptides that are connected to the particular protein. The protein with the most peptides will then be the razor protein.\n",
    "\n",
    "We remove this protein and the respective peptides and continue with the extraction from the cluster until no more peptides are present.\n",
    "\n",
    "For efficient implementation, the proteins and peptides are encoded as indexes. To distinguish proteins from peptides, proteins have a leading 'p'.\n",
    "\n",
    "* [1] Tyanova, S., Temu, T. & Cox, J. The MaxQuant computational platform for mass spectrometry-based shotgun proteomics. Nat Protoc 11, 2301–2319 (2016). https://doi.org/10.1038/nprot.2016.136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import networkx as nx\n",
    "\n",
    "def assign_proteins(data: pd.DataFrame, pept_dict: dict) -> (pd.DataFrame, dict):\n",
    "    \"\"\"\n",
    "    Assign psms to proteins. \n",
    "    This function appends the dataframe with a column 'n_possible_proteins' which indicates how many proteins a psm could be matched to.\n",
    "    It returns the appended dataframe and a dictionary `found_proteins` where each protein is mapped to the psms indices.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): psms table of scored and filtered search results from alphapept.\n",
    "        pept_dict (dict): dictionary that matches peptide sequences to proteins\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: psms table of search results from alphapept appended with the number of matched proteins. \n",
    "        dict: dictionary mapping psms indices to proteins. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    data['n_possible_proteins'] = data['sequence'].apply(lambda x: len(pept_dict[x]))\n",
    "    unique_peptides = (data['n_possible_proteins'] == 1).sum()\n",
    "    shared_peptides = (data['n_possible_proteins'] > 1).sum()\n",
    "\n",
    "    logging.info(f'A total of {unique_peptides:,} unique and {shared_peptides:,} shared peptides.')\n",
    "    \n",
    "    sub = data[data['n_possible_proteins']==1]\n",
    "    psms_to_protein = sub['sequence'].apply(lambda x: pept_dict[x])\n",
    "\n",
    "    found_proteins = {}\n",
    "    for idx, _ in enumerate(psms_to_protein):\n",
    "        idx_ = psms_to_protein.index[idx]\n",
    "        p_str = 'p' + str(_[0])\n",
    "        if p_str in found_proteins:\n",
    "            found_proteins[p_str] = found_proteins[p_str] + [str(idx_)]\n",
    "        else:\n",
    "            found_proteins[p_str] = [str(idx_)] \n",
    "    \n",
    "    return data, found_proteins\n",
    "\n",
    "def get_shared_proteins(data: pd.DataFrame, found_proteins: dict, pept_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Assign peptides to razor proteins. \n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): psms table of scored and filtered search results from alphapept, appended with `n_possible_proteins`.\n",
    "        found_proteins (dict): dictionary mapping psms indices to proteins\n",
    "        pept_dict (dict): dictionary mapping peptide indices to the originating proteins as a list\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary mapping peptides to razor proteins\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    G = nx.Graph()\n",
    "\n",
    "    sub = data[data['n_possible_proteins']>1]\n",
    "\n",
    "    for i in range(len(sub)):\n",
    "        seq, score = sub.iloc[i][['sequence','score']]\n",
    "        idx = sub.index[i]\n",
    "        possible_proteins = pept_dict[seq]\n",
    "\n",
    "        for p in possible_proteins:\n",
    "            G.add_edge(str(idx), 'p'+str(p), score=score)\n",
    "            \n",
    "    connected_groups = np.array([list(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)], dtype=object)\n",
    "    n_groups = len(connected_groups)\n",
    "\n",
    "    logging.info('A total of {} ambigious proteins'.format(len(connected_groups)))\n",
    "\n",
    "    #Solving with razor:\n",
    "    found_proteins_razor = {}\n",
    "    for a in connected_groups[::-1]:\n",
    "        H = G.subgraph(a).copy()\n",
    "        shared_proteins = list(np.array(a)[np.array(list(i[0] == 'p' for i in a))])\n",
    "\n",
    "        while len(shared_proteins) > 0:\n",
    "            neighbors_list = []\n",
    "\n",
    "            for node in shared_proteins:\n",
    "                shared_peptides = list(H.neighbors(node))\n",
    "\n",
    "                if node in G:\n",
    "                    if node in found_proteins.keys():\n",
    "                        shared_peptides += found_proteins[node]\n",
    "\n",
    "                n_neigbhors = len(shared_peptides)\n",
    "\n",
    "                neighbors_list.append((n_neigbhors, node, shared_peptides))\n",
    "\n",
    "\n",
    "            #Check if we have a protein_group (e.g. they share the same everythin)\n",
    "            neighbors_list.sort()\n",
    "\n",
    "            # Check for protein group\n",
    "            node_ = [neighbors_list[-1][1]]\n",
    "            idx = 1\n",
    "            while idx < len(neighbors_list): #Check for protein groups\n",
    "                if neighbors_list[-idx][0] == neighbors_list[-idx-1][0]: #lenght check\n",
    "                    if set(neighbors_list[-idx][2]) == set(neighbors_list[-idx-1][2]): #identical peptides\n",
    "                        node_.append(neighbors_list[-idx-1][1])\n",
    "                        idx += 1\n",
    "                    else:\n",
    "                        break   \n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            #Remove the last entry:  \n",
    "            shared_peptides = neighbors_list[-1][2]\n",
    "            for node in node_:\n",
    "                shared_proteins.remove(node)\n",
    "\n",
    "            for _ in shared_peptides:\n",
    "                if _ in H:\n",
    "                    H.remove_node(_)\n",
    "\n",
    "            if len(shared_peptides) > 0:\n",
    "                if len(node_) > 1:\n",
    "                    node_ = tuple(node_)\n",
    "                else:\n",
    "                    node_ = node_[0]\n",
    "\n",
    "                found_proteins_razor[node_] = shared_peptides\n",
    "            \n",
    "    return found_proteins_razor\n",
    "\n",
    "\n",
    "\n",
    "def get_protein_groups(data: pd.DataFrame, pept_dict: dict, fasta_dict: dict, decoy = False, callback = None, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to perform protein grouping by razor approach.\n",
    "    This function calls `assign_proteins` and `get_shared_proteins`.\n",
    "    ToDo: implement callback for solving\n",
    "    Each protein is indicated with a p -> protein index\n",
    " \n",
    "    Args:\n",
    "        data (pd.DataFrame): psms table of scored and filtered search results from alphapept.\n",
    "        pept_dict (dict): A dictionary mapping peptide indices to the originating proteins as a list.\n",
    "        fasta_dict (dict): A dictionary with fasta sequences.\n",
    "        decoy (bool, optional): Defaults to False.\n",
    "        callback (bool, optional): Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: alphapept results table now including protein level information.\n",
    "    \"\"\"\n",
    "    data, found_proteins = assign_proteins(data, pept_dict)\n",
    "    found_proteins_razor = get_shared_proteins(data, found_proteins, pept_dict)\n",
    "\n",
    "    report = data.copy()\n",
    " \n",
    "    assignment = np.zeros(len(report), dtype=object)\n",
    "    assignment[:] = ''\n",
    "    assignment_pg = assignment.copy()\n",
    "    \n",
    "    assignment_idx = assignment.copy()\n",
    "    assignment_idx[:] = ''\n",
    "    \n",
    "    razor = assignment.copy()\n",
    "    razor[:] = False\n",
    "    \n",
    "    if decoy:\n",
    "        add = 'REV__'\n",
    "    else:\n",
    "        add = ''\n",
    "\n",
    "    for protein_str in found_proteins.keys():\n",
    "        protein = int(protein_str[1:])\n",
    "        protein_name = add+fasta_dict[protein]['name']\n",
    "        indexes = [int(_) for _ in found_proteins[protein_str]]\n",
    "        assignment[indexes] = protein_name\n",
    "        assignment_pg[indexes] = protein_name\n",
    "        assignment_idx[indexes] = str(protein)\n",
    "\n",
    "    for protein_str in found_proteins_razor.keys():\n",
    "        indexes = [int(_) for _ in found_proteins_razor[protein_str]]  \n",
    "        \n",
    "        if isinstance(protein_str, tuple):\n",
    "            proteins = [int(_[1:]) for _ in protein_str]\n",
    "            protein_name = ','.join([add+fasta_dict[_]['name'] for _ in proteins])\n",
    "            protein = ','.join([str(_) for _ in proteins])\n",
    "        \n",
    "        else:            \n",
    "            protein = int(protein_str[1:])   \n",
    "            protein_name = add+fasta_dict[protein]['name']\n",
    "        \n",
    "        assignment[indexes] = protein_name\n",
    "        assignment_pg[indexes] = protein_name\n",
    "        assignment_idx[indexes] = str(protein)\n",
    "        razor[indexes] = True\n",
    "\n",
    "    report['protein'] = assignment\n",
    "    report['protein_group'] = assignment_pg\n",
    "    report['razor'] = razor\n",
    "    report['protein_idx'] = assignment_idx\n",
    "\n",
    "    return report\n",
    "\n",
    "def perform_protein_grouping(data: pd.DataFrame, pept_dict: dict, fasta_dict: dict, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Wrapper function to perform protein grouping by razor approach\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): psms table of scored and filtered search results from alphapept.\n",
    "        pept_dict (dict): A dictionary mapping peptide indices to the originating proteins as a list.\n",
    "        fasta_dict (dict): A dictionary with fasta sequences.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: alphapept results table now including protein level information.\n",
    "    \"\"\"\n",
    "    data_sub = data[['sequence','score','decoy']]\n",
    "    data_sub_unique = data_sub.groupby(['sequence','decoy'], as_index=False).agg({\"score\": \"max\"})\n",
    "\n",
    "    targets = data_sub_unique[data_sub_unique.decoy == False]\n",
    "    targets = targets.reset_index(drop=True)\n",
    "    protein_targets = get_protein_groups(targets, pept_dict, fasta_dict, **kwargs)\n",
    "    \n",
    "    protein_targets['decoy_protein'] = False\n",
    "\n",
    "    decoys = data_sub_unique[data_sub_unique.decoy == True]\n",
    "    decoys = decoys.reset_index(drop=True)\n",
    "    protein_decoys = get_protein_groups(decoys, pept_dict, fasta_dict, decoy=True, **kwargs)\n",
    "\n",
    "    protein_decoys['decoy_protein'] = True\n",
    "\n",
    "    protein_groups = pd.concat([protein_targets, protein_decoys])\n",
    "    protein_groups_app = protein_groups[['sequence','decoy','protein','protein_group','razor','protein_idx','decoy_protein','n_possible_proteins']]\n",
    "    protein_report = pd.merge(data,\n",
    "                                protein_groups_app,\n",
    "                                how = 'inner',\n",
    "                                on = ['sequence','decoy'],\n",
    "                                validate=\"many_to_one\")\n",
    "    \n",
    "    \n",
    "    return protein_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_get_protein_groups():\n",
    "    pept_dict = {}\n",
    "\n",
    "    pept_dict['seq0'] = [0] #unique\n",
    "    pept_dict['seq1'] = [1] #unique\n",
    "    pept_dict['seq2'] = [2] #unique\n",
    "    pept_dict['seq3'] = [3] #unique\n",
    "    pept_dict['seq4'] = [4] #unique\n",
    "    pept_dict['seq5'] = [5] #unique\n",
    "    pept_dict['seq345'] = [3,4,5] #multiple\n",
    "    pept_dict['seq34'] = [3,4] #multiple\n",
    "    pept_dict['seq45'] = [4,5] #multiple\n",
    "    pept_dict['seq35'] = [3,5] #multiple\n",
    "    \n",
    "\n",
    "    fasta_dict = {}\n",
    "    fasta_dict[0] = {'name':'P0'}\n",
    "    fasta_dict[1] = {'name':'P1'}\n",
    "    fasta_dict[2] = {'name':'P2'}\n",
    "    fasta_dict[3] = {'name':'P3'}\n",
    "    fasta_dict[4] = {'name':'P4'}\n",
    "    fasta_dict[5] = {'name':'P5'}\n",
    "\n",
    "    test_case = ['seq0','seq1','seq2','seq3','seq4','seq5']\n",
    "    data = pd.DataFrame({'sequence':test_case, 'score':[1 for _ in test_case]})\n",
    "    res = get_protein_groups(data, pept_dict, fasta_dict)\n",
    "    assert res['razor'].sum() == 0\n",
    "\n",
    "    #sequence 3,4 & 3,5 are present -> P3 will be razor\n",
    "    test_case = ['seq0','seq1','seq2','seq3','seq4','seq5','seq34','seq35']\n",
    "    data = pd.DataFrame({'sequence':test_case, 'score':[1 for _ in test_case]})\n",
    "    res = get_protein_groups(data, pept_dict, fasta_dict)\n",
    "    assert res[res['sequence'] == 'seq34'][['protein', 'razor']].values.tolist()[0] == ['P3', True]\n",
    "    assert res[res['sequence'] == 'seq35'][['protein', 'razor']].values.tolist()[0] == ['P3', True]\n",
    "\n",
    "    #sequence 3,4,5 & 3,4, & 4,5 are present -> P4 will be razor\n",
    "    test_case = ['seq0','seq1','seq2','seq3','seq4','seq5','seq345','seq34','seq45']\n",
    "    data = pd.DataFrame({'sequence':test_case, 'score':[1 for _ in test_case]})\n",
    "    res = get_protein_groups(data, pept_dict, fasta_dict)\n",
    "    assert res[res['sequence'] == 'seq345'][['protein', 'razor']].values.tolist()[0] == ['P4', True]\n",
    "    assert res[res['sequence'] == 'seq34'][['protein', 'razor']].values.tolist()[0] == ['P4', True]\n",
    "    assert res[res['sequence'] == 'seq45'][['protein', 'razor']].values.tolist()[0] == ['P4', True]\n",
    "    \n",
    "    # protein group case:\n",
    "    test_case = ['seq35']\n",
    "    data = pd.DataFrame({'sequence':test_case, 'score':[1 for _ in test_case]})\n",
    "    res = get_protein_groups(data, pept_dict, fasta_dict)    \n",
    "    assert res[res['sequence'] == 'seq35'][['protein', 'razor']].values.tolist()[0] == ['P5,P3', True]\n",
    " \n",
    "test_get_protein_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_cut_fdr():\n",
    "    import random\n",
    "    import string\n",
    "    from collections import Counter\n",
    "    # Generate dummy data\n",
    "    n_samples = 10000\n",
    "    test_data = np.random.rand(n_samples)\n",
    "    df = pd.DataFrame(test_data, columns=['score'])\n",
    "    df['decoy'] = (np.random.rand(n_samples) + df['score']) < 0.5\n",
    "\n",
    "    df['filename'] = np.repeat(['file1','file2','file3','file4'], 2500)\n",
    "\n",
    "    sequences = []\n",
    "    i = 0\n",
    "    while i < 5000:\n",
    "        i += 1\n",
    "        sequences.append(''.join(random.choices(string.ascii_uppercase, k=50)))\n",
    "    df['sequence'] = np.random.choice(sequences, 10000, replace=True)\n",
    "\n",
    "    proteins = []\n",
    "    i = 0\n",
    "    while i < 500:\n",
    "        i += 1\n",
    "        proteins.append(''.join(random.choices(string.ascii_uppercase, k=50)))\n",
    "    df['protein'] = np.random.choice(proteins, 10000, replace=True)\n",
    "\n",
    "    for fdr_level in [0.01, 0.02, 0.05, 0.1, 0.2, 0.4]:\n",
    "        cutoff_value, cutoff = cut_fdr(df,fdr_level = fdr_level, plot=False)\n",
    "        assert cutoff.iloc[-1]['fdr'] <= fdr_level\n",
    "        count_fdr = len(cutoff[cutoff.decoy])/len(cutoff[cutoff.target])\n",
    "        assert  count_fdr <= fdr_level\n",
    "        sequence_res = cut_global_fdr(df, plot=False)\n",
    "        sequence_count_fdr = len(np.unique(sequence_res[sequence_res.decoy].sequence))/len(np.unique(sequence_res[~ sequence_res.decoy].sequence))\n",
    "        assert len(np.unique(sequence_res.filename)) == 4\n",
    "        assert Counter(sequence_res.sequence).most_common(1)[0][1] > 1\n",
    "        assert sequence_count_fdr <= fdr_level\n",
    "        protein_res = cut_global_fdr(df, analyte_level=\"protein\", plot=False)\n",
    "        protein_count_fdr = len(np.unique(protein_res[protein_res.decoy].protein))/len(np.unique(protein_res[~ protein_res.decoy].protein))\n",
    "        assert len(np.unique(protein_res.filename)) == 4\n",
    "        assert protein_count_fdr <= fdr_level\n",
    "        assert Counter(protein_res.sequence).most_common(1)[0][1] > 1\n",
    "        \n",
    "test_cut_fdr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "ion_dict = {}\n",
    "ion_dict[0] = ''\n",
    "ion_dict[1] = '-H20'\n",
    "ion_dict[2] = '-NH3'\n",
    "\n",
    "def get_ion(i: int, df: pd.DataFrame, fragment_ions: pd.DataFrame)-> (list, np.ndarray):\n",
    "    \"\"\"\n",
    "    Helper function to extract the ion-hits for a given DataFrame index.\n",
    "    This function extracts the hit type and the intensities.\n",
    "    E.g.: ['b1','y1'], np.array([10,20]).\n",
    "    \n",
    "    Args:\n",
    "        i (int): Row index for the DataFrame\n",
    "        df (pd.DataFrame): DataFrame with PSMs\n",
    "        fragment_ions (pd.DataFrame): DataFrame with ion hits\n",
    "\n",
    "    Returns:\n",
    "        list: List with strings that describe the ion type.\n",
    "        np.ndarray: Array with intensity information\n",
    "    \"\"\"\n",
    "    start = df['fragment_ion_idx'].iloc[i]\n",
    "    end = df['n_fragments_matched'].iloc[i]+start\n",
    "\n",
    "    ion = [('b'+str(int(_))).replace('b-','y') for _ in fragment_ions.iloc[start:end]['ion_index']]\n",
    "    losses = [ion_dict[int(_)] for _ in fragment_ions.iloc[start:end]['fragment_ion_type']]\n",
    "    ion = [a+b for a,b in zip(ion, losses)]\n",
    "    ints = fragment_ions.iloc[start:end]['fragment_ion_int'].astype('int').values\n",
    "    \n",
    "    return ion, ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_get_ion():\n",
    "    df = pd.DataFrame({'fragment_ion_idx':[1], 'n_fragments_matched':[3]})\n",
    "    fragment_ions = pd.DataFrame({'ion_index':[-1,1,-1,1],'fragment_ion_type':[0,0,1,2],'fragment_ion_int':[1,2,3,4]})\n",
    "    i = 0\n",
    "\n",
    "    ion, ints = get_ion(i, df, fragment_ions)\n",
    "\n",
    "    assert ion == ['b1', 'y1-H20', 'b1-NH3']\n",
    "    assert np.allclose(ints, np.array([2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ecdf(data:np.ndarray)-> (np.ndarray, np.ndarray):\n",
    "    \"\"\"Compute ECDF.\n",
    "    Helper function to calculate the ECDF of a score distribution.\n",
    "    This is later used to normalize the score from an arbitrary range to [0,1].\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Array containting the score.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array containg the score, sorted.\n",
    "        np.ndarray: Noramalized counts.  \n",
    "\n",
    "    \"\"\"\n",
    "    x = np.sort(data)\n",
    "    n = x.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_ecdf():\n",
    "    x,y = ecdf(np.array([4, 2, 3, 1]))\n",
    "\n",
    "    assert np.allclose(x, np.array([1, 2, 3, 4]))\n",
    "    assert np.allclose(y, np.array([0.25, 0.5 , 0.75, 1.  ]))\n",
    "    \n",
    "test_ecdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "To call the functions from the interface with a process pool, we define the helper functions `score_hdf` and `protein_grouping_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from scipy.interpolate import interp1d\n",
    "from typing import Callable, Union\n",
    "\n",
    "#This function has no unit test and is covered by the quick_test\n",
    "def score_hdf(to_process: tuple, callback: Callable = None, parallel: bool=False) -> Union[bool, str]:\n",
    "    \"\"\"Apply scoring on an hdf file to be called from a parallel pool.\n",
    "    This function does not raise errors but returns the exception as a string.\n",
    "    Args:\n",
    "        to_process: (int, dict): Tuple containg a file index and the settings.\n",
    "        callback: (Callable): Optional callback\n",
    "        parallel: (bool): Parallel flag (unused).\n",
    "\n",
    "    Returns:\n",
    "        Union[bool, str]: True if no eo exception occured, the exception if things failed.\n",
    " \n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info('Calling score_hdf')\n",
    "    \n",
    "    index, settings = to_process\n",
    "    \n",
    "    try:\n",
    "        index, settings = to_process\n",
    "        file_name = settings['experiment']['file_paths'][index]\n",
    "        base_file_name, ext = os.path.splitext(file_name)\n",
    "        ms_file = base_file_name+\".ms_data.hdf\"\n",
    "\n",
    "        skip = False\n",
    "\n",
    "        ms_file_ = alphapept.io.MS_Data_File(ms_file, is_overwritable=True)\n",
    "\n",
    "        try:\n",
    "            df = ms_file_.read(dataset_name='second_search')\n",
    "            logging.info('Found second search psms for scoring.')\n",
    "        except KeyError:\n",
    "            try:\n",
    "                df = ms_file_.read(dataset_name='first_search')\n",
    "                logging.info('No second search psms for scoring found. Using first search.')\n",
    "            except KeyError:\n",
    "                df = pd.DataFrame()    \n",
    "\n",
    "        if len(df) == 0:\n",
    "            skip = True\n",
    "            logging.info('Dataframe does not contain data. Skipping scoring step.')\n",
    "\n",
    "        if not skip:\n",
    "            df_ = get_ML_features(df, **settings['fasta'])\n",
    "            \n",
    "            if settings[\"score\"][\"method\"] == 'random_forest':\n",
    "                try:\n",
    "                    classifier, features = train_RF(df_, n_jobs = settings['general']['n_processes'], ini_score= settings['score']['ml_ini_score'])\n",
    "                    df_['score'] = classifier.predict_proba(df_[features])[:,1]\n",
    "                except ValueError as e:\n",
    "                    logging.info('ML failed. Defaulting to morpheus score')\n",
    "                    logging.info(f\"{e}\")\n",
    "                    \n",
    "                    logging.info('Converting morpheus score to probabilities')\n",
    "                    \n",
    "                    x_, y_ = ecdf(df_['score'].values)\n",
    "                    f = interp1d(x_, y_, bounds_error = False, fill_value=(y_.min(), y_.max()))\n",
    "                \n",
    "                    df_['score'] = df_['x_tandem'].apply(lambda x: f(x))\n",
    "                    \n",
    "            elif settings[\"score\"][\"method\"] == 'x_tandem':\n",
    "                df_['score'] = df_['x_tandem']\n",
    "            elif settings[\"score\"][\"method\"] == 'generic_score':\n",
    "                df_['score'] = df_['generic_score']\n",
    "            elif settings[\"score\"][\"method\"] == 'morpheus':\n",
    "                df_['score'] = df_['hits']\n",
    "            else:\n",
    "                try:\n",
    "                    import importlib\n",
    "                    alphapept_plugin = importlib.import_module(settings[\"score\"][\"method\"]+\".alphapept_plugin\")\n",
    "                    df = alphapept_plugin.score_alphapept(df, index, settings)\n",
    "                except Exception as e:\n",
    "                    raise NotImplementedError('Scoring method {} not implemented. Other exception info: {}'.format(settings[\"score\"][\"method\"], e))\n",
    "            \n",
    "            \n",
    "            #Save identifications\n",
    "            ids = df_.copy()\n",
    "            ids = filter_score(ids)\n",
    "            \n",
    "            agg_cval, ids = cut_fdr(ids, plot=False, cut=False)\n",
    "\n",
    "            logging.info('Saving identifications to ms_data file.')  \n",
    "            ms_file_.write(ids, dataset_name=\"identifications\")\n",
    "            logging.info('Saving identifications to ms_data file complete.')  \n",
    "            ids.to_csv(base_file_name+'_ids.csv')\n",
    "            logging.info('Saving identifications to csv file complete.')\n",
    "            \n",
    "            \n",
    "            df = filter_with_score(df_)\n",
    "                 \n",
    "            df_pfdr = cut_global_fdr(df, analyte_level='precursor',  plot=False, fdr_level = settings[\"search\"][\"peptide_fdr\"], **settings['search'])\n",
    "\n",
    "            logging.info('FDR on peptides complete. For {} FDR found {:,} targets and {:,} decoys.'.format(settings[\"search\"][\"peptide_fdr\"], df_pfdr['target'].sum(), df_pfdr['decoy'].sum()) )\n",
    "                    \n",
    "            df = df_pfdr\n",
    "\n",
    "            try:\n",
    "                logging.info('Extracting fragment_ions')\n",
    "                fragment_ions = ms_file_.read(dataset_name='fragment_ions')\n",
    "\n",
    "                ion_list = []\n",
    "                ion_ints = []\n",
    "\n",
    "                for i in range(len(df)):\n",
    "                    ion, ints = get_ion(i, df, fragment_ions)\n",
    "                    ion_list.append(ion)\n",
    "                    ion_ints.append(ints)\n",
    "\n",
    "                df['fragment_ion_int'] = ion_ints\n",
    "                df['fragment_ion_type'] = ion_list\n",
    "\n",
    "\n",
    "                logging.info('Extracting fragment_ions complete.')\n",
    "\n",
    "            except KeyError:\n",
    "                logging.info('No fragment_ions present.')\n",
    "                    \n",
    "            export_df = df.reset_index(drop=True)\n",
    "            \n",
    "            #if 'level_0' in export_df.keys(): #Todo: Why is this in here?\n",
    "            #    export_df.drop(columns=['level_0'])\n",
    "            #   logging.info('Dropped level_0 from df.')\n",
    "\n",
    "            #Note: Peptide FDR can be misleading here as we don't filter here, so this has not the set peptide fdr.\n",
    "            logging.info(f'Exporting peptide_fdr on {file_name}.')\n",
    "            ms_file_.write(export_df, dataset_name=\"peptide_fdr\")\n",
    "\n",
    "            logging.info(f'Scoring of files {file_name} complete.')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.info(f'Scoring of file {index} failed. Exception {e}')\n",
    "        return f\"{e}\" #Can't return exception object, cast as string\n",
    "    \n",
    "\n",
    "import alphapept.utils\n",
    "\n",
    "#This function has no unit test and is covered by the quick_test    \n",
    "def protein_grouping_all(settings:dict, pept_dict:dict, fasta_dict:dict, callback=None):\n",
    "    \"\"\"Apply protein grouping on all files in an experiment.\n",
    "    This function will load all dataframes (peptide_fdr level) and perform protein grouping.\n",
    "    \n",
    "    Args:\n",
    "        settings: (dict): Settings file for the experiment\n",
    "        pept_dict: (dict): A peptide dictionary.\n",
    "        fast_dict: (dict): A FASTA dictionary.\n",
    "        callback: (Callable): Optional callback. \n",
    "    \"\"\"\n",
    "    \n",
    "    df = alphapept.utils.assemble_df(settings, field = 'peptide_fdr', callback=None)\n",
    "    if len(df) > 0:\n",
    "        df_pg = perform_protein_grouping(df, pept_dict, fasta_dict, callback = None)\n",
    "\n",
    "        df_pg = cut_global_fdr(df_pg, analyte_level='protein_group',  plot=False, fdr_level = settings[\"search\"][\"protein_fdr\"], **settings['search'])\n",
    "        logging.info('FDR on proteins complete. For {} FDR found {:,} targets and {:,} decoys. A total of {:,} proteins found.'.format(settings[\"search\"][\"protein_fdr\"], df_pg['target'].sum(), df_pg['decoy'].sum(), len(set(df_pg['protein']))))\n",
    "\n",
    "        path = settings['experiment']['results_path']\n",
    "\n",
    "        base, ext = os.path.splitext(path)\n",
    "\n",
    "        df_pg.to_hdf(\n",
    "            path,\n",
    "            'protein_fdr'\n",
    "        )\n",
    "\n",
    "        logging.info('Saving complete.')\n",
    "        \n",
    "    else:\n",
    "        logging.info('No peptides for grouping present. Skipping.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
