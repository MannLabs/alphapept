{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching\n",
    "\n",
    "> Functions related to matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Alignment\n",
    "\n",
    "For matching MS2-identifications to MS1-features, we first need to align the datasets on top of each other to be able to transfer identifications correctly. Datasets are aligned by comparing shared precursors and calculating the median offset.\n",
    "When comparing all files to each other, we get an overdetermined linear equation system. By solving this, we find offset parameters that minimize the shift of all files to each other.\n",
    "Offset is either applied relative (mz, mobility) or absolute (rt).\n",
    "\n",
    "### Relative offset\n",
    "\n",
    "For some parameters, we would like to have a relative correction of values. Consider the case of different `mz`-values, e.g. 300 and 600. If we assume that the offset is larger for larger m/z values, we would not want an absolute correction of e.g. +0.5 Da (300.5 and 600.5) but rather a relative correction of e.g. +0.1% (300.3 and 600.6).\n",
    "\n",
    "### Absolute correction\n",
    "\n",
    "In contrast to the relative correction, sometimes absolute correction is more applicable. Consider the case of retention time. Here one would rather not expect a relative offset but rather an absolute offset. As an example, consider a lag time of 0.5 Minutes. This would be constant for all retention times and not differ e.g., for later retention times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_distance(table_1: pd.DataFrame, table_2: pd.DataFrame, offset_dict: dict, calib: bool = False) -> (list, int):\n",
    "    \"\"\"Calculate the distance between two precursors for different columns\n",
    "    Distance can either be relative or absolute.\n",
    "    \n",
    "    An example for a minimal offset_dict is: offset_dict = {'mass':'absolute'}\n",
    "\n",
    "    Args:\n",
    "        table_1 (pd.DataFrame): Dataframe with precusor data.\n",
    "        table_2 (pd.DataFrame): Dataframe with precusor data.\n",
    "        offset_dict (dict): Dictionary with column names and how the distance should be calculated.\n",
    "        calib (bool): Flag to indicate that distances should be calculated on calibrated columns. Defaults to False.\n",
    "        \n",
    "    Raises:\n",
    "        KeyError: If either table_1 or table_2 is not indexed by precursor\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    if table_1.index.name != 'precursor':\n",
    "        raise KeyError('table_1 is not indexed by precursor')\n",
    "\n",
    "    if table_2.index.name != 'precursor':\n",
    "        raise KeyError('table_2 is not indexed by precursor')\n",
    "\n",
    "    shared_precursors = list(set(table_1.index).intersection(set(table_2.index)))\n",
    "\n",
    "    table_1_ = table_1.loc[shared_precursors]\n",
    "    table_2_ = table_2.loc[shared_precursors]\n",
    "\n",
    "    table_1_ = table_1_.groupby('precursor').mean()\n",
    "    table_2_ = table_2_.groupby('precursor').mean()\n",
    "\n",
    "    deltas = []\n",
    "\n",
    "    for col in offset_dict:\n",
    "        if calib:\n",
    "            col_ = col+'_calib'\n",
    "        else:\n",
    "            col_ = col\n",
    "\n",
    "        if offset_dict[col] == 'absolute':\n",
    "            deltas.append(np.nanmedian(table_1_[col_] - table_2_[col_]))\n",
    "        elif offset_dict[col] == 'relative':\n",
    "            deltas.append(np.nanmedian((table_1_[col_] - table_2_[col_]) / (table_1_[col_] + table_2_[col_]) * 2))\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Calculating delta for {offset_dict[col_]} not implemented.\")\n",
    "\n",
    "    return deltas, len(shared_precursors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_calcualte_distance():\n",
    "    a = pd.DataFrame({'precursor':['A','B','C','D','E'], 'mass':[10, 20, 30, 40, 50]}).set_index('precursor')\n",
    "    b = pd.DataFrame({'precursor':['A','B','C','D','F'], 'mass':[11, 21, 31, 41, 51]}).set_index('precursor')\n",
    "    offset_dict = {'mass':'absolute'}\n",
    "\n",
    "    delta, n_shared = calculate_distance(a, a, offset_dict)\n",
    "    assert (delta[0] == 0) & (n_shared == 5)\n",
    "\n",
    "    delta, n_shared = calculate_distance(a, b, offset_dict)\n",
    "    assert (delta[0] == -1.0) & (n_shared == 4)\n",
    "\n",
    "test_calcualte_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "def calib_table(table: pd.DataFrame, delta: pd.Series, offset_dict: dict):\n",
    "    \"\"\"\n",
    "    Apply offset to a table. Different operations for offsets exist.\n",
    "    Offsets will be saved with a '_calib'-suffix. If this does not already exist,\n",
    "    it will be created.\n",
    "\n",
    "    Args:\n",
    "        table_1 (pd.DataFrame): Dataframe with data.\n",
    "        delta (pd.Series): Series cotaining the offset.\n",
    "        offset_dict (dict): Dictionary with column names and how the distance should be calculated.\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: If the type of vonversion is not implemented.\n",
    "    \"\"\"\n",
    "    for col in offset_dict:\n",
    "\n",
    "        if (col not in table.columns) and (col+'_apex' in table.columns):\n",
    "            col_ = col+'_apex'\n",
    "        else:\n",
    "            col_ = col\n",
    "\n",
    "        if offset_dict[col] == 'absolute':\n",
    "            table[col+'_calib'] =  table[col_]-delta[col]\n",
    "        elif offset_dict[col] == 'relative':\n",
    "            table[col+'_calib'] = (1-delta[col_])*table[col]\n",
    "        else:\n",
    "            raise NotImplementedError(offset_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_calib_table():\n",
    "    a = pd.DataFrame({'precursor':['A','B','C','D','E'], 'mass':[10, 20, 30, 40, 50]}).set_index('precursor')\n",
    "    delta = pd.Series({'mass':[1]})\n",
    "    offset_dict = {'mass':'absolute'}\n",
    "    calib_table(a, delta, offset_dict)\n",
    "\n",
    "    assert 'mass_calib' in a\n",
    "    assert np.allclose(a['mass_calib'], a['mass']-1)\n",
    "\n",
    "test_calib_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import logging\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def align(deltas: pd.DataFrame, filenames: list, weights:np.ndarray=None) -> np.ndarray:\n",
    "    \"\"\"Align multiple datasets.\n",
    "    This function creates a matrix to represent the shifts from each dataset to another.\n",
    "    This effectively is an overdetermined equation system and is solved with a linear regression.\n",
    "\n",
    "    Args:\n",
    "        deltas (pd.DataFrame): Distances from each dataset to another.\n",
    "        filenames (list): The filenames of the datasts that were compared.\n",
    "        weights (np.ndarray, optional): Distances can be weighted by their number of shared elements. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: alignment values.\n",
    "    \"\"\"\n",
    "    matrix = []\n",
    "\n",
    "    for i in range(len(deltas)):\n",
    "        start, end = deltas.index[i]\n",
    "\n",
    "        start_idx = filenames.index(start)\n",
    "        end_idx = filenames.index(end)\n",
    "\n",
    "        lines = np.zeros(len(filenames)-1)\n",
    "        lines[start_idx:end_idx] = 1\n",
    "        matrix.append(lines)\n",
    "\n",
    "    # Remove nan values\n",
    "    not_nan = ~deltas.isnull().any(axis=1)\n",
    "    matrix = np.array(matrix)\n",
    "    matrix = matrix[not_nan]\n",
    "    deltas_ = deltas[not_nan]\n",
    "\n",
    "    if len(deltas) < matrix.shape[1]:\n",
    "        logging.info('Low overlap between datasets detected. Alignment may fail.')\n",
    "\n",
    "    if weights is not None:\n",
    "        reg = LinearRegression(fit_intercept=False).fit(matrix, deltas_.values, sample_weight = weights[not_nan])\n",
    "        score= reg.score(matrix, deltas_.values)\n",
    "    else:\n",
    "        reg = LinearRegression(fit_intercept=False).fit(matrix, deltas_.values)\n",
    "        score= reg.score(matrix, deltas_.values)\n",
    "\n",
    "    logging.info(f\"Regression score is {score}\")\n",
    "\n",
    "    x = reg.predict(np.eye(len(filenames)-1))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_align():\n",
    "    deltas = pd.DataFrame({'filename':[('A', 'B'), ('A', 'C'), ('B', 'C')],'mass_offset': [1, -1, -2] }).set_index('filename')\n",
    "    filenames = ['A','B','C']\n",
    "\n",
    "    assert np.allclose(align(deltas, filenames), np.array([[1],[-2]]))\n",
    "\n",
    "test_align()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import alphapept.io\n",
    "import os\n",
    "from typing import Callable\n",
    "\n",
    "def calculate_deltas(combos: list, calib:bool = False, callback:Callable=None) -> (pd.DataFrame, np.ndarray, dict):\n",
    "\n",
    "    \"\"\"Wrapper function to calculate the distances of multiple files.\n",
    "    \n",
    "    In here, we define the offset_dict to make a relative comparison for mz and mobility and absolute for rt.\n",
    "    \n",
    "    TODO: This function could be speed-up by parallelization\n",
    "\n",
    "    Args:\n",
    "        combos (list): A list containing tuples of filenames that should be compared.\n",
    "        calib (bool): Boolean flag to indicate distance should be calculated on calibrated data.\n",
    "        callback (Callable): A callback function to track progress.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing the deltas of the files\n",
    "        np.ndarray: Numpy array containing the weights of each comparison (i.e. number of shared elements)\n",
    "        dict: Offset dictionary whicch was used for comparing. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    offset_dict = {}\n",
    "    deltas = pd.DataFrame()\n",
    "    weights = []\n",
    "\n",
    "    for i, combo in enumerate(combos):\n",
    "        file1 = os.path.splitext(combo[0])[0] + '.ms_data.hdf'\n",
    "        file2 = os.path.splitext(combo[1])[0] + '.ms_data.hdf'\n",
    "        df_1 = alphapept.io.MS_Data_File(file1).read(dataset_name=\"peptide_fdr\").set_index('precursor')\n",
    "        df_2 = alphapept.io.MS_Data_File(file2).read(dataset_name=\"peptide_fdr\").set_index('precursor')\n",
    "\n",
    "        if not offset_dict:\n",
    "            offset_dict = {'mz':'relative', 'rt':'absolute'}\n",
    "            if 'mobility' in df_1.columns:\n",
    "                logging.info(\"Also using mobility for calibration.\")\n",
    "                offset_dict['mobility'] = 'relative'\n",
    "            cols = list(offset_dict.keys())\n",
    "\n",
    "        if len(deltas) == 0:\n",
    "             deltas = pd.DataFrame(columns = cols)\n",
    "\n",
    "        dists, weight = calculate_distance(df_1, df_2, offset_dict, calib = calib)\n",
    "        deltas = deltas.append(pd.DataFrame([dists], columns = cols, index=[combo]))\n",
    "\n",
    "        weights.append(weight)\n",
    "\n",
    "        if callback:\n",
    "            callback((i+1)/len(combos))\n",
    "\n",
    "    return deltas, np.array(weights), offset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphapept.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'alphapept.io' has no attribute 'MS_Data_File'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21708/15706991.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtest_calculate_deltas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21708/15706991.py\u001b[0m in \u001b[0;36mtest_calculate_deltas\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../testfiles/test.raw'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcombos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdeltas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_deltas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21708/3688533148.py\u001b[0m in \u001b[0;36mcalculate_deltas\u001b[1;34m(combos, calib, callback)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mfile1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.ms_data.hdf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mfile2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.ms_data.hdf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mdf_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malphapept\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMS_Data_File\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"peptide_fdr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'precursor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mdf_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malphapept\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMS_Data_File\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"peptide_fdr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'precursor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'alphapept.io' has no attribute 'MS_Data_File'"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "def test_calculate_deltas():\n",
    "    filename = '../testfiles/test.raw'\n",
    "    combos = [(filename, filename)]\n",
    "    deltas, weights, offset_dict = calculate_deltas(combos)\n",
    "    assert np.allclose(deltas.values, np.array((0,0)))\n",
    "\n",
    "test_calculate_deltas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import os\n",
    "import functools\n",
    "\n",
    "#There is no unit test for align_files and align_datasets as they are wrappers and should be covered by the quick_test\n",
    "def align_files(filenames: list, alignment: pd.DataFrame, offset_dict: dict):\n",
    "    \"\"\"\n",
    "    Wrapper function that aligns a list of files.\n",
    "    \n",
    "    Args:\n",
    "        filenames (list): A list with raw file names.\n",
    "        alignment (pd.DataFrame): A pandas dataframe containing the alignment information.\n",
    "        offset_dict (dict): Dictionary with column names and how the distance should be calculated.\n",
    "    \"\"\"\n",
    "    for idx, filename in enumerate(filenames):\n",
    "\n",
    "        file = os.path.splitext(filename)[0] + '.ms_data.hdf'\n",
    "\n",
    "        for column in ['peptide_fdr', 'feature_table']:\n",
    "            df = alphapept.io.MS_Data_File(file).read(dataset_name=column)\n",
    "            calib_table(df, alignment.iloc[idx], offset_dict)\n",
    "            logging.info(f\"Saving {file} - {column}.\")\n",
    "            ms_file = alphapept.io.MS_Data_File(file, is_overwritable=True)\n",
    "\n",
    "            ms_file.write(df, dataset_name=column)\n",
    "\n",
    "\n",
    "def align_datasets(settings:dict, callback:callable=None):\n",
    "    \"\"\"\n",
    "    Wrapper function that aligns all experimental files specified a settings file.\n",
    "    \n",
    "    Args:\n",
    "        settings (dict): A list with raw file names.\n",
    "        callback (Callable): Callback function to indicate progress.\n",
    "    \"\"\"\n",
    "    filenames = settings['experiment']['file_paths']\n",
    "\n",
    "    if callback:\n",
    "        def progress_wrapper(current, step, n_steps):\n",
    "            callback((step/n_steps)+(current/n_steps))\n",
    "\n",
    "        cb = functools.partial(progress_wrapper, 0, 2)\n",
    "    else:\n",
    "        cb = None\n",
    "\n",
    "    if len(filenames) > 1:\n",
    "        combos = list(combinations(filenames, 2))\n",
    "\n",
    "        deltas, weights, offset_dict = calculate_deltas(combos, callback=cb)\n",
    "\n",
    "        cols = list(offset_dict.keys())\n",
    "\n",
    "        before_sum = deltas.abs().sum().to_dict()\n",
    "        before_mean = deltas.abs().mean().to_dict()\n",
    "\n",
    "        logging.info(f'Total deviation before calibration {before_sum}')\n",
    "        logging.info(f'Mean deviation before calibration {before_mean}')\n",
    "\n",
    "        logging.info(f'Solving equation system')\n",
    "\n",
    "        alignment = pd.DataFrame(align(deltas, filenames, weights), columns = cols)\n",
    "        alignment = pd.concat([pd.DataFrame(np.zeros((1, alignment.shape[1])), columns= cols), alignment])\n",
    "        alignment -= alignment.mean()\n",
    "\n",
    "        logging.info(f'Solving equation system complete.')\n",
    "\n",
    "        logging.info(f'Applying offset')\n",
    "\n",
    "        align_files(filenames, -alignment, offset_dict)\n",
    "\n",
    "        if cb:\n",
    "            cb = functools.partial(progress_wrapper, 1, 2)\n",
    "\n",
    "        deltas, weights, offset_dict = calculate_deltas(combos, calib=True, callback=cb)\n",
    "\n",
    "        after_sum = deltas.abs().sum().to_dict()\n",
    "        after_mean = deltas.abs().mean().to_dict()\n",
    "\n",
    "        logging.info(f'Total deviation after calibration {after_sum}')\n",
    "        logging.info(f'Mean deviation after calibration {after_mean}')\n",
    "\n",
    "        change_sum = {k:v/before_sum[k] for k,v in after_sum.items()}\n",
    "        change_mean = {k:v/before_mean[k] for k,v in after_mean.items()}\n",
    "\n",
    "        logging.info(f'Change (after/before) total deviation {change_sum}')\n",
    "        logging.info(f'Change (after/before) mean deviation {change_mean}')\n",
    "\n",
    "    else:\n",
    "        logging.info('Only 1 dataset present. Skipping alignment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching \n",
    "\n",
    "Transfer MS2 identifications to similar MS1 features.\n",
    "\n",
    "For \"match-between-runs\" we start with aligning datasets. To create a reference we use for matching, we combine all datasets of a matching group. When using the default settings, the matching group consists of all files. We then group the dataset by precursor and calculate it's average properties (rt, mz, mobility). By combining several files we further are able to calculate a standard deviation. This allows us to know where and with what deviation we would expect an MS1 feature and have the corresponding identification. This is our matching reference. In the matching step, we go through each dataset individually and check if there are precursors in the reference that were not identified in this dataset. We then perform a nearest-neighbor lookup to find if any MS1 features exist that are in close proximity to the reference. The distance metric we use is normed by the median standard of the deviation. Lastly we assess the confidence in a transfered identifcation by using the [`Mahalanobis`](https://stats.stackexchange.com/questions/331283/how-to-calculate-the-probability-of-a-data-point-belonging-to-a-multivariate-nor) distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from scipy import stats\n",
    "def get_probability(df: pd.DataFrame, ref: pd.DataFrame, sigma:pd.DataFrame, index:int)-> float:\n",
    "    \"\"\"Probablity estimate of a transfered identification using the Mahalanobis distance.\n",
    "    \n",
    "    The function calculates the probability that a feature is a reference feature.\n",
    "    The reference features containing std deviations so that a probability can be estimated.\n",
    "    \n",
    "    It is required that the data frames are matched, meaning that the first entry in df matches to the first entry in ref.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset containing transferered features\n",
    "        ref (pd.DataFrame): Dataset containing reference features\n",
    "        sigma (pd.DataFrame): Dataset containing the standard deviations of the reference features\n",
    "        index (int): Index to the datframes that should be compared\n",
    "\n",
    "    Returns:\n",
    "        float: Mahalanobis distance\n",
    "    \"\"\"\n",
    "\n",
    "    sigma = sigma.iloc[index].values\n",
    "    sigma = sigma*np.eye(len(sigma))\n",
    "\n",
    "    mu = ref.iloc[index].values\n",
    "    x = df.iloc[index].values\n",
    "\n",
    "    try:\n",
    "        m_dist_x = np.dot((x-mu).transpose(), np.linalg.inv(sigma))\n",
    "        m_dist_x = np.dot(m_dist_x, (x-mu))\n",
    "        _ = stats.chi2.cdf(m_dist_x, len(mu))\n",
    "    except Exception as e:\n",
    "        _ = np.nan\n",
    "\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element: (ideal match): 0.00\n",
      "Second element: (rt slightly off): 0.12\n",
      "Third element: (mass completely off): 1.00\n"
     ]
    }
   ],
   "source": [
    "#Example usage\n",
    "\n",
    "a = pd.DataFrame({'mass':[100,200,300],'rt':[1,2,3]})\n",
    "b = pd.DataFrame({'mass':[100,200,302],'rt':[1,2.5,3]})\n",
    "std = pd.DataFrame({'mass':[0.1,0.1,0.1],'rt':[1,1,1]})\n",
    "\n",
    "print(f\"First element: (ideal match): {get_probability(a, b, std, 0):.2f}\")\n",
    "print(f\"Second element: (rt slightly off): {get_probability(a, b, std, 1):.2f}\")\n",
    "print(f\"Third element: (mass completely off): {get_probability(a, b, std, 2):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_probability():\n",
    "    a = pd.DataFrame({'mass':[100,200,300],'rt':[1,2,3]})\n",
    "    b = pd.DataFrame({'mass':[100,200,302],'rt':[1,2.5,3]})\n",
    "    std = pd.DataFrame({'mass':[0.1,0.1,0.1],'rt':[1,1,1]})\n",
    "\n",
    "    assert np.allclose(get_probability(a, b, std, 0), 0.00)\n",
    "    assert np.allclose(get_probability(a, b, std, 2), 1)\n",
    "    \n",
    "test_get_probability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from sklearn.neighbors import KDTree\n",
    "from alphapept.utils import assemble_df\n",
    "\n",
    "# This function is a wrapper function and has currently has no unit test \n",
    "# The function will be revised when implementing issue #255: https://github.com/MannLabs/alphapept/issues/255\n",
    "def match_datasets(settings:dict, callback:Callable = None):\n",
    "    \"\"\"Match datasets: Wrapper function to match datasets based on a settings file.\n",
    "    \n",
    "    Args:\n",
    "        settings (dict): Dictionary containg specifications of the run\n",
    "        callback (Callable): Callback function to indicate progress.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(settings['experiment']['file_paths']) > 2:\n",
    "        xx = alphapept.utils.assemble_df(settings, field='peptide_fdr')\n",
    "\n",
    "        base_col = ['precursor']\n",
    "        alignment_cols = ['mz_calib','rt_calib']\n",
    "        extra_cols = ['score','decoy','target']\n",
    "\n",
    "        if 'mobility' in xx.columns:\n",
    "            alignment_cols += ['mobility_calib']\n",
    "            use_mobility = True\n",
    "        else:\n",
    "            use_mobility = False\n",
    "\n",
    "        grouped = xx[base_col + alignment_cols + extra_cols].groupby('precursor').mean()\n",
    "        std_ = xx[base_col + alignment_cols].groupby('precursor').std()\n",
    "\n",
    "        grouped[[_+'_std' for _ in alignment_cols]] = std_\n",
    "\n",
    "        std_range = np.nanmedian(std_.values, axis=0)\n",
    "\n",
    "        match_p_min = settings['matching']['match_p_min']\n",
    "        match_d_min = settings['matching']['match_d_min']\n",
    "\n",
    "        filenames = settings['experiment']['file_paths']\n",
    "\n",
    "        lookup_dict = xx.set_index('precursor')[['sequence']].to_dict()\n",
    "\n",
    "        for idx, filename in enumerate(filenames):\n",
    "            file = os.path.splitext(filename)[0] + '.ms_data.hdf'\n",
    "\n",
    "            df = alphapept.io.MS_Data_File(file).read(dataset_name='peptide_fdr')\n",
    "            features = alphapept.io.MS_Data_File(file).read(dataset_name='feature_table')\n",
    "            features['feature_idx'] = features.index\n",
    "\n",
    "            matching_set = set(grouped.index) - set(df['precursor'])\n",
    "            logging.info(f'Trying to match file {file} with database of {len(matching_set):,} unidentified candidates')\n",
    "\n",
    "            mz_range = std_range[0]\n",
    "            rt_range = std_range[1]\n",
    "\n",
    "            tree_points = features[alignment_cols].values\n",
    "            tree_points[:,0] = tree_points[:,0]/mz_range\n",
    "            tree_points[:,1] = tree_points[:,1]/rt_range\n",
    "\n",
    "            query_points = grouped.loc[matching_set][alignment_cols].values\n",
    "            query_points[:,0] = query_points[:,0]/mz_range\n",
    "            query_points[:,1] = query_points[:,1]/rt_range\n",
    "\n",
    "            if use_mobility:\n",
    "                logging.info(\"Using mobility\")\n",
    "                i_range = std_range[2]\n",
    "\n",
    "                tree_points[:,2] = tree_points[:,2]/i_range\n",
    "                query_points[:,2] = query_points[:,2]/i_range\n",
    "\n",
    "            matching_tree = KDTree(tree_points, metric=\"minkowski\")\n",
    "\n",
    "            dist, idx = matching_tree.query(query_points, k=1)\n",
    "\n",
    "            matched = features.iloc[idx[:,0]]\n",
    "\n",
    "            for _ in extra_cols:\n",
    "                matched[_] = grouped.loc[matching_set, _].values\n",
    "\n",
    "            to_keep = dist < match_d_min\n",
    "\n",
    "            matched = matched[to_keep]\n",
    "\n",
    "            ref = grouped.loc[matching_set][alignment_cols][to_keep]\n",
    "            sigma = std_.loc[matching_set][to_keep]\n",
    "\n",
    "            logging.info(f'{len(matched):,} possible features for matching based on distance of {match_d_min}')\n",
    "\n",
    "            matched['matching_p'] = [get_probability(matched[alignment_cols], ref, sigma, i) for i in range(len(matched))]\n",
    "            matched['precursor'] = grouped.loc[matching_set][to_keep].index.values\n",
    "\n",
    "            matched = matched[matched['matching_p']< match_p_min]\n",
    "\n",
    "            logging.info(f'{len(matched):,} possible features for matching based on probability of {match_p_min}')\n",
    "\n",
    "            matched['type'] = 'matched'\n",
    "\n",
    "            for _ in lookup_dict.keys():\n",
    "                matched[_] = [lookup_dict[_][x] for x in matched['precursor']]\n",
    "\n",
    "            df['type'] = 'msms'\n",
    "            df['matching_p'] = np.nan\n",
    "\n",
    "            shared_columns = set(matched.columns).intersection(set(df.columns))\n",
    "\n",
    "            df_ = pd.concat([df, matched[shared_columns]], ignore_index=True)\n",
    "\n",
    "            logging.info(f\"Saving {file} - peptide_fdr.\")\n",
    "            ms_file = alphapept.io.MS_Data_File(file, is_overwritable=True)\n",
    "\n",
    "            ms_file.write(df_, dataset_name='peptide_fdr')\n",
    "    else:\n",
    "        logging.info('Less than 3 datasets present. Skipping matching.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted 12_performance.ipynb.\n",
      "Converted 13_export.ipynb.\n",
      "Converted 14_display.ipynb.\n",
      "Converted 15_label.ipynb.\n",
      "Converted additional_code.ipynb.\n",
      "Converted contributing.ipynb.\n",
      "Converted file_formats.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
