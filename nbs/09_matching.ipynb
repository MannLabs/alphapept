{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching\n",
    "\n",
    "> Functions related to matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Alignment\n",
    "\n",
    "Align datasets via comparing shared precursors and calculating the median offset.\n",
    "All files will be compared with each other, and a linear equation system is used to calculate the best offset.\n",
    "\n",
    "Offset is either applied relative (mz, mobility) or absolute (rt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "import pandas as pd\n",
    "from itertools import combinations \n",
    "import numpy as np\n",
    "import os\n",
    "import alphapept.io\n",
    "import functools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def calculate_distance(table_1, table_2, offset_cols, calib = False):\n",
    "    \"\"\"\n",
    "    Calculate the distance, either relative or absolute\n",
    "    TODO: We could use a weighting factor\n",
    "    \"\"\"\n",
    "    \n",
    "    shared_precursors = list(set(table_1.index).intersection(set(table_2.index)))\n",
    "    \n",
    "    table_1_ = table_1.loc[shared_precursors]\n",
    "    table_2_ = table_2.loc[shared_precursors] \n",
    "    \n",
    "    table_1_ = table_1_.groupby('precursor').mean()\n",
    "    table_2_ = table_2_.groupby('precursor').mean()\n",
    "    \n",
    "    deltas = []\n",
    "    \n",
    "    for col in list(offset_cols.keys()):\n",
    "        if calib:\n",
    "            col_ = col+'_calib'\n",
    "        else:\n",
    "            col_ = col\n",
    "            \n",
    "        if offset_cols[col] == 'absolute':\n",
    "            deltas.append(np.nanmedian(table_1_[col_] - table_2_[col_]))\n",
    "        elif offset_cols[col] == 'relative':\n",
    "            deltas.append(np.nanmedian((table_1_[col_] - table_2_[col_]) / (table_1_[col_] + table_2_[col_]) * 2))\n",
    "        else:\n",
    "            raise NotImplementedError(offset_cols[col_])\n",
    "            \n",
    "    return deltas, len(shared_precursors)\n",
    "\n",
    "def calib_table(table, delta, offset_cols):\n",
    "    \"\"\"\n",
    "    Apply offset to a table\n",
    "    If not _calib table exist, create a new one.\n",
    "    \n",
    "    \"\"\"\n",
    "    for col in list(offset_cols.keys()):\n",
    "        \n",
    "        if (col not in table.columns) and (col+'_apex' in table.columns):\n",
    "            col_ = col+'_apex'\n",
    "        else:\n",
    "            col_ = col\n",
    "\n",
    "        if offset_cols[col] == 'absolute':\n",
    "            table[col+'_calib'] =  table[col_]-delta[col]\n",
    "        elif offset_cols[col] == 'relative':\n",
    "            table[col+'_calib'] = (1-delta[col_])*table[col]\n",
    "        else:\n",
    "            raise NotImplementedError(offset_cols[col])\n",
    "            \n",
    "def align(deltas, filenames, weights=None):\n",
    "    \"\"\"\n",
    "    Solve equation system\n",
    "    \"\"\"\n",
    "    matrix = []\n",
    "\n",
    "    for i in range(len(deltas)):\n",
    "        start, end = deltas.index[i]\n",
    "\n",
    "        start_idx = filenames.index(start)\n",
    "        end_idx = filenames.index(end)\n",
    "\n",
    "        lines = np.zeros(len(filenames)-1)\n",
    "        lines[start_idx:end_idx] = 1\n",
    "        matrix.append(lines)\n",
    "        \n",
    "    # Remove nan values\n",
    "    \n",
    "    not_nan = ~deltas.isnull().any(axis=1)\n",
    "    matrix = np.array(matrix)\n",
    "    matrix = matrix[not_nan]\n",
    "    deltas_ = deltas[not_nan]\n",
    "    \n",
    "    if len(deltas) < matrix.shape[1]:\n",
    "        logging.info('Low overlap between datasets detected. Alignment may fail.')\n",
    "        \n",
    "    if weights is not None:\n",
    "        reg = LinearRegression(fit_intercept=False).fit(matrix, deltas_.values, sample_weight = weights[not_nan])\n",
    "        score= reg.score(matrix, deltas_.values)\n",
    "    else:\n",
    "        reg = LinearRegression(fit_intercept=False).fit(matrix, deltas_.values)\n",
    "        score= reg.score(matrix, deltas_.values)\n",
    "        \n",
    "    logging.info(f\"Regression score is {score}\")\n",
    "\n",
    "    x= reg.predict(np.eye(len(filenames)-1))\n",
    "\n",
    "    #x = np.linalg.lstsq(matrix, deltas_.values, rcond=None)[0] #Alternative w/o weights\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def calculate_deltas(combos, calib = False, callback=None):\n",
    "    \"\"\"\n",
    "    Calculate offsets for multiple files\n",
    "    TODO: Parallelize\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    offset_cols = {}\n",
    "\n",
    "    callback = None\n",
    "\n",
    "    deltas = pd.DataFrame()\n",
    "    weights = []\n",
    "\n",
    "    for i, combo in enumerate(combos):\n",
    "\n",
    "        file1 = os.path.splitext(combo[0])[0] + '.ms_data.hdf'\n",
    "        file2 = os.path.splitext(combo[1])[0] + '.ms_data.hdf'\n",
    "\n",
    "        df_1 = alphapept.io.MS_Data_File(file1).read(dataset_name=\"peptide_fdr\")\n",
    "        df_2 = alphapept.io.MS_Data_File(file2).read(dataset_name=\"peptide_fdr\")\n",
    "\n",
    "        if not offset_cols:\n",
    "            offset_cols = {'mz':'relative', 'rt':'absolute'}\n",
    "            if 'mobility' in df_1.columns:\n",
    "                logging.info(\"Also using mobility for calibration.\")\n",
    "                offset_cols['mobility'] = 'relative'\n",
    "            cols = list(offset_cols.keys())\n",
    "\n",
    "        if len(deltas) == 0:\n",
    "             deltas = pd.DataFrame(columns = cols)\n",
    "\n",
    "        dists, weight = calculate_distance(df_1, df_2, offset_cols, calib = calib)\n",
    "        deltas = deltas.append(pd.DataFrame([dists], columns = cols, index=[combo]))\n",
    "\n",
    "        weights.append(weight)\n",
    "        \n",
    "        if callback:\n",
    "            callback((i+1)/len(combos))\n",
    "        \n",
    "    return deltas, np.array(weights), offset_cols\n",
    "\n",
    "\n",
    "def align_files(filenames, alignment, offset_cols):\n",
    "\n",
    "    for idx, filename in enumerate(filenames):\n",
    "        \n",
    "        file = os.path.splitext(filename)[0] + '.ms_data.hdf'\n",
    "\n",
    "        for column in ['peptide_fdr', 'feature_table']:\n",
    "            df = alphapept.io.MS_Data_File(file).read(dataset_name=column)\n",
    "            calib_table(df, alignment.iloc[idx], offset_cols)\n",
    "            logging.info(f\"Saving {file} - {column}.\")\n",
    "            ms_file = alphapept.io.MS_Data_File(file, is_overwritable=True)\n",
    "        \n",
    "            ms_file.write(df, dataset_name=column)\n",
    "\n",
    "\n",
    "def align_datasets(settings, callback=None):\n",
    "    filenames = settings['experiment']['file_paths']\n",
    "    \n",
    "    if callback:\n",
    "        def progress_wrapper(current, step, n_steps):\n",
    "            callback(step+current/n_steps)\n",
    "            \n",
    "        cb = functools.partial(progress_wrapper, 0, 2)\n",
    "    else:\n",
    "        cb = None\n",
    "        \n",
    "    if len(filenames) > 1:\n",
    "        combos = list(combinations(filenames, 2))\n",
    "\n",
    "        deltas, weights, offset_cols = calculate_deltas(combos, callback=cb)\n",
    "\n",
    "        cols = list(offset_cols.keys())\n",
    "\n",
    "        before_sum = deltas.abs().sum().to_dict()\n",
    "        before_mean = deltas.abs().mean().to_dict()\n",
    "\n",
    "        logging.info(f'Total deviation before calibration {before_sum}')\n",
    "        logging.info(f'Mean deviation before calibration {before_mean}')\n",
    "\n",
    "        logging.info(f'Solving equation system')\n",
    "\n",
    "        alignment = pd.DataFrame(align(deltas, filenames, weights), columns = cols)\n",
    "        alignment = pd.concat([pd.DataFrame(np.zeros((1, alignment.shape[1])), columns= cols), alignment])\n",
    "        alignment -= alignment.mean()\n",
    "\n",
    "        logging.info(f'Solving equation system complete.')\n",
    "\n",
    "        logging.info(f'Applying offset')\n",
    "\n",
    "        align_files(filenames, -alignment, offset_cols)\n",
    "        \n",
    "        if cb:\n",
    "            cb = functools.partial(progress_wrapper, 1, 2)\n",
    "\n",
    "        deltas, weights, offset_cols = calculate_deltas(combos, calib=True, callback=cb)\n",
    "\n",
    "        after_sum = deltas.abs().sum().to_dict()\n",
    "        after_mean = deltas.abs().mean().to_dict()\n",
    "\n",
    "        logging.info(f'Total deviation after calibration {after_sum}')\n",
    "        logging.info(f'Mean deviation after calibration {after_mean}')\n",
    "\n",
    "        change_sum = {k:v/before_sum[k] for k,v in after_sum.items()}\n",
    "        change_mean = {k:v/before_mean[k] for k,v in after_mean.items()}\n",
    "\n",
    "        logging.info(f'Change (after/before) total deviation {change_sum}')\n",
    "        logging.info(f'Change (after/before) mean deviation {change_mean}')\n",
    "    \n",
    "    else:\n",
    "        logging.info('Only 1 dataset present. Skipping alignment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching \n",
    "\n",
    "Transfer MS2 identifications to similar MS1 fatures.\n",
    "\n",
    "Brief outline of the computational task\n",
    "\n",
    "- Start with aligned datasets\n",
    "- Combine all datasets in one dataframe\n",
    "- Group by percursor and calculate expected location (rt, mz, mobility) and standard deviation to calculate a reference\n",
    "- For each dataset, calculate the subset of precursors that were not identified but are present in the reference\n",
    "- Search for the closest neighbor for each element in the subset in the identified features\n",
    "- The distance is calculated using the median standard deviation of the reference\n",
    "- Use the `Mahalanobis` distance to calculate a probability that one featue belongs to the distribution\n",
    "\n",
    "For Mahalanobis distance, see here:\n",
    "https://stats.stackexchange.com/questions/331283/how-to-calculate-the-probability-of-a-data-point-belonging-to-a-multivariate-nor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "from alphapept.utils import assemble_df\n",
    "from scipy import stats\n",
    "\n",
    "def get_probability(df, ref, sigma, index):\n",
    "\n",
    "    sigma = sigma.iloc[index].values\n",
    "    sigma = sigma*np.eye(len(sigma))\n",
    "    \n",
    "    mu = ref.iloc[index].values\n",
    "    \n",
    "    x = df.iloc[index].values\n",
    "\n",
    "    try:\n",
    "        m_dist_x = np.dot((x-mu).transpose(), np.linalg.inv(sigma))\n",
    "        m_dist_x = np.dot(m_dist_x, (x-mu))\n",
    "        _ = stats.chi2.cdf(m_dist_x, len(mu))\n",
    "    except Exception as e:\n",
    "        _ = np.nan\n",
    "    \n",
    "    return _\n",
    "\n",
    "def match_datasets(settings, callback = None):\n",
    "    \n",
    "    if len(settings['experiment']['file_paths']) > 2:\n",
    "        xx = alphapept.utils.assemble_df(settings, field='peptide_fdr')\n",
    "\n",
    "        base_col = ['precursor']\n",
    "        alignment_cols = ['mz_calib','rt_calib']\n",
    "        extra_cols = ['score','decoy','target']\n",
    "\n",
    "        if 'mobility' in xx.columns:\n",
    "            alignment_cols += ['mobility_calib']\n",
    "            use_mobility = True\n",
    "        else:\n",
    "            use_mobility = False\n",
    "\n",
    "        grouped = xx[base_col + alignment_cols + extra_cols].groupby('precursor').mean()\n",
    "        std_ = xx[base_col + alignment_cols].groupby('precursor').std()\n",
    "\n",
    "        grouped[[_+'_std' for _ in alignment_cols]] = std_\n",
    "\n",
    "        std_range = np.nanmedian(std_.values, axis=0)\n",
    "\n",
    "        min_match_p = settings['matching']['min_match_p']\n",
    "        min_match_d = settings['matching']['min_match_d']\n",
    "\n",
    "        filenames = settings['experiment']['file_paths']\n",
    "\n",
    "        lookup_dict = xx.set_index('precursor')[['sequence']].to_dict()\n",
    "\n",
    "        for idx, filename in enumerate(filenames):\n",
    "            file = os.path.splitext(filename)[0] + '.ms_data.hdf'\n",
    "\n",
    "            df = alphapept.io.MS_Data_File(file).read(dataset_name='peptide_fdr')\n",
    "            features = alphapept.io.MS_Data_File(file).read(dataset_name='feature_table')\n",
    "            features['feature_idx'] = features.index\n",
    "\n",
    "            matching_set = set(grouped.index) - set(df['precursor'])\n",
    "            logging.info(f'Trying to match file {file} with database of {len(matching_set):,} unidentified candidates')\n",
    "\n",
    "            mz_range = std_range[0]\n",
    "            rt_range = std_range[1]\n",
    "\n",
    "            tree_points = features[alignment_cols].values\n",
    "            tree_points[:,0] = tree_points[:,0]/mz_range \n",
    "            tree_points[:,1] = tree_points[:,1]/rt_range\n",
    "\n",
    "            query_points = grouped.loc[matching_set][alignment_cols].values\n",
    "            query_points[:,0] = query_points[:,0]/mz_range \n",
    "            query_points[:,1] = query_points[:,1]/rt_range\n",
    "\n",
    "            if use_mobility:\n",
    "                logging.info(\"Using mobility\")\n",
    "                i_range = std_range[2]\n",
    "\n",
    "                tree_points[:,2] = tree_points[:,2]/i_range\n",
    "                query_points[:,2] = query_points[:,2]/i_range\n",
    "\n",
    "            matching_tree = KDTree(tree_points, metric=\"minkowski\")\n",
    "\n",
    "            dist, idx = matching_tree.query(query_points, k=1)\n",
    "\n",
    "            matched = features.iloc[idx[:,0]]\n",
    "\n",
    "            for _ in extra_cols:\n",
    "                matched[_] = grouped.loc[matching_set, _].values\n",
    "\n",
    "            to_keep = dist < min_match_d\n",
    "\n",
    "            matched = matched[to_keep]\n",
    "\n",
    "            ref = grouped.loc[matching_set][alignment_cols][to_keep]\n",
    "            sigma = std_.loc[matching_set][to_keep]\n",
    "\n",
    "            logging.info(f'{len(matched):,} possible features for matching based on distance of {min_match_d}')\n",
    "\n",
    "            matched['matching_p'] = [get_probability(matched[alignment_cols], ref, sigma, i) for i in range(len(matched))]\n",
    "            matched['precursor'] = grouped.loc[matching_set][to_keep].index.values\n",
    "\n",
    "            matched = matched[matched['matching_p']< min_match_p]\n",
    "\n",
    "            logging.info(f'{len(matched):,} possible features for matching based on probability of {min_match_p}')\n",
    "\n",
    "            matched['type'] = 'matched'\n",
    "\n",
    "            for _ in lookup_dict.keys():\n",
    "                matched[_] = [lookup_dict[_][x] for x in matched['precursor']]\n",
    "\n",
    "            df['type'] = 'msms'\n",
    "            df['matching_p'] = np.nan\n",
    "\n",
    "            shared_columns = set(matched.columns).intersection(set(df.columns))\n",
    "\n",
    "            df_ = pd.concat([df, matched[shared_columns]], ignore_index=True)\n",
    "\n",
    "            logging.info(f\"Saving {file} - peptide_fdr.\")\n",
    "            ms_file = alphapept.io.MS_Data_File(file, is_overwritable=True)\n",
    "\n",
    "            ms_file.write(df_, dataset_name='peptide_fdr')\n",
    "    else:\n",
    "        logging.info('Less than 3 datasets present. Skipping matching.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted 12_speed.ipynb.\n",
      "Converted 13_export.ipynb.\n",
      "Converted Alignment Revision.ipynb.\n",
      "Converted Alignment_revision2-Copy1.ipynb.\n",
      "Converted Alignment_revision2.ipynb.\n",
      "Converted FF Fix.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted Untitled1.ipynb.\n",
      "Converted Untitled2.ipynb.\n",
      "Converted Untitled3.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
