{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp recalibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recalibration\n",
    "\n",
    "> Functions related to recalibrating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains everything to perform a recalibration.\n",
    "\n",
    "Current ToDo here:\n",
    "\n",
    "- Write introduction / comment more on functions\n",
    "- Describe approach\n",
    "- Cite relevant papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from alphapept.score import score_x_tandem\n",
    "import alphapept.io\n",
    "from multiprocessing import Pool\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import logging\n",
    "\n",
    "def transform(x, _, scaling_dict):\n",
    "    \n",
    "    if _ not in scaling_dict:\n",
    "        raise NotImplementError(f\"Column {_} not in scaling_dict\")\n",
    "    else:\n",
    "        type_, scale_ = scaling_dict[_]\n",
    "        \n",
    "        if type_ == 'relative':\n",
    "            return np.log(x, out=np.zeros_like(x), where=(x>0))/scale_\n",
    "        elif type_ == 'absolute':\n",
    "            return x/scale_\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Type {type_} not known.\")\n",
    "\n",
    "\n",
    "def get_calibration(df, features, outlier_std = 3, n_neighbors = 100, ppm_range = 20, rt_range = 0.5, mob_range = 0.3, callback = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Calibration\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(df) > n_neighbors:\n",
    "        target = 'o_mass_ppm'\n",
    "        cols = ['mz','rt']\n",
    "\n",
    "        if 'mobility' in df.columns:\n",
    "            cols += ['mobility']\n",
    "\n",
    "        scaling_dict = {}\n",
    "        scaling_dict['mz'] = ('relative', ppm_range/1e6)\n",
    "        scaling_dict['rt'] = ('absolute', rt_range)\n",
    "        scaling_dict['mobility'] = ('relative', mob_range)\n",
    "\n",
    "        # Remove outliers for calibration\n",
    "        o_mass_std = np.abs(df['o_mass_ppm'].std())\n",
    "        o_mass_mean = df['o_mass_ppm'].mean()\n",
    "    \n",
    "        #df_sub = df[~df['feature_idx'].isna()].copy() #Remove unmatched features\n",
    "        df_sub = df.query('o_mass_ppm < @o_mass_mean+@outlier_std*@o_mass_std and o_mass_ppm > @o_mass_mean-@outlier_std*@o_mass_std').copy()\n",
    "\n",
    "        tree_points = df_sub[cols].values\n",
    "\n",
    "        for idx, _ in enumerate(df_sub[cols].columns):\n",
    "            tree_points[:, idx] = transform(tree_points[:, idx], _, scaling_dict)\n",
    "\n",
    "        target_points = features[[_+'_matched' for _ in cols]].values\n",
    "\n",
    "        for idx, _ in enumerate(df_sub[cols].columns):\n",
    "            target_points[:, idx] = transform(target_points[:, idx], _, scaling_dict)\n",
    "\n",
    "        neigh = KNeighborsRegressor(n_neighbors=n_neighbors, weights = 'distance')\n",
    "        neigh.fit(tree_points, df_sub[target].values)\n",
    "\n",
    "        y_hat = neigh.predict(target_points)    \n",
    "\n",
    "        corrected_mass = (1-y_hat/1e6) * features['mass_matched']\n",
    "\n",
    "        return corrected_mass, y_hat.std()\n",
    "    \n",
    "    else:\n",
    "        logging.info('Not enough data points present. Skipping recalibration.')\n",
    "        return features['mass_matched'], np.abs(df['o_mass_ppm'].std())\n",
    "\n",
    "    \n",
    "    \n",
    "def calibrate_hdf(to_process, callback = None, parallel=False):\n",
    "\n",
    "    # TODO Only features are calibrated, not raw MS1 signals.\n",
    "    # What if features are not present?\n",
    "    \n",
    "    try:\n",
    "        index, settings = to_process\n",
    "        file_name = settings['experiment']['file_paths'][index]\n",
    "        base_file_name, ext = os.path.splitext(file_name)\n",
    "        ms_file = base_file_name+\".ms_data.hdf\"\n",
    "        ms_file_ = alphapept.io.MS_Data_File(ms_file, is_overwritable=True)\n",
    "\n",
    "        features = ms_file_.read(dataset_name='features')\n",
    "\n",
    "        try:\n",
    "            psms =  ms_file_.read(dataset_name='first_search')\n",
    "        except KeyError: #no elements in search\n",
    "            psms = pd.DataFrame()\n",
    "\n",
    "        if len(psms) > 0 :\n",
    "            df = score_x_tandem(\n",
    "                psms,\n",
    "                fdr_level=settings[\"search\"][\"peptide_fdr\"],\n",
    "                plot=False,\n",
    "                verbose=False,\n",
    "                **settings[\"search\"]\n",
    "            )\n",
    "            corrected_mass, o_mass_ppm_std = get_calibration(\n",
    "                df,\n",
    "                features,\n",
    "                **settings[\"calibration\"]\n",
    "            )\n",
    "            ms_file_.write(\n",
    "                corrected_mass,\n",
    "                dataset_name=\"corrected_mass\",\n",
    "                group_name=\"features\"\n",
    "            )\n",
    "        else:\n",
    "            \n",
    "            ms_file_.write(\n",
    "                features['mass_matched'],\n",
    "                dataset_name=\"corrected_mass\",\n",
    "                group_name=\"features\"\n",
    "            )\n",
    "            \n",
    "            o_mass_ppm_std = 0\n",
    "                        \n",
    "        ms_file_.write(\n",
    "            o_mass_ppm_std,\n",
    "            dataset_name=\"corrected_mass\",\n",
    "            group_name=\"features\",\n",
    "            attr_name=\"estimated_max_precursor_ppm\"\n",
    "        )\n",
    "        logging.info(f'Calibration of file {ms_file} complete.')\n",
    "        \n",
    "        \n",
    "        # Calibration of fragments\n",
    "        \n",
    "        skip = False\n",
    "        \n",
    "        try:\n",
    "            logging.info(f'Calibrating fragments')\n",
    "            ions = ms_file_.read(dataset_name='ions')\n",
    "        except KeyError:\n",
    "            logging.info('No ions to calibrate fragment masses found')\n",
    "                         \n",
    "            skip = True\n",
    "                         \n",
    "        if not skip:\n",
    "            delta_ppm = ((ions['db_mass'] - ions['ion_mass'])/((ions['db_mass'] + ions['ion_mass'])/2)*1e6).values\n",
    "            median_offset = -np.median(delta_ppm)\n",
    "            std_offset = np.std(delta_ppm)\n",
    "            mass_list_ms2 = ms_file_.read(dataset_name = 'mass_list_ms2', group_name = \"Raw/MS2_scans\")\n",
    "            \n",
    "            try:\n",
    "                offset = ms_file_.read(dataset_name = 'corrected_fragment_mzs')\n",
    "            except KeyError:\n",
    "                offset = np.zeros(len(mass_list_ms2))\n",
    "                \n",
    "            offset += median_offset\n",
    "                         \n",
    "            logging.info(f'Median fragment offset {median_offset:.2f} - std {std_offset:.2f} ppm')\n",
    "\n",
    "            ms_file_.write(\n",
    "                offset,\n",
    "                dataset_name=\"corrected_fragment_mzs\",\n",
    "            )\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f'Calibration of file {ms_file} failed. Exception {e}.')\n",
    "        return f\"{e}\" #Can't return exception object, cast as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calbration with regards to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "import alphapept.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from matplotlib import pyplot as plt\n",
    "import numba\n",
    "import scipy.signal\n",
    "import scipy.interpolate\n",
    "import alphapept.fasta\n",
    "\n",
    "def get_db_targets(\n",
    "    db_file_name,\n",
    "    max_ppm=100,\n",
    "    min_distance=0.5,\n",
    "    ms_level=2,\n",
    "):\n",
    "    if ms_level == 1:\n",
    "        db_mzs_ = alphapept.fasta.read_database(db_file_name, 'precursors')\n",
    "    elif ms_level == 2:\n",
    "        db_mzs_ = alphapept.fasta.read_database(db_file_name, 'fragmasses')\n",
    "    else:\n",
    "        raise ValueError(f\"{ms_level} is not a valid ms level\")\n",
    "    tmp_result = np.bincount(\n",
    "        (\n",
    "            np.log10(\n",
    "                db_mzs_[\n",
    "                    np.isfinite(db_mzs_) & (db_mzs_ > 0)\n",
    "                ].flatten()\n",
    "            ) * 10**6\n",
    "        ).astype(np.int64)\n",
    "    )\n",
    "    db_mz_distribution = np.zeros_like(tmp_result)\n",
    "    for i in range(1, max_ppm):\n",
    "        db_mz_distribution[i:] += tmp_result[:-i]\n",
    "        db_mz_distribution[:-i] += tmp_result[i:]\n",
    "    peaks = scipy.signal.find_peaks(db_mz_distribution, distance=max_ppm)[0]\n",
    "    db_targets = 10 ** (peaks / 10**6)\n",
    "#     db_vals = db_mz_distribution[peaks]\n",
    "#     plt.vlines(db_targets, 0, db_vals)\n",
    "    db_array = np.zeros(int(db_targets[-1]) + 1, dtype=np.float64)\n",
    "    last_int_mz = -1\n",
    "    last_mz = -1\n",
    "    for mz in db_targets:\n",
    "        mz_int = int(mz)\n",
    "        if (mz_int != last_int_mz) & (mz > (last_mz + min_distance)):\n",
    "            db_array[mz_int] = mz\n",
    "        else:\n",
    "            db_array[mz_int] = 0\n",
    "        last_int_mz = mz_int\n",
    "        last_mz = mz\n",
    "    return db_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def align_run_to_db(\n",
    "    ms_data_file_name,\n",
    "    db_array,\n",
    "    max_ppm_distance=1000000,\n",
    "    rt_step_size=0.1,\n",
    "    plot_ppms=False,\n",
    "    ms_level=2,\n",
    "):\n",
    "    ms_data = alphapept.io.MS_Data_File(ms_data_file_name)\n",
    "    if ms_level == 1:\n",
    "        mzs = ms_data.read(dataset_name=\"mass_matched\", group_name=\"features\")\n",
    "        rts = ms_data.read(dataset_name=\"rt_matched\", group_name=\"features\")\n",
    "    elif ms_level == 2:\n",
    "        mzs = ms_data.read(dataset_name=\"Raw/MS2_scans/mass_list_ms2\")\n",
    "        inds = ms_data.read(dataset_name=\"Raw/MS2_scans/indices_ms2\")\n",
    "        precursor_rts = ms_data.read(dataset_name=\"Raw/MS2_scans/rt_list_ms2\")\n",
    "        rts = np.repeat(precursor_rts, np.diff(inds))\n",
    "    else:\n",
    "        raise ValueError(f\"{ms_level} is not a valid ms level\")\n",
    "    \n",
    "    selected = mzs.astype(np.int64)\n",
    "    ds = np.zeros((3, len(selected)))\n",
    "    if len(db_array) < len(selected) + 1:\n",
    "        tmp = np.zeros(len(selected) + 1)\n",
    "        tmp[:len(db_array)] = db_array\n",
    "        db_array = tmp\n",
    "    ds[0] = mzs - db_array[selected - 1]\n",
    "    ds[1] = mzs - db_array[selected]\n",
    "    ds[2] = mzs - db_array[selected + 1]\n",
    "    min_ds = np.take_along_axis(\n",
    "        ds,\n",
    "        np.expand_dims(np.argmin(np.abs(ds), axis=0), axis=0),\n",
    "        axis=0\n",
    "    ).squeeze(axis=0)\n",
    "    ppm_ds = min_ds / mzs * 10**6\n",
    "\n",
    "    selected = np.abs(ppm_ds) < max_ppm_distance\n",
    "    selected &= np.isfinite(rts)\n",
    "    rt_order = np.argsort(rts)\n",
    "    rt_order = rt_order[selected[rt_order]]\n",
    "\n",
    "    \n",
    "    ordered_rt = rts[rt_order]\n",
    "    ordered_ppm = ppm_ds[rt_order]\n",
    "    \n",
    "    rt_idx_break = np.searchsorted(\n",
    "        ordered_rt,\n",
    "        np.arange(ordered_rt[0], ordered_rt[-1], rt_step_size),\n",
    "        \"left\"\n",
    "    )\n",
    "    median_ppms = np.empty(len(rt_idx_break) - 1)\n",
    "    for i in range(len(median_ppms)):\n",
    "        median_ppms[i] = np.median(\n",
    "            ordered_ppm[rt_idx_break[i]: rt_idx_break[i + 1]]\n",
    "        )\n",
    "\n",
    "    if plot_ppms:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(\n",
    "            rt_step_size + np.arange(\n",
    "                ordered_rt[0],\n",
    "                ordered_rt[-1],\n",
    "                rt_step_size\n",
    "            )[:-1],\n",
    "            median_ppms\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "    estimated_errors = scipy.interpolate.griddata(\n",
    "        rt_step_size / 2 + np.arange(\n",
    "            ordered_rt[0],\n",
    "            ordered_rt[-1] - 2 * rt_step_size,\n",
    "            rt_step_size\n",
    "        ),\n",
    "        median_ppms,\n",
    "        rts,\n",
    "        fill_value=0,\n",
    "        method=\"linear\",\n",
    "        rescale=True\n",
    "    )\n",
    "    \n",
    "    estimated_errors[~np.isfinite(estimated_errors)] = 0\n",
    "        \n",
    "    return estimated_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def calibrate_fragments(\n",
    "    db_file_name,\n",
    "    ms_data_file_name,\n",
    "    ms_level=2\n",
    "):\n",
    "    db_array = get_db_targets(\n",
    "        db_file_name,\n",
    "        max_ppm=100,\n",
    "        min_distance=0.5,\n",
    "        ms_level=ms_level,\n",
    "    )\n",
    "    estimated_errors = align_run_to_db(\n",
    "        ms_data_file_name,\n",
    "        db_array=db_array,\n",
    "        ms_level=ms_level,\n",
    "        plot_ppms=False,\n",
    "    )\n",
    "    \n",
    "    ms_file = alphapept.io.MS_Data_File(ms_data_file_name, is_overwritable=True)\n",
    "    ms_file.write(\n",
    "        estimated_errors,\n",
    "        dataset_name=\"corrected_fragment_mzs\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted 12_speed.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted playground.ipynb.\n",
      "Converted Untitled.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphapept]",
   "language": "python",
   "name": "conda-env-alphapept-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
