{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp recalibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recalibration\n",
    "\n",
    "> Functions related to recalibrating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains evertyhing related to recalibration of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalibration after search\n",
    "\n",
    "### Precursor mass calibration\n",
    "\n",
    "Recalibration refers to the computational step where masses are recalibrated after a first search. The identified peptides are used to calculate the deviations of experimental masses to their theoretical masses. After recalibration, a second search with decreased precursor tolerance is performed. \n",
    "\n",
    "The recalibration is largely motivated by the software lock mass paper:\n",
    "\n",
    "[Cox J, Michalski A, Mann M. Software lock mass by two-dimensional minimization of peptide mass errors. J Am Soc Mass Spectrom. 2011;22(8):1373-1380. doi:10.1007/s13361-011-0142-8](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3231580/)\n",
    "\n",
    "Here, mass offsets are piecewise linearly approximated. The positions for approximation need to fulfill a number of criteria (e.g., a minimum number of samples and a minimum distance). The AlphaPept implementation is slightly modified by employing a more general `KNeighborsRegressor`-approach. In brief, the calibration is calculated for each point individually by estimating the deviation from its identified neighbors in n-dimensional space (e.g., retention time, mass, mobility).\n",
    "\n",
    "More specifically, the algorithm consists of the following steps:\n",
    "\n",
    "1. Outlier removal: We remove outliers from the identified peptides by only accepting identifications with a mass offset that is within n (default 3) standard deviations to the mean.\n",
    "2. For each point, we perform a neighbors lookup of the next n (default 100) neighbors. For the neighbor's lookup we need to scale the axis, which is done with a transform function either absolute or relative.\n",
    "3. Next, we perform a regression based on the neighbors to determine the mass offset. The contribution of each neighbor is weighted by their distance.\n",
    "\n",
    "### Fragment mass calibration\n",
    "\n",
    "The fragment mass calibration is based on the identified ions (i.e., b-hits and y-hits). For each hit, we calculate the offset to its theoretical mass. The correction is then applied by taking the median offset in ppm and applying it globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def remove_outliers(\n",
    "    df:  pd.DataFrame,\n",
    "    outlier_std: float) -> pd.DataFrame:\n",
    "    \"\"\"Helper function to remove outliers from a dataframe.\n",
    "    Outliers are removed based on the precursor offset mass (o_mass).\n",
    "    All values within x standard deviations to the median are kept.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe that contains a o_mass_ppm-column.\n",
    "        outlier_std (float): Range of standard deviations to filter outliers\n",
    "\n",
    "    Raises:\n",
    "        ValueError: An error if the column is not present in the dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe w/o outliers.\n",
    "    \"\"\"    \n",
    "\n",
    "    if 'o_mass_ppm' not in df.columns:\n",
    "        raise ValueError(f\"Column o_mass_ppm not in df\")\n",
    "    else:\n",
    "        # Remove outliers for calibration\n",
    "        o_mass_std = np.abs(df['o_mass_ppm'].std())\n",
    "        o_mass_median = df['o_mass_ppm'].median()\n",
    "\n",
    "        df_sub = df.query('o_mass_ppm < @o_mass_median+@outlier_std*@o_mass_std and o_mass_ppm > @o_mass_median-@outlier_std*@o_mass_std').copy()\n",
    "\n",
    "        return df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_remove_outliers():\n",
    "\n",
    "    df = pd.DataFrame({'o_mass_ppm':[1,1,1,1,1,3,5]})\n",
    "\n",
    "    assert remove_outliers(df, 2).values.max() == 3\n",
    "    assert remove_outliers(df, 1).values.max() == 1\n",
    "    \n",
    "test_remove_outliers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def transform(\n",
    "    x:  np.ndarray,\n",
    "    column: str,\n",
    "    scaling_dict: dict) -> np.ndarray:\n",
    "    \"\"\"Helper function to transform an input array for neighbors lookup used for calibration\n",
    "\n",
    "    Note: The scaling_dict stores information about how scaling is applied and is defined in get_calibration\n",
    "\n",
    "    Relative transformation: Compare distances relatively, for mz that is ppm, for mobility %.\n",
    "    Absolute transformation: Compare distance absolute, for RT it is the timedelta.\n",
    "\n",
    "    An example definition is below:\n",
    "\n",
    "    scaling_dict = {}\n",
    "    scaling_dict['mz'] = ('relative', calib_mz_range/1e6)\n",
    "    scaling_dict['rt'] = ('absolute', calib_rt_range)\n",
    "    scaling_dict['mobility'] = ('relative', calib_mob_range)\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input array.\n",
    "        column (str): String to lookup what scaling should be applied.\n",
    "        scaling_dict (dict): Lookup dict to retrieve the scaling operation and factor for the column.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: An error if the column is not present in the dict.\n",
    "        NotImplementedError: An error if the column is not present in the dict.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A scaled array.\n",
    "    \"\"\"\n",
    "    if column not in scaling_dict:\n",
    "        raise KeyError(f\"Column {_} not in scaling_dict\")\n",
    "    else:\n",
    "        type_, scale_ = scaling_dict[column]\n",
    "\n",
    "        if type_ == 'relative':\n",
    "            return np.log(x, out=np.zeros_like(x), where=(x>0))/scale_\n",
    "        elif type_ == 'absolute':\n",
    "            return x/scale_\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Type {type_} not known.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_transform():\n",
    "    \n",
    "    scaling_dict = {'A':('relative', 10), 'B':('absolute', 20)}\n",
    "    \n",
    "    x = np.array([1,2,3,4,5], dtype=float)\n",
    "    \n",
    "    assert np.allclose(transform(x, 'A', scaling_dict), np.log(x)/10)\n",
    "    assert np.allclose(transform(x, 'B', scaling_dict), x/20)\n",
    "    \n",
    "test_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "def kneighbors_calibration(df: pd.DataFrame, features: pd.DataFrame, cols: list, target: str, scaling_dict: dict, calib_n_neighbors: int) -> np.ndarray:\n",
    "    \"\"\"Calibration using a KNeighborsRegressor.\n",
    "    Input arrays from are transformed to be used with a nearest-neighbor approach.\n",
    "    Based on neighboring points a calibration is calculated for each input point.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe that contains identified peptides (w/o outliers).\n",
    "        features (pd.DataFrame): Features dataframe for which the masses are calibrated.\n",
    "        cols (list): List of input columns for the calibration.\n",
    "        target (str): Target column on which offset is calculated.\n",
    "        scaling_dict (dict): A dictionary that contains how scaling operations are applied.\n",
    "        calib_n_neighbors (int): Number of neighbors for calibration.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array with calibrated masses.\n",
    "    \"\"\"    \n",
    "\n",
    "    data = df[cols]\n",
    "    tree_points = data.values\n",
    "\n",
    "    for idx, _ in enumerate(data.columns):\n",
    "        tree_points[:, idx] = transform(tree_points[:, idx], _, scaling_dict)\n",
    "\n",
    "    target_points = features[[_+'_matched' for _ in cols]].values\n",
    "\n",
    "    for idx, _ in enumerate(data.columns):\n",
    "        target_points[:, idx] = transform(target_points[:, idx], _, scaling_dict)\n",
    "\n",
    "    neigh = KNeighborsRegressor(n_neighbors=calib_n_neighbors, weights = 'distance')\n",
    "    neigh.fit(tree_points, df[target].values)\n",
    "\n",
    "    y_hat = neigh.predict(target_points)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_kneighbors_calibration():\n",
    "    \n",
    "    scaling_dict = {'A':('relative', 10), 'B':('absolute', 20)}\n",
    "    df = pd.DataFrame({'o_mass_ppm':[1,1,1,2], 'A':np.array([1,2,3,4],dtype=float)})\n",
    "    features = pd.DataFrame({'mass':[1,1,1,1], 'A_matched':np.array([1,2,3,4],dtype=float)})\n",
    "\n",
    "    cols = ['A']\n",
    "    target = 'o_mass_ppm'\n",
    "    calib_n_neighbors = 3\n",
    "\n",
    "    assert np.allclose(kneighbors_calibration(df, features, cols, target, scaling_dict, calib_n_neighbors), np.array([1,1,1,2]))\n",
    "\n",
    "test_kneighbors_calibration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "\n",
    "def get_calibration(\n",
    "    df: pd.DataFrame,\n",
    "    features:pd.DataFrame,\n",
    "    outlier_std: float = 3,\n",
    "    calib_n_neighbors: int = 100,\n",
    "    calib_mz_range: int = 20,\n",
    "    calib_rt_range: float = 0.5,\n",
    "    calib_mob_range: float = 0.3,\n",
    "    **kwargs) -> (np.ndarray, float):    \n",
    "    \"\"\"Wrapper function to get calibrated values for the precursor mass.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe that contains identified peptides.\n",
    "        features (pd.DataFrame): Features dataframe for which the masses are calibrated.\n",
    "        outlier_std (float, optional): Range in standard deviations for outlier removal. Defaults to 3.\n",
    "        calib_n_neighbors (int, optional): Number of neighbors used for regression. Defaults to 100. \n",
    "        calib_mz_range (int, optional): Scaling factor for mz range. Defaults to 20.\n",
    "        calib_rt_range (float, optional): Scaling factor for rt_range. Defaults to 0.5.\n",
    "        calib_mob_range (float, optional): Scaling factor for mobility range. Defaults to 0.3.\n",
    "        **kwargs: Arbitrary keyword arguments so that settings can be passes as whole.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        corrected_mass (np.ndarray): The calibrated mass\n",
    "        y_hat_std (float): The standard deviation of the precursor offset after calibration\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if len(df) > calib_n_neighbors:\n",
    "\n",
    "        target = 'o_mass_ppm'\n",
    "        cols = ['mz','rt']\n",
    "\n",
    "        if 'mobility' in df.columns:\n",
    "            cols += ['mobility']\n",
    "\n",
    "        scaling_dict = {}\n",
    "        scaling_dict['mz'] = ('relative', calib_mz_range/1e6)\n",
    "        scaling_dict['rt'] = ('absolute', calib_rt_range)\n",
    "        scaling_dict['mobility'] = ('relative', calib_mob_range)\n",
    "\n",
    "        df_sub = remove_outliers(df, outlier_std)\n",
    "        y_hat = kneighbors_calibration(df, features, cols, target, scaling_dict, calib_n_neighbors)\n",
    "\n",
    "        corrected_mass = (1-y_hat/1e6) * features['mass_matched']\n",
    "\n",
    "        y_hat_std = y_hat.std()\n",
    "        return corrected_mass, y_hat_std\n",
    "\n",
    "    else:\n",
    "        logging.info('Not enough data points present. Skipping recalibration.')\n",
    "        return features['mass_matched'], np.abs(df['o_mass_ppm'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_get_calibration():\n",
    "    df = pd.DataFrame({'o_mass_ppm':[0,0,0,0], \n",
    "                       'mz':np.array([10,10,10,10], dtype=float),\n",
    "                       'rt':np.array([1,2,3,4], dtype=float)})\n",
    "\n",
    "    features = pd.DataFrame({'mass_matched':[100,100,100,100], \n",
    "                       'mz_matched':np.array([10,10,10,10], dtype=float),\n",
    "                       'rt_matched':np.array([1,2,3,4], dtype=float)})\n",
    "\n",
    "\n",
    "    corrected_mass, y_hat_std = get_calibration(df, features, calib_n_neighbors=3)\n",
    "\n",
    "    assert np.allclose(corrected_mass.values, np.array([100,100,100,100]))\n",
    "    assert y_hat_std == 0\n",
    "    \n",
    "    \n",
    "    # Test calibration on files\n",
    "    \n",
    "    import alphapept.io\n",
    "\n",
    "    ms_data = alphapept.io.MS_Data_File('../testfiles/test.ms_data.hdf')\n",
    "    features = ms_data.read(dataset_name=\"features\")\n",
    "    df = ms_data.read(dataset_name=\"first_search\")\n",
    "    \n",
    "    corrected_mass, y_hat_std = get_calibration(df, features, calib_n_neighbors = 10)\n",
    "    \n",
    "    assert y_hat_std < df['o_mass_ppm'].std()\n",
    "\n",
    "\n",
    "test_get_calibration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from typing import Union\n",
    "import alphapept.io\n",
    "from alphapept.score import score_x_tandem\n",
    "import os\n",
    "\n",
    "\n",
    "def calibrate_hdf(\n",
    "    to_process: tuple, callback=None, parallel=True) -> Union[str,bool]:\n",
    "    \"\"\"Wrapper function to get calibrate a hdf file when using the parallel executor.\n",
    "    The function loads the respective dataframes from the hdf, calls the calibration function and applies the offset.\n",
    "\n",
    "    Args:\n",
    "        to_process (tuple): Tuple that contains the file index and the settings dictionary.\n",
    "        callback ([type], optional): Placeholder for callback (unused).\n",
    "        parallel (bool, optional): Placeholder for parallel usage (unused).\n",
    "\n",
    "    Returns:\n",
    "        Union[str,bool]: Either True as boolean when calibration is successfull or the Error message as string.\n",
    "    \"\"\"    \n",
    "\n",
    "    try:\n",
    "        index, settings = to_process\n",
    "        file_name = settings['experiment']['file_paths'][index]\n",
    "        base_file_name, ext = os.path.splitext(file_name)\n",
    "        ms_file = base_file_name+\".ms_data.hdf\"\n",
    "        ms_file_ = alphapept.io.MS_Data_File(ms_file, is_overwritable=True)\n",
    "\n",
    "        features = ms_file_.read(dataset_name='features')\n",
    "\n",
    "        try:\n",
    "            psms =  ms_file_.read(dataset_name='first_search')\n",
    "        except KeyError: #no elements in search\n",
    "            psms = pd.DataFrame()\n",
    "\n",
    "        if len(psms) > 0 :\n",
    "            df = score_x_tandem(\n",
    "                psms,\n",
    "                fdr_level=settings[\"search\"][\"peptide_fdr\"],\n",
    "                plot=False,\n",
    "                verbose=False,\n",
    "                **settings[\"search\"]\n",
    "            )\n",
    "            corrected_mass, o_mass_ppm_std = get_calibration(\n",
    "                df,\n",
    "                features,\n",
    "                **settings[\"calibration\"]\n",
    "            )\n",
    "            ms_file_.write(\n",
    "                corrected_mass,\n",
    "                dataset_name=\"corrected_mass\",\n",
    "                group_name=\"features\"\n",
    "            )\n",
    "        else:\n",
    "\n",
    "            ms_file_.write(\n",
    "                features['mass_matched'],\n",
    "                dataset_name=\"corrected_mass\",\n",
    "                group_name=\"features\"\n",
    "            )\n",
    "\n",
    "            o_mass_ppm_std = 0\n",
    "\n",
    "        ms_file_.write(\n",
    "            o_mass_ppm_std,\n",
    "            dataset_name=\"corrected_mass\",\n",
    "            group_name=\"features\",\n",
    "            attr_name=\"estimated_max_precursor_ppm\"\n",
    "        )\n",
    "        logging.info(f'Calibration of file {ms_file} complete.')\n",
    "\n",
    "\n",
    "        # Calibration of fragments\n",
    "\n",
    "        skip = False\n",
    "\n",
    "        try:\n",
    "            logging.info(f'Calibrating fragments')\n",
    "            ions = ms_file_.read(dataset_name='ions')\n",
    "        except KeyError:\n",
    "            logging.info('No ions to calibrate fragment masses found')\n",
    "\n",
    "            skip = True\n",
    "\n",
    "        if not skip:\n",
    "            delta_ppm = ((ions['db_mass'] - ions['ion_mass'])/((ions['db_mass'] + ions['ion_mass'])/2)*1e6).values\n",
    "            median_offset = -np.median(delta_ppm)\n",
    "            std_offset = np.std(delta_ppm)\n",
    "            mass_list_ms2 = ms_file_.read(dataset_name = 'mass_list_ms2', group_name = \"Raw/MS2_scans\")\n",
    "\n",
    "            try:\n",
    "                offset = ms_file_.read(dataset_name = 'corrected_fragment_mzs')\n",
    "            except KeyError:\n",
    "                offset = np.zeros(len(mass_list_ms2))\n",
    "\n",
    "            offset += median_offset\n",
    "\n",
    "            logging.info(f'Median fragment offset {median_offset:.2f} - std {std_offset:.2f} ppm')\n",
    "\n",
    "            ms_file_.write(\n",
    "                offset,\n",
    "                dataset_name=\"corrected_fragment_mzs\",\n",
    "            )\n",
    "            \n",
    "            ms_file_.write(std_offset, dataset_name=\"estimated_max_fragment_ppm\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f'Calibration of file {ms_file} failed. Exception {e}.')\n",
    "        return f\"{e}\" #Can't return exception object, cast as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database calibration\n",
    "\n",
    "Another way to calibrate the fragment and precursor masses is by directly comparing them to a previously generated theoretical mass database. Here, peaks in the distribution of databases are used to align the experimental masses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import scipy.stats\n",
    "import scipy.signal\n",
    "import scipy.interpolate\n",
    "import alphapept.fasta\n",
    "\n",
    "#The following function does not have an own unit test but is run by test_calibrate_fragments.\n",
    "def get_db_targets(\n",
    "    db_file_name: str,\n",
    "    max_ppm: int=100,\n",
    "    min_distance: float=0.5,\n",
    "    ms_level: int=2,\n",
    ") ->np.ndarray:\n",
    "    \"\"\"Function to extract database targets for database-calibration.\n",
    "    Based on the FASTA database it finds masses that occur often. These will be used for calibration.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        db_file_name (str): Path to the database.\n",
    "        max_ppm (int, optional): Maximum distance in ppm between two peaks. Defaults to 100.\n",
    "        min_distance (float, optional): Minimum distance between two calibration peaks. Defaults to 0.5.\n",
    "        ms_level (int, optional): MS-Level used for calibration, either precursors (1) or fragmasses (2). Defaults to 2.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: When ms_level is not valid.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Numpy array with calibration masses.\n",
    "    \"\"\"    \n",
    "\n",
    "    if ms_level == 1:\n",
    "        db_mzs_ = alphapept.fasta.read_database(db_file_name, 'precursors')\n",
    "    elif ms_level == 2:\n",
    "        db_mzs_ = alphapept.fasta.read_database(db_file_name, 'fragmasses')\n",
    "    else:\n",
    "        raise ValueError(f\"{ms_level} is not a valid ms level\")\n",
    "    tmp_result = np.bincount(\n",
    "        (\n",
    "            np.log10(\n",
    "                db_mzs_[\n",
    "                    np.isfinite(db_mzs_) & (db_mzs_ > 0)\n",
    "                ].flatten()\n",
    "            ) * 10**6\n",
    "        ).astype(np.int64)\n",
    "    )\n",
    "    db_mz_distribution = np.zeros_like(tmp_result)\n",
    "    for i in range(1, max_ppm):\n",
    "        db_mz_distribution[i:] += tmp_result[:-i]\n",
    "        db_mz_distribution[:-i] += tmp_result[i:]\n",
    "    peaks = scipy.signal.find_peaks(db_mz_distribution, distance=max_ppm)[0]\n",
    "    db_targets = 10 ** (peaks / 10**6)\n",
    "    db_array = np.zeros(int(db_targets[-1]) + 1, dtype=np.float64)\n",
    "    last_int_mz = -1\n",
    "    last_mz = -1\n",
    "    for mz in db_targets:\n",
    "        mz_int = int(mz)\n",
    "        if (mz_int != last_int_mz) & (mz > (last_mz + min_distance)):\n",
    "            db_array[mz_int] = mz\n",
    "        else:\n",
    "            db_array[mz_int] = 0\n",
    "        last_int_mz = mz_int\n",
    "        last_mz = mz\n",
    "    return db_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#The following function does not have an own unit test but is run by test_calibrate_fragments.\n",
    "def align_run_to_db(\n",
    "    ms_data_file_name: str,\n",
    "    db_array: np.ndarray,\n",
    "    max_ppm_distance: int=1000000,\n",
    "    rt_step_size:float =0.1,\n",
    "    plot_ppms: bool=False,\n",
    "    ms_level: int=2,\n",
    ") ->np.ndarray:\n",
    "    \"\"\"Function align a run to it's theoretical FASTA database.\n",
    "\n",
    "    Args:\n",
    "        ms_data_file_name (str): Path to the run.\n",
    "        db_array (np.ndarray): Numpy array containing the database targets.\n",
    "        max_ppm_distance (int, optional): Maximum distance in ppm. Defaults to 1000000.\n",
    "        rt_step_size (float, optional): Stepsize for rt calibration. Defaults to 0.1.\n",
    "        plot_ppms (bool, optional): Flag to indicate plotting. Defaults to False.\n",
    "        ms_level (int, optional): ms_level for calibration. Defaults to 2.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: When ms_level is not valid.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Estimated errors\n",
    "    \"\"\"\n",
    "\n",
    "    ms_data = alphapept.io.MS_Data_File(ms_data_file_name)\n",
    "    if ms_level == 1:\n",
    "        mzs = ms_data.read(dataset_name=\"mass_matched\", group_name=\"features\")\n",
    "        rts = ms_data.read(dataset_name=\"rt_matched\", group_name=\"features\")\n",
    "    elif ms_level == 2:\n",
    "        mzs = ms_data.read(dataset_name=\"Raw/MS2_scans/mass_list_ms2\")\n",
    "        inds = ms_data.read(dataset_name=\"Raw/MS2_scans/indices_ms2\")\n",
    "        precursor_rts = ms_data.read(dataset_name=\"Raw/MS2_scans/rt_list_ms2\")\n",
    "        rts = np.repeat(precursor_rts, np.diff(inds))\n",
    "    else:\n",
    "        raise ValueError(f\"{ms_level} is not a valid ms level\")\n",
    "\n",
    "    selected = mzs.astype(np.int64)\n",
    "    ds = np.zeros((3, len(selected)))\n",
    "    if len(db_array) < len(selected) + 1:\n",
    "        tmp = np.zeros(len(selected) + 1)\n",
    "        tmp[:len(db_array)] = db_array\n",
    "        db_array = tmp\n",
    "    ds[0] = mzs - db_array[selected - 1]\n",
    "    ds[1] = mzs - db_array[selected]\n",
    "    ds[2] = mzs - db_array[selected + 1]\n",
    "    min_ds = np.take_along_axis(\n",
    "        ds,\n",
    "        np.expand_dims(np.argmin(np.abs(ds), axis=0), axis=0),\n",
    "        axis=0\n",
    "    ).squeeze(axis=0)\n",
    "    ppm_ds = min_ds / mzs * 10**6\n",
    "\n",
    "    selected = np.abs(ppm_ds) < max_ppm_distance\n",
    "    selected &= np.isfinite(rts)\n",
    "    rt_order = np.argsort(rts)\n",
    "    rt_order = rt_order[selected[rt_order]]\n",
    "\n",
    "\n",
    "    ordered_rt = rts[rt_order]\n",
    "    ordered_ppm = ppm_ds[rt_order]\n",
    "\n",
    "    rt_idx_break = np.searchsorted(\n",
    "        ordered_rt,\n",
    "        np.arange(ordered_rt[0], ordered_rt[-1], rt_step_size),\n",
    "        \"left\"\n",
    "    )\n",
    "    median_ppms = np.empty(len(rt_idx_break) - 1)\n",
    "    for i in range(len(median_ppms)):\n",
    "        median_ppms[i] = np.median(\n",
    "            ordered_ppm[rt_idx_break[i]: rt_idx_break[i + 1]]\n",
    "        )\n",
    "\n",
    "    if plot_ppms:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(\n",
    "            rt_step_size + np.arange(\n",
    "                ordered_rt[0],\n",
    "                ordered_rt[-1],\n",
    "                rt_step_size\n",
    "            )[:-1],\n",
    "            median_ppms\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    estimated_errors = scipy.interpolate.griddata(\n",
    "        rt_step_size / 2 + np.arange(\n",
    "            ordered_rt[0],\n",
    "            ordered_rt[-1] - 2 * rt_step_size,\n",
    "            rt_step_size\n",
    "        ),\n",
    "        median_ppms,\n",
    "        rts,\n",
    "        fill_value=0,\n",
    "        method=\"linear\",\n",
    "        rescale=True\n",
    "    )\n",
    "\n",
    "    estimated_errors[~np.isfinite(estimated_errors)] = 0\n",
    "\n",
    "    return estimated_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def calibrate_fragments(\n",
    "    db_file_name: str,\n",
    "    ms_data_file_name: str,\n",
    "    ms_level: int=2,\n",
    "    write = True,\n",
    "    plot_ppms = False,\n",
    "):\n",
    "    \"\"\"Wrapper function to calibrate fragments.\n",
    "    Calibrated values are saved to corrected_fragment_mzs\n",
    "\n",
    "    Args:\n",
    "        db_file_name (str): Path to database\n",
    "        ms_data_file_name (str): Path to ms_data file\n",
    "        ms_level (int, optional): MS-level for calibration. Defaults to 2.\n",
    "        write (bool, optional): Boolean flag for test purposes to avoid writing to testfile. Defaults to True.\n",
    "        plot_ppms (bool, optional):  Boolean flag to plot the calibration. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    db_array = get_db_targets(\n",
    "        db_file_name,\n",
    "        max_ppm=100,\n",
    "        min_distance=0.5,\n",
    "        ms_level=ms_level,\n",
    "    )\n",
    "    estimated_errors = align_run_to_db(\n",
    "        ms_data_file_name,\n",
    "        db_array=db_array,\n",
    "        ms_level=ms_level,\n",
    "        plot_ppms=plot_ppms,\n",
    "    )\n",
    "\n",
    "    if write:\n",
    "        ms_file = alphapept.io.MS_Data_File(ms_data_file_name, is_overwritable=True)\n",
    "        ms_file.write(\n",
    "            estimated_errors,\n",
    "            dataset_name=\"corrected_fragment_mzs\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_calibrate_fragments():\n",
    "    \"\"\"\n",
    "    This tests if the function can be called with the sample files.\n",
    "    TODO: add functional tests\n",
    "    \n",
    "    \"\"\"\n",
    "    calibrate_fragments('../testfiles/database.hdf', '../testfiles/test.ms_data.hdf', write=False)\n",
    "    \n",
    "    \n",
    "test_calibrate_fragments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted 12_performance.ipynb.\n",
      "Converted 13_export.ipynb.\n",
      "Converted additional_code.ipynb.\n",
      "Converted contributing.ipynb.\n",
      "Converted file_formats.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphapept] *",
   "language": "python",
   "name": "conda-env-alphapept-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
