{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search\n",
    "\n",
    "> Functions related to the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all functions related to searching and getting peptide-spectrum-matches (PSMs). When searching, we compare how similar an experimental spectrum is to a theoretical spectrum. As described in the FASTA notebook, we can calculate theoretical fragment masses for a given peptide sequence and get theoretical spectra. Typically, we calculate a database with all possible spectra and compare our experimental data. It could be that the database is too large to be saved on disc; here we keep the database in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import warnings\n",
    "from numba import NumbaPendingDeprecationWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=NumbaPendingDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Fragments\n",
    "\n",
    "To efficiently compare two fragments, we use a pointer based approach. We start with two sorted arrays, the `query_frag` that contains the m/z positions of the query spectrum and the `db_frag` which contains the database fragment that is compared against to. The two pointers compare each m/z position with each other and check wheter they are within a certain tolerance `mtol`. Depending on their delta, either of the pointers is advanced. The function returns an arrray named `hits` that is the same length as the database spectrum and encodes the hit positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "\n",
    "@njit(nogil=True)\n",
    "def compare_frags(query_frag, db_frag, mtol, ppm=False):\n",
    "    \"\"\"\n",
    "    Compare query and database frags and find hits\n",
    "    \"\"\"\n",
    "    q_max = len(query_frag)\n",
    "    d_max = len(db_frag)\n",
    "    hits = np.zeros(d_max, dtype=np.int16)\n",
    "    q, d = 0, 0  # q > query, d > database\n",
    "    while q < q_max and d < d_max:\n",
    "        mass1 = query_frag[q]\n",
    "        mass2 = db_frag[d]\n",
    "        delta_mass = mass1 - mass2\n",
    "\n",
    "        if ppm:\n",
    "            sum_mass = mass1 + mass2\n",
    "            mass_difference = 2 * delta_mass / sum_mass * 1e6\n",
    "        else:\n",
    "            mass_difference = delta_mass\n",
    "\n",
    "        if abs(mass_difference) <= mtol:\n",
    "            hits[d] = q + 1  # Save query position +1 (zero-indexing)\n",
    "            d += 1\n",
    "            q += 1  # Only one query for each db element\n",
    "        elif delta_mass < 0:\n",
    "            q += 1\n",
    "        elif delta_mass > 0:\n",
    "            d += 1\n",
    "\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, 0], dtype=int16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "query_frag = np.array([100, 200, 300, 400])\n",
    "db_frag = np.array([150, 200, 300, 450])\n",
    "\n",
    "# Hits: Query 2 -> Db 2 and Query 3 -> Db 3\n",
    "\n",
    "compare_frags(query_frag, db_frag, mtol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_compare_frags():\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    mtol = 1\n",
    "\n",
    "    db_frag = query_frag.copy()\n",
    "\n",
    "    # Self-Comparison: no of hits should be same as length\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == len(query_frag)\n",
    "\n",
    "    # Self-Comparison: above but in tolerance\n",
    "    hits = compare_frags(query_frag, db_frag + mtol - 0.01, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == len(query_frag)\n",
    "\n",
    "    # Self-Comparison: below but in tolerance\n",
    "    hits = compare_frags(query_frag, db_frag - mtol + 0.01, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == len(query_frag)\n",
    "\n",
    "    # Self-Comparison: above tolerance, no hits\n",
    "    hits = compare_frags(query_frag, db_frag + mtol + 0.01, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 0\n",
    "\n",
    "    # Special case 1: First and last\n",
    "    db_frag = np.array([100, 400])\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 2\n",
    "\n",
    "    # Special case 2: Two queries matching the same DB frag\n",
    "    query_frag = np.array([100, 100.5])\n",
    "    db_frag = np.array([100, 200, 300])\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 1\n",
    "\n",
    "    # Special case 3: Two db frags matching the same query frag\n",
    "    db_frag = np.array([100, 100.5])\n",
    "    query_frag = np.array([100, 200, 300])\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 1\n",
    "    \n",
    "test_compare_frags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows us to easily compare a query spectrum against a theoretical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from alphapept import constants\n",
    "from alphapept.fasta import get_frag_dict, parse\n",
    "import alphapept.io\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "peptide = 'PEPTIDE'\n",
    "\n",
    "# Theoretical Spectrum\n",
    "\n",
    "frag_dict = get_frag_dict(parse(peptide), constants.mass_dict)\n",
    "db_frag = list(frag_dict.values())\n",
    "db_frag.sort()\n",
    "\n",
    "db_int = [100 for _ in db_frag]\n",
    "\n",
    "# Experimental Spectrum, dummy data\n",
    "\n",
    "query_frag = np.array([98.06, 227.10, 263.08, 548.06, 653.31])\n",
    "query_int = np.array([20, 80, 30, 30, 50])\n",
    "\n",
    "hits = compare_frags(query_frag, db_frag, mtol=1)\n",
    "\n",
    "hitpos = hits[hits > 0] - 1\n",
    "hit_x = query_frag[hitpos]\n",
    "hit_y = query_int[hitpos]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "plt.vlines(query_frag, 0, query_int, \"r\", label=\"Query\", alpha=0.5)\n",
    "\n",
    "plt.plot(hit_x, hit_y, \"ro\", label=\"Hit\", alpha=0.5)\n",
    "\n",
    "for _ in frag_dict.keys():\n",
    "    plt.text(frag_dict[_], 104, _, fontsize=12, alpha = 0.8)\n",
    "    \n",
    "plt.title('Theoretical Spectrum for {}'.format(peptide))\n",
    "plt.xlabel('Mass')\n",
    "plt.ylabel('Intensity')\n",
    "plt.legend()\n",
    "plt.ylim([0,110])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Spectra\n",
    "\n",
    "To compare multiple spectra against a database, we first need some helper functions. Per default, AlphaPept calculates in Dalton. To use ppm boundaries, we need the function `ppm_to_dalton` for conversion. \n",
    "\n",
    "To minimize the search space, we typically only compare spectra with precursors in the same mass range as defined by `prec_tol`. To look up the limits for search, we define the function `get_idxs`, which is a wrapper to the fast `searchsorted` method from NumPy.\n",
    "\n",
    "The actual search takes place in `compare_specs_single` and `compare_specs_parallel`, a single-core and multicore method for comparing spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import prange\n",
    "@njit\n",
    "def ppm_to_dalton(mass, prec_tol):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    return mass / 1e6 * prec_tol\n",
    "\n",
    "\n",
    "def get_idxs(db_masses, query_masses, prec_tol, ppm):\n",
    "    \"\"\"\n",
    "    Function to get upper and lower limits to define search range.\n",
    "\n",
    "    \"\"\"\n",
    "    if ppm:\n",
    "        dalton_offset = ppm_to_dalton(query_masses, prec_tol)\n",
    "    else:\n",
    "        dalton_offset = prec_tol\n",
    "\n",
    "    idxs_lower = db_masses.searchsorted(query_masses - dalton_offset, side=\"left\")\n",
    "    idxs_higher = db_masses.searchsorted(query_masses + dalton_offset, side=\"right\")\n",
    "\n",
    "    return idxs_lower, idxs_higher\n",
    "\n",
    "@njit\n",
    "def compare_spectrum(query_idx, idxs_lower, idxs_higher, query_indices, query_frags, query_ints, db_indices, db_frags, best_hits, score, frag_tol, ppm):\n",
    "    \"\"\"\n",
    "    Compares a spectrum and writes to best_his, score array\n",
    "    \"\"\" \n",
    "    idx_low = idxs_lower[query_idx]\n",
    "    idx_high = idxs_higher[query_idx]\n",
    "\n",
    "    query_idx_start = query_indices[query_idx]\n",
    "    query_idx_end = query_indices[query_idx + 1]\n",
    "    query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "    query_int = query_ints[query_idx_start:query_idx_end]\n",
    "\n",
    "    query_int_sum = np.sum(query_int)\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    for db_idx in range(idx_low, idx_high):    \n",
    "        db_idx_start = db_indices[db_idx]\n",
    "        db_idx_end = db_indices[db_idx + 1]\n",
    "        db_frag = db_frags[db_idx_start:db_idx_end]\n",
    "        hits = compare_frags(query_frag, db_frag, frag_tol, ppm)\n",
    "        n_hits = np.sum(hits>0)\n",
    "        if n_hits > 0:\n",
    "            matched_int = np.sum(query_int[hits[hits>0]-1]) / query_int_sum\n",
    "            matches.append((n_hits + matched_int, db_idx))\n",
    "            \n",
    "    matches.sort()\n",
    "    matches = matches[::-1]\n",
    "\n",
    "    len_ = min((len(matches), best_hits.shape[1]))            \n",
    "    best_hits[query_idx, 0:len_] = [_[1] for _ in matches[:len_]]\n",
    "    score[query_idx, 0:len_] = [_[0] for _ in matches[:len_]]\n",
    "    \n",
    "    \n",
    "from alphapept.speed import parallel_compiled_func\n",
    "\n",
    "@parallel_compiled_func(cpu_only=True)\n",
    "def compare_spectrum_parallel(query_idx, query_masses, idxs_lower, idxs_higher, query_indices, query_frags, query_ints, db_indices, db_frags, best_hits, score, frag_tol, ppm):\n",
    "    \"\"\"\n",
    "    Compares a spectrum and writes to best_his, score array\n",
    "    \"\"\" \n",
    "    idx_low = idxs_lower[query_idx]\n",
    "    idx_high = idxs_higher[query_idx]\n",
    "\n",
    "    query_idx_start = query_indices[query_idx]\n",
    "    query_idx_end = query_indices[query_idx + 1]\n",
    "    query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "    query_int = query_ints[query_idx_start:query_idx_end]\n",
    "\n",
    "    query_int_sum = np.sum(query_int)\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    for db_idx in range(idx_low, idx_high):    \n",
    "        db_idx_start = db_indices[db_idx]\n",
    "        db_idx_end = db_indices[db_idx + 1]\n",
    "        db_frag = db_frags[db_idx_start:db_idx_end]\n",
    "        hits = compare_frags(query_frag, db_frag, frag_tol, ppm)\n",
    "        n_hits = np.sum(hits>0)\n",
    "        if n_hits > 0:\n",
    "            matched_int = np.sum(query_int[hits[hits>0]-1]) / query_int_sum\n",
    "            matches.append((n_hits + matched_int, db_idx))\n",
    "            \n",
    "    matches.sort()\n",
    "    matches = matches[::-1]\n",
    "\n",
    "    len_ = min((len(matches), best_hits.shape[1]))            \n",
    "    best_hits[query_idx, 0:len_] = [_[1] for _ in matches[:len_]]\n",
    "    score[query_idx, 0:len_] = [_[0] for _ in matches[:len_]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "from alphapept.fasta import read_database\n",
    "\n",
    "def query_data_to_features(query_data):\n",
    "\n",
    "    # if we dont use the feature finder we extract them from the query data..\n",
    "\n",
    "    query_masses = query_data['prec_mass_list2']\n",
    "    query_mz = query_data['mono_mzs2']\n",
    "    query_rt = query_data['rt_list_ms2']\n",
    "\n",
    "    features = pd.DataFrame(np.array([query_masses, query_mz, query_rt]).T, columns = ['mass_matched', 'mz_matched', 'rt_matched'])\n",
    "\n",
    "    features['feature_idx'] = features.index #Index to query_data\n",
    "    features['query_idx']  = np.arange(len(query_masses))\n",
    "\n",
    "    features = features.sort_values('mass_matched', ascending=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_psms(\n",
    "    query_data,\n",
    "    db_data,\n",
    "    features,\n",
    "    parallel,\n",
    "    frag_tol,\n",
    "    prec_tol,\n",
    "    ppm,\n",
    "    min_frag_hits,\n",
    "    callback = None,\n",
    "    m_offset_calibrated = None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper function to extract psms from dataset\n",
    "\n",
    "    Args:\n",
    "        db_masses: database precursor masses\n",
    "        query_masses: query precursor masses\n",
    "        prec_tol: mass offset in dalton or ppm\n",
    "        ppm: flag for ppm or dalton\n",
    "        callback: Callback function, e.g. for progress bar\n",
    "    Returns:\n",
    "        idxs_lower: lower search range\n",
    "        idxs_higher: upper search range\n",
    "    Raises:\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(db_data, str):\n",
    "        db_masses = read_database(db_data, array_name = 'precursors')\n",
    "        db_frags = read_database(db_data, array_name = 'fragmasses')\n",
    "        db_indices = read_database(db_data, array_name = 'indices')\n",
    "    else:\n",
    "        db_masses = db_data['precursors']\n",
    "        db_frags = db_data['fragmasses']\n",
    "        db_indices = db_data['indices']\n",
    "        \n",
    "    query_indices = query_data[\"indices_ms2\"]\n",
    "    query_frags = query_data['mass_list_ms2']\n",
    "    query_ints = query_data['int_list_ms2']\n",
    "    \n",
    "    if features is not None:\n",
    "        if m_offset_calibrated:\n",
    "            prec_tol = m_offset_calibrated\n",
    "            query_masses = features['corrected_mass'].values\n",
    "        else:\n",
    "            query_masses = features['mass_matched'].values\n",
    "        query_mz = features['mz_matched'].values\n",
    "        query_rt = features['rt_matched'].values\n",
    "        query_selection = features['query_idx'].values\n",
    "        indices = np.zeros(len(query_selection) + 1, np.int64)\n",
    "        indices[1:] = np.diff(query_indices)[query_selection]\n",
    "        indices = np.cumsum(indices)\n",
    "        query_frags = np.concatenate(\n",
    "            [\n",
    "                query_frags[s: e] for s, e in zip(\n",
    "                    query_indices[query_selection], query_indices[query_selection + 1]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        query_ints = np.concatenate(\n",
    "            [\n",
    "                query_ints[s: e] for s, e in zip(\n",
    "                    query_indices[query_selection], query_indices[query_selection + 1]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        query_indices = indices\n",
    "    else:\n",
    "        if m_offset_calibrated:\n",
    "            prec_tol = m_offset_calibrated\n",
    "        \n",
    "        query_masses = query_data['prec_mass_list2']\n",
    "        query_mz = query_data['mono_mzs2']\n",
    "        query_rt = query_data['rt_list_ms2']\n",
    "    \n",
    "    idxs_lower, idxs_higher = get_idxs(\n",
    "        db_masses,\n",
    "        query_masses,\n",
    "        prec_tol,\n",
    "        ppm\n",
    "    )\n",
    "\n",
    "    n_queries = len(query_masses)\n",
    "    n_db = len(db_masses)\n",
    "    top_n = 5\n",
    "    \n",
    "    best_hits = np.zeros((n_queries, top_n))-1\n",
    "    score = np.zeros((n_queries, top_n))\n",
    "    \n",
    "    logging.info(f'Performing search on {n_queries:,} query and {n_db:,} db entries with frag_tol = {frag_tol:.2f} and prec_tol = {prec_tol:.2f}.')\n",
    "    \n",
    "    if False:\n",
    "        for query_idx in range(n_queries):\n",
    "            compare_spectrum(query_idx, idxs_lower, idxs_higher, query_indices, query_frags, query_ints, db_indices, db_frags, best_hits, score, frag_tol, ppm)\n",
    "\n",
    "            if callback is not None:\n",
    "                callback((query_idx+1)/n_queries)\n",
    "                \n",
    "    compare_spectrum_parallel(np.arange(n_queries), idxs_lower, idxs_higher, query_indices, query_frags, query_ints, db_indices, db_frags, best_hits, score, frag_tol, ppm)\n",
    "            \n",
    "    query_idx, db_idx_ = np.where(score > min_frag_hits)\n",
    "    db_idx = best_hits[query_idx, db_idx_]\n",
    "    score_ = score[query_idx, db_idx_]\n",
    "\n",
    "    psms = np.array(\n",
    "        list(zip(query_idx, db_idx, score_)), dtype=[(\"query_idx\", int), (\"db_idx\", int), (\"hits\", float)]\n",
    "    )\n",
    "\n",
    "    logging.info('Found {:,} psms.'.format(len(psms)))\n",
    "\n",
    "    return psms, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting columns for scoring\n",
    "\n",
    "The basic fragment comparison only counts the number of hits when comparing a theoretical spectrum to an experimental one. Based on the number of hits, we can drastically reduce the number of candidates one wants to analyze for an in-depth comparison, which requires additional features. The following section describes several functions which extract parameters to compare spectrum matches better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frag Delta\n",
    "\n",
    "`frag_delta` calculates the the substracts the experimental fragment masses from the theoretical fragment masses for each hit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def frag_delta(query_frag, db_frag, hits):\n",
    "    \"\"\"\n",
    "    Calculate the mass difference for a given array of hits in Dalton and ppm\n",
    "    \"\"\"\n",
    "\n",
    "    delta_m = db_frag[hits > 0] - query_frag[hits[hits > 0] - 1]\n",
    "    delta_m_ppm = (\n",
    "        2 * delta_m / (db_frag[hits > 0] + query_frag[hits[hits > 0] - 1]) * 1e6\n",
    "    )\n",
    "\n",
    "    return delta_m, delta_m_ppm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_frag_delta():\n",
    "    mtol = 10\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([101, 202, 303, 404])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    delta_m, delta_m_ppm = frag_delta(query_frag, db_frag, hits)\n",
    "\n",
    "    assert np.sum(delta_m) == 10\n",
    "    \n",
    "test_frag_delta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def intensity_fraction(query_int, hits):\n",
    "    \"\"\"\n",
    "    Calculate the fraction of matched intensity\n",
    "    \"\"\"\n",
    "    total_intensity = np.sum(query_int)\n",
    "    if total_intensity != 0:\n",
    "        matched_intensity = np.sum(query_int[hits[hits > 0] - 1])\n",
    "        i_frac = matched_intensity / total_intensity\n",
    "    else:\n",
    "        i_frac = 0\n",
    "\n",
    "    return i_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_intensity_fraction():\n",
    "    mtol = 1\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    i_frac = intensity_fraction(query_int, hits)\n",
    "\n",
    "    assert i_frac == 40 / 100\n",
    "\n",
    "test_intensity_fraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def intensity_product(query_int, hits, db_int=None):\n",
    "    \"\"\"\n",
    "    Calculate the dot product of matched query intensity to db intensity\n",
    "    \"\"\"\n",
    "\n",
    "    matched_query_int = query_int[hits[hits > 0] - 1]\n",
    "    if db_int is None:\n",
    "        matched_intensity = np.sum(matched_query_int)\n",
    "    else:\n",
    "        matched_db_int = db_int[hits > 0]\n",
    "        matched_intensity = np.sum(matched_query_int*matched_db_int)\n",
    "\n",
    "    return matched_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_intensity_product():\n",
    "    mtol = 1\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert intensity_product(query_int, hits) == 40\n",
    "\n",
    "    query_frag = np.array([100, 200, 300, 400, 600])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40, 50])\n",
    "    db_int = np.array([10, 20, 30, 40])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert intensity_product(query_int, hits, db_int = db_int) == 10*10+30*20\n",
    "    \n",
    "test_intensity_product()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B & Y - Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def b_y_hits(frag_type, hits):\n",
    "    \"\"\"\n",
    "    Count the number of b and y hits\n",
    "    hits usually start with b-ions > 0, then y-ions < 1\n",
    "    \"\"\"\n",
    "    hits_index = hits > 0\n",
    "\n",
    "    hit_types = frag_type[hits_index]\n",
    "\n",
    "    b_hits = np.sum(hit_types > 0)\n",
    "    y_hits = np.sum(hit_types < 0)\n",
    "\n",
    "    return b_hits, y_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_b_y_hits():\n",
    "    # TODO: Write a test to make sure the by hits are correct..\n",
    "    mtol = 1\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40])\n",
    "    frag_type = np.array([1, -1, 2, -2])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "\n",
    "    b_hit, y_hit = b_y_hits(frag_type, hits)\n",
    "\n",
    "    assert (b_hit) == 1\n",
    "    assert (y_hit) == 1\n",
    "\n",
    "    frag_type = np.array([-1, -2, -3, -4])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "\n",
    "    b_hit, y_hit = b_y_hits(frag_type, hits)\n",
    "    \n",
    "    assert (b_hit) == 0\n",
    "    assert (y_hit) == 2\n",
    "\n",
    "    frag_type = np.array([1, 2, 3, 4])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "\n",
    "    b_hit, y_hit = b_y_hits(frag_type, hits)\n",
    "\n",
    "    assert (b_hit) == 2\n",
    "    assert (y_hit) == 0\n",
    "    \n",
    "test_b_y_hits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting Score columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numpy.lib.recfunctions import append_fields, drop_fields\n",
    "\n",
    "\n",
    "def add_column(recarray, column, name):\n",
    "    \"\"\"\n",
    "    Function to add a column with given name to recarray\n",
    "    \"\"\"\n",
    "    if hasattr(recarray, name):\n",
    "        recarray = drop_fields(recarray, name, usemask=False, asrecarray=True)\n",
    "    recarray = append_fields(\n",
    "        recarray, name, column, dtypes=column.dtype, usemask=False, asrecarray=True\n",
    "    )\n",
    "    return recarray\n",
    "\n",
    "\n",
    "def remove_column(recarray, name):\n",
    "    \"\"\"\n",
    "    Function to remove a column from recarray\n",
    "    \"\"\"\n",
    "    if hasattr(recarray, name):\n",
    "        recarray = drop_fields(recarray, name, usemask=False, asrecarray=True)\n",
    "    return recarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting\n",
    "\n",
    "When performing a database search, we need to know what experimental spectrum we are comparing with which database entry. \n",
    "We distinguish three indices:\n",
    "* query_idx\n",
    "* raw_idx\n",
    "* feature_idx\n",
    "Initially, the get_psms functions accepts experimental data in the form of `query_data`. Here, the `query_idx` refers to the index to `query_data`. However, this might not be the same index as of the raw data. This is due to the implementation of the matching of MS1-features to MS2 spectra. Here we allow multiple matches and implement this by repeating the respective spectra. \n",
    "\n",
    "We then add the two columns `feature_idx` and `raw_idx` to the psms to later be able to distinguish where the match originated from. Here, `raw_idx` refers to the original spectrum.\n",
    "\n",
    "When not applying feature finding, `raw_idx` and `query_idx` are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba.typed import List\n",
    "@njit\n",
    "def get_hits(query_frag, query_int, db_frag, db_int, frag_type, mtol, ppm, losses):\n",
    "    \n",
    "    max_array_size = len(db_frag)*len(losses)\n",
    "\n",
    "    ions = np.zeros((max_array_size, 8))\n",
    "\n",
    "    pointer = 0\n",
    "    \n",
    "    query_range = np.arange(len(query_frag))\n",
    "    db_range = np.arange(len(db_frag))\n",
    "\n",
    "    for idx, off in enumerate(losses):\n",
    "        hits = compare_frags(query_frag, db_frag-off, mtol, ppm)\n",
    "        n_hits = np.sum(hits>0)\n",
    "        \n",
    "        hitpos = hits[hits > 0] - 1\n",
    "        hit = hits > 0\n",
    "        \n",
    "        ions[pointer:pointer+n_hits,0] = frag_type[hits>0] #type\n",
    "        ions[pointer:pointer+n_hits,1] = idx #ion-index\n",
    "        \n",
    "        ions[pointer:pointer+n_hits,2] = query_int[hitpos] #query int\n",
    "        ions[pointer:pointer+n_hits,3] = db_int[hit] #db int\n",
    "        \n",
    "        ions[pointer:pointer+n_hits,4] = query_frag[hitpos] #query mass\n",
    "        ions[pointer:pointer+n_hits,5] = db_frag[hit]-off # db mass  \n",
    "        \n",
    "        ions[pointer:pointer+n_hits,6] = query_range[hitpos] # index to query entry\n",
    "        ions[pointer:pointer+n_hits,7] = db_range[hit] # index to db entry\n",
    "    \n",
    "        pointer += n_hits\n",
    "\n",
    "    ions = ions[:pointer,:]\n",
    "        \n",
    "    return ions\n",
    "\n",
    "@njit\n",
    "def score(\n",
    "    psms,\n",
    "    query_masses,\n",
    "    query_masses_raw,\n",
    "    query_frags,\n",
    "    query_ints,\n",
    "    query_indices,\n",
    "    db_masses,\n",
    "    db_frags,\n",
    "    frag_types,\n",
    "    mtol,\n",
    "    db_indices,\n",
    "    ppm,\n",
    "    psms_dtype,\n",
    "    db_ints = None,\n",
    "    parallel = False\n",
    "):\n",
    "\n",
    "    psms_ = np.zeros(len(psms), dtype=psms_dtype)  \n",
    "    \n",
    "    losses = [0, 18.01056468346, 17.03052] #H2O, NH3\n",
    "    \n",
    "    ions_ = List()\n",
    "    \n",
    "    ion_count = 0\n",
    "    \n",
    "    for i in range(len(psms)):\n",
    "        query_idx = psms[i][\"query_idx\"]\n",
    "        db_idx = psms[i][\"db_idx\"]\n",
    "        query_idx_start = query_indices[query_idx]\n",
    "        query_idx_end = query_indices[query_idx + 1]\n",
    "        query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "        query_int = query_ints[query_idx_start:query_idx_end]\n",
    "        db_frag = db_frags[db_indices[db_idx]:db_indices[db_idx+1]]\n",
    "        frag_type = frag_types[db_indices[db_idx]:db_indices[db_idx+1]]\n",
    "\n",
    "        if db_ints is None:\n",
    "            db_int = np.zeros(len(db_frag))\n",
    "        else:\n",
    "            db_int = db_ints[i]\n",
    "\n",
    "        ions = get_hits(query_frag, query_int, db_frag, db_int, frag_type, mtol, ppm, losses)\n",
    "\n",
    "        psms_['o_mass'][i] = query_masses[query_idx] - db_masses[db_idx]\n",
    "        psms_['o_mass_ppm'][i] = 2 * psms_['o_mass'][i] / (query_masses[query_idx]  + db_masses[db_idx] ) * 1e6\n",
    "        \n",
    "        psms_['o_mass_raw'][i] = query_masses_raw[query_idx] - db_masses[db_idx]\n",
    "        psms_['o_mass_ppm_raw'][i] = 2 * psms_['o_mass'][i] / (query_masses_raw[query_idx]  + db_masses[db_idx] ) * 1e6\n",
    "\n",
    "        psms_['delta_m'][i] = np.mean(ions[:,4]-ions[:,5])\n",
    "        psms_['delta_m_ppm'][i] = np.mean(2 * psms_['delta_m'][i] / (ions[:,4]  + ions[:,5] ) * 1e6)\n",
    "\n",
    "        psms_['total_int'][i] = np.sum(query_int)\n",
    "        psms_['matched_int'][i] = np.sum(ions[:,2])\n",
    "        psms_['matched_int_ratio'][i] = psms_['matched_int'][i] / psms_['total_int'][i]\n",
    "        psms_['int_ratio'][i] = np.mean(ions[:,3]/ions[:,2])\n",
    "        \n",
    "        psms_['b_hits'][i] = np.sum(ions[ions[:,1]==0][:,0]>0)\n",
    "        psms_['y_hits'][i] = np.sum(ions[ions[:,1]==0][:,0]<0)\n",
    "        \n",
    "        psms_['b-H2O_hits'][i] = np.sum(ions[ions[:,1]==1][:,0]>0)\n",
    "        psms_['y-H2O_hits'][i] = np.sum(ions[ions[:,1]==1][:,0]<0)\n",
    "        \n",
    "        psms_['b-NH3_hits'][i] = np.sum(ions[ions[:,1]==2][:,0]>0)\n",
    "        psms_['y-NH3_hits'][i] = np.sum(ions[ions[:,1]==2][:,0]<0)\n",
    "        \n",
    "        n_ions = len(ions)\n",
    "        \n",
    "        psms_['n_ions'][i] = n_ions\n",
    "        psms_['ion_idx'][i] = ion_count\n",
    "        \n",
    "        ion_count += n_ions\n",
    "        ions_.append(ions)\n",
    "        \n",
    "    return psms_, ions_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from numba.typed import Dict\n",
    "def get_sequences(psms, db_seqs):\n",
    "    \"\"\"\n",
    "    Get sequences to add them to a recarray\n",
    "    \"\"\"\n",
    "    sequence_list = db_seqs[psms[\"db_idx\"]]\n",
    "\n",
    "    return sequence_list\n",
    "\n",
    "\n",
    "def get_score_columns(\n",
    "    psms,\n",
    "    query_data,\n",
    "    db_data,\n",
    "    features,\n",
    "    parallel,\n",
    "    frag_tol,\n",
    "    prec_tol,\n",
    "    ppm,\n",
    "    m_offset_calibrated=None,\n",
    "    **kwargs\n",
    "):\n",
    "    logging.info('Extracting columns for scoring.')\n",
    "    query_indices = query_data[\"indices_ms2\"]\n",
    "    query_charges = query_data['charge2']\n",
    "    query_frags = query_data['mass_list_ms2']\n",
    "    query_ints = query_data['int_list_ms2']\n",
    "    query_scans = query_data['scan_list_ms2']\n",
    "\n",
    "    if 'prec_id2' in query_data.keys():\n",
    "        bruker = True\n",
    "        query_prec_id = query_data['prec_id2']\n",
    "    else:\n",
    "        bruker = False\n",
    "\n",
    "    if isinstance(db_data, str):\n",
    "        db_masses = read_database(db_data, array_name = 'precursors')\n",
    "        db_frags = read_database(db_data, array_name = 'fragmasses')\n",
    "        db_indices = read_database(db_data, array_name = 'indices')\n",
    "        frag_types = read_database(db_data, array_name = 'fragtypes')\n",
    "\n",
    "        try:\n",
    "            db_ints = read_database(db_data, array_name = 'db_ints')\n",
    "        except KeyError:\n",
    "            db_ints = None\n",
    "                \n",
    "    else:\n",
    "        db_masses = db_data['precursors']\n",
    "        db_frags = db_data['fragmasses']\n",
    "        db_indices = db_data['indices']\n",
    "        frag_types = db_data['fragtypes']\n",
    "\n",
    "        if 'db_ints' in db_data.keys():\n",
    "            db_ints = db_data['db_ints']\n",
    "        else:\n",
    "            db_ints = None\n",
    "\n",
    "    if features is not None:\n",
    "        if m_offset_calibrated:\n",
    "            query_masses = features['corrected_mass'].values\n",
    "        else:\n",
    "            query_masses = features['mass_matched'].values\n",
    "            \n",
    "        query_masses_raw = features['mass_matched'].values\n",
    "        \n",
    "        query_mz = features['mz_matched'].values\n",
    "        query_rt = features['rt_matched'].values\n",
    "        query_charges = query_charges[features['query_idx'].values]\n",
    "        query_scans = query_scans[features['query_idx'].values]\n",
    "\n",
    "        if bruker:\n",
    "            query_prec_id = query_prec_id[features['query_idx'].values]\n",
    "\n",
    "        query_selection = features['query_idx'].values\n",
    "        indices = np.zeros(len(query_selection) + 1, np.int64)\n",
    "        indices[1:] = np.diff(query_indices)[query_selection]\n",
    "        indices = np.cumsum(indices)\n",
    "        query_frags = np.concatenate(\n",
    "            [\n",
    "                query_frags[s: e] for s, e in zip(\n",
    "                    query_indices[query_selection], query_indices[query_selection + 1]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        query_ints = np.concatenate(\n",
    "            [\n",
    "                query_ints[s: e] for s, e in zip(\n",
    "                    query_indices[query_selection], query_indices[query_selection + 1]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        query_indices = indices\n",
    "    else:\n",
    "        #TODO: This code is outdated, callin with features = None will crash.\n",
    "        query_masses = query_data['prec_mass_list2']\n",
    "        query_masses_raw = query_data['prec_mass_list2']\n",
    "        query_mz = query_data['mono_mzs2']\n",
    "        query_rt = query_data['rt_list_ms2']\n",
    "\n",
    "\n",
    "    loss_dict = Dict()\n",
    "    loss_dict[''] = 0.0\n",
    "    loss_dict['-H2O'] = 18.01056468346\n",
    "    loss_dict['-NH3'] = 17.03052\n",
    "\n",
    "    float_fields = ['o_mass', 'o_mass_ppm', 'o_mass_raw','o_mass_ppm_raw','delta_m','delta_m_ppm','matched_int_ratio','int_ratio']\n",
    "    int_fields = ['total_int','matched_int','n_ions','ion_idx'] + [a+_+'_hits' for _ in loss_dict for a in ['b','y']]\n",
    "\n",
    "    psms_dtype = np.dtype([(_,np.float32) for _ in float_fields] + [(_,np.int64) for _ in int_fields])\n",
    "\n",
    "    psms_, ions_,  = score(\n",
    "        psms,\n",
    "        query_masses,\n",
    "        query_masses_raw,\n",
    "        query_frags,\n",
    "        query_ints,\n",
    "        query_indices,\n",
    "        db_masses,\n",
    "        db_frags,\n",
    "        frag_types,\n",
    "        frag_tol,\n",
    "        db_indices,\n",
    "        ppm,\n",
    "        psms_dtype)\n",
    "\n",
    "    ions_ = np.vstack(ions_)\n",
    "\n",
    "    for _ in psms_.dtype.names:\n",
    "        psms = add_column(psms, psms_[_], _)\n",
    "\n",
    "    rts = np.array(query_rt)[psms[\"query_idx\"]]\n",
    "    psms = add_column(psms, rts, 'rt')\n",
    "\n",
    "    if isinstance(db_data, str):\n",
    "        db_seqs = read_database(db_data, array_name = 'seqs').astype(str)\n",
    "    else:\n",
    "        db_seqs = db_data['seqs']\n",
    "\n",
    "    seqs = get_sequences(psms, db_seqs)\n",
    "\n",
    "    del db_seqs\n",
    "\n",
    "    psms = add_column(psms, seqs, \"sequence\")\n",
    "\n",
    "    mass = np.array(query_masses)[psms[\"query_idx\"]]\n",
    "    mz = np.array(query_mz)[psms[\"query_idx\"]]\n",
    "    charge = np.array(query_charges)[psms[\"query_idx\"]]\n",
    "\n",
    "    psms = add_column(psms, mass, \"mass\")\n",
    "    psms = add_column(psms, mz, \"mz\")\n",
    "    psms = add_column(psms, charge, \"charge\")\n",
    "\n",
    "    psms = add_column(psms, np.char.add(np.char.add(psms['sequence'],\"_\"), psms['charge'].astype(int).astype(str)), 'precursor')\n",
    "\n",
    "    if features is not None:\n",
    "        psms = add_column(psms, features.loc[psms['query_idx']]['feature_idx'].values, 'feature_idx')\n",
    "        psms = add_column(psms, features.loc[psms['query_idx']]['query_idx'].values, 'raw_idx')\n",
    "\n",
    "        for key in ['int_sum','int_apex','rt_start','rt_apex','rt_end','fwhm','dist','mobility']:\n",
    "            if key in features.keys():\n",
    "                psms = add_column(psms, features.loc[psms['query_idx']][key].values, key)\n",
    "\n",
    "    scan_no = np.array(query_scans)[psms[\"query_idx\"]]\n",
    "    if bruker:\n",
    "        psms = add_column(psms, scan_no, \"parent\")\n",
    "        psms = add_column(psms, np.array(query_prec_id)[psms[\"query_idx\"]], 'precursor_idx')\n",
    "        psms = add_column(psms, psms['feature_idx']+1, 'feature_id') #Bruker\n",
    "    else:\n",
    "        psms = add_column(psms, scan_no, \"scan_no\")\n",
    "\n",
    "    logging.info(f'Extracted columns from {len(psms):,} spectra.')\n",
    "\n",
    "    return psms, ions_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def plot_hit(\n",
    "    df,\n",
    "    index,\n",
    "    db_indices,\n",
    "    db_frags,\n",
    "    frag_types,\n",
    "    query_frags,\n",
    "    query_ints,\n",
    "    query_indices,\n",
    "    ppm,\n",
    "    frag_tol,\n",
    "    db_ints = None,\n",
    "    **kwargs\n",
    "):\n",
    "    spectrum = df.iloc[index]\n",
    "\n",
    "    sequence = spectrum[\"sequence\"]\n",
    "\n",
    "    db_idx = spectrum[\"db_idx\"]\n",
    "    query_idx = spectrum[\"query_idx\"]\n",
    "\n",
    "    intensity_fraction = spectrum[\"matched_int\"] / spectrum[\"total_int\"]\n",
    "\n",
    "    db_bound = db_indices[db_idx]\n",
    "    db_frag = db_frags[:, db_idx] [:db_bound]\n",
    "    if db_ints is not None:\n",
    "        db_int = db_ints[:, db_idx] [:db_bound]\n",
    "    else:\n",
    "        db_int = np.ones(len(db_frag))\n",
    "\n",
    "    db_int = db_int / np.max(db_int) * 100\n",
    "\n",
    "    frag_type = frag_types[:, db_idx] [:db_bound]\n",
    "\n",
    "    query_idx_start = query_indices[query_idx]\n",
    "    query_idx_end = query_indices[query_idx + 1]\n",
    "    query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "    query_int = query_ints[query_idx_start:query_idx_end]\n",
    "\n",
    "    query_int = query_int / np.max(query_int) * 100\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, frag_tol, ppm)\n",
    "\n",
    "    n_hits = np.sum(hits > 0)\n",
    "\n",
    "    hitpos = hits[hits > 0] - 1\n",
    "\n",
    "    hit_x = query_frag[hitpos]\n",
    "    hit_y = query_int[hitpos]\n",
    "\n",
    "    # create an axis\n",
    "    ax = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "\n",
    "    plt.vlines(query_frag, 0, query_int, \"r\", label=\"Query\", alpha=0.5)\n",
    "\n",
    "    plt.plot(hit_x, hit_y, \"ro\", label=\"Hit\", alpha=0.5)\n",
    "\n",
    "    figure_title = \"PSM Match for Spectra: {} - sequence {} \\nHits {} - Intensity Fraction {:.2f} %\".format(\n",
    "        query_idx, sequence, n_hits, intensity_fraction * 100\n",
    "    )\n",
    "    plt.title(figure_title)\n",
    "\n",
    "    plt.xlabel(\"Mass\")\n",
    "    plt.ylabel(\"Relative Intensity (%)\")\n",
    "    plt.ylim([0, 110])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "from alphapept.fasta import get_frag_dict, parse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_psms(query_data, df, index, mass_dict, ppm=True, frag_tol=20):\n",
    "    \"\"\"\n",
    "    Plot a psms\n",
    "    \"\"\"\n",
    "    spectrum = df.iloc[index]\n",
    "\n",
    "    sequence = spectrum[\"sequence\"]\n",
    "    db_idx = spectrum[\"db_idx\"]\n",
    "    query_idx = spectrum[\"query_idx\"]\n",
    "\n",
    "    if 'matched_int' in spectrum.index:\n",
    "        intensity_fraction = spectrum[\"matched_int\"] / spectrum[\"total_int\"]\n",
    "    else:\n",
    "        intensity_fraction = np.nan\n",
    "        matched_int = np.nan\n",
    "\n",
    "    frag_dict = get_frag_dict(parse(sequence), mass_dict)\n",
    "    frag_dict_r = {v: k for k, v in frag_dict.items()}\n",
    "\n",
    "    db_frag = list(frag_dict.values())\n",
    "    db_frag.sort()\n",
    "\n",
    "    db_int = [100 for _ in db_frag]\n",
    "\n",
    "    query_frags = query_data['mass_list_ms2']\n",
    "    query_ints = query_data['int_list_ms2']\n",
    "\n",
    "    query_idx_start = query_indices[query_idx]\n",
    "    query_idx_end = query_indices[query_idx + 1]\n",
    "    query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "    query_int = query_ints[query_idx_start:query_idx_end]\n",
    "\n",
    "    query_int = query_int / np.max(query_int) * 100\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, frag_tol, ppm)\n",
    "\n",
    "    n_hits = np.sum(hits > 0)\n",
    "\n",
    "    hitpos = hits[hits > 0] - 1\n",
    "\n",
    "    hit_x = query_frag[hitpos]\n",
    "    hit_y = query_int[hitpos]\n",
    "\n",
    "    # create an axis\n",
    "    ax = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "\n",
    "    plt.vlines(query_frag, 0, query_int, \"r\", label=\"Query\", alpha=0.5)\n",
    "\n",
    "    plt.plot(hit_x, hit_y, \"ro\", label=\"Hit\", alpha=0.5)\n",
    "\n",
    "    figure_title = \"Peptide-Spectrum-Match for Spectra: {} - sequence {} \\nHits {} - Intensity Fraction {:.2f} %\".format(\n",
    "        query_idx, sequence, n_hits, intensity_fraction * 100\n",
    "    )\n",
    "\n",
    "    db_hits = np.array(db_frag)[hits>0]\n",
    "    ion_hits = [frag_dict_r[_] for _ in db_hits]\n",
    "\n",
    "    for _ in frag_dict.keys():\n",
    "\n",
    "        if _ in ion_hits:\n",
    "            color = 'r'\n",
    "        else:\n",
    "            color = 'k'\n",
    "\n",
    "        if _[0] == 'y':\n",
    "            plt.text(frag_dict[_], 110, _, fontsize=12, alpha = 0.8, color=color)\n",
    "        else:\n",
    "            plt.text(frag_dict[_], 104, _, fontsize=12, alpha = 0.8, color=color)\n",
    "\n",
    "    plt.title(figure_title)\n",
    "\n",
    "    plt.xlabel(\"Mass\")\n",
    "    plt.ylabel(\"Relative Intensity (%)\")\n",
    "    plt.ylim([0, 120])\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def perform_search(query_files, db_masses, db_frags, db_indices, db_seqs, frag_types, plot, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to search and score one or multiple MS runs by the X!Tandem approach.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(query_files, str):\n",
    "        kwargs['query_path'] = query_files\n",
    "        psms_all = score_psms(db_masses, db_frags, db_indices, db_seqs, frag_types, plot=plot, **kwargs)\n",
    "        psms_all['filename'] = query_files\n",
    "    elif isinstance(query_files, list):\n",
    "        psms_all = []\n",
    "        for file in query_files:\n",
    "            kwargs['query_path'] = file\n",
    "            psms = score_psms(db_masses, db_frags, db_indices, db_seqs, frag_types, plot=plot, **kwargs)\n",
    "            psms['filename'] = file\n",
    "            psms_all.append(psms)\n",
    "        psms_all = pd.concat(psms_all, ignore_index=True)\n",
    "    else:\n",
    "        raise Exception('query_files should be either a string or a list. The selected query_files argument is of type: {}'.format(type(query_files)))\n",
    "    return psms_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching with database\n",
    "\n",
    "We save intermediate results to hdf5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "import alphapept.io\n",
    "import alphapept.fasta\n",
    "\n",
    "def store_hdf(df, path, key, replace=False, swmr = False):\n",
    "    \"\"\"\n",
    "    Stores in hdf\n",
    "    \"\"\"\n",
    "    ms_file = alphapept.io.MS_Data_File(path.file_name, is_overwritable=True)\n",
    "    \n",
    "    if replace:\n",
    "        ms_file.write(df, dataset_name=key, swmr = swmr)\n",
    "    else:\n",
    "        try:\n",
    "            df.to_hdf(path, key=key, append=True)\n",
    "            #TODO, append is not implemented yet\n",
    "        except (ValueError, AttributeError):\n",
    "            try:\n",
    "                old_df = ms_file.read(dataset_name=key, swmr = swmr)\n",
    "                new_df = pd.concat([old_df, df])\n",
    "                ms_file.write(new_df, dataset_name=key, swmr = swmr)\n",
    "            except KeyError: # File is created new\n",
    "                ms_file.write(df, dataset_name=key, swmr = swmr)\n",
    "    \n",
    "    \n",
    "def search_db(to_process, callback = None, parallel=False, first_search = True):\n",
    "    \"\"\"\n",
    "    Perform a databse search. One file at a time.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        index, settings = to_process\n",
    "        file_name = settings['experiment']['file_paths'][index]\n",
    "        base_file_name, ext = os.path.splitext(file_name)\n",
    "        ms_file = base_file_name+\".ms_data.hdf\"\n",
    "\n",
    "        skip = False\n",
    "        feature_calibration = False\n",
    "        \n",
    "        ms_file_ = alphapept.io.MS_Data_File(\n",
    "            f\"{ms_file}\"\n",
    "        )\n",
    "        \n",
    "        if not first_search:\n",
    "            try:\n",
    "                calibration = float(ms_file_.read(group_name = 'features', dataset_name='corrected_mass', attr_name='estimated_max_precursor_ppm'))\n",
    "                if calibration == 0:\n",
    "                    logging.info('Calibration is 0, skipping second database search.')\n",
    "                    skip = True\n",
    "\n",
    "                else:\n",
    "                    settings['search']['m_offset_calibrated'] = calibration*settings['search']['calibration_std']\n",
    "                    calib = settings['search']['m_offset_calibrated']\n",
    "                    logging.info(f\"Found calibrated prec_tol with value {calib:.2f}\")\n",
    "            except KeyError as e:\n",
    "                logging.info(f'{e}')\n",
    "        \n",
    "    \n",
    "        if not skip:\n",
    "            db_data_path = settings['experiment']['database_path']\n",
    "\n",
    "    #         TODO calibrated_fragments should be included in settings\n",
    "            query_data = ms_file_.read_DDA_query_data(\n",
    "                calibrated_fragments=True,\n",
    "                database_file_name=settings['experiment']['database_path']\n",
    "            )\n",
    "\n",
    "            features = ms_file_.read(dataset_name=\"features\")\n",
    "\n",
    "            psms, num_specs_compared = get_psms(query_data, db_data_path, features, **settings[\"search\"])\n",
    "            if len(psms) > 0:\n",
    "                psms, ions = get_score_columns(psms, query_data, db_data_path, features, **settings[\"search\"])\n",
    "\n",
    "                if first_search:\n",
    "                    logging.info('Saving first_search results to {}'.format(ms_file))\n",
    "                    save_field = 'first_search'\n",
    "                else:\n",
    "                    logging.info('Saving second_search results to {}'.format(ms_file))\n",
    "                    save_field = 'second_search'\n",
    "\n",
    "                store_hdf(pd.DataFrame(psms), ms_file_, save_field, replace=True)\n",
    "                ion_columns = ['ion_index','ion_type','ion_int','db_int','ion_mass','db_mass','query_idx','db_idx']\n",
    "                store_hdf(pd.DataFrame(ions, columns = ion_columns), ms_file_, 'ions', replace=True)\n",
    "            else:\n",
    "                logging.info('No psms found.')\n",
    "            \n",
    "        logging.info(f'Search of file {file_name} complete.')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f'Search of file {file_name} failed. Exception {e}.')\n",
    "        return f\"{e}\" #Can't return exception object, cast as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Large Fasta and or Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from alphapept.fasta import blocks, generate_peptides, add_to_pept_dict\n",
    "from alphapept.io import list_to_numpy_f32\n",
    "from alphapept.fasta import block_idx, generate_fasta_list, generate_spectra, check_peptide\n",
    "from alphapept import constants\n",
    "mass_dict = constants.mass_dict\n",
    "import os\n",
    "import alphapept.speed\n",
    "\n",
    "def search_fasta_block(to_process):\n",
    "    \"\"\"\n",
    "    Search fasta block\n",
    "    For searches with big fasta files or unspecific searches\n",
    "    \"\"\"\n",
    "\n",
    "    fasta_index, fasta_block, ms_files, settings = to_process\n",
    "\n",
    "    settings_ = settings[0]\n",
    "    spectra_block = settings_['fasta']['spectra_block']\n",
    "    to_add = List()\n",
    "\n",
    "    psms_container = [list() for _ in ms_files]\n",
    "\n",
    "    f_index = 0\n",
    "\n",
    "    pept_dict = {}\n",
    "    for element in fasta_block:\n",
    "        sequence = element[\"sequence\"]\n",
    "        mod_peptides = generate_peptides(sequence, **settings_['fasta'])\n",
    "        \n",
    "        pept_dict, added_peptides = add_to_pept_dict(pept_dict, mod_peptides, fasta_index+f_index)\n",
    "\n",
    "        if len(added_peptides) > 0:\n",
    "            to_add.extend(added_peptides)\n",
    "            \n",
    "        f_index += 1\n",
    "        \n",
    "\n",
    "    if len(to_add) > 0:\n",
    "        for seq_block in blocks(to_add, spectra_block):\n",
    "                    \n",
    "            spectra = generate_spectra(seq_block, mass_dict)\n",
    "            \n",
    "            precmasses, seqs, fragmasses, fragtypes = zip(*spectra)\n",
    "            sortindex = np.argsort(precmasses)\n",
    "                        \n",
    "            fragmasses = np.array(fragmasses, dtype=object)[sortindex]\n",
    "            fragtypes = np.array(fragtypes, dtype=object)[sortindex]\n",
    "\n",
    "            lens = [len(_) for _ in fragmasses]\n",
    "\n",
    "            n_frags = sum(lens)\n",
    "            \n",
    "            \n",
    "            frags = np.zeros(n_frags, dtype=fragmasses[0].dtype)\n",
    "            frag_types = np.zeros(n_frags, dtype=fragtypes[0].dtype)\n",
    "\n",
    "            indices = np.zeros(len(lens) + 1, np.int64)\n",
    "            indices[1:] = lens\n",
    "            indices = np.cumsum(indices)\n",
    "\n",
    "            #Fill data\n",
    "\n",
    "            for _ in range(len(indices)-1):\n",
    "                start = indices[_]\n",
    "                end = indices[_+1]\n",
    "                frags[start:end] = fragmasses[_]\n",
    "                frag_types[start:end] = fragtypes[_]\n",
    "\n",
    "            db_data = {}\n",
    "\n",
    "            db_data[\"precursors\"] = np.array(precmasses)[sortindex]\n",
    "            db_data[\"seqs\"] = np.array(seqs)[sortindex]\n",
    "\n",
    "            db_data[\"fragmasses\"] = frags\n",
    "            db_data[\"fragtypes\"] = frag_types\n",
    "            db_data[\"indices\"] = indices\n",
    "\n",
    "            for file_idx, ms_file in enumerate(ms_files):\n",
    "                query_data = alphapept.io.MS_Data_File(\n",
    "                    f\"{ms_file}\"\n",
    "                ).read_DDA_query_data(swmr=True)\n",
    "\n",
    "                try:\n",
    "                    features = alphapept.io.MS_Data_File(\n",
    "                        ms_file\n",
    "                    ).read(dataset_name=\"features\",swmr=True)\n",
    "                except FileNotFoundError:\n",
    "                    features = None\n",
    "                except KeyError:\n",
    "                    features = None\n",
    "                \n",
    "                psms, num_specs_compared = get_psms(query_data, db_data, features, **settings[file_idx][\"search\"])\n",
    "\n",
    "                if len(psms) > 0:\n",
    "                    #This could be speed up..\n",
    "                    psms, ions = get_score_columns(psms, query_data, db_data, features, **settings[file_idx][\"search\"])\n",
    "\n",
    "                    fasta_indices = [set(x for x in pept_dict[_]) for _ in psms['sequence']]\n",
    "\n",
    "                    psms_df = pd.DataFrame(psms)\n",
    "                    psms_df['fasta_index'] = fasta_indices\n",
    "\n",
    "                    psms_container[file_idx].append(psms_df)\n",
    "\n",
    "    return psms_container, len(to_add)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def filter_top_n(temp, top_n = 10):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and keeps only the top n entries. \n",
    "    Combines fasta indices for sequences.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    pept_dict_ = {}\n",
    "\n",
    "    for k, v in temp[['sequence','fasta_index']].values:\n",
    "        if k in pept_dict_:\n",
    "            new_set = pept_dict_[k]\n",
    "            if isinstance(v, set):\n",
    "                new_set.update(v)\n",
    "            else:\n",
    "                new_set.add(v) \n",
    "            pept_dict_[k] = new_set\n",
    "        else:\n",
    "            pept_dict_[k] = set(v)\n",
    "\n",
    "    temp['fasta_index'] = [pept_dict_[_] for _ in temp['sequence']]\n",
    "    temp = temp.drop_duplicates(subset = ['raw_idx','sequence','hits','feature_idx'])\n",
    "    temp = temp.sort_values('hits', ascending = False).groupby('raw_idx').head(top_n)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "def search_parallel(settings, calibration = None, callback = None):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    fasta_list, fasta_dict = generate_fasta_list(fasta_paths = settings['experiment']['fasta_paths'], **settings['fasta'])\n",
    "\n",
    "    fasta_block = settings['fasta']['fasta_block']\n",
    "\n",
    "    ms_file_path = []\n",
    "\n",
    "    for _ in settings['experiment']['file_paths']:\n",
    "        base, ext = os.path.splitext(_)\n",
    "        ms_file_path.append(base + '.ms_data.hdf')\n",
    "\n",
    "    if calibration:\n",
    "        custom_settings = []\n",
    "        for _ in calibration:\n",
    "            settings_ = copy.deepcopy(settings)\n",
    "            settings_[\"search\"][\"m_offset_calibrated\"] = _\n",
    "            custom_settings.append(settings_)\n",
    "    else:\n",
    "        custom_settings = [settings for _ in ms_file_path]\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Number of FASTA entries: {len(fasta_list):,} - FASTA settings {settings['fasta']}\")\n",
    "    to_process = [(idx_start, fasta_list[idx_start:idx_end], ms_file_path, custom_settings) for idx_start, idx_end in block_idx(len(fasta_list), fasta_block)]\n",
    "\n",
    "    n_processes = settings['general']['n_processes']\n",
    "\n",
    "    n_seqs_ = 0\n",
    "    \n",
    "    for _ in ms_file_path:\n",
    "        ms_file = alphapept.io.MS_Data_File(_+'_', is_new_file=True) #Create temporary files for writing\n",
    "        \n",
    "    df_cache = {} \n",
    "                \n",
    "    with alphapept.speed.AlphaPool(n_processes) as p:\n",
    "        max_ = len(to_process)\n",
    "\n",
    "        for i, (_, n_seqs) in enumerate(p.imap_unordered(search_fasta_block, to_process)):\n",
    "            n_seqs_ += n_seqs\n",
    "\n",
    "\n",
    "            logging.info(f'Block {i+1} of {max_} complete - {((i+1)/max_*100):.2f} % - created peptides {n_seqs:,} ')\n",
    "            for j in range(len(_)): #Temporary hdf files for avoiding saving issues\n",
    "                output = [_ for _ in _[j]]\n",
    "                if len(output) > 0:  \n",
    "\n",
    "                    psms = pd.concat(output)\n",
    "\n",
    "                    if ms_file_path[j] in df_cache:\n",
    "                        df_cache[ms_file_path[j]] = filter_top_n(pd.concat([df_cache[ms_file_path[j]], psms]))\n",
    "                    else:\n",
    "                        df_cache[ms_file_path[j]] = psms\n",
    "\n",
    "\n",
    "            if callback:\n",
    "                callback((i+1)/max_)\n",
    "                 \n",
    "    for _ in ms_file_path:    \n",
    "        if _ in df_cache:\n",
    "            x = df_cache[_]\n",
    "            ms_file = alphapept.io.MS_Data_File(_) \n",
    "            \n",
    "            x['fasta_index'] = x['fasta_index'].apply(lambda x: ','.join(str(_) for _ in x))\n",
    "            \n",
    "            if calibration:\n",
    "                store_hdf(x, ms_file, 'second_search', replace = True)\n",
    "            else:\n",
    "                store_hdf(x, ms_file, 'first_search', replace = True)\n",
    "                    \n",
    "    logging.info(f'Complete. Created peptides {n_seqs_:,}')\n",
    "    \n",
    "    return fasta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted 12_speed.ipynb.\n",
      "Converted 13_export.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted XX_file_formats.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
