{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search\n",
    "\n",
    "> Functions related to the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all functions related to searching and getting peptide-spectrum-matches (PSMs). When searching, we compare how similar an experimental spectrum is to a theoretical spectrum. As described in the FASTA notebook, we can calculate theoretical fragment masses for a given peptide sequence and get theoretical spectra. Typically, we calculate a database with all possible spectra and compare our experimental data. It could be that the database is too large to be saved on disc; here we keep the database in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import warnings\n",
    "from numba import NumbaPendingDeprecationWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=NumbaPendingDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Fragments\n",
    "\n",
    "To efficiently compare two fragments, we use a pointer based approach. We start with two sorted arrays, the `query_frag` that contains the m/z positions of the query spectrum and the `db_frag` which contains the database fragment that is compared against to. The two pointers compare each m/z position with each other and check wheter they are within a certain tolerance `mtol`. Depending on their delta, either of the pointers is advanced. The function returns an arrray named `hits` that is the same length as the database spectrum and encodes the hit positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "\n",
    "@njit(nogil=True)\n",
    "def compare_frags(query_frag, db_frag, mtol, ppm=False):\n",
    "    \"\"\"\n",
    "    Compare query and database frags and find hits\n",
    "    \"\"\"\n",
    "    q_max = len(query_frag)\n",
    "    d_max = len(db_frag)\n",
    "    hits = np.zeros(d_max, dtype=np.int16)\n",
    "    q, d = 0, 0  # q > query, d > database\n",
    "    while q < q_max and d < d_max:\n",
    "        mass1 = query_frag[q]\n",
    "        mass2 = db_frag[d]\n",
    "        delta_mass = mass1 - mass2\n",
    "\n",
    "        if ppm:\n",
    "            sum_mass = mass1 + mass2\n",
    "            mass_difference = 2 * delta_mass / sum_mass * 1e6\n",
    "        else:\n",
    "            mass_difference = delta_mass\n",
    "\n",
    "        if abs(mass_difference) <= mtol:\n",
    "            hits[d] = q + 1  # Save query position +1 (zero-indexing)\n",
    "            d += 1\n",
    "            q += 1  # Only one query for each db element\n",
    "        elif delta_mass < 0:\n",
    "            q += 1\n",
    "        elif delta_mass > 0:\n",
    "            d += 1\n",
    "\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, 0], dtype=int16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "query_frag = np.array([100, 200, 300, 400])\n",
    "db_frag = np.array([150, 200, 300, 450])\n",
    "\n",
    "# Hits: Query 2 -> Db 2 and Query 3 -> Db 3\n",
    "\n",
    "compare_frags(query_frag, db_frag, mtol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_compare_frags():\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    mtol = 1\n",
    "\n",
    "    db_frag = query_frag.copy()\n",
    "\n",
    "    # Self-Comparison: no of hits should be same as length\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == len(query_frag)\n",
    "\n",
    "    # Self-Comparison: above but in tolerance\n",
    "    hits = compare_frags(query_frag, db_frag + mtol - 0.01, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == len(query_frag)\n",
    "\n",
    "    # Self-Comparison: below but in tolerance\n",
    "    hits = compare_frags(query_frag, db_frag - mtol + 0.01, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == len(query_frag)\n",
    "\n",
    "    # Self-Comparison: above tolerance, no hits\n",
    "    hits = compare_frags(query_frag, db_frag + mtol + 0.01, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 0\n",
    "\n",
    "    # Special case 1: First and last\n",
    "    db_frag = np.array([100, 400])\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 2\n",
    "\n",
    "    # Special case 2: Two queries matching the same DB frag\n",
    "    query_frag = np.array([100, 100.5])\n",
    "    db_frag = np.array([100, 200, 300])\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 1\n",
    "\n",
    "    # Special case 3: Two db frags matching the same query frag\n",
    "    db_frag = np.array([100, 100.5])\n",
    "    query_frag = np.array([100, 200, 300])\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 1\n",
    "    \n",
    "test_compare_frags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows us to easily compare a query spectrum against a theoretical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'alphapept'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-84fbbf7c6838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malphapept\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malphapept\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_frag_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malphapept\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'alphapept'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from alphapept import constants\n",
    "from alphapept.fasta import get_frag_dict, parse\n",
    "import alphapept.io\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "peptide = 'PEPTIDE'\n",
    "\n",
    "# Theoretical Spectrum\n",
    "\n",
    "frag_dict = get_frag_dict(parse(peptide), constants.mass_dict)\n",
    "db_frag = list(frag_dict.values())\n",
    "db_frag.sort()\n",
    "\n",
    "db_int = [100 for _ in db_frag]\n",
    "\n",
    "# Experimental Spectrum, dummy data\n",
    "\n",
    "query_frag = np.array([98.06, 227.10, 263.08, 548.06, 653.31])\n",
    "query_int = np.array([20, 80, 30, 30, 50])\n",
    "\n",
    "hits = compare_frags(query_frag, db_frag, mtol=1)\n",
    "\n",
    "hitpos = hits[hits > 0] - 1\n",
    "hit_x = query_frag[hitpos]\n",
    "hit_y = query_int[hitpos]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "plt.vlines(query_frag, 0, query_int, \"r\", label=\"Query\", alpha=0.5)\n",
    "\n",
    "plt.plot(hit_x, hit_y, \"ro\", label=\"Hit\", alpha=0.5)\n",
    "\n",
    "for _ in frag_dict.keys():\n",
    "    plt.text(frag_dict[_], 104, _, fontsize=12, alpha = 0.8)\n",
    "    \n",
    "plt.title('Theoretical Spectrum for {}'.format(peptide))\n",
    "plt.xlabel('Mass')\n",
    "plt.ylabel('Intensity')\n",
    "plt.legend()\n",
    "plt.ylim([0,110])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Spectra\n",
    "\n",
    "To compare multiple spectra against a database, we first need some helper functions. Per default, AlphaPept calculates in Dalton. To use ppm boundaries, we need the function `ppm_to_dalton` for conversion. \n",
    "\n",
    "To minimize the search space, we typically only compare spectra with precursors in the same mass range as defined by `m_offset`. To look up the limits for search, we define the function `get_idxs`, which is a wrapper to the fast `searchsorted` method from NumPy.\n",
    "\n",
    "The actual search takes place in `compare_specs_single` and `compare_specs_parallel`, a single-core and multicore method for comparing spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import prange\n",
    "@njit\n",
    "def ppm_to_dalton(mass, m_offset):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    return mass / 1e6 * m_offset\n",
    "\n",
    "\n",
    "def get_idxs(db_masses, query_masses, m_offset, ppm):\n",
    "    \"\"\"\n",
    "    Function to get upper and lower limits to define search range.\n",
    "\n",
    "    \"\"\"\n",
    "    if ppm:\n",
    "        dalton_offset = ppm_to_dalton(query_masses, m_offset)\n",
    "    else:\n",
    "        dalton_offset = m_offset\n",
    "\n",
    "    idxs_lower = db_masses.searchsorted(query_masses - dalton_offset, side=\"left\")\n",
    "    idxs_higher = db_masses.searchsorted(query_masses + dalton_offset, side=\"right\")\n",
    "\n",
    "    return idxs_lower, idxs_higher\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compare_specs_parallel(\n",
    "    frag_hits,\n",
    "    query_masses,\n",
    "    query_frags,\n",
    "    query_indices,\n",
    "    db_masses,\n",
    "    db_frags,\n",
    "    idxs_lower,\n",
    "    idxs_higher,\n",
    "    mtol,\n",
    "    db_indices,\n",
    "    chunk=(0, 1),\n",
    "    offset=False,\n",
    "    ppm=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare spectra, parallelized version\n",
    "    \"\"\"\n",
    "    num_specs_compared = 0\n",
    "    if chunk == (0, 0):\n",
    "        for query_idx in prange(len(query_masses)):\n",
    "            for db_idx in range(idxs_lower[query_idx] , idxs_higher[query_idx] ):\n",
    "                num_specs_compared += 1\n",
    "                query_idx_start = query_indices[query_idx]\n",
    "                query_idx_end = query_indices[query_idx + 1]\n",
    "                query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "                db_frag = db_frags[db_indices[db_idx]:db_indices[db_idx+1]]\n",
    "                o_mass = query_masses[query_idx]  - db_masses[db_idx]\n",
    "                hits = compare_frags(query_frag, db_frag, mtol, ppm)\n",
    "                if offset is True:  # Mirrored search\n",
    "                    hits += compare_frags(query_frag, db_frag + o_mass, mtol, ppm)\n",
    "                frag_hits[query_idx, db_idx - idxs_lower[query_idx] ] = np.sum(hits > 0)\n",
    "    else:\n",
    "        # Chunked version\n",
    "        current_index = chunk[0]\n",
    "        n_chunks = chunk[1]\n",
    "        m_subset = np.arange(len(query_masses))[current_index::n_chunks]\n",
    "\n",
    "        for index in prange(len(m_subset)):\n",
    "            query_idx = m_subset[index]\n",
    "            for db_idx in range(idxs_lower[query_idx] , idxs_higher[query_idx] ):\n",
    "                num_specs_compared += 1\n",
    "                query_idx_start = query_indices[query_idx]\n",
    "                query_idx_end = query_indices[query_idx + 1]\n",
    "                query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "                db_frag = db_frags[db_indices[db_idx]:db_indices[db_idx+1]]\n",
    "                o_mass = query_masses[query_idx]  - db_masses[db_idx]\n",
    "                hits = compare_frags(query_frag, db_frag, mtol, ppm)\n",
    "                if offset is True:  # Mirrored search\n",
    "                    hits += compare_frags(query_frag, db_frag + o_mass, mtol, ppm)\n",
    "                frag_hits[query_idx, db_idx - idxs_lower[query_idx] ] = np.sum(hits > 0)\n",
    "\n",
    "    return frag_hits, num_specs_compared\n",
    "\n",
    "\n",
    "@njit\n",
    "def compare_specs_single(\n",
    "    frag_hits,\n",
    "    query_masses,\n",
    "    query_frags,\n",
    "    query_indices,\n",
    "    db_masses,\n",
    "    db_frags,\n",
    "    idxs_lower,\n",
    "    idxs_higher,\n",
    "    mtol,\n",
    "    db_indices,\n",
    "    chunk=(0, 1),\n",
    "    offset=False,\n",
    "    ppm=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare spectra, single core version\n",
    "\n",
    "    \"\"\"\n",
    "    num_specs_compared = 0\n",
    "\n",
    "    if chunk == (0, 0):\n",
    "        for query_idx in range(len(query_masses)):\n",
    "            for db_idx in range(idxs_lower[query_idx] , idxs_higher[query_idx] ):\n",
    "                num_specs_compared += 1\n",
    "                query_idx_start = query_indices[query_idx]\n",
    "                query_idx_end = query_indices[query_idx + 1]\n",
    "                query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "                db_frag = db_frags[db_indices[db_idx]:db_indices[db_idx+1]]\n",
    "                o_mass = query_masses[query_idx]  - db_masses[db_idx]\n",
    "                hits = compare_frags(query_frag, db_frag, mtol, ppm)\n",
    "                if offset is True:  # Mirrored search\n",
    "                    hits += compare_frags(query_frag, db_frag + o_mass, mtol, ppm)\n",
    "                frag_hits[query_idx, db_idx - idxs_lower[query_idx] ] = np.sum(hits > 0)\n",
    "    else:\n",
    "        # Chunked version\n",
    "        current_index = chunk[0]\n",
    "        n_chunks = chunk[1]\n",
    "\n",
    "        m_subset = np.arange(len(query_masses))[current_index::n_chunks]\n",
    "\n",
    "        for index in range(len(m_subset)):\n",
    "            query_idx = m_subset[index]\n",
    "            for db_idx in range(idxs_lower[query_idx] , idxs_higher[query_idx] ):\n",
    "                num_specs_compared += 1\n",
    "                query_idx_start = query_indices[query_idx]\n",
    "                query_idx_end = query_indices[query_idx + 1]\n",
    "                query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "                db_frag = db_frags[db_indices[db_idx]:db_indices[db_idx+1]]\n",
    "                o_mass = query_masses[query_idx]  - db_masses[db_idx]\n",
    "                hits = compare_frags(query_frag, db_frag, mtol, ppm)\n",
    "                if offset is True:  # Mirrored search\n",
    "                    hits += compare_frags(query_frag, db_frag + o_mass, mtol, ppm)\n",
    "                frag_hits[query_idx, db_idx - idxs_lower[query_idx] ] = np.sum(hits > 0)\n",
    "\n",
    "    return frag_hits, num_specs_compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "from alphapept.fasta import read_database\n",
    "\n",
    "def query_data_to_features(query_data):\n",
    "\n",
    "    # if we dont use the feature finder we extract them from the query data..\n",
    "\n",
    "    query_masses = query_data['prec_mass_list2']\n",
    "    query_mz = query_data['mono_mzs2']\n",
    "    query_rt = query_data['rt_list_ms2']\n",
    "\n",
    "    features = pd.DataFrame(np.array([query_masses, query_mz, query_rt]).T, columns = ['mass_matched', 'mz_matched', 'rt_matched'])\n",
    "\n",
    "    features['feature_idx'] = features.index #Index to query_data\n",
    "    features['query_idx']  = np.arange(len(query_masses))\n",
    "\n",
    "    features = features.sort_values('mass_matched', ascending=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_psms(\n",
    "    query_data,\n",
    "    db_data,\n",
    "    features,\n",
    "    parallel,\n",
    "    m_tol,\n",
    "    m_offset,\n",
    "    ppm,\n",
    "    min_frag_hits,\n",
    "    callback = None,\n",
    "    m_offset_calibrated = None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper function to extract psms from dataset\n",
    "\n",
    "    Args:\n",
    "        db_masses: database precursor masses\n",
    "        query_masses: query precursor masses\n",
    "        m_offset: mass offset in dalton or ppm\n",
    "        ppm: flag for ppm or dalton\n",
    "        callback: Callback function, e.g. for progress bar\n",
    "    Returns:\n",
    "        idxs_lower: lower search range\n",
    "        idxs_higher: upper search range\n",
    "    Raises:\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(db_data, str):\n",
    "        db_masses = read_database(db_data, array_name = 'precursors')\n",
    "        db_frags = read_database(db_data, array_name = 'fragmasses')\n",
    "        db_indices = read_database(db_data, array_name = 'indices')\n",
    "    else:\n",
    "        db_masses = db_data['precursors']\n",
    "        db_frags = db_data['fragmasses']\n",
    "        db_indices = db_data['indices']\n",
    "        \n",
    "    query_indices = query_data[\"indices_ms2\"]\n",
    "    query_frags = query_data['mass_list_ms2']\n",
    "    \n",
    "    \n",
    "\n",
    "    if features is not None:\n",
    "        if m_offset_calibrated:\n",
    "            m_offset = m_offset_calibrated\n",
    "            query_masses = features['corrected_mass'].values\n",
    "        else:\n",
    "            query_masses = features['mass_matched'].values\n",
    "        query_mz = features['mz_matched'].values\n",
    "        query_rt = features['rt_matched'].values\n",
    "        query_selection = features['query_idx'].values\n",
    "        indices = np.zeros(len(query_selection) + 1, np.int64)\n",
    "        indices[1:] = np.diff(query_indices)[query_selection]\n",
    "        indices = np.cumsum(indices)\n",
    "        query_frags = np.concatenate(\n",
    "            [\n",
    "                query_frags[s: e] for s, e in zip(\n",
    "                    query_indices[query_selection], query_indices[query_selection + 1]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        query_indices = indices\n",
    "    else:\n",
    "        if m_offset_calibrated:\n",
    "            m_offset = m_offset_calibrated\n",
    "        query_masses = query_data['prec_mass_list2']\n",
    "        query_mz = query_data['mono_mzs2']\n",
    "        query_rt = query_data['rt_list_ms2']\n",
    "    \n",
    "#     idxs_lower, idxs_higher = get_idxs(db_masses, query_masses, m_offset, ppm)\n",
    "    idxs_lower, idxs_higher = get_idxs(\n",
    "        db_masses,\n",
    "        query_masses,\n",
    "        m_offset,\n",
    "        ppm\n",
    "    )\n",
    "    frag_hits = np.zeros(\n",
    "        (len(query_masses), np.max(idxs_higher - idxs_lower)), dtype=int\n",
    "    )\n",
    "\n",
    "    logging.info(f'Performing search on {len(query_masses):,} query and {len(db_masses):,} db entries with m_tol = {m_tol:.2f} and m_offset = {m_offset:.2f}.')\n",
    "\n",
    "    if callback is None:\n",
    "        chunk = (0, 0)\n",
    "        offset = False\n",
    "\n",
    "        if parallel:\n",
    "            frag_hits, num_specs_compared = compare_specs_parallel(\n",
    "                frag_hits,\n",
    "                query_masses,\n",
    "                query_frags,\n",
    "                query_indices,\n",
    "                db_masses,\n",
    "                db_frags,\n",
    "                idxs_lower,\n",
    "                idxs_higher,\n",
    "                m_tol,\n",
    "                db_indices,\n",
    "                chunk,\n",
    "                offset,\n",
    "                ppm,\n",
    "            )\n",
    "        else:\n",
    "            frag_hits, num_specs_compared = compare_specs_single(\n",
    "                frag_hits,\n",
    "                query_masses,\n",
    "                query_frags,\n",
    "                query_indices,\n",
    "                db_masses,\n",
    "                db_frags,\n",
    "                idxs_lower,\n",
    "                idxs_higher,\n",
    "                m_tol,\n",
    "                db_indices,\n",
    "                chunk,\n",
    "                offset,\n",
    "                ppm,\n",
    "            )\n",
    "\n",
    "    else: #chunk the data for progess bar until numba has proper progress bar\n",
    "        n_chunks = 100\n",
    "        num_specs_compared = 0\n",
    "\n",
    "        for current_chunk in range(n_chunks):\n",
    "            chunk = (current_chunk, n_chunks)\n",
    "            offset = False\n",
    "            if parallel:\n",
    "                frag_hits, num_specs_compared_chunk = compare_specs_parallel(\n",
    "                    frag_hits,\n",
    "                    query_masses,\n",
    "                    query_frags,\n",
    "                    query_indices,\n",
    "                    db_masses,\n",
    "                    db_frags,\n",
    "                    idxs_lower,\n",
    "                    idxs_higher,\n",
    "                    m_tol,\n",
    "                    db_indices,\n",
    "                    chunk,\n",
    "                    offset,\n",
    "                    ppm,\n",
    "                )\n",
    "            else:\n",
    "                frag_hits, num_specs_compared_chunk = compare_specs_single(\n",
    "                    frag_hits,\n",
    "                    query_masses,\n",
    "                    query_frags,\n",
    "                    query_indices,\n",
    "                    db_masses,\n",
    "                    db_frags,\n",
    "                    idxs_lower,\n",
    "                    idxs_higher,\n",
    "                    m_tol,\n",
    "                    db_indices,\n",
    "                    chunk,\n",
    "                    offset,\n",
    "                    ppm,\n",
    "                )\n",
    "\n",
    "            if callback is not None:\n",
    "                callback((current_chunk+1)/n_chunks)\n",
    "            num_specs_compared += num_specs_compared_chunk\n",
    "\n",
    "    hit_query, hit_db = np.where(frag_hits >= min_frag_hits)\n",
    "    hits = frag_hits[hit_query, hit_db]\n",
    "    hit_db += idxs_lower[hit_query]\n",
    "            \n",
    "    del db_masses\n",
    "    del db_frags\n",
    "    del db_indices\n",
    "\n",
    "    psms = np.array(\n",
    "        list(zip(hit_query, hit_db, hits)), dtype=[(\"query_idx\", int), (\"db_idx\", int), (\"hits\", int)]\n",
    "    )\n",
    "\n",
    "    logging.info('Compared {:,} spectra and found {:,} psms.'.format(num_specs_compared, len(psms)))\n",
    "\n",
    "    return psms, num_specs_compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting columns for scoring\n",
    "\n",
    "The basic fragment comparison only counts the number of hits when comparing a theoretical spectrum to an experimental one. Based on the number of hits, we can drastically reduce the number of candidates one wants to analyze for an in-depth comparison, which requires additional features. The following section describes several functions which extract parameters to compare spectrum matches better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frag Delta\n",
    "\n",
    "`frag_delta` calculates the the substracts the experimental fragment masses from the theoretical fragment masses for each hit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def frag_delta(query_frag, db_frag, hits):\n",
    "    \"\"\"\n",
    "    Calculate the mass difference for a given array of hits in Dalton and ppm\n",
    "    \"\"\"\n",
    "\n",
    "    delta_m = db_frag[hits > 0] - query_frag[hits[hits > 0] - 1]\n",
    "    delta_m_ppm = (\n",
    "        2 * delta_m / (db_frag[hits > 0] + query_frag[hits[hits > 0] - 1]) * 1e6\n",
    "    )\n",
    "\n",
    "    return delta_m, delta_m_ppm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_frag_delta():\n",
    "    mtol = 10\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([101, 202, 303, 404])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    delta_m, delta_m_ppm = frag_delta(query_frag, db_frag, hits)\n",
    "\n",
    "    assert np.sum(delta_m) == 10\n",
    "    \n",
    "test_frag_delta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def intensity_fraction(query_int, hits):\n",
    "    \"\"\"\n",
    "    Calculate the fraction of matched intensity\n",
    "    \"\"\"\n",
    "    total_intensity = np.sum(query_int)\n",
    "    if total_intensity != 0:\n",
    "        matched_intensity = np.sum(query_int[hits[hits > 0] - 1])\n",
    "        i_frac = matched_intensity / total_intensity\n",
    "    else:\n",
    "        i_frac = 0\n",
    "\n",
    "    return i_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_intensity_fraction():\n",
    "    mtol = 1\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    i_frac = intensity_fraction(query_int, hits)\n",
    "\n",
    "    assert i_frac == 40 / 100\n",
    "\n",
    "test_intensity_fraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def intensity_product(query_int, hits, db_int=None):\n",
    "    \"\"\"\n",
    "    Calculate the dot product of matched query intensity to db intensity\n",
    "    \"\"\"\n",
    "\n",
    "    matched_query_int = query_int[hits[hits > 0] - 1]\n",
    "    if db_int is None:\n",
    "        matched_intensity = np.sum(matched_query_int)\n",
    "    else:\n",
    "        matched_db_int = db_int[hits > 0]\n",
    "        matched_intensity = np.sum(matched_query_int*matched_db_int)\n",
    "\n",
    "    return matched_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_intensity_product():\n",
    "    mtol = 1\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert intensity_product(query_int, hits) == 40\n",
    "\n",
    "    query_frag = np.array([100, 200, 300, 400, 600])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40, 50])\n",
    "    db_int = np.array([10, 20, 30, 40])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert intensity_product(query_int, hits, db_int = db_int) == 10*10+30*20\n",
    "    \n",
    "test_intensity_product()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B & Y - Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def b_y_hits(frag_type, hits):\n",
    "    \"\"\"\n",
    "    Count the number of b and y hits\n",
    "    hits usually start with b-ions > 0, then y-ions < 1\n",
    "    \"\"\"\n",
    "    hits_index = hits > 0\n",
    "\n",
    "    hit_types = frag_type[hits_index]\n",
    "\n",
    "    b_hits = np.sum(hit_types > 0)\n",
    "    y_hits = np.sum(hit_types < 0)\n",
    "\n",
    "    return b_hits, y_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_b_y_hits():\n",
    "    # TODO: Write a test to make sure the by hits are correct..\n",
    "    mtol = 1\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40])\n",
    "    frag_type = np.array([1, -1, 2, -2])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "\n",
    "    b_hit, y_hit = b_y_hits(frag_type, hits)\n",
    "\n",
    "    assert (b_hit) == 1\n",
    "    assert (y_hit) == 1\n",
    "\n",
    "    frag_type = np.array([-1, -2, -3, -4])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "\n",
    "    b_hit, y_hit = b_y_hits(frag_type, hits)\n",
    "    \n",
    "    assert (b_hit) == 0\n",
    "    assert (y_hit) == 2\n",
    "\n",
    "    frag_type = np.array([1, 2, 3, 4])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "\n",
    "    b_hit, y_hit = b_y_hits(frag_type, hits)\n",
    "\n",
    "    assert (b_hit) == 2\n",
    "    assert (y_hit) == 0\n",
    "    \n",
    "test_b_y_hits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting Score columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numpy.lib.recfunctions import append_fields, drop_fields\n",
    "\n",
    "\n",
    "def add_column(recarray, column, name):\n",
    "    \"\"\"\n",
    "    Function to add a column with given name to recarray\n",
    "    \"\"\"\n",
    "    if hasattr(recarray, name):\n",
    "        recarray = drop_fields(recarray, name, usemask=False, asrecarray=True)\n",
    "    recarray = append_fields(\n",
    "        recarray, name, column, dtypes=column.dtype, usemask=False, asrecarray=True\n",
    "    )\n",
    "    return recarray\n",
    "\n",
    "\n",
    "def remove_column(recarray, name):\n",
    "    \"\"\"\n",
    "    Function to remove a column from recarray\n",
    "    \"\"\"\n",
    "    if hasattr(recarray, name):\n",
    "        recarray = drop_fields(recarray, name, usemask=False, asrecarray=True)\n",
    "    return recarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting\n",
    "\n",
    "When performing a database search, we need to know what experimental spectrum we are comparing with which database entry. \n",
    "We distinguish three indices:\n",
    "* query_idx\n",
    "* raw_idx\n",
    "* feature_idx\n",
    "Initially, the get_psms functions accepts experimental data in the form of `query_data`. Here, the `query_idx` refers to the index to `query_data`. However, this might not be the same index as of the raw data. This is due to the implementation of the matching of MS1-features to MS2 spectra. Here we allow multiple matches and implement this by repeating the respective spectra. \n",
    "\n",
    "We then add the two columns `feature_idx` and `raw_idx` to the psms to later be able to distinguish where the match originated from. Here, `raw_idx` refers to the original spectrum.\n",
    "\n",
    "When not applying feature finding, `raw_idx` and `query_idx` are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba.typed import List\n",
    "@njit\n",
    "def get_hits(query_frag, query_int, db_frag, db_int, frag_type, mtol, ppm, losses):\n",
    "    \n",
    "    max_array_size = len(db_frag)*len(losses)\n",
    "\n",
    "    ions = np.zeros((max_array_size, 8))\n",
    "\n",
    "    pointer = 0\n",
    "    \n",
    "    query_range = np.arange(len(query_frag))\n",
    "    db_range = np.arange(len(db_frag))\n",
    "\n",
    "    for idx, off in enumerate(losses):\n",
    "        hits = compare_frags(query_frag, db_frag-off, mtol, ppm)\n",
    "        n_hits = np.sum(hits>0)\n",
    "        \n",
    "        hitpos = hits[hits > 0] - 1\n",
    "        hit = hits > 0\n",
    "        \n",
    "        ions[pointer:pointer+n_hits,0] = frag_type[hits>0] #type\n",
    "        ions[pointer:pointer+n_hits,1] = idx #ion-index\n",
    "        \n",
    "        ions[pointer:pointer+n_hits,2] = query_int[hitpos] #query int\n",
    "        ions[pointer:pointer+n_hits,3] = db_int[hit] #db int\n",
    "        \n",
    "        ions[pointer:pointer+n_hits,4] = query_frag[hitpos] #query mass\n",
    "        ions[pointer:pointer+n_hits,5] = db_frag[hit]-off # db mass  \n",
    "        \n",
    "        ions[pointer:pointer+n_hits,6] = query_range[hitpos] # index to query entry\n",
    "        ions[pointer:pointer+n_hits,7] = db_range[hit] # index to db entry\n",
    "    \n",
    "        pointer += n_hits\n",
    "\n",
    "    ions = ions[:pointer,:]\n",
    "        \n",
    "    return ions\n",
    "\n",
    "@njit\n",
    "def score(\n",
    "    psms,\n",
    "    query_masses,\n",
    "    query_masses_raw,\n",
    "    query_frags,\n",
    "    query_ints,\n",
    "    query_indices,\n",
    "    db_masses,\n",
    "    db_frags,\n",
    "    frag_types,\n",
    "    mtol,\n",
    "    db_indices,\n",
    "    ppm,\n",
    "    psms_dtype,\n",
    "    db_ints = None,\n",
    "    parallel = False\n",
    "):\n",
    "\n",
    "    psms_ = np.zeros(len(psms), dtype=psms_dtype)  \n",
    "    \n",
    "    losses = [0, 18.01056468346, 17.03052] #H2O, NH3\n",
    "    \n",
    "    ions_ = List()\n",
    "    \n",
    "    ion_count = 0\n",
    "    \n",
    "    for i in range(len(psms)):\n",
    "        query_idx = psms[i][\"query_idx\"]\n",
    "        db_idx = psms[i][\"db_idx\"]\n",
    "        query_idx_start = query_indices[query_idx]\n",
    "        query_idx_end = query_indices[query_idx + 1]\n",
    "        query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "        query_int = query_ints[query_idx_start:query_idx_end]\n",
    "        db_frag = db_frags[db_indices[db_idx]:db_indices[db_idx+1]]\n",
    "        frag_type = frag_types[db_indices[db_idx]:db_indices[db_idx+1]]\n",
    "\n",
    "        if db_ints is None:\n",
    "            db_int = np.zeros(len(db_frag))\n",
    "        else:\n",
    "            db_int = db_ints[i]\n",
    "\n",
    "        ions = get_hits(query_frag, query_int, db_frag, db_int, frag_type, mtol, ppm, losses)\n",
    "\n",
    "        psms_['o_mass'][i] = query_masses[query_idx] - db_masses[db_idx]\n",
    "        psms_['o_mass_ppm'][i] = 2 * psms_['o_mass'][i] / (query_masses[query_idx]  + db_masses[db_idx] ) * 1e6\n",
    "        \n",
    "        psms_['o_mass_raw'][i] = query_masses_raw[query_idx] - db_masses[db_idx]\n",
    "        psms_['o_mass_ppm_raw'][i] = 2 * psms_['o_mass'][i] / (query_masses_raw[query_idx]  + db_masses[db_idx] ) * 1e6\n",
    "\n",
    "        psms_['delta_m'][i] = np.mean(ions[:,4]-ions[:,5])\n",
    "        psms_['delta_m_ppm'][i] = np.mean(2 * psms_['delta_m'][i] / (ions[:,4]  + ions[:,5] ) * 1e6)\n",
    "\n",
    "        psms_['total_int'][i] = np.sum(query_int)\n",
    "        psms_['matched_int'][i] = np.sum(ions[:,2])\n",
    "        psms_['matched_int_ratio'][i] = psms_['matched_int'][i] / psms_['total_int'][i]\n",
    "        psms_['int_ratio'][i] = np.mean(ions[:,3]/ions[:,2])\n",
    "        \n",
    "        psms_['b_hits'][i] = np.sum(ions[ions[:,1]==0][:,0]>0)\n",
    "        psms_['y_hits'][i] = np.sum(ions[ions[:,1]==0][:,0]<0)\n",
    "        \n",
    "        psms_['b-H2O_hits'][i] = np.sum(ions[ions[:,1]==1][:,0]>0)\n",
    "        psms_['y-H2O_hits'][i] = np.sum(ions[ions[:,1]==1][:,0]<0)\n",
    "        \n",
    "        psms_['b-NH3_hits'][i] = np.sum(ions[ions[:,1]==2][:,0]>0)\n",
    "        psms_['y-NH3_hits'][i] = np.sum(ions[ions[:,1]==2][:,0]<0)\n",
    "        \n",
    "        n_ions = len(ions)\n",
    "        \n",
    "        psms_['n_ions'][i] = n_ions\n",
    "        psms_['ion_idx'][i] = ion_count\n",
    "        \n",
    "        ion_count += n_ions\n",
    "        ions_.append(ions)\n",
    "        \n",
    "    return psms_, ions_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from numba.typed import Dict\n",
    "def get_sequences(psms, db_seqs):\n",
    "    \"\"\"\n",
    "    Get sequences to add them to a recarray\n",
    "    \"\"\"\n",
    "    sequence_list = db_seqs[psms[\"db_idx\"]]\n",
    "\n",
    "    return sequence_list\n",
    "\n",
    "\n",
    "def get_score_columns(\n",
    "    psms,\n",
    "    query_data,\n",
    "    db_data,\n",
    "    features,\n",
    "    parallel,\n",
    "    m_tol,\n",
    "    m_offset,\n",
    "    ppm,\n",
    "    m_offset_calibrated=None,\n",
    "    **kwargs\n",
    "):\n",
    "    logging.info('Extracting columns for scoring.')\n",
    "    query_indices = query_data[\"indices_ms2\"]\n",
    "    query_charges = query_data['charge2']\n",
    "    query_frags = query_data['mass_list_ms2']\n",
    "    query_ints = query_data['int_list_ms2']\n",
    "    query_scans = query_data['scan_list_ms2']\n",
    "\n",
    "    if 'prec_id2' in query_data.keys():\n",
    "        bruker = True\n",
    "        query_prec_id = query_data['prec_id2']\n",
    "    else:\n",
    "        bruker = False\n",
    "\n",
    "    if isinstance(db_data, str):\n",
    "        db_masses = read_database(db_data, array_name = 'precursors')\n",
    "        db_frags = read_database(db_data, array_name = 'fragmasses')\n",
    "        db_indices = read_database(db_data, array_name = 'indices')\n",
    "        frag_types = read_database(db_data, array_name = 'fragtypes')\n",
    "\n",
    "        try:\n",
    "            db_ints = read_database(db_data, array_name = 'db_ints')\n",
    "        except KeyError:\n",
    "            db_ints = None\n",
    "                \n",
    "    else:\n",
    "        db_masses = db_data['precursors']\n",
    "        db_frags = db_data['fragmasses']\n",
    "        db_indices = db_data['indices']\n",
    "        frag_types = db_data['fragtypes']\n",
    "\n",
    "        if 'db_ints' in db_data.keys():\n",
    "            db_ints = db_data['db_ints']\n",
    "        else:\n",
    "            db_ints = None\n",
    "\n",
    "    if features is not None:\n",
    "        if m_offset_calibrated:\n",
    "            query_masses = features['corrected_mass'].values\n",
    "        else:\n",
    "            query_masses = features['mass_matched'].values\n",
    "            \n",
    "        query_masses_raw = features['mass_matched'].values\n",
    "        \n",
    "        query_mz = features['mz_matched'].values\n",
    "        query_rt = features['rt_matched'].values\n",
    "        query_charges = query_charges[features['query_idx'].values]\n",
    "        query_scans = query_scans[features['query_idx'].values]\n",
    "\n",
    "        if bruker:\n",
    "            query_prec_id = query_prec_id[features['query_idx'].values]\n",
    "\n",
    "        query_selection = features['query_idx'].values\n",
    "        indices = np.zeros(len(query_selection) + 1, np.int64)\n",
    "        indices[1:] = np.diff(query_indices)[query_selection]\n",
    "        indices = np.cumsum(indices)\n",
    "        query_frags = np.concatenate(\n",
    "            [\n",
    "                query_frags[s: e] for s, e in zip(\n",
    "                    query_indices[query_selection], query_indices[query_selection + 1]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        query_ints = np.concatenate(\n",
    "            [\n",
    "                query_ints[s: e] for s, e in zip(\n",
    "                    query_indices[query_selection], query_indices[query_selection + 1]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        query_indices = indices\n",
    "    else:\n",
    "        #TODO: This code is outdated, callin with features = None will crash.\n",
    "        query_masses = query_data['prec_mass_list2']\n",
    "        query_masses_raw = query_data['prec_mass_list2']\n",
    "        query_mz = query_data['mono_mzs2']\n",
    "        query_rt = query_data['rt_list_ms2']\n",
    "\n",
    "\n",
    "    loss_dict = Dict()\n",
    "    loss_dict[''] = 0.0\n",
    "    loss_dict['-H2O'] = 18.01056468346\n",
    "    loss_dict['-NH3'] = 17.03052\n",
    "\n",
    "    float_fields = ['o_mass', 'o_mass_ppm', 'o_mass_raw','o_mass_ppm_raw','delta_m','delta_m_ppm','matched_int_ratio','int_ratio']\n",
    "    int_fields = ['total_int','matched_int','n_ions','ion_idx'] + [a+_+'_hits' for _ in loss_dict for a in ['b','y']]\n",
    "\n",
    "    psms_dtype = np.dtype([(_,np.float32) for _ in float_fields] + [(_,np.int64) for _ in int_fields])\n",
    "\n",
    "    psms_, ions_,  = score(\n",
    "        psms,\n",
    "        query_masses,\n",
    "        query_masses_raw,\n",
    "        query_frags,\n",
    "        query_ints,\n",
    "        query_indices,\n",
    "        db_masses,\n",
    "        db_frags,\n",
    "        frag_types,\n",
    "        m_tol,\n",
    "        db_indices,\n",
    "        ppm,\n",
    "        psms_dtype)\n",
    "\n",
    "    ions_ = np.vstack(ions_)\n",
    "\n",
    "    for _ in psms_.dtype.names:\n",
    "        psms = add_column(psms, psms_[_], _)\n",
    "\n",
    "    rts = np.array(query_rt)[psms[\"query_idx\"]]\n",
    "    psms = add_column(psms, rts, 'rt')\n",
    "\n",
    "    if isinstance(db_data, str):\n",
    "        db_seqs = read_database(db_data, array_name = 'seqs').astype(str)\n",
    "    else:\n",
    "        db_seqs = db_data['seqs']\n",
    "\n",
    "    seqs = get_sequences(psms, db_seqs)\n",
    "\n",
    "    del db_seqs\n",
    "\n",
    "    psms = add_column(psms, seqs, \"sequence\")\n",
    "\n",
    "    mass = np.array(query_masses)[psms[\"query_idx\"]]\n",
    "    mz = np.array(query_mz)[psms[\"query_idx\"]]\n",
    "    charge = np.array(query_charges)[psms[\"query_idx\"]]\n",
    "\n",
    "    psms = add_column(psms, mass, \"mass\")\n",
    "    psms = add_column(psms, mz, \"mz\")\n",
    "    psms = add_column(psms, charge, \"charge\")\n",
    "\n",
    "    psms = add_column(psms, np.char.add(np.char.add(psms['sequence'],\"_\"), psms['charge'].astype(int).astype(str)), 'precursor')\n",
    "\n",
    "    if features is not None:\n",
    "        psms = add_column(psms, features.loc[psms['query_idx']]['feature_idx'].values, 'feature_idx')\n",
    "        psms = add_column(psms, features.loc[psms['query_idx']]['query_idx'].values, 'raw_idx')\n",
    "\n",
    "        for key in ['int_sum','int_apex','rt_start','rt_apex','rt_end','fwhm','dist','mobility']:\n",
    "            if key in features.keys():\n",
    "                psms = add_column(psms, features.loc[psms['query_idx']][key].values, key)\n",
    "\n",
    "    scan_no = np.array(query_scans)[psms[\"query_idx\"]]\n",
    "    if bruker:\n",
    "        psms = add_column(psms, scan_no, \"parent\")\n",
    "        psms = add_column(psms, np.array(query_prec_id)[psms[\"query_idx\"]], 'precursor_idx')\n",
    "        psms = add_column(psms, psms['feature_idx']+1, 'feature_id') #Bruker\n",
    "    else:\n",
    "        psms = add_column(psms, scan_no, \"scan_no\")\n",
    "\n",
    "    logging.info(f'Extracted columns from {len(psms):,} spectra.')\n",
    "\n",
    "    return psms, ions_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def plot_hit(\n",
    "    df,\n",
    "    index,\n",
    "    db_indices,\n",
    "    db_frags,\n",
    "    frag_types,\n",
    "    query_frags,\n",
    "    query_ints,\n",
    "    query_indices,\n",
    "    ppm,\n",
    "    m_tol,\n",
    "    db_ints = None,\n",
    "    **kwargs\n",
    "):\n",
    "    spectrum = df.iloc[index]\n",
    "\n",
    "    sequence = spectrum[\"sequence\"]\n",
    "\n",
    "    db_idx = spectrum[\"db_idx\"]\n",
    "    query_idx = spectrum[\"query_idx\"]\n",
    "\n",
    "    intensity_fraction = spectrum[\"matched_int\"] / spectrum[\"total_int\"]\n",
    "\n",
    "    db_bound = db_indices[db_idx]\n",
    "    db_frag = db_frags[:, db_idx] [:db_bound]\n",
    "    if db_ints is not None:\n",
    "        db_int = db_ints[:, db_idx] [:db_bound]\n",
    "    else:\n",
    "        db_int = np.ones(len(db_frag))\n",
    "\n",
    "    db_int = db_int / np.max(db_int) * 100\n",
    "\n",
    "    frag_type = frag_types[:, db_idx] [:db_bound]\n",
    "\n",
    "    query_idx_start = query_indices[query_idx]\n",
    "    query_idx_end = query_indices[query_idx + 1]\n",
    "    query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "    query_int = query_ints[query_idx_start:query_idx_end]\n",
    "\n",
    "    query_int = query_int / np.max(query_int) * 100\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, m_tol, ppm)\n",
    "\n",
    "    n_hits = np.sum(hits > 0)\n",
    "\n",
    "    hitpos = hits[hits > 0] - 1\n",
    "\n",
    "    hit_x = query_frag[hitpos]\n",
    "    hit_y = query_int[hitpos]\n",
    "\n",
    "    # create an axis\n",
    "    ax = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "\n",
    "    plt.vlines(query_frag, 0, query_int, \"r\", label=\"Query\", alpha=0.5)\n",
    "\n",
    "    plt.plot(hit_x, hit_y, \"ro\", label=\"Hit\", alpha=0.5)\n",
    "\n",
    "    figure_title = \"PSM Match for Spectra: {} - sequence {} \\nHits {} - Intensity Fraction {:.2f} %\".format(\n",
    "        query_idx, sequence, n_hits, intensity_fraction * 100\n",
    "    )\n",
    "    plt.title(figure_title)\n",
    "\n",
    "    plt.xlabel(\"Mass\")\n",
    "    plt.ylabel(\"Relative Intensity (%)\")\n",
    "    plt.ylim([0, 110])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "from alphapept.fasta import get_frag_dict, parse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_psms(query_data, df, index, mass_dict, ppm=True, m_tol=20):\n",
    "    \"\"\"\n",
    "    Plot a psms\n",
    "    \"\"\"\n",
    "    spectrum = df.iloc[index]\n",
    "\n",
    "    sequence = spectrum[\"sequence\"]\n",
    "    db_idx = spectrum[\"db_idx\"]\n",
    "    query_idx = spectrum[\"query_idx\"]\n",
    "\n",
    "    if 'matched_int' in spectrum.index:\n",
    "        intensity_fraction = spectrum[\"matched_int\"] / spectrum[\"total_int\"]\n",
    "    else:\n",
    "        intensity_fraction = np.nan\n",
    "        matched_int = np.nan\n",
    "\n",
    "    frag_dict = get_frag_dict(parse(sequence), mass_dict)\n",
    "    frag_dict_r = {v: k for k, v in frag_dict.items()}\n",
    "\n",
    "    db_frag = list(frag_dict.values())\n",
    "    db_frag.sort()\n",
    "\n",
    "    db_int = [100 for _ in db_frag]\n",
    "\n",
    "    query_frags = query_data['mass_list_ms2']\n",
    "    query_ints = query_data['int_list_ms2']\n",
    "\n",
    "    query_idx_start = query_indices[query_idx]\n",
    "    query_idx_end = query_indices[query_idx + 1]\n",
    "    query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "    query_int = query_ints[query_idx_start:query_idx_end]\n",
    "\n",
    "    query_int = query_int / np.max(query_int) * 100\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, m_tol, ppm)\n",
    "\n",
    "    n_hits = np.sum(hits > 0)\n",
    "\n",
    "    hitpos = hits[hits > 0] - 1\n",
    "\n",
    "    hit_x = query_frag[hitpos]\n",
    "    hit_y = query_int[hitpos]\n",
    "\n",
    "    # create an axis\n",
    "    ax = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "\n",
    "    plt.vlines(query_frag, 0, query_int, \"r\", label=\"Query\", alpha=0.5)\n",
    "\n",
    "    plt.plot(hit_x, hit_y, \"ro\", label=\"Hit\", alpha=0.5)\n",
    "\n",
    "    figure_title = \"Peptide-Spectrum-Match for Spectra: {} - sequence {} \\nHits {} - Intensity Fraction {:.2f} %\".format(\n",
    "        query_idx, sequence, n_hits, intensity_fraction * 100\n",
    "    )\n",
    "\n",
    "    db_hits = np.array(db_frag)[hits>0]\n",
    "    ion_hits = [frag_dict_r[_] for _ in db_hits]\n",
    "\n",
    "    for _ in frag_dict.keys():\n",
    "\n",
    "        if _ in ion_hits:\n",
    "            color = 'r'\n",
    "        else:\n",
    "            color = 'k'\n",
    "\n",
    "        if _[0] == 'y':\n",
    "            plt.text(frag_dict[_], 110, _, fontsize=12, alpha = 0.8, color=color)\n",
    "        else:\n",
    "            plt.text(frag_dict[_], 104, _, fontsize=12, alpha = 0.8, color=color)\n",
    "\n",
    "    plt.title(figure_title)\n",
    "\n",
    "    plt.xlabel(\"Mass\")\n",
    "    plt.ylabel(\"Relative Intensity (%)\")\n",
    "    plt.ylim([0, 120])\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def perform_search(query_files, db_masses, db_frags, db_indices, db_seqs, frag_types, plot, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to search and score one or multiple MS runs by the X!Tandem approach.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(query_files, str):\n",
    "        kwargs['query_path'] = query_files\n",
    "        psms_all = score_psms(db_masses, db_frags, db_indices, db_seqs, frag_types, plot=plot, **kwargs)\n",
    "        psms_all['filename'] = query_files\n",
    "    elif isinstance(query_files, list):\n",
    "        psms_all = []\n",
    "        for file in query_files:\n",
    "            kwargs['query_path'] = file\n",
    "            psms = score_psms(db_masses, db_frags, db_indices, db_seqs, frag_types, plot=plot, **kwargs)\n",
    "            psms['filename'] = file\n",
    "            psms_all.append(psms)\n",
    "        psms_all = pd.concat(psms_all, ignore_index=True)\n",
    "    else:\n",
    "        raise Exception('query_files should be either a string or a list. The selected query_files argument is of type: {}'.format(type(query_files)))\n",
    "    return psms_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching with database\n",
    "\n",
    "We save intermediate results to hdf5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "import alphapept.io\n",
    "import alphapept.fasta\n",
    "\n",
    "def store_hdf(df, path, key, replace=False, swmr = False):\n",
    "    \"\"\"\n",
    "    Stores in hdf\n",
    "    \"\"\"\n",
    "    ms_file = alphapept.io.MS_Data_File(path.file_name, is_overwritable=True)\n",
    "    \n",
    "    if replace:\n",
    "        ms_file.write(df, dataset_name=key, swmr = swmr)\n",
    "    else:\n",
    "        try:\n",
    "            df.to_hdf(path, key=key, append=True)\n",
    "            #TODO, append is not implemented yet\n",
    "        except (ValueError, AttributeError):\n",
    "            try:\n",
    "                old_df = ms_file.read(dataset_name=key, swmr = swmr)\n",
    "                new_df = pd.concat([old_df, df])\n",
    "                ms_file.write(new_df, dataset_name=key, swmr = swmr)\n",
    "            except KeyError: # File is created new\n",
    "                ms_file.write(df, dataset_name=key, swmr = swmr)\n",
    "    \n",
    "    \n",
    "def search_db(to_process, callback = None, parallel=False, first_search = True):\n",
    "    \"\"\"\n",
    "    Perform a databse search. One file at a time.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        index, settings = to_process\n",
    "        file_name = settings['experiment']['file_paths'][index]\n",
    "        base_file_name, ext = os.path.splitext(file_name)\n",
    "        ms_file = base_file_name+\".ms_data.hdf\"\n",
    "\n",
    "        skip = False\n",
    "        feature_calibration = False\n",
    "        \n",
    "        ms_file_ = alphapept.io.MS_Data_File(\n",
    "            f\"{ms_file}\"\n",
    "        )\n",
    "        \n",
    "        if not first_search:\n",
    "            try:\n",
    "                calibration = float(ms_file_.read(group_name = 'features', dataset_name='corrected_mass', attr_name='estimated_max_precursor_ppm'))\n",
    "                if calibration == 0:\n",
    "                    logging.info('Calibration is 0, skipping second database search.')\n",
    "                    skip = True\n",
    "\n",
    "                else:\n",
    "                    settings['search']['m_offset_calibrated'] = calibration*settings['search']['calibration_std']\n",
    "                    calib = settings['search']['m_offset_calibrated']\n",
    "                    logging.info(f\"Found calibrated m_offset with value {calib:.2f}\")\n",
    "            except KeyError as e:\n",
    "                logging.info(f'{e}')\n",
    "        \n",
    "    \n",
    "        if not skip:\n",
    "            db_data_path = settings['experiment']['database_path']\n",
    "\n",
    "    #         TODO calibrated_fragments should be included in settings\n",
    "            query_data = ms_file_.read_DDA_query_data(\n",
    "                calibrated_fragments=True,\n",
    "                database_file_name=settings['experiment']['database_path']\n",
    "            )\n",
    "\n",
    "            features = ms_file_.read(dataset_name=\"features\")\n",
    "\n",
    "            psms, num_specs_compared = get_psms(query_data, db_data_path, features, **settings[\"search\"])\n",
    "            if len(psms) > 0:\n",
    "                psms, ions = get_score_columns(psms, query_data, db_data_path, features, **settings[\"search\"])\n",
    "\n",
    "                if first_search:\n",
    "                    logging.info('Saving first_search results to {}'.format(ms_file))\n",
    "                    save_field = 'first_search'\n",
    "                else:\n",
    "                    logging.info('Saving second_search results to {}'.format(ms_file))\n",
    "                    save_field = 'second_search'\n",
    "\n",
    "                store_hdf(pd.DataFrame(psms), ms_file_, save_field, replace=True)\n",
    "                ion_columns = ['ion_index','ion_type','ion_int','db_int','ion_mass','db_mass','query_idx','db_idx']\n",
    "                store_hdf(pd.DataFrame(ions, columns = ion_columns), ms_file_, 'ions', replace=True)\n",
    "            else:\n",
    "                logging.info('No psms found.')\n",
    "            \n",
    "        logging.info(f'Search of file {file_name} complete.')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f'Search of file {file_name} failed. Exception {e}.')\n",
    "        return f\"{e}\" #Can't return exception object, cast as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Large Fasta and or Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from alphapept.fasta import blocks, generate_peptides, add_to_pept_dict\n",
    "from alphapept.io import list_to_numpy_f32\n",
    "from alphapept.fasta import block_idx, generate_fasta_list, generate_spectra, check_peptide\n",
    "from alphapept import constants\n",
    "mass_dict = constants.mass_dict\n",
    "import os\n",
    "import alphapept.speed\n",
    "\n",
    "def search_fasta_block(to_process):\n",
    "    \"\"\"\n",
    "    Search fasta block\n",
    "    For searches with big fasta files or unspecific searches\n",
    "    \"\"\"\n",
    "\n",
    "    fasta_index, fasta_block, ms_files, settings = to_process\n",
    "\n",
    "    settings_ = settings[0]\n",
    "    spectra_block = settings_['fasta']['spectra_block']\n",
    "    to_add = List()\n",
    "\n",
    "    psms_container = [list() for _ in ms_files]\n",
    "\n",
    "    f_index = 0\n",
    "\n",
    "    pept_dict = {}\n",
    "    for element in fasta_block:\n",
    "        sequence = element[\"sequence\"]\n",
    "        mod_peptides = generate_peptides(sequence, **settings_['fasta'])\n",
    "        \n",
    "        pept_dict, added_peptides = add_to_pept_dict(pept_dict, mod_peptides, fasta_index+f_index)\n",
    "\n",
    "        if len(added_peptides) > 0:\n",
    "            to_add.extend(added_peptides)\n",
    "            \n",
    "        f_index += 1\n",
    "        \n",
    "\n",
    "    if len(to_add) > 0:\n",
    "        for seq_block in blocks(to_add, spectra_block):\n",
    "                    \n",
    "            spectra = generate_spectra(seq_block, mass_dict)\n",
    "            \n",
    "            precmasses, seqs, fragmasses, fragtypes = zip(*spectra)\n",
    "            sortindex = np.argsort(precmasses)\n",
    "                        \n",
    "            fragmasses = np.array(fragmasses, dtype=object)[sortindex]\n",
    "            fragtypes = np.array(fragtypes, dtype=object)[sortindex]\n",
    "\n",
    "            lens = [len(_) for _ in fragmasses]\n",
    "\n",
    "            n_frags = sum(lens)\n",
    "            \n",
    "            \n",
    "            frags = np.zeros(n_frags, dtype=fragmasses[0].dtype)\n",
    "            frag_types = np.zeros(n_frags, dtype=fragtypes[0].dtype)\n",
    "\n",
    "            indices = np.zeros(len(lens) + 1, np.int64)\n",
    "            indices[1:] = lens\n",
    "            indices = np.cumsum(indices)\n",
    "\n",
    "            #Fill data\n",
    "\n",
    "            for _ in range(len(indices)-1):\n",
    "                start = indices[_]\n",
    "                end = indices[_+1]\n",
    "                frags[start:end] = fragmasses[_]\n",
    "                frag_types[start:end] = fragtypes[_]\n",
    "\n",
    "            db_data = {}\n",
    "\n",
    "            db_data[\"precursors\"] = np.array(precmasses)[sortindex]\n",
    "            db_data[\"seqs\"] = np.array(seqs)[sortindex]\n",
    "\n",
    "            db_data[\"fragmasses\"] = frags\n",
    "            db_data[\"fragtypes\"] = frag_types\n",
    "            db_data[\"indices\"] = indices\n",
    "\n",
    "            for file_idx, ms_file in enumerate(ms_files):\n",
    "                query_data = alphapept.io.MS_Data_File(\n",
    "                    f\"{ms_file}\"\n",
    "                ).read_DDA_query_data(swmr=True)\n",
    "\n",
    "                try:\n",
    "                    features = alphapept.io.MS_Data_File(\n",
    "                        ms_file\n",
    "                    ).read(dataset_name=\"features\",swmr=True)\n",
    "                except FileNotFoundError:\n",
    "                    features = None\n",
    "                except KeyError:\n",
    "                    features = None\n",
    "                \n",
    "                psms, num_specs_compared = get_psms(query_data, db_data, features, **settings[file_idx][\"search\"])\n",
    "\n",
    "                if len(psms) > 0:\n",
    "                    #This could be speed up..\n",
    "                    psms, ions = get_score_columns(psms, query_data, db_data, features, **settings[file_idx][\"search\"])\n",
    "\n",
    "                    fasta_indices = [set(x for x in pept_dict[_]) for _ in psms['sequence']]\n",
    "\n",
    "                    psms_df = pd.DataFrame(psms)\n",
    "                    psms_df['fasta_index'] = fasta_indices\n",
    "\n",
    "                    psms_container[file_idx].append(psms_df)\n",
    "\n",
    "    return psms_container, len(to_add)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def filter_top_n(temp, top_n = 10):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and keeps only the top n entries. \n",
    "    Combines fasta indices for sequences.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    pept_dict_ = {}\n",
    "\n",
    "    for k, v in temp[['sequence','fasta_index']].values:\n",
    "        if k in pept_dict_:\n",
    "            new_set = pept_dict_[k]\n",
    "            if isinstance(v, set):\n",
    "                new_set.update(v)\n",
    "            else:\n",
    "                new_set.add(v) \n",
    "            pept_dict_[k] = new_set\n",
    "        else:\n",
    "            pept_dict_[k] = set(v)\n",
    "\n",
    "    temp['fasta_index'] = [pept_dict_[_] for _ in temp['sequence']]\n",
    "    temp = temp.drop_duplicates(subset = ['raw_idx','sequence','hits','feature_idx'])\n",
    "    temp = temp.sort_values('hits', ascending = False).groupby('raw_idx').head(top_n)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "def search_parallel(settings, calibration = None, callback = None):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    fasta_list, fasta_dict = generate_fasta_list(fasta_paths = settings['experiment']['fasta_paths'], **settings['fasta'])\n",
    "\n",
    "    fasta_block = settings['fasta']['fasta_block']\n",
    "\n",
    "    ms_file_path = []\n",
    "\n",
    "    for _ in settings['experiment']['file_paths']:\n",
    "        base, ext = os.path.splitext(_)\n",
    "        ms_file_path.append(base + '.ms_data.hdf')\n",
    "\n",
    "    if calibration:\n",
    "        custom_settings = []\n",
    "        for _ in calibration:\n",
    "            settings_ = copy.deepcopy(settings)\n",
    "            settings_[\"search\"][\"m_offset_calibrated\"] = _\n",
    "            custom_settings.append(settings_)\n",
    "    else:\n",
    "        custom_settings = [settings for _ in ms_file_path]\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Number of FASTA entries: {len(fasta_list):,} - FASTA settings {settings['fasta']}\")\n",
    "    to_process = [(idx_start, fasta_list[idx_start:idx_end], ms_file_path, custom_settings) for idx_start, idx_end in block_idx(len(fasta_list), fasta_block)]\n",
    "\n",
    "    n_processes = settings['general']['n_processes']\n",
    "\n",
    "    n_seqs_ = 0\n",
    "    \n",
    "    for _ in ms_file_path:\n",
    "        ms_file = alphapept.io.MS_Data_File(_+'_', is_new_file=True) #Create temporary files for writing\n",
    "        \n",
    "    df_cache = {} \n",
    "                \n",
    "    with alphapept.speed.AlphaPool(n_processes) as p:\n",
    "        max_ = len(to_process)\n",
    "\n",
    "        for i, (_, n_seqs) in enumerate(p.imap_unordered(search_fasta_block, to_process)):\n",
    "            n_seqs_ += n_seqs\n",
    "\n",
    "\n",
    "            logging.info(f'Block {i+1} of {max_} complete - {((i+1)/max_*100):.2f} % - created peptides {n_seqs:,} ')\n",
    "            for j in range(len(_)): #Temporary hdf files for avoiding saving issues\n",
    "                output = [_ for _ in _[j]]\n",
    "                if len(output) > 0:  \n",
    "\n",
    "                    psms = pd.concat(output)\n",
    "\n",
    "                    if ms_file_path[j] in df_cache:\n",
    "                        df_cache[ms_file_path[j]] = filter_top_n(pd.concat([df_cache[ms_file_path[j]], psms]))\n",
    "                    else:\n",
    "                        df_cache[ms_file_path[j]] = psms\n",
    "\n",
    "\n",
    "            if callback:\n",
    "                callback((i+1)/max_)\n",
    "                 \n",
    "    for _ in ms_file_path:    \n",
    "        if _ in df_cache:\n",
    "            x = df_cache[_]\n",
    "            ms_file = alphapept.io.MS_Data_File(_) \n",
    "            \n",
    "            x['fasta_index'] = x['fasta_index'].apply(lambda x: ','.join(str(_) for _ in x))\n",
    "            \n",
    "            if calibration:\n",
    "                store_hdf(x, ms_file, 'second_search', replace = True)\n",
    "            else:\n",
    "                store_hdf(x, ms_file, 'first_search', replace = True)\n",
    "                    \n",
    "    logging.info(f'Complete. Created peptides {n_seqs_:,}')\n",
    "    \n",
    "    return fasta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
