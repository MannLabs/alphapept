{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search\n",
    "\n",
    "> Functions related to the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all functions related to the searching and getting peptide-spectrum-matches (PSMS).\n",
    "\n",
    "Current ToDo here:\n",
    "\n",
    "- Most of the functions are not very well described yet\n",
    "- A key ingredient of speed is the frag_hits matrix to store the results which should be described\n",
    "- Introductory text to give an overview would be nice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import warnings\n",
    "from numba import NumbaPendingDeprecationWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=NumbaPendingDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Fragments\n",
    "\n",
    "To efficiently compare two fragments, we use a pointer based approach. We start with two sorted arrays, the `query_frag` that contains the m/z positions of the query spectrum and the `db_frag` which contains the database fragment that is compared against to. The two pointers compare each m/z position with each other and check wheter they are within a certain tolerance `mtol`. Depending on their delta, either of the pointers is advanced. The function returns an arrray named `hits` that is the same length as the database spectrum and encodes the hit positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "\n",
    "@njit(nogil=True)\n",
    "def compare_frags(query_frag, db_frag, mtol, ppm=False):\n",
    "    \"\"\"\n",
    "    Compare query and database frags and find hits\n",
    "    \"\"\"\n",
    "    q_max = len(query_frag)\n",
    "    d_max = len(db_frag)\n",
    "    hits = np.zeros(d_max, dtype=np.int16)\n",
    "    q, d = 0, 0  # q > query, d > database\n",
    "    while q < q_max and d < d_max:\n",
    "        mass1 = query_frag[q]\n",
    "        mass2 = db_frag[d]\n",
    "        delta_mass = mass1 - mass2\n",
    "\n",
    "        if ppm:\n",
    "            sum_mass = mass1 + mass2\n",
    "            mass_difference = 2 * delta_mass / sum_mass * 1e6\n",
    "        else:\n",
    "            mass_difference = delta_mass\n",
    "\n",
    "        if abs(mass_difference) <= mtol:\n",
    "            hits[d] = q + 1  # Save query position +1 (zero-indexing)\n",
    "            d += 1\n",
    "            q += 1  # Only one query for each db element\n",
    "        elif delta_mass < 0:\n",
    "            q += 1\n",
    "        elif delta_mass > 0:\n",
    "            d += 1\n",
    "\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, 0], dtype=int16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "query_frag = np.array([100, 200, 300, 400])\n",
    "db_frag = np.array([150, 200, 300, 450])\n",
    "\n",
    "# Hits: Query 2 -> Db 2 and Query 3 -> Db 3\n",
    "\n",
    "compare_frags(query_frag, db_frag, mtol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_compare_frags():\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    mtol = 1\n",
    "\n",
    "    db_frag = query_frag.copy()\n",
    "\n",
    "    # Self-Comparison: no of hits should be same as length\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == len(query_frag)\n",
    "\n",
    "    # Self-Comparison: above but in tolerance\n",
    "    hits = compare_frags(query_frag, db_frag + mtol - 0.01, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == len(query_frag)\n",
    "\n",
    "    # Self-Comparison: below but in tolerance\n",
    "    hits = compare_frags(query_frag, db_frag - mtol + 0.01, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == len(query_frag)\n",
    "\n",
    "    # Self-Comparison: above tolerance, no hits\n",
    "    hits = compare_frags(query_frag, db_frag + mtol + 0.01, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 0\n",
    "\n",
    "    # Special case 1: First and last\n",
    "    db_frag = np.array([100, 400])\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 2\n",
    "\n",
    "    # Special case 2: Two queries matching the same DB frag\n",
    "    query_frag = np.array([100, 100.5])\n",
    "    db_frag = np.array([100, 200, 300])\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 1\n",
    "\n",
    "    # Special case 3: Two db frags matching the same query frag\n",
    "    db_frag = np.array([100, 100.5])\n",
    "    query_frag = np.array([100, 200, 300])\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert np.sum(hits > 0) == 1\n",
    "    \n",
    "test_compare_frags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows us to easily compare a query spectrum against a theoretical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from alphapept import constants\n",
    "from alphapept.fasta import get_frag_dict, parse\n",
    "import alphapept.io\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "peptide = 'PEPTIDE'\n",
    "\n",
    "# Theoretical Spectrum\n",
    "\n",
    "frag_dict = get_frag_dict(parse(peptide), constants.mass_dict)\n",
    "db_frag = list(frag_dict.values())\n",
    "db_frag.sort()\n",
    "\n",
    "db_int = [100 for _ in db_frag]\n",
    "\n",
    "# Experimental Spectrum, dummy data\n",
    "\n",
    "query_frag = np.array([98.06, 227.10, 263.08, 548.06, 653.31])\n",
    "query_int = np.array([20, 80, 30, 30, 50])\n",
    "\n",
    "hits = compare_frags(query_frag, db_frag, mtol=1)\n",
    "\n",
    "hitpos = hits[hits > 0] - 1\n",
    "hit_x = query_frag[hitpos]\n",
    "hit_y = query_int[hitpos]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "plt.vlines(query_frag, 0, query_int, \"r\", label=\"Query\", alpha=0.5)\n",
    "\n",
    "plt.plot(hit_x, hit_y, \"ro\", label=\"Hit\", alpha=0.5)\n",
    "\n",
    "for _ in frag_dict.keys():\n",
    "    plt.text(frag_dict[_], 104, _, fontsize=12, alpha = 0.8)\n",
    "    \n",
    "plt.title('Theoretical Spectrum for {}'.format(peptide))\n",
    "plt.xlabel('Mass')\n",
    "plt.ylabel('Intensity')\n",
    "plt.legend()\n",
    "plt.ylim([0,110])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Spectra\n",
    "\n",
    "To compare multiple spectra against a database, we first need some helper functions. Per default, AlphaPept calculates in Dalton. To use ppm boundaries, we need the function `ppm_to_dalton` for conversion. \n",
    "\n",
    "To minimize the search space, we typically only compare spectra with precursors in the same mass range as defined by `m_offset`. To look up the limits for search, we define the function `get_idxs`, which is a wrapper to the fast `searchsorted` method from NumPy.\n",
    "\n",
    "The actual search takes place in `compare_specs_single` and `compare_specs_parallel`, a single-core and multicore method for comparing spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import prange\n",
    "@njit\n",
    "def ppm_to_dalton(mass, m_offset):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    return mass / 1e6 * m_offset\n",
    "\n",
    "\n",
    "def get_idxs(db_masses, query_masses, m_offset, ppm):\n",
    "    \"\"\"\n",
    "    Function to get upper and lower limits to define search range.\n",
    "\n",
    "    \"\"\"\n",
    "    if ppm:\n",
    "        dalton_offset = ppm_to_dalton(query_masses, m_offset)\n",
    "    else:\n",
    "        dalton_offset = m_offset\n",
    "\n",
    "    idxs_lower = db_masses.searchsorted(query_masses - dalton_offset, side=\"left\")\n",
    "    idxs_higher = db_masses.searchsorted(query_masses + dalton_offset, side=\"right\")\n",
    "\n",
    "    return idxs_lower, idxs_higher\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compare_specs_parallel(\n",
    "    frag_hits,\n",
    "    query_masses,\n",
    "    query_frags,\n",
    "    query_indices,\n",
    "    db_masses,\n",
    "    db_frags,\n",
    "    idxs_lower,\n",
    "    idxs_higher,\n",
    "    mtol,\n",
    "    query_bounds,\n",
    "    db_bounds,\n",
    "    chunk=(0, 1),\n",
    "    offset=False,\n",
    "    ppm=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare spectra, parallelized version\n",
    "    \"\"\"\n",
    "    num_specs_compared = 0\n",
    "    if chunk == (0, 0):\n",
    "        for query_idx in prange(len(query_masses)):\n",
    "            for db_idx in range(idxs_lower[query_idx] , idxs_higher[query_idx] ):\n",
    "                num_specs_compared += 1\n",
    "                query_idx_start = query_indices[query_idx]\n",
    "                query_idx_end = query_indices[query_idx + 1]\n",
    "                query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "                db_frag = db_frags[:, db_idx] [: db_bounds[db_idx] ]\n",
    "                o_mass = query_masses[query_idx]  - db_masses[db_idx]\n",
    "                hits = compare_frags(query_frag, db_frag, mtol, ppm)\n",
    "                if offset is True:  # Mirrored search\n",
    "                    hits += compare_frags(query_frag, db_frag + o_mass, mtol, ppm)\n",
    "                frag_hits[query_idx, db_idx - idxs_lower[query_idx] ] = np.sum(hits > 0)\n",
    "    else:\n",
    "        # Chunked version\n",
    "        current_index = chunk[0]\n",
    "        n_chunks = chunk[1]\n",
    "        m_subset = np.arange(len(query_masses))[current_index::n_chunks]\n",
    "\n",
    "        for index in prange(len(m_subset)):\n",
    "            query_idx = m_subset[index]\n",
    "            for db_idx in range(idxs_lower[query_idx] , idxs_higher[query_idx] ):\n",
    "                num_specs_compared += 1\n",
    "                query_idx_start = query_indices[query_idx]\n",
    "                query_idx_end = query_indices[query_idx + 1]\n",
    "                query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "                db_frag = db_frags[:, db_idx] [: db_bounds[db_idx] ]\n",
    "                o_mass = query_masses[query_idx]  - db_masses[db_idx]\n",
    "                hits = compare_frags(query_frag, db_frag, mtol, ppm)\n",
    "                if offset is True:  # Mirrored search\n",
    "                    hits += compare_frags(query_frag, db_frag + o_mass, mtol, ppm)\n",
    "                frag_hits[query_idx, db_idx - idxs_lower[query_idx] ] = np.sum(hits > 0)\n",
    "\n",
    "    return frag_hits, num_specs_compared\n",
    "\n",
    "\n",
    "@njit\n",
    "def compare_specs_single(\n",
    "    frag_hits,\n",
    "    query_masses,\n",
    "    query_frags,\n",
    "    query_indices,\n",
    "    db_masses,\n",
    "    db_frags,\n",
    "    idxs_lower,\n",
    "    idxs_higher,\n",
    "    mtol,\n",
    "    query_bounds,\n",
    "    db_bounds,\n",
    "    chunk=(0, 1),\n",
    "    offset=False,\n",
    "    ppm=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare spectra, single core version\n",
    "\n",
    "    \"\"\"\n",
    "    num_specs_compared = 0\n",
    "\n",
    "    if chunk == (0, 0):\n",
    "        for query_idx in range(len(query_masses)):\n",
    "            for db_idx in range(idxs_lower[query_idx] , idxs_higher[query_idx] ):\n",
    "                num_specs_compared += 1\n",
    "                query_idx_start = query_indices[query_idx]\n",
    "                query_idx_end = query_indices[query_idx + 1]\n",
    "                query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "                db_frag = db_frags[:, db_idx] [: db_bounds[db_idx] ]\n",
    "                o_mass = query_masses[query_idx]  - db_masses[db_idx]\n",
    "                hits = compare_frags(query_frag, db_frag, mtol, ppm)\n",
    "                if offset is True:  # Mirrored search\n",
    "                    hits += compare_frags(query_frag, db_frag + o_mass, mtol, ppm)\n",
    "                frag_hits[query_idx, db_idx - idxs_lower[query_idx] ] = np.sum(hits > 0)\n",
    "    else:\n",
    "        # Chunked version\n",
    "        current_index = chunk[0]\n",
    "        n_chunks = chunk[1]\n",
    "\n",
    "        m_subset = np.arange(len(query_masses))[current_index::n_chunks]\n",
    "\n",
    "        for index in range(len(m_subset)):\n",
    "            query_idx = m_subset[index]\n",
    "            for db_idx in range(idxs_lower[query_idx] , idxs_higher[query_idx] ):\n",
    "                num_specs_compared += 1\n",
    "                query_idx_start = query_indices[query_idx]\n",
    "                query_idx_end = query_indices[query_idx + 1]\n",
    "                query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "                db_frag = db_frags[:, db_idx] [: db_bounds[db_idx] ]\n",
    "                o_mass = query_masses[query_idx]  - db_masses[db_idx]\n",
    "                hits = compare_frags(query_frag, db_frag, mtol, ppm)\n",
    "                if offset is True:  # Mirrored search\n",
    "                    hits += compare_frags(query_frag, db_frag + o_mass, mtol, ppm)\n",
    "                frag_hits[query_idx, db_idx - idxs_lower[query_idx] ] = np.sum(hits > 0)\n",
    "\n",
    "    return frag_hits, num_specs_compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def query_data_to_features(query_data):\n",
    "\n",
    "    # if we dont use the feature finder we extract them from the query data..\n",
    "\n",
    "    query_masses = query_data['prec_mass_list2']\n",
    "    query_mz = query_data['mono_mzs2']\n",
    "    query_rt = query_data['rt_list_ms2']\n",
    "\n",
    "    features = pd.DataFrame(np.array([query_masses, query_mz, query_rt]).T, columns = ['mass_matched', 'mz_matched', 'rt_matched'])\n",
    "\n",
    "    features['feature_idx'] = features.index #Index to query_data\n",
    "    features['query_idx']  = np.arange(len(query_masses))\n",
    "\n",
    "    features = features.sort_values('mass_matched', ascending=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_psms(\n",
    "    query_data,\n",
    "    db_data,\n",
    "    features,\n",
    "    parallel,\n",
    "    m_tol,\n",
    "    m_offset,\n",
    "    ppm,\n",
    "    min_frag_hits,\n",
    "    callback = None,\n",
    "    m_offset_calibrated = None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper function to extract psms from dataset\n",
    "\n",
    "    Args:\n",
    "        db_masses: database precursor masses\n",
    "        query_masses: query precursor masses\n",
    "        m_offset: mass offset in dalton or ppm\n",
    "        ppm: flag for ppm or dalton\n",
    "        callback: Callback function, e.g. for progress bar\n",
    "    Returns:\n",
    "        idxs_lower: lower search range\n",
    "        idxs_higher: upper search range\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    db_masses = db_data['precursors']\n",
    "    db_frags = db_data['fragmasses']\n",
    "    db_bounds = db_data['bounds']\n",
    "    \n",
    "    query_indices = query_data[\"indices_ms2\"]\n",
    "    query_bounds = query_data['bounds']\n",
    "    query_frags = query_data['mass_list_ms2']\n",
    "\n",
    "    if features is not None:\n",
    "        if m_offset_calibrated:\n",
    "            m_offset = m_offset_calibrated\n",
    "            query_masses = features['corrected_mass'].values\n",
    "        else:\n",
    "            query_masses = features['mass_matched'].values\n",
    "        query_mz = features['mz_matched'].values\n",
    "        query_rt = features['rt_matched'].values\n",
    "        query_bounds = query_bounds[features['query_idx'].values]\n",
    "        query_selection = features['query_idx'].values\n",
    "        indices = np.zeros(len(query_selection) + 1, np.int64)\n",
    "        indices[1:] = np.diff(query_indices)[query_selection]\n",
    "        indices = np.cumsum(indices)\n",
    "        query_frags = np.concatenate(\n",
    "            [\n",
    "                query_frags[s: e] for s, e in zip(\n",
    "                    query_indices[query_selection], query_indices[query_selection + 1]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        query_indices = indices\n",
    "    else:\n",
    "        if m_offset_calibrated:\n",
    "            m_offset = m_offset_calibrated\n",
    "        query_masses = query_data['prec_mass_list2']\n",
    "        query_mz = query_data['mono_mzs2']\n",
    "        query_rt = query_data['rt_list_ms2']\n",
    "    \n",
    "#     idxs_lower, idxs_higher = get_idxs(db_masses, query_masses, m_offset, ppm)\n",
    "    idxs_lower, idxs_higher = get_idxs(\n",
    "        db_masses,\n",
    "        query_masses,\n",
    "        m_offset,\n",
    "        ppm\n",
    "    )\n",
    "    frag_hits = np.zeros(\n",
    "        (len(query_masses), np.max(idxs_higher - idxs_lower)), dtype=int\n",
    "    )\n",
    "\n",
    "    logging.info('Performing search on {:,} query and {:,} db entries with m_tol = {} and m_offset = {}.'.format(len(query_masses), len(db_masses), m_tol, m_offset))\n",
    "\n",
    "    if callback is None:\n",
    "        chunk = (0, 0)\n",
    "        offset = False\n",
    "\n",
    "        if parallel:\n",
    "            frag_hits, num_specs_compared = compare_specs_parallel(\n",
    "                frag_hits,\n",
    "                query_masses,\n",
    "                query_frags,\n",
    "                query_indices,\n",
    "                db_masses,\n",
    "                db_frags,\n",
    "                idxs_lower,\n",
    "                idxs_higher,\n",
    "                m_tol,\n",
    "                query_bounds,\n",
    "                db_bounds,\n",
    "                chunk,\n",
    "                offset,\n",
    "                ppm,\n",
    "            )\n",
    "        else:\n",
    "            frag_hits, num_specs_compared = compare_specs_single(\n",
    "                frag_hits,\n",
    "                query_masses,\n",
    "                query_frags,\n",
    "                query_indices,\n",
    "                db_masses,\n",
    "                db_frags,\n",
    "                idxs_lower,\n",
    "                idxs_higher,\n",
    "                m_tol,\n",
    "                query_bounds,\n",
    "                db_bounds,\n",
    "                chunk,\n",
    "                offset,\n",
    "                ppm,\n",
    "            )\n",
    "\n",
    "    else: #chunk the data for progess bar until numba has proper progress bar\n",
    "        n_chunks = 100\n",
    "        num_specs_compared = 0\n",
    "\n",
    "        for current_chunk in range(n_chunks):\n",
    "            chunk = (current_chunk, n_chunks)\n",
    "            offset = False\n",
    "            if parallel:\n",
    "                frag_hits, num_specs_compared_chunk = compare_specs_parallel(\n",
    "                    frag_hits,\n",
    "                    query_masses,\n",
    "                    query_frags,\n",
    "                    query_indices,\n",
    "                    db_masses,\n",
    "                    db_frags,\n",
    "                    idxs_lower,\n",
    "                    idxs_higher,\n",
    "                    m_tol,\n",
    "                    query_bounds,\n",
    "                    db_bounds,\n",
    "                    chunk,\n",
    "                    offset,\n",
    "                    ppm,\n",
    "                )\n",
    "            else:\n",
    "                frag_hits, num_specs_compared_chunk = compare_specs_single(\n",
    "                    frag_hits,\n",
    "                    query_masses,\n",
    "                    query_frags,\n",
    "                    query_indices,\n",
    "                    db_masses,\n",
    "                    db_frags,\n",
    "                    idxs_lower,\n",
    "                    idxs_higher,\n",
    "                    m_tol,\n",
    "                    query_bounds,\n",
    "                    db_bounds,\n",
    "                    chunk,\n",
    "                    offset,\n",
    "                    ppm,\n",
    "                )\n",
    "\n",
    "            if callback is not None:\n",
    "                callback((current_chunk+1)/n_chunks)\n",
    "            num_specs_compared += num_specs_compared_chunk\n",
    "\n",
    "    hit_query, hit_db = np.where(frag_hits >= min_frag_hits)\n",
    "    hits = frag_hits[hit_query, hit_db]\n",
    "    hit_db += idxs_lower[hit_query]\n",
    "\n",
    "    psms = np.array(\n",
    "        list(zip(hit_query, hit_db, hits)), dtype=[(\"query_idx\", int), (\"db_idx\", int), (\"hits\", int)]\n",
    "    )\n",
    "\n",
    "    logging.info('Compared {:,} spectra and found {:,} psms.'.format(num_specs_compared, len(psms)))\n",
    "\n",
    "    return psms, num_specs_compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting columns for scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frag Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def frag_delta(query_frag, db_frag, hits):\n",
    "    \"\"\"\n",
    "    Calculate the mass difference for a given array of hits in Dalton and ppm\n",
    "    \"\"\"\n",
    "\n",
    "    delta_m = db_frag[hits > 0] - query_frag[hits[hits > 0] - 1]\n",
    "    delta_m_ppm = (\n",
    "        2 * delta_m / (db_frag[hits > 0] + query_frag[hits[hits > 0] - 1]) * 1e6\n",
    "    )\n",
    "\n",
    "    return delta_m, delta_m_ppm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_frag_delta():\n",
    "    mtol = 10\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([101, 202, 303, 404])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    delta_m, delta_m_ppm = frag_delta(query_frag, db_frag, hits)\n",
    "\n",
    "    assert np.sum(delta_m) == 10\n",
    "    \n",
    "test_frag_delta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def intensity_fraction(query_int, hits):\n",
    "    \"\"\"\n",
    "    Calculate the fraction of matched intensity\n",
    "    \"\"\"\n",
    "    total_intensity = np.sum(query_int)\n",
    "    if total_intensity != 0:\n",
    "        matched_intensity = np.sum(query_int[hits[hits > 0] - 1])\n",
    "        i_frac = matched_intensity / total_intensity\n",
    "    else:\n",
    "        i_frac = 0\n",
    "\n",
    "    return i_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_intensity_fraction():\n",
    "    mtol = 1\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    i_frac = intensity_fraction(query_int, hits)\n",
    "\n",
    "    assert i_frac == 40 / 100\n",
    "\n",
    "test_intensity_fraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def intensity_product(query_int, hits, db_int=None):\n",
    "    \"\"\"\n",
    "    Calculate the dot product of matched query intensity to db intensity\n",
    "    \"\"\"\n",
    "\n",
    "    matched_query_int = query_int[hits[hits > 0] - 1]\n",
    "    if db_int is None:\n",
    "        matched_intensity = np.sum(matched_query_int)\n",
    "    else:\n",
    "        matched_db_int = db_int[hits > 0]\n",
    "        matched_intensity = np.sum(matched_query_int*matched_db_int)\n",
    "\n",
    "    return matched_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_intensity_product():\n",
    "    mtol = 1\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert intensity_product(query_int, hits) == 40\n",
    "\n",
    "    query_frag = np.array([100, 200, 300, 400, 600])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40, 50])\n",
    "    db_int = np.array([10, 20, 30, 40])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "    assert intensity_product(query_int, hits, db_int = db_int) == 10*10+30*20\n",
    "    \n",
    "test_intensity_product()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass density scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used to calculate scores that take the relative abundance of fragment ion masses in the database into account. The underlying hypothesis is that matched ions in less populated m/z regions are worth more that ions matched in highly populated m/z regions. \n",
    "\n",
    "The `get_frag_mass_density_norm` takes fragment ion masses `frags` as input and creates both an array of m/z bins and the respective negaive m/z density values. The `mz_factor` determines the m/z bin-width. A factor of 0.5 results in m/z bins of 20 Da, while a factor of 10 results in m/z bins of 0.1 Da. \n",
    "\n",
    "The score calculated by `mass_density_weighted_fragment_sum` corresponds to the sum of fragment ion hits weighted by the negative mass density. A fragment ion match in a sparsely populated m/z region therefor counts nearly 1, while a match in a densely populated region contributes with a low weight, e.g. 0.1.\n",
    "\n",
    "The score calculated by `mass_density_weighted_intensity_sum` corresponds to a matched intensity score weighted by the negative mass density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@njit\n",
    "def get_frag_mass_density_norm(frags, mz_factor=0.05):\n",
    "    frags_f = frags.flatten()\n",
    "    frags_f_p = frags_f[frags_f >= 0]\n",
    "    frags_f_p = (frags_f_p*mz_factor).astype(np.int64)\n",
    "    mz_space = np.arange(0,np.max(frags_f_p)+1,1)/mz_factor\n",
    "    mz_count = np.bincount(frags_f_p)\n",
    "\n",
    "    if np.percentile(mz_count,90) > 0:\n",
    "        if np.max(mz_count)/np.percentile(mz_count,90) > 50:\n",
    "            mz_count_norm = np.zeros(len(mz_count))\n",
    "            for i in prange(len(mz_count)):\n",
    "                if mz_count[i] > 0:\n",
    "                    mz_count_norm[i] = np.log(mz_count[i])\n",
    "                else:\n",
    "                    mz_count_norm[i] = 0\n",
    "            max_int = np.max(mz_count_norm)\n",
    "            mz_count_norm = mz_count_norm/max_int\n",
    "        else:\n",
    "            max_int = np.max(mz_count)\n",
    "            mz_count_norm = mz_count/max_int\n",
    "    else:\n",
    "        max_int = np.max(mz_count)\n",
    "        mz_count_norm = mz_count/max_int\n",
    "\n",
    "    percentile_range = np.arange(0,0.9,0.1)\n",
    "    mz_percentiles = np.searchsorted(percentile_range, mz_count_norm, side='left')\n",
    "\n",
    "    neg_mz_density = np.zeros(len(mz_space))\n",
    "    for i in prange(len(mz_space)):\n",
    "        neg_mz_density[i] = (10-mz_percentiles[i])/10\n",
    "\n",
    "    return neg_mz_density, mz_space\n",
    "\n",
    "@njit\n",
    "def mass_density_weighted_fragment_sum(query_frag, hits, mz_space, mz_density, mz_factor=0.05):\n",
    "    matched_query_frag = query_frag[hits[hits > 0]-1]\n",
    "    matched_query_frag_bin = (matched_query_frag*mz_factor).astype(np.int64)\n",
    "\n",
    "    matched_query_frag_density = np.zeros(len(matched_query_frag_bin))\n",
    "    for i in prange(len(matched_query_frag_bin)):\n",
    "        matched_query_frag_density[i] = mz_density[matched_query_frag_bin[i]]\n",
    "    return np.sum(matched_query_frag_density)\n",
    "\n",
    "@njit\n",
    "def mass_density_weighted_intensity_sum(query_int, query_frag, hits, mz_space, mz_density, mz_factor=0.05):\n",
    "    matched_query_frag = query_frag[hits[hits > 0]-1]\n",
    "    matched_query_frag_bin = (matched_query_frag*mz_factor).astype(np.int64)\n",
    "\n",
    "    matched_query_frag_density = np.zeros(len(matched_query_frag_bin))\n",
    "    for i in prange(len(matched_query_frag_bin)):\n",
    "        matched_query_frag_density[i] = mz_density[matched_query_frag_bin[i]]\n",
    "\n",
    "    matched_query_int = query_int[hits[hits > 0] - 1]\n",
    "\n",
    "    if len(matched_query_frag_density) > 0:\n",
    "        weighted_intensity_sum = np.sum(matched_query_int*matched_query_frag_density)\n",
    "        if weighted_intensity_sum!=0:\n",
    "            weighted_intensity_sum = np.log(weighted_intensity_sum)\n",
    "        else:\n",
    "            weighted_intensity_sum = 0\n",
    "    else:\n",
    "        weighted_intensity_sum = 0\n",
    "\n",
    "    return weighted_intensity_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_frag_mass_density_norm():\n",
    "    db_frags = np.array([-1, 10, 20, 100, 110, -1])\n",
    "\n",
    "    neg_mz_density, mz_space = get_frag_mass_density_norm(db_frags, mz_factor=0.05)\n",
    "    assert len(mz_space) == 6\n",
    "    assert mz_space[0] == 0\n",
    "    assert all(neg_mz_density == np.array([0.5, 0.5, 1. , 1. , 1. , 0.1]))\n",
    "\n",
    "    neg_mz_density, mz_space = get_frag_mass_density_norm(db_frags, mz_factor=10)\n",
    "    assert len(mz_space) == 1101\n",
    "    assert mz_space[0] == 0\n",
    "    assert all(neg_mz_density[0:5] == np.array([1., 1., 1., 1., 1.]))\n",
    "    assert neg_mz_density[100] == 0.1\n",
    "\n",
    "test_get_frag_mass_density_norm()\n",
    "    \n",
    "def test_mass_density_weighted_fragment_sum():\n",
    "    mtol = 1\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "\n",
    "    db_frags = np.array([-1, 100, 200, 300, 400, -1])\n",
    "    \n",
    "    neg_mz_density, mz_space = get_frag_mass_density_norm(db_frags, mz_factor=0.05)\n",
    "    assert mass_density_weighted_fragment_sum(query_frag, hits, mz_space, neg_mz_density, mz_factor=0.05) == 0.2\n",
    "\n",
    "    neg_mz_density, mz_space = get_frag_mass_density_norm(db_frags, mz_factor=10)\n",
    "    assert np.round(mass_density_weighted_fragment_sum(query_frag, hits, mz_space, neg_mz_density, mz_factor=10), decimals=3) == 0.2\n",
    "\n",
    "test_mass_density_weighted_fragment_sum()\n",
    "\n",
    "def test_mass_density_weighted_intensity_sum():\n",
    "    mtol = 1\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "\n",
    "    db_frags = np.array([-1, 100, 200, 300, 400, -1])\n",
    "    neg_mz_density, mz_space = get_frag_mass_density_norm(db_frags, mz_factor=0.05)\n",
    "    assert np.round(mass_density_weighted_intensity_sum(query_int, query_frag, hits, mz_space, neg_mz_density, mz_factor=0.05), decimals=3) == 1.386\n",
    "    \n",
    "test_mass_density_weighted_intensity_sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B & Y - Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def b_y_hits(frag_type, hits):\n",
    "    \"\"\"\n",
    "    Count the number of b and y hits\n",
    "    hits usually start with b-ions > 0, then y-ions < 1\n",
    "    \"\"\"\n",
    "    hits_index = hits > 0\n",
    "\n",
    "    hit_types = frag_type[hits_index]\n",
    "\n",
    "    b_hits = np.sum(hit_types > 0)\n",
    "    y_hits = np.sum(hit_types < 0)\n",
    "\n",
    "    return b_hits, y_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_b_y_hits():\n",
    "    # TODO: Write a test to make sure the by hits are correct..\n",
    "    mtol = 1\n",
    "    query_frag = np.array([100, 200, 300, 400])\n",
    "    db_frag = np.array([100, 300, 500, 700])\n",
    "    query_int = np.array([10, 20, 30, 40])\n",
    "    frag_type = np.array([1, -1, 2, -2])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "\n",
    "    b_hit, y_hit = b_y_hits(frag_type, hits)\n",
    "\n",
    "    assert (b_hit) == 1\n",
    "    assert (y_hit) == 1\n",
    "\n",
    "    frag_type = np.array([-1, -2, -3, -4])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "\n",
    "    b_hit, y_hit = b_y_hits(frag_type, hits)\n",
    "    \n",
    "    assert (b_hit) == 0\n",
    "    assert (y_hit) == 2\n",
    "\n",
    "    frag_type = np.array([1, 2, 3, 4])\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm=False)\n",
    "\n",
    "    b_hit, y_hit = b_y_hits(frag_type, hits)\n",
    "\n",
    "    assert (b_hit) == 2\n",
    "    assert (y_hit) == 0\n",
    "    \n",
    "test_b_y_hits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting Score columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numpy.lib.recfunctions import append_fields, drop_fields\n",
    "\n",
    "\n",
    "def add_column(recarray, column, name):\n",
    "    \"\"\"\n",
    "    Function to add a column with given name to recarray\n",
    "    \"\"\"\n",
    "    if hasattr(recarray, name):\n",
    "        recarray = drop_fields(recarray, name, usemask=False, asrecarray=True)\n",
    "    recarray = append_fields(\n",
    "        recarray, name, column, dtypes=column.dtype, usemask=False, asrecarray=True\n",
    "    )\n",
    "    return recarray\n",
    "\n",
    "\n",
    "def remove_column(recarray, name):\n",
    "    \"\"\"\n",
    "    Function to remove a column from recarray\n",
    "    \"\"\"\n",
    "    if hasattr(recarray, name):\n",
    "        recarray = drop_fields(recarray, name, usemask=False, asrecarray=True)\n",
    "    return recarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting\n",
    "\n",
    "When performing a database search, we need to know what experimental spectrum we are comparing with which database entry. \n",
    "We distinguish three indices:\n",
    "* query_idx\n",
    "* raw_idx\n",
    "* feature_idx\n",
    "Initially, the get_psms functions accepts experimental data in the form of `query_data`. Here, the `query_idx` refers to the index to `query_data`. However, this might not be the same index as of the raw data. This is due to the implementation of the matching of MS1-features to MS2 spectra. Here we allow multiple matches and implement this by repeating the respective spectra. \n",
    "\n",
    "We then add the two columns `feature_idx` and `raw_idx` to the psms to later be able to distinguish where the match originated from. Here, `raw_idx` refers to the original spectrum.\n",
    "\n",
    "When not applying feature finding, `raw_idx` and `query_idx` are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba.typed import List\n",
    "@njit\n",
    "def get_hits(query_frag, query_int, db_frag, db_int, frag_type, mtol, ppm, losses):\n",
    "    \n",
    "    max_array_size = len(db_frag)*len(losses)\n",
    "\n",
    "    ions = np.zeros((max_array_size, 8))\n",
    "\n",
    "    pointer = 0\n",
    "    \n",
    "    query_range = np.arange(len(query_frag))\n",
    "    db_range = np.arange(len(db_frag))\n",
    "\n",
    "    for idx, off in enumerate(losses):\n",
    "        hits = compare_frags(query_frag, db_frag-off, mtol, ppm)\n",
    "        n_hits = np.sum(hits>0)\n",
    "        \n",
    "        hitpos = hits[hits > 0] - 1\n",
    "        hit = hits > 0\n",
    "        \n",
    "        ions[pointer:pointer+n_hits,0] = frag_type[hits>0] #type\n",
    "        ions[pointer:pointer+n_hits,1] = idx #ion-index\n",
    "        \n",
    "        ions[pointer:pointer+n_hits,2] = query_int[hitpos] #query int\n",
    "        ions[pointer:pointer+n_hits,3] = db_int[hit] #db int\n",
    "        \n",
    "        ions[pointer:pointer+n_hits,4] = query_frag[hitpos] #query mass\n",
    "        ions[pointer:pointer+n_hits,5] = db_frag[hit]-off # db mass  \n",
    "        \n",
    "        ions[pointer:pointer+n_hits,6] = query_range[hitpos] # index to db entry\n",
    "        ions[pointer:pointer+n_hits,7] = db_range[hit] # index to query entry\n",
    "    \n",
    "        pointer += n_hits\n",
    "\n",
    "    ions = ions[:pointer,:]\n",
    "        \n",
    "    return ions\n",
    "\n",
    "@njit\n",
    "def score(\n",
    "    psms,\n",
    "    query_masses,\n",
    "    query_frags,\n",
    "    query_ints,\n",
    "    query_indices,\n",
    "    db_masses,\n",
    "    db_frags,\n",
    "    frag_types,\n",
    "    mtol,\n",
    "    query_bounds,\n",
    "    db_bounds,\n",
    "    ppm,\n",
    "    psms_dtype,\n",
    "    db_ints = None,\n",
    "    parallel = False\n",
    "):\n",
    "\n",
    "    psms_ = np.zeros(len(psms), dtype=psms_dtype)  \n",
    "    \n",
    "    losses = [0, 18.01056468346, 17.03052] #H2O, NH3\n",
    "    \n",
    "    ions_ = List()\n",
    "    \n",
    "    ion_count = 0\n",
    "    \n",
    "    for i in range(len(psms)):\n",
    "        query_idx = psms[i][\"query_idx\"]\n",
    "        db_idx = psms[i][\"db_idx\"]\n",
    "        query_idx_start = query_indices[query_idx]\n",
    "        query_idx_end = query_indices[query_idx + 1]\n",
    "        query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "        query_int = query_ints[query_idx_start:query_idx_end]\n",
    "        db_frag = db_frags[:, db_idx] [: db_bounds[db_idx] ]\n",
    "        frag_type = frag_types[:, db_idx] [: db_bounds[db_idx] ]\n",
    "\n",
    "        if db_ints is None:\n",
    "            db_int = np.zeros(len(db_frag))\n",
    "        else:\n",
    "            db_int = db_ints[i]\n",
    "\n",
    "        ions = get_hits(query_frag, query_int, db_frag, db_int, frag_type, mtol, ppm, losses)\n",
    "\n",
    "        psms_['o_mass'][i] = query_masses[query_idx] - db_masses[db_idx]\n",
    "        psms_['o_mass_ppm'][i] = 2 * psms_['o_mass'][i] / (query_masses[query_idx]  + db_masses[db_idx] ) * 1e6\n",
    "\n",
    "        psms_['delta_m'][i] = np.mean(ions[:,4]-ions[:,5])\n",
    "        psms_['delta_m_ppm'][i] = np.mean(2 * psms_['delta_m'][i] / (ions[:,4]  + ions[:,5] ) * 1e6)\n",
    "\n",
    "        psms_['total_int'][i] = np.sum(query_int)\n",
    "        psms_['matched_int'][i] = np.sum(ions[:,2])\n",
    "        psms_['matched_int_ratio'][i] = psms_['matched_int'][i] / psms_['total_int'][i]\n",
    "        psms_['int_ratio'][i] = np.mean(ions[:,3]/ions[:,2])\n",
    "        \n",
    "        psms_['b_hits'][i] = np.sum(ions[ions[:,1]==0][:,0]>0)\n",
    "        psms_['y_hits'][i] = np.sum(ions[ions[:,1]==0][:,0]<0)\n",
    "        \n",
    "        psms_['b-H2O_hits'][i] = np.sum(ions[ions[:,1]==1][:,0]>0)\n",
    "        psms_['y-H2O_hits'][i] = np.sum(ions[ions[:,1]==1][:,0]<0)\n",
    "        \n",
    "        psms_['b-NH3_hits'][i] = np.sum(ions[ions[:,1]==2][:,0]>0)\n",
    "        psms_['y-NH3_hits'][i] = np.sum(ions[ions[:,1]==2][:,0]<0)\n",
    "        \n",
    "        n_ions = len(ions)\n",
    "        \n",
    "        psms_['n_ions'][i] = n_ions\n",
    "        psms_['ion_idx'][i] = ion_count\n",
    "        \n",
    "        ion_count += n_ions\n",
    "        ions_.append(ions)\n",
    "        \n",
    "    return psms_, ions_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from numba.typed import Dict\n",
    "def get_sequences(psms, db_seqs):\n",
    "    \"\"\"\n",
    "    Get sequences to add them to a recarray\n",
    "    \"\"\"\n",
    "    sequence_list = db_seqs[psms[\"db_idx\"]]\n",
    "\n",
    "    return sequence_list\n",
    "\n",
    "\n",
    "def get_score_columns(\n",
    "    psms,\n",
    "    query_data,\n",
    "    db_data,\n",
    "    features,\n",
    "    parallel,\n",
    "    m_tol,\n",
    "    m_offset,\n",
    "    ppm,\n",
    "    m_offset_calibrated=None,\n",
    "    **kwargs\n",
    "):\n",
    "    logging.info('Extracting columns for scoring.')\n",
    "    query_indices = query_data[\"indices_ms2\"]\n",
    "    query_bounds = query_data['bounds']\n",
    "    query_charges = query_data['charge2']\n",
    "    query_frags = query_data['mass_list_ms2']\n",
    "    query_ints = query_data['int_list_ms2']\n",
    "\n",
    "    db_masses = db_data['precursors']\n",
    "    db_frags = db_data['fragmasses']\n",
    "    db_bounds = db_data['bounds']\n",
    "    frag_types = db_data['fragtypes']\n",
    "\n",
    "    db_seqs = db_data['seqs']\n",
    "\n",
    "    if 'db_ints' in db_data.keys():\n",
    "        db_ints = db_data['db_ints']\n",
    "    else:\n",
    "        db_ints = None\n",
    "\n",
    "    if features is not None:\n",
    "        if m_offset_calibrated:\n",
    "            query_masses = features['corrected_mass'].values\n",
    "        else:\n",
    "            query_masses = features['mass_matched'].values\n",
    "        query_mz = features['mz_matched'].values\n",
    "        query_rt = features['rt_matched'].values\n",
    "        query_bounds = query_bounds[features['query_idx'].values]\n",
    "        query_charges = query_charges[features['query_idx'].values]\n",
    "        query_selection = features['query_idx'].values\n",
    "        indices = np.zeros(len(query_selection) + 1, np.int64)\n",
    "        indices[1:] = np.diff(query_indices)[query_selection]\n",
    "        indices = np.cumsum(indices)\n",
    "        query_frags = np.concatenate(\n",
    "            [\n",
    "                query_frags[s: e] for s, e in zip(\n",
    "                    query_indices[query_selection], query_indices[query_selection + 1]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        query_ints = np.concatenate(\n",
    "            [\n",
    "                query_ints[s: e] for s, e in zip(\n",
    "                    query_indices[query_selection], query_indices[query_selection + 1]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        query_indices = indices\n",
    "    else:\n",
    "        query_masses = query_data['prec_mass_list2']\n",
    "        query_mz = query_data['mono_mzs2']\n",
    "        query_rt = query_data['rt_list_ms2']\n",
    "        \n",
    "\n",
    "    loss_dict = Dict()\n",
    "    loss_dict[''] = 0.0\n",
    "    loss_dict['-H2O'] = 18.01056468346\n",
    "    loss_dict['-NH3'] = 17.03052\n",
    "\n",
    "    float_fields = ['o_mass', 'o_mass_ppm','delta_m','delta_m_ppm','matched_int_ratio','int_ratio']\n",
    "    int_fields = ['total_int','matched_int','n_ions','ion_idx'] + [a+_+'_hits' for _ in loss_dict for a in ['b','y']] \n",
    "\n",
    "    psms_dtype = np.dtype([(_,np.float32) for _ in float_fields] + [(_,np.int64) for _ in int_fields])\n",
    "\n",
    "    psms_, ions_,  = score(\n",
    "        psms,\n",
    "        query_masses,\n",
    "        query_frags,\n",
    "        query_ints,\n",
    "        query_indices,\n",
    "        db_masses,\n",
    "        db_frags,\n",
    "        frag_types,\n",
    "        m_tol,\n",
    "        query_bounds,\n",
    "        db_bounds,\n",
    "        ppm,\n",
    "        psms_dtype)\n",
    "    \n",
    "    ions_ = np.vstack(ions_)\n",
    "\n",
    "    for _ in psms_.dtype.names:\n",
    "        psms = add_column(psms, psms_[_], _)\n",
    "        \n",
    "    rts = np.array(query_rt)[psms[\"query_idx\"]]\n",
    "    psms = add_column(psms, rts, 'rt')\n",
    "\n",
    "    seqs = get_sequences(psms, db_seqs)\n",
    "    psms = add_column(psms, seqs, \"sequence\")\n",
    "\n",
    "    mass = np.array(query_masses)[psms[\"query_idx\"]]\n",
    "    mz = np.array(query_mz)[psms[\"query_idx\"]]\n",
    "    charge = np.array(query_charges)[psms[\"query_idx\"]]\n",
    "\n",
    "    psms = add_column(psms, mass, \"mass\")\n",
    "    psms = add_column(psms, mz, \"mz\")\n",
    "    psms = add_column(psms, charge, \"charge\")\n",
    "    \n",
    "    psms = add_column(psms, np.char.add(np.char.add(psms['sequence'],\"_\"), psms['charge'].astype(int).astype(str)), 'precursor')\n",
    "\n",
    "    if features is not None:\n",
    "        psms = add_column(psms, features.loc[psms['query_idx']]['feature_idx'].values, 'feature_idx')\n",
    "        psms = add_column(psms, features.loc[psms['query_idx']]['query_idx'].values, 'raw_idx')\n",
    "        \n",
    "        for key in ['int_sum','int_apex','rt_start','rt_apex','rt_end','fwhm','dist','mobility']:\n",
    "            if key in features.keys():\n",
    "                psms = add_column(psms, features.loc[psms['query_idx']][key].values, key)\n",
    "\n",
    "    logging.info(f'Extracted columns from {len(psms):,} spectra.')\n",
    "\n",
    "    return psms, ions_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def plot_hit(\n",
    "    df,\n",
    "    index,\n",
    "    db_bounds,\n",
    "    db_frags,\n",
    "    frag_types,\n",
    "    query_bounds,\n",
    "    query_frags,\n",
    "    query_ints,\n",
    "    query_indices,\n",
    "    ppm,\n",
    "    m_tol,\n",
    "    db_ints = None,\n",
    "    **kwargs\n",
    "):\n",
    "    spectrum = df.iloc[index]\n",
    "\n",
    "    sequence = spectrum[\"sequence\"]\n",
    "\n",
    "    db_idx = spectrum[\"db_idx\"]\n",
    "    query_idx = spectrum[\"query_idx\"]\n",
    "\n",
    "    intensity_fraction = spectrum[\"matched_int\"] / spectrum[\"total_int\"]\n",
    "\n",
    "    db_bound = db_bounds[db_idx]\n",
    "    db_frag = db_frags[:, db_idx] [:db_bound]\n",
    "    if db_ints is not None:\n",
    "        db_int = db_ints[:, db_idx] [:db_bound]\n",
    "    else:\n",
    "        db_int = np.ones(len(db_frag))\n",
    "\n",
    "    db_int = db_int / np.max(db_int) * 100\n",
    "\n",
    "    frag_type = frag_types[:, db_idx] [:db_bound]\n",
    "\n",
    "    query_bound = query_bounds[query_idx]\n",
    "    query_idx_start = query_indices[query_idx]\n",
    "    query_idx_end = query_indices[query_idx + 1]\n",
    "    query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "    query_int = query_ints[query_idx_start:query_idx_end]\n",
    "\n",
    "    query_int = query_int / np.max(query_int) * 100\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, m_tol, ppm)\n",
    "\n",
    "    n_hits = np.sum(hits > 0)\n",
    "\n",
    "    hitpos = hits[hits > 0] - 1\n",
    "\n",
    "    hit_x = query_frag[hitpos]\n",
    "    hit_y = query_int[hitpos]\n",
    "\n",
    "    # create an axis\n",
    "    ax = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "\n",
    "    plt.vlines(query_frag, 0, query_int, \"r\", label=\"Query\", alpha=0.5)\n",
    "\n",
    "    plt.plot(hit_x, hit_y, \"ro\", label=\"Hit\", alpha=0.5)\n",
    "\n",
    "    figure_title = \"PSM Match for Spectra: {} - sequence {} \\nHits {} - Intensity Fraction {:.2f} %\".format(\n",
    "        query_idx, sequence, n_hits, intensity_fraction * 100\n",
    "    )\n",
    "    plt.title(figure_title)\n",
    "\n",
    "    plt.xlabel(\"Mass\")\n",
    "    plt.ylabel(\"Relative Intensity (%)\")\n",
    "    plt.ylim([0, 110])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "from alphapept.fasta import get_frag_dict, parse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_psms(query_data, df, index, mass_dict, ppm=True, m_tol=20):\n",
    "    \"\"\"\n",
    "    Plot a psms\n",
    "    \"\"\"\n",
    "    spectrum = df.iloc[index]\n",
    "\n",
    "    sequence = spectrum[\"sequence\"]\n",
    "    db_idx = spectrum[\"db_idx\"]\n",
    "    query_idx = spectrum[\"query_idx\"]\n",
    "\n",
    "    if 'matched_int' in spectrum.index:\n",
    "        intensity_fraction = spectrum[\"matched_int\"] / spectrum[\"total_int\"]\n",
    "    else:\n",
    "        intensity_fraction = np.nan\n",
    "        matched_int = np.nan\n",
    "\n",
    "    frag_dict = get_frag_dict(parse(sequence), mass_dict)\n",
    "    frag_dict_r = {v: k for k, v in frag_dict.items()}\n",
    "\n",
    "    db_frag = list(frag_dict.values())\n",
    "    db_frag.sort()\n",
    "\n",
    "    db_int = [100 for _ in db_frag]\n",
    "\n",
    "    query_bounds = query_data['bounds']\n",
    "    query_frags = query_data['mass_list_ms2']\n",
    "    query_ints = query_data['int_list_ms2']\n",
    "\n",
    "    query_bound = query_bounds[query_idx]\n",
    "    query_idx_start = query_indices[query_idx]\n",
    "    query_idx_end = query_indices[query_idx + 1]\n",
    "    query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "    query_int = query_ints[query_idx_start:query_idx_end]\n",
    "\n",
    "    query_int = query_int / np.max(query_int) * 100\n",
    "\n",
    "    hits = compare_frags(query_frag, db_frag, m_tol, ppm)\n",
    "\n",
    "    n_hits = np.sum(hits > 0)\n",
    "\n",
    "    hitpos = hits[hits > 0] - 1\n",
    "\n",
    "    hit_x = query_frag[hitpos]\n",
    "    hit_y = query_int[hitpos]\n",
    "\n",
    "    # create an axis\n",
    "    ax = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "\n",
    "    plt.vlines(query_frag, 0, query_int, \"r\", label=\"Query\", alpha=0.5)\n",
    "\n",
    "    plt.plot(hit_x, hit_y, \"ro\", label=\"Hit\", alpha=0.5)\n",
    "\n",
    "    figure_title = \"Peptide-Spectrum-Match for Spectra: {} - sequence {} \\nHits {} - Intensity Fraction {:.2f} %\".format(\n",
    "        query_idx, sequence, n_hits, intensity_fraction * 100\n",
    "    )\n",
    "\n",
    "    db_hits = np.array(db_frag)[hits>0]\n",
    "    ion_hits = [frag_dict_r[_] for _ in db_hits]\n",
    "\n",
    "    for _ in frag_dict.keys():\n",
    "\n",
    "        if _ in ion_hits:\n",
    "            color = 'r'\n",
    "        else:\n",
    "            color = 'k'\n",
    "\n",
    "        if _[0] == 'y':\n",
    "            plt.text(frag_dict[_], 110, _, fontsize=12, alpha = 0.8, color=color)\n",
    "        else:\n",
    "            plt.text(frag_dict[_], 104, _, fontsize=12, alpha = 0.8, color=color)\n",
    "\n",
    "    plt.title(figure_title)\n",
    "\n",
    "    plt.xlabel(\"Mass\")\n",
    "    plt.ylabel(\"Relative Intensity (%)\")\n",
    "    plt.ylim([0, 120])\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def perform_search(query_files, db_masses, db_frags, db_bounds, db_seqs, frag_types, plot, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to search and score one or multiple MS runs by the X!Tandem approach.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(query_files, str):\n",
    "        kwargs['query_path'] = query_files\n",
    "        psms_all = score_psms(db_masses, db_frags, db_bounds, db_seqs, frag_types, plot=plot, **kwargs)\n",
    "        psms_all['filename'] = query_files\n",
    "    elif isinstance(query_files, list):\n",
    "        psms_all = []\n",
    "        for file in query_files:\n",
    "            kwargs['query_path'] = file\n",
    "            psms = score_psms(db_masses, db_frags, db_bounds, db_seqs, frag_types, plot=plot, **kwargs)\n",
    "            psms['filename'] = file\n",
    "            psms_all.append(psms)\n",
    "        psms_all = pd.concat(psms_all, ignore_index=True)\n",
    "    else:\n",
    "        raise Exception('query_files should be either a string or a list. The selected query_files argument is of type: {}'.format(type(query_files)))\n",
    "    return psms_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching with database\n",
    "\n",
    "We save intermediate results to hdf5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "import alphapept.io\n",
    "import alphapept.fasta\n",
    "\n",
    "def store_hdf(df, path, key, replace=False, swmr = False):\n",
    "    \"\"\"\n",
    "    Stores in hdf\n",
    "    \"\"\"\n",
    "    ms_file = alphapept.io.MS_Data_File(path.file_name, is_overwritable=True)\n",
    "    \n",
    "    if replace:\n",
    "        ms_file.write(df, dataset_name=key)\n",
    "    else:\n",
    "        try:\n",
    "            df.to_hdf(path, key=key, append=True)\n",
    "            #TODO, append is not implemented yet\n",
    "        except (ValueError, AttributeError):\n",
    "            try:\n",
    "                old_df = ms_file.read(dataset_name=key, swmr = swmr)\n",
    "                new_df = pd.concat([old_df, df])\n",
    "                ms_file.write(new_df, dataset_name=key, swmr = swmr)\n",
    "            except KeyError:\n",
    "                ms_file.write(df, dataset_name=key, swmr= swmr)\n",
    "\n",
    "def search_db(to_process):\n",
    "    \"\"\"\n",
    "    Perform a databse search. One file at a time.\n",
    "    \"\"\"\n",
    "\n",
    "    ms_file, settings = to_process\n",
    "\n",
    "    skip = False\n",
    "    feature_calibration = False\n",
    "\n",
    "    if 'm_offset_calibrated' in settings[\"search\"]:\n",
    "        calibration = settings['search']['m_offset_calibrated']\n",
    "        logging.info('Found calibrated m_offset with value {}'.format(calibration))\n",
    "        feature_calibration = True\n",
    "        if calibration == 0:\n",
    "            logging.info('Calibration is 0, skipping second database search.')\n",
    "            skip = True\n",
    "\n",
    "    if not skip:\n",
    "        db_data = alphapept.fasta.read_database(\n",
    "            settings['fasta']['database_path']\n",
    "        )\n",
    "\n",
    "\n",
    "        ms_file = alphapept.io.MS_Data_File(\n",
    "            f\"{ms_file}\"\n",
    "        )\n",
    "\n",
    "    \n",
    "#         TODO calibrated_fragments should be included in settings\n",
    "        query_data = ms_file.read_DDA_query_data(\n",
    "            calibrated_fragments=True,\n",
    "            database_file_name=settings['fasta']['database_path']\n",
    "        )\n",
    "\n",
    "        features = ms_file.read(dataset_name=\"features\")\n",
    "\n",
    "        psms, num_specs_compared = get_psms(query_data, db_data, features, **settings[\"search\"])\n",
    "        if len(psms) > 0:\n",
    "            psms, ions = get_score_columns(psms, query_data, db_data, features, **settings[\"search\"])\n",
    "\n",
    "        if 'm_offset_calibrated' in settings[\"search\"]:\n",
    "            logging.info('Saving second_search results to {}'.format(ms_file))\n",
    "            save_field = 'second_search'\n",
    "        else:\n",
    "            logging.info('Saving first_search results to {}'.format(ms_file))\n",
    "            save_field = 'first_search'\n",
    "        \n",
    "        store_hdf(pd.DataFrame(psms), ms_file, save_field, replace=True)\n",
    "        ion_columns = ['ion_index','ion_type','ion_int','db_int','ion_mass','db_mass','query_idx','db_idx']\n",
    "        store_hdf(pd.DataFrame(ions, columns = ion_columns), ms_file, 'ions', replace=True)\n",
    "\n",
    "\n",
    "def search_parallel_db(settings, calibration = None, callback = None):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    ms_files = []\n",
    "\n",
    "    for _ in settings['experiment']['file_paths']:\n",
    "        base, ext = os.path.splitext(_)\n",
    "        ms_files.append(base + '.ms_data.hdf')\n",
    "\n",
    "    if calibration:\n",
    "        custom_settings = []\n",
    "        for _ in calibration:\n",
    "            settings_ = copy.deepcopy(settings)\n",
    "            settings_[\"search\"][\"m_offset_calibrated\"] = _\n",
    "            custom_settings.append(settings_)\n",
    "    else:\n",
    "        custom_settings = [settings for _ in ms_files]\n",
    "\n",
    "    n_processes = settings['general']['n_processes']\n",
    "\n",
    "    to_process = [(ms_files[i], custom_settings[i]) for i in range(len(ms_files))]\n",
    "\n",
    "    if len(to_process) == 1:\n",
    "        ms_file, settings_ = to_process[0]\n",
    "        settings_['search']['parallel'] = True\n",
    "        search_db((ms_file, settings_))\n",
    "    else:\n",
    "        with Pool(n_processes) as p:\n",
    "            max_ = len(to_process)\n",
    "            for i, _ in enumerate(p.imap_unordered(search_db, to_process)):\n",
    "                if callback:\n",
    "                    callback((i+1)/max_)\n",
    "\n",
    "    db_data = alphapept.fasta.read_database(\n",
    "        settings['fasta']['database_path']\n",
    "    )\n",
    "\n",
    "    return db_data['fasta_dict'].item(), db_data['pept_dict'].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Large Fasta and or Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from alphapept.fasta import blocks, generate_peptides, add_to_pept_dict\n",
    "from alphapept.io import list_to_numpy_f32\n",
    "from alphapept.fasta import block_idx, generate_fasta_list, generate_spectra, check_peptide\n",
    "from alphapept import constants\n",
    "mass_dict = constants.mass_dict\n",
    "\n",
    "def search_fasta_block(to_process):\n",
    "    \"\"\"\n",
    "    Search fasta block\n",
    "    For searches with big fasta files or unspecific searches\n",
    "    \"\"\"\n",
    "\n",
    "    fasta_index, fasta_block, ms_files, settings = to_process\n",
    "\n",
    "    settings_ = settings[0]\n",
    "    spectra_block = settings_['fasta']['spectra_block']\n",
    "    to_add = List()\n",
    "\n",
    "    psms_container = [list() for _ in ms_files]\n",
    "\n",
    "    f_index = 0\n",
    "\n",
    "    pept_dict = {}\n",
    "    for element in fasta_block:\n",
    "        sequence = element[\"sequence\"]\n",
    "        mod_peptides = generate_peptides(sequence, **settings_['fasta'])\n",
    "\n",
    "        pept_dict, added_peptides = add_to_pept_dict(pept_dict, mod_peptides, fasta_index+f_index)\n",
    "\n",
    "        if len(added_peptides) > 0:\n",
    "            to_add.extend(added_peptides)\n",
    "        f_index += 1\n",
    "\n",
    "    if len(to_add) > 0:\n",
    "        for seq_block in blocks(to_add, spectra_block):\n",
    "            spectra = generate_spectra(seq_block, mass_dict)\n",
    "            precmasses, seqs, fragmasses, fragtypes = zip(*spectra)\n",
    "            sortindex = np.argsort(precmasses)\n",
    "\n",
    "            db_data = {}\n",
    "            db_data['precursors'] = np.array(precmasses)[sortindex]\n",
    "            db_data['seqs'] = np.array(seqs)[sortindex]\n",
    "            db_data['fragmasses']  = list_to_numpy_f32(np.array(fragmasses, dtype=object)[sortindex])\n",
    "            db_data['fragtypes'] = list_to_numpy_f32(np.array(fragtypes, dtype=object)[sortindex])\n",
    "            db_data['bounds'] = np.sum(db_data['fragmasses']>=0,axis=0).astype(np.int64)\n",
    "\n",
    "            for file_idx, ms_file in enumerate(ms_files):\n",
    "                query_data = alphapept.io.MS_Data_File(\n",
    "                    f\"{ms_file}\"\n",
    "                ).read_DDA_query_data(swmr=True)\n",
    "\n",
    "                try:\n",
    "                    features = alphapept.io.MS_Data_File(\n",
    "                        ms_file\n",
    "                    ).read(dataset_name=\"features\",swmr=True)\n",
    "                except FileNotFoundError:\n",
    "                    features = None\n",
    "                except KeyError:\n",
    "                    features = None\n",
    "        \n",
    "                psms, num_specs_compared = get_psms(query_data, db_data, features, **settings[file_idx][\"search\"])\n",
    "\n",
    "                if len(psms) > 0:\n",
    "                    psms, ions = get_score_columns(psms, query_data, db_data, features, **settings[file_idx][\"search\"])\n",
    "\n",
    "                    fasta_indices = [','.join([str(x) for x in pept_dict[_]]) for _ in psms['sequence']]\n",
    "\n",
    "                    psms_df = pd.DataFrame(psms)\n",
    "                    psms_df['fasta_index'] = fasta_indices\n",
    "\n",
    "                    psms_container[file_idx].append(psms_df)\n",
    "\n",
    "    return psms_container, len(to_add)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def search_parallel(settings, calibration = None, callback = None):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    fasta_list, fasta_dict = generate_fasta_list(**settings['fasta'])\n",
    "\n",
    "    fasta_block = settings['fasta']['fasta_block']\n",
    "\n",
    "    ms_file_path = []\n",
    "\n",
    "    for _ in settings['experiment']['file_paths']:\n",
    "        base, ext = os.path.splitext(_)\n",
    "        ms_file_path.append(base + '.ms_data.hdf')\n",
    "\n",
    "    if calibration:\n",
    "        custom_settings = []\n",
    "        for _ in calibration:\n",
    "            settings_ = copy.deepcopy(settings)\n",
    "            settings_[\"search\"][\"m_offset_calibrated\"] = _\n",
    "            custom_settings.append(settings_)\n",
    "    else:\n",
    "        custom_settings = [settings for _ in ms_file_path]\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Number of FASTA entries: {len(fasta_list):,} - FASTA settings {settings['fasta']}\")\n",
    "    to_process = [(idx_start, fasta_list[idx_start:idx_end], ms_file_path, custom_settings) for idx_start, idx_end in block_idx(len(fasta_list), fasta_block)]\n",
    "\n",
    "    n_processes = settings['general']['n_processes']\n",
    "\n",
    "    n_seqs_ = 0\n",
    "    with Pool(n_processes) as p:\n",
    "        max_ = len(to_process)\n",
    "        \n",
    "        for i, (_, n_seqs) in enumerate(p.imap_unordered(search_fasta_block, to_process)):\n",
    "            n_seqs_ += n_seqs\n",
    "            logging.info(f'Block {i+1} of {max_} complete - {(i+1)/max_*100:.2f} % - created peptides {n_seqs:,}')\n",
    "            for j in range(len(_)):\n",
    "                ms_file = alphapept.io.MS_Data_File(ms_file_path[j])\n",
    "                output = [_ for _ in _[j]]\n",
    "                if len(output) > 0:\n",
    "                    if calibration:\n",
    "                        store_hdf(pd.concat(output), ms_file, 'second_search', swmr = True)\n",
    "                    else:\n",
    "                        store_hdf(pd.concat(output), ms_file, 'first_search', swmr = True)\n",
    "\n",
    "            if callback:\n",
    "                callback((i+1)/max_)\n",
    "    \n",
    "    logging.info(f'Complete. Created peptides {n_seqs_:,}')\n",
    "    \n",
    "    return fasta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted ML_Test.ipynb.\n",
      "Converted Untitled.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphapept] *",
   "language": "python",
   "name": "conda-env-alphapept-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
