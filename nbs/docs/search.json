[
  {
    "objectID": "interface.html",
    "href": "interface.html",
    "title": "Interface",
    "section": "",
    "text": "Several generic utility functions include:\n\nCallback function to track progress\nLogging function\nVersion/hardware/settings checks\n\n\nsource\n\n\n\n tqdm_wrapper (pbar, update:float)\n\nUpdate a qdm progress bar.\nArgs: pbar (type): a tqd,.tqdm objet. update (float): The new value for the progressbar.\n\nsource\n\n\n\n\n check_version_and_hardware (settings:dict)\n\nShow platform and python information and parse settings.\nArgs: settings (dict): A dictionary with settings how to process the data.\nReturns: dict: The parsed settings.\n\nsource\n\n\n\n\n wrapped_partial (func:<built-infunctioncallable>, *args, **kwargs)\n\nWrap a function with partial args and kwargs.\nArgs: func (callable): The function to be wrapped. *args (type): Args to be wrapped. **kwargs (type): Kwargs to be wrapped.\nReturns: callable: The wrapped function."
  },
  {
    "objectID": "interface.html#functions",
    "href": "interface.html#functions",
    "title": "Interface",
    "section": "Functions",
    "text": "Functions\nThe implemented functions are as follows:\n\nCreate database\nImport raw data\nPerform feature finding\nSearch data with fasta\nRecalibrate\nScore data with fasta\nPerform LFQ\nExport results\nRun whole workflow\n\nThe last command allows to run the whole pipeline at once.\n\nsource\n\ncreate_database\n\n create_database (settings:dict, logger_set:bool=False,\n                  settings_parsed:bool=False, callback:<built-\n                  infunctioncallable>=None)\n\nCreate the search database.\nArgs: settings (dict): A dictionary with settings how to process the data. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\nRaises: FileNotFoundError: If the FASTA file is not found.\n\nsource\n\n\nimport_raw_data\n\n import_raw_data (settings:dict, logger_set:bool=False,\n                  settings_parsed:bool=False, callback:<built-\n                  infunctioncallable>=None)\n\nImport raw data.\nArgs: settings (dict): A dictionary with settings how to process the data. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\n\nsource\n\n\nfeature_finding\n\n feature_finding (settings:dict, logger_set:bool=False,\n                  settings_parsed:bool=False, callback:<built-\n                  infunctioncallable>=None)\n\nFind features.\nArgs: settings (dict): A dictionary with settings how to process the data. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\n\nsource\n\n\nsearch_data\n\n search_data (settings:dict, first_search:bool=True,\n              logger_set:bool=False, settings_parsed:bool=False,\n              callback:<built-infunctioncallable>=None)\n\nCreate the search database.\nArgs: settings (dict): A dictionary with settings how to process the data. first_search (bool): If True, save the intermediary results as first search. Otherwise, calibrate mz_values are used and results are saved as second search. Defaults to True. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\nRaises: FileNotFoundError: If the FASTA file is not found.\n\nsource\n\n\nrecalibrate_data\n\n recalibrate_data (settings:dict, logger_set:bool=False,\n                   settings_parsed:bool=False, callback:<built-\n                   infunctioncallable>=None)\n\nRecalibrate mz values.\nArgs: settings (dict): A dictionary with settings how to process the data. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\n\nsource\n\n\nscore\n\n score (settings:dict, pept_dict:dict=None, fasta_dict:dict=None,\n        logger_set:bool=False, settings_parsed:bool=False,\n        callback:<built-infunctioncallable>=None)\n\nScore PSMs and calculate FDR.\nArgs: settings (dict): A dictionary with settings how to process the data. pept_dict (dict): A dictionary with peptides. Defaults to None. fasta_dict (dict): A dictionary with fasta sequences. Defaults to None. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\n\nsource\n\n\nisobaric_labeling\n\n isobaric_labeling (settings:dict, logger_set:bool=False,\n                    settings_parsed:bool=False, callback:<built-\n                    infunctioncallable>=None)\n\nSearch for isobaric labels.\nArgs: settings (dict): A dictionary with settings how to process the data. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\n\nsource\n\n\nprotein_grouping\n\n protein_grouping (settings:dict, pept_dict:dict=None,\n                   fasta_dict:dict=None, logger_set:bool=False,\n                   settings_parsed:bool=False, callback:<built-\n                   infunctioncallable>=None)\n\nGroup peptides into proteins.\nArgs: settings (dict): A dictionary with settings how to process the data. pept_dict (dict): A dictionary with peptides. Defaults to None. fasta_dict (dict): A dictionary with fasta sequences. Defaults to None. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\n\nsource\n\n\nmatch\n\n match (settings:dict, logger_set:bool=False, settings_parsed:bool=False,\n        callback:<built-infunctioncallable>=None)\n\nMatch datasets.\nArgs: settings (dict): A dictionary with settings how to process the data. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\n\nsource\n\n\nalign\n\n align (settings:dict, logger_set:bool=False, settings_parsed:bool=False,\n        callback:<built-infunctioncallable>=None)\n\nAlign multiple samples.\nArgs: settings (dict): A dictionary with settings how to process the data. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\n\nsource\n\n\nread_label_intensity\n\n read_label_intensity (df:pandas.core.frame.DataFrame,\n                       label:<class'NamedTuple'>)\n\nReads the label intensities from peptides and sums them by protein group.\nArgs: df (pd.DataFrame): Table with peptide information. label (NamedTuple): Label used for the experiment.\nReturns: pd.DataFrame: Summary protein table containing proteins and their intensity for each channel.\n\nsource\n\n\nquantification\n\n quantification (settings:dict, logger_set:bool=False,\n                 settings_parsed:bool=False, callback:<built-\n                 infunctioncallable>=None)\n\nNormalize and quantify datasets.\nArgs: settings (dict): A dictionary with settings how to process the data. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\n\nsource\n\n\nexport\n\n export (settings:dict, logger_set:bool=False, settings_parsed:bool=False,\n         callback:<built-infunctioncallable>=None)\n\nExport settings.\nArgs: settings (dict): A dictionary with settings how to process the data. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: the parsed settings.\n\nsource\n\n\nrun_complete_workflow\n\n run_complete_workflow (settings:dict, progress:bool=False,\n                        logger_set:bool=False, settings_parsed:bool=False,\n                        callback:<built-infunctioncallable>=None,\n                        callback_overall:<built-infunctioncallable>=None,\n                        callback_task:<built-infunctioncallable>=None,\n                        logfile:str=None)\n\nRun all AlphaPept steps from a settings dict.\nArgs: settings (dict): A dictionary with settings how to process the data. progress (bool): Track progress. Defaults to False. logger_set (bool): If False, reset the default logger. Defaults to False. settings_parsed (bool): If True, reparse the settings. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None. callback_overall (callable): Same as callback, but for the overall progress. Defaults to None. callback_task (callable): Same as callback, but for the task progress. Defaults to None. logfile (str): The name of a logfile. Defaults to None.\nReturns: dict: the parsed settings.\n\nsource\n\n\nparallel_execute\n\n parallel_execute (settings:dict, step:<built-infunctioncallable>,\n                   callback:<built-infunctioncallable>=None)\n\nGeneric function to execute worklow steps in parallel on a per-file basis.\nArgs: settings (dict): The settings for processing the step function. step (callable): A function that accepts settings as input parameter. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: dict: The settings after processing.\nRaises: NotImplementedError: When the step is feature finding on files other then Thermo or Bruker.\n\nsource\n\n\nget_summary\n\n get_summary (settings:dict, summary:dict)\n\nAppend file summary statistics to a summary dictionary.\nArgs: settings (dict): A dictionary with settings how to process the data. summary (dict): A dictionary with summary statistics of the experiment.\nReturns: dict: The summary in which file summary statistcs are appended.\n\nsource\n\n\nget_file_summary\n\n get_file_summary (ms_data:alphapept.io.MS_Data_File, fields:list)\n\nGet summarize statitics from an MS_Data file.\nArgs: ms_data (alphapept.io.MS_Data_File): An MS_Data file which has been fully identified and quantified. fields (list): A list with colum names to calculate summary statistics.\nReturns: dict: A dictionary with summary statistics.\n\nsource\n\n\nextract_median_unique\n\n extract_median_unique (settings:dict, fields:list,\n                        summary_type='filename')\n\nExtract the medion protein FDR and number of unique proteins.\nArgs: settings (dict): A dictionary with settings how to process the data. fields (list): A list with colum names to calculate summary statistics. summary_type (str): A str of column name used for summarizing (‘filename’ or ‘sample_group’)\nReturns: tuple: Two arrays with the median protein FDR per file/sample_group and the unique number of protein hits"
  },
  {
    "objectID": "interface.html#cli",
    "href": "interface.html#cli",
    "title": "Interface",
    "section": "CLI",
    "text": "CLI\nAll workflow functions can be called with the command line interface (CLI). To implement this CLI, we use the click package.\nIn brief, click allows to create a CLI with minimal effort by simply adding decorators to already defined functions. These decorators create a help text for each function and describe all their parameters. Functions that are decorated by click can be added to a central run_cli functions to be incorporated in the CLI automatically.\nWhile AlphaTims allows modular execution of individual steps to process MS data, it is common for these steps to be combined and reuse multiple parameters. We therefore opt to use a singe YAML settings file containing all parameters in dictionary format as a single parameter instead of providing all parameters individually to each function.\n\n\n\n\n <Command gui> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Command workflow> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Command export> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Command quantify> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Command match> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Command align> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Command score> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Command recalibrate> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Command search> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Command features> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Command import> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Command database> (*args:Any, **kwargs:Any)\n\n\n\n\n\n\n <Group cli-overview> (*args:Any, **kwargs:Any)\n\n\nsource\n\n\nrun_cli\n\n run_cli ()\n\nRun the command line interface.\n\nsource\n\n\nis_port_in_use\n\n is_port_in_use (port:int)\n\n\nsource\n\n\nbcolors\n\n bcolors ()\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "How to contribute",
    "section": "",
    "text": "Report bugs at https://github.com/MannLabs/alphapept/issues.\nIf you are reporting a bug, please include:\n\nYour operating system name and version.\nAny details about your local setup that might be helpful in troubleshooting.\nDetailed steps to reproduce the bug."
  },
  {
    "objectID": "contributing.html#fix-bugs",
    "href": "contributing.html#fix-bugs",
    "title": "How to contribute",
    "section": "Fix Bugs",
    "text": "Fix Bugs\nLook through the GitHub issues for bugs. Anything tagged with “bug” and “help wanted” is open to whoever wants to implement it."
  },
  {
    "objectID": "contributing.html#implement-features",
    "href": "contributing.html#implement-features",
    "title": "How to contribute",
    "section": "Implement Features",
    "text": "Implement Features\nLook through the GitHub issues for features. Anything tagged with “enhancement” and “help wanted” is open to whoever wants to implement it."
  },
  {
    "objectID": "contributing.html#write-documentation",
    "href": "contributing.html#write-documentation",
    "title": "How to contribute",
    "section": "Write Documentation",
    "text": "Write Documentation\nAlphaPept could always use more documentation, whether as part of the official AlphaPept documentation, in docstrings, or even on the web in blog posts, articles, and such. See below for our docstring convention."
  },
  {
    "objectID": "contributing.html#submit-feedback",
    "href": "contributing.html#submit-feedback",
    "title": "How to contribute",
    "section": "Submit Feedback",
    "text": "Submit Feedback\nThe best way to send feedback is to file an issue at https://github.com/MannLabs/alphapept/issues.\nIf you are proposing a feature:\n\nExplain in detail how it would work.\nKeep the scope as narrow as possible, to make it easier to implement.\nRemember that this is a volunteer-driven project, and that contributions are welcome :)"
  },
  {
    "objectID": "contributing.html#get-started",
    "href": "contributing.html#get-started",
    "title": "How to contribute",
    "section": "Get Started!",
    "text": "Get Started!\nReady to contribute? Here’s how to set up alphapept for local development.\n\nFork the alphapept repo on GitHub.\nClone your fork locally:\ngit clone git@github.com:your_name_here/alphapept.git\nFollow the installation instructions in the readme to install a alphapept environment.\nCreate a branch for local development::\ngit checkout -b name-of-your-bugfix-or-feature\nNow you can make your changes locally.\nSee below in the Notes for Programmers about how to use the nbdev environment.\nCommit your changes and push your branch to GitHub::\ngit add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\nSubmit a pull request through the GitHub website."
  },
  {
    "objectID": "contributing.html#pr-submission-guidelines",
    "href": "contributing.html#pr-submission-guidelines",
    "title": "How to contribute",
    "section": "PR submission guidelines",
    "text": "PR submission guidelines\n\nKeep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needed to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs, and it most likely gets rejected.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise, each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exceptional case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Again, use common sense where you’d choose one way over another."
  },
  {
    "objectID": "contributing.html#notes-for-programmers",
    "href": "contributing.html#notes-for-programmers",
    "title": "How to contribute",
    "section": "Notes for Programmers",
    "text": "Notes for Programmers\n\nLiteral Programming\nA key feature is the use of nbdev. We like to keep the entrance barrier low to attract new coders to contribute to the AlphaPept package. For this, we see nbedv as an ideal tool to document and modify code. To install nbdev use pip install nbdev and then install the git hooks in the folder where your GitHub repository is cloned (nbdev_install_git_hooks). The key commands for nbdev are:\n\nnbdev_build_lib: build the library from the notebooks\nnbdev_test_nbs: test all notebooks\nnbdev_clean_nbs: clean the notebooks (strip from unnecessary metadata)\n\n\n\nDocstring convention\nThe docstring convention we adhere to is from the Google stylegyuide.\nBelow a sample docstring (Note the empty space after Args/Returns/Raises):\ndef sample_function(a_string: str, a_int: int, a_flag: bool) -> str:\n    \"\"\"Sample function that performs a sample calculation.\n\n    Args:\n        a_string (str): A string.\n        a_int (int): An integer.\n        a_flag (bool): A flag.\n\n    Returns:\n        str: A string based on the input\n    \"\"\"    \n    \n\nExamples:\n\npandas dataframes: df: pd.DataFrame\nnumpy arrays: x:np.ndarray. You can further specify the dimension (e.g. of numpy arrays) in the docstring.\nargs and kwargs: list and dict. It is generally not needed to type-hint these but state in the Args what they are and what can be done with them.\ncallbacks: typing.Callable, e.g. Callable[[Arg1Type, Arg2Type], ReturnType]\n\n\n\nUseful tricks:\n\nWhen using Visual Studio Code, you can use the Python Docstring Generator to automatically generate the docstrings with this format.\nWhen changing the docstrings in the code, you can propagate the changes back to the notebooks with nbdev_update_lib [filename].\n\n\n\n\n\n\n\nNote\n\n\n\nType hints are highly recommended but not mandatory (see also PEP484). As the Legacy codebase had no type hints, we are slowly moving towards more type hinting coverage.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdding additional elements (e.g lines like ——-) to the docstring could break how the documentation is displayed. Below is the function from above but exported to show how it would appear in the documentation.\n\n\n\nsource\n\n\n\nsample_function\n\n sample_function (a_string:str, a_int:int, a_flag:bool)\n\nSample function that performs a sample calculation.\nArgs: a_string (str): A string. a_int (int): An integer. a_flag (bool): A flag.\nReturns: str: A string based on the input"
  },
  {
    "objectID": "contributing.html#tests",
    "href": "contributing.html#tests",
    "title": "How to contribute",
    "section": "Tests",
    "text": "Tests\nIn order to make AlphaPept a sustainable package, it is imperative that all functions have tests. This is not only to ensure the proper execution of the function but also for the long run when wanting to keep up-to-date with package updates. For tracking package updates, we rely on dependabot. For continuous integration, we use GitHub Actions.\n\nUnit Tests\nWithin the nbdev notebooks, we try to write small tests for each function. They should be lightweight so that running (and hence testing) the notebooks should not take too long. You define your code in a cell with the #export-flag and you can write a text in the following cell. To prevent the cell from showing up in the documentation, you can add a #hide-flag. To be flexible and maybe migrate from the notebook tests to pytest we write the tests with leading test_ and the function name and execute it in the cell.\n\nExample\nCell that defines the code:\n#export\n\ndef function_a():\n    return 5\nNext cell with the test function and calling the test.\ndef test_function_a():\n    assert function_a() == 5\n\ntest_function_a()\n\n\n\n\n\n\nNote\n\n\n\nAlphaPept contains a testfolder testfiles with testfiles which are used for some functions to perform tests on files.\n\n\n\n\n\nGitHub Actions\nWe use GitHub Actions as part of the continuous integration framework.\nWithin the repository, the actions are defined as yml files in ./.github/workflows/.\nCurrently, the following actions are present:\n\ncla_assistant: Action that runs a check if a contributor has signed the CLA\ngpu_test: Helper to run GPU tests. This is not part of the automated pipeline\nmain: Main CI/CD from nbdev. This installs the library and runs all notebooks\nperformance_test: This action freezes the environment and runs multiple test cases specified in test_ci.py. Used to benchmark new versions.\nquick_test: This action runs the whole AlphaPept pipeline on small test files\nrelease: This action triggers the installer compilation and creates a release draft\n\nThe quick_test and performance_test are hooked to the MongoDB, so results are reported here.\nSome actions are executed on each push (e.g. quick_test, others need to be triggered manually (e.g. release).\n\n\nAdding an integration test\n\n\n\n\n\n\nNote\n\n\n\nUnless you are a core developer, there should not be the need to add integration tests. Feel free to create a PR in case.\n\n\nThe best way to add an integration test is by specifying a new test in test_ci.py and then adding a job to the performance_test action. test_ci.py contains a file dictionary with links to files. The idea here is as follows: On the local machine, there should be a BASE_DIR that contains all files used for running the test cases. If the files are not present, the script will download them with the URL from the file dictionary. To define a test case, one needs to initiate a TestRun-class and provide the files to be used. Running a test will always take the current default_settings.yaml and modifying the respective files. You can add subsequent analysis (e.g., such as calculating the false discovery FDR for a mixed species experiment by adding a function to the test class.\nTo test an integration test locally, simply call the test_ci.py with the respective test (e.g. thermo_irt) like so: python test_ci.py thermo_irt.\n\n\nIntegrating Actions\n\n\n\n\n\n\nNote\n\n\n\nUnless you are a core developer, there should not be the need to add GitHub Actions. Feel free to create a PR in case.\n\n\nNew actions can be added by creating a .yml file and placing it in the workflows folder. We can distinguish local actions, which run on our self-hosted runners and cloud actions that run on GitHub servers.\nThe local runners are meant to run computationally intensive tasks such as the performance_test and the quick_test, whereas the cloud actions are meant for unit tests.\n\n\n\n\n\n\nNote\n\n\n\nIf you fork the repository, you will not be able to execute tests on the local runners. They are restricted to the MannLabs repository."
  },
  {
    "objectID": "contributing.html#test-coverage",
    "href": "contributing.html#test-coverage",
    "title": "How to contribute",
    "section": "Test Coverage",
    "text": "Test Coverage\nIdeally, we would like to track test coverage. However, this isn’t easy when having numba-compiled functions. Any solutions to this are greatly appreciated."
  },
  {
    "objectID": "contributing.html#linting",
    "href": "contributing.html#linting",
    "title": "How to contribute",
    "section": "Linting",
    "text": "Linting\nIt is planned to include a GitHub action that automatically runs a linter on the code.\n\nCallbacks\nAs AlphaPept is intended to be the backend of a tool with GUI, we ideally want to be able to get a progress bar out of the major functions. For this, we can pass a callback-argument to the major functions. If the argument is passed, it will return the current progress in the range from 0 to 1.\n\n\nVersion bumping\nWe are using the python package bump2version. You can use this to bump the version number (install via pip install bump2version). Currently specified is: bump2version: (major, minor, patch):\n\ne.g.: bump2version patch for a patch"
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Performance",
    "section": "",
    "text": "A first step of compilation can be achieved by using NumPy arrays which are already heavily c-optimized. Net we consider three kinds of compilation: * Python This allows to use no compilation * Numba This allows to use just-in-time (JIT) compilation. * Cuda This allows compilation on the GPU.\nAll of these compilation approaches can be combined with parallelization approaches. We consider the following possibilities: * No parallelization Not all functionality can be parallelized. * Multithreading This is only performant when Python’s global interpreter lock (GIL) is released or when mostly using input-/output (IO) functions. * GPU This is only available if an NVIDIA GPU is available and properly configured.\nNote that not all compilation approaches can sensibly be combined with all parallelization approaches.\nNext we import all libraries, taking into account that not every machine has a GPU (with NVidia cuda cores) available:\n\nsource\n\nis_valid_compilation_mode\n\n is_valid_compilation_mode (compilation_mode:str)\n\nCheck if the provided string is a valid compilation mode.\nArgs: compilation_mode (str): The compilation mode to verify.\nRaises: ModuleNotFoundError: When trying to use an unavailable GPU. NotImplementedError: When the compilation mode is not valid.\nBy default, we will use cuda if it is available. If not, numba-multithread will be used as default.\nTo consistently use multiple threads or processes, we can set a global MAX_WORKER_COUNT parameter.\n\nsource\n\n\nset_worker_count\n\n set_worker_count (worker_count:int=1, set_global:bool=True)\n\nParse and set the (global) number of threads.\nArgs: worker_count (int): The number of workers. If larger than available cores, it is trimmed to the available maximum. If 0, it is set to the maximum cores available. If negative, it indicates how many cores NOT to use. Default is 1 set_global (bool): If False, the number of workers is only parsed to a valid value. If True, the number of workers is saved as a global variable. Default is True.\nReturns: int: The parsed worker_count.\nCompiled functions are intended to be very fast. However, they do not have the same flexibility as pure Python functions. In general, we recommend to use staticly defined compilation functions for optimal performance. We provide the option to define a default compilation mode for decorated functions, while also allowing to define the compilation mode for each individual function.\nNOTE: Compiled functions are by default expected to be performed on a single thread. Thus, ‘cuda’ funtions are always assumed to be device functions which makes them callable from within the GPU, unless explicitly stated otherwise. Similarly, ‘numba’ functions are always assumed to bo ‘nopython’ and ‘nogil’.\nNOTE If the global compilation mode is set to Python, all decorators default to python, even if a specific compilation_mode is provided.\nIn addition, we allow to enable dynamic compilation, meaning the compilation mode of functions can be changed at runtime. Do note that this comes at the cost of some performance, as compilation needs to be done at runtime as well. Moreover, functions that are defined with dynamic compilation can not be called from within other compiled functions (with the exception of ‘python’ compilation, which means no compilation is actually performe|d).\nNOTE: Dynamic compilation must be enabled before functions are decorated to take effect at runtime, otherwise they are statically compiled with the current settings at the time they are defined! Alternatively, statically compiled functions of a an ‘imported_module’ can reloaded (and thus statically be recompiled) with the commands:\nimport importlib\nimportlib.reload(imported_module)\n\nsource\n\n\ncompile_function\n\n compile_function (_func:<built-infunctioncallable>=None,\n                   compilation_mode:str=None, **decorator_kwargs)\n\nA decorator to compile a given function.\nNumba functions are by default set to use nogil=True and nopython=True, unless explicitly defined otherwise. Cuda functions are by default set to use device=True, unless explicitly defined otherwise..\nArgs: compilation_mode (str): The compilation mode to use. Will be checked with is_valid_compilation_mode. If None, the global COMPILATION_MODE will be used as soon as the function is decorated for static compilation. If DYNAMIC_COMPILATION_ENABLED, the function will always be compiled at runtime and thus by default returns a Python function. Static recompilation can be enforced by reimporting a module containing the function with importlib.reload(imported_module). If COMPILATION_MODE is Python and not DYNAMIC_COMPILATION_ENABLED, no compilation will be used. Default is None **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.\nReturns: callable: A decorated function that is compiled.\n\nsource\n\n\nset_compilation_mode\n\n set_compilation_mode (compilation_mode:str=None,\n                       enable_dynamic_compilation:bool=None)\n\nSet the global compilation mode to use.\nArgs: compilation_mode (str): The compilation mode to use. Will be checked with is_valid_compilation_mode. Default is None enable_dynamic_compilation (bool): Enable dynamic compilation. If enabled, code will generally be slower and no other functions can be called from within a compiled function anymore, as they are compiled at runtime. WARNING: Enabling this is strongly disadvised in almost all cases! Default is None.\nTesting yields the expected results:\n\nimport types\n\nset_compilation_mode(compilation_mode=\"numba-multithread\")\n\n@compile_function(compilation_mode=\"python\")\ndef test_func_python(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n    \n@compile_function(compilation_mode=\"numba\")\ndef test_func_numba(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n\nset_compilation_mode(enable_dynamic_compilation=True)\n\n@compile_function\ndef test_func_dynamic_runtime(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n\nset_compilation_mode(enable_dynamic_compilation=False, compilation_mode=\"numba-multithread\")\n\n@compile_function\ndef test_func_static_runtime_numba(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n\na = np.zeros(1, dtype=np.int64)\nassert(isinstance(test_func_python, types.FunctionType))\ntest_func_python(a)\nassert(np.all(a == np.ones(1)))\n\na = np.zeros(1)\nassert(isinstance(test_func_numba, numba.core.registry.CPUDispatcher))\ntest_func_numba(a)\nassert(np.all(a == np.ones(1)))\n\nif __GPU_AVAILABLE:\n    @compile_function(compilation_mode=\"cuda\", device=None)\n    def test_func_cuda(x):\n        \"\"\"Docstring test\"\"\"\n        x[0] += 1\n\n    # Cuda function cannot be tested from outside the GPU\n    a = np.zeros(1)\n    assert(isinstance(test_func_cuda, numba.cuda.compiler.Dispatcher))\n    test_func_cuda.forall(1,1)(a)\n    assert(np.all(a == np.ones(1)))\n\nset_compilation_mode(compilation_mode=\"python\")\na = np.zeros(1)\nassert(isinstance(test_func_static_runtime_numba, numba.core.registry.CPUDispatcher))\ntest_func_static_runtime_numba(a)\nassert(np.all(a == np.ones(1)))\n\nset_compilation_mode(compilation_mode=\"python\")\na = np.zeros(1)\nassert(isinstance(test_func_dynamic_runtime, types.FunctionType))\ntest_func_dynamic_runtime(a)\nassert(np.all(a == np.ones(1)))\n\nset_compilation_mode(compilation_mode=\"numba\")\na = np.zeros(1)\nassert(isinstance(test_func_dynamic_runtime, types.FunctionType))\ntest_func_dynamic_runtime(a)\nassert(np.all(a == np.ones(1)))\n\n# # Cuda function cannot be tested from outside the GPU\n# set_compilation_mode(compilation_mode=\"cuda\")\n# a = np.zeros(1)\n# assert(isinstance(test_func_dynamic_runtime, types.FunctionType))\n# test_func_dynamic_runtime.forall(1,1)(a)\n# assert(np.all(a == np.ones(1)))\n\nNext, we define the ‘performance_function’ decorator to take full advantage of both compilation and parallelization for maximal performance. Note that a ‘performance_function’ can not return values. Instead, it should store results in provided buffer arrays.\n\nsource\n\n\nperformance_function\n\n performance_function (_func:<built-infunctioncallable>=None,\n                       worker_count:int=None, compilation_mode:str=None,\n                       **decorator_kwargs)\n\nA decorator to compile a given function and allow multithreading over an multiple indices.\nNOTE This should only be used on functions that are compilable. Functions that need to be decorated need to have an index argument as first argument. If an iterable is provided to the decorated function, the original (compiled) function will be applied to all elements of this iterable. The most efficient way to provide iterables are with ranges, but numpy arrays work as well. Functions can not return values, results should be stored in buffer arrays inside thge function instead.\nArgs: worker_count (int): The number of workers to use for multithreading. If None, the global MAX_WORKER_COUNT is used at runtime. Default is None. compilation_mode (str): The compilation mode to use. Will be forwarded to the compile_function decorator. **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.\nReturns: callable: A decorated function that is compiled and parallelized.\nWe test this function with a simple smoothing algorithm.\n\ndef smooth_func(index, in_array, out_array, window_size):\n    min_index = max(index - window_size, 0)\n    max_index = min(index + window_size + 1, len(in_array))\n    smooth_value = 0\n    for i in range(min_index, max_index):\n        smooth_value += in_array[i]\n    out_array[index] += smooth_value / (max_index - min_index)\n\n\nset_compilation_mode(compilation_mode=\"numba-multithread\")\nset_worker_count(0)\narray_size = 10**6\nsmooth_factor = 10**4\n\n# python test\nin_array = np.arange(array_size)\nout_array = np.zeros_like(in_array)\n\nfunc = performance_function(compilation_mode=\"python\")(smooth_func)\n\n\n# numba test\nin_array = np.arange(array_size)\nout_array = np.zeros_like(in_array)\n\nfunc = performance_function(compilation_mode=\"numba\")(smooth_func)\n\n\n# numba-multithread test\nin_array = np.arange(array_size)\nout_array = np.zeros_like(in_array)\n\nfunc = performance_function(compilation_mode=\"numba-multithread\")(smooth_func)\n\n\n# cuda test\nif __GPU_AVAILABLE:\n    in_array = cupy.arange(array_size)\n    out_array = cupy.zeros_like(in_array)\n\n    func = performance_function(compilation_mode=\"cuda\")(smooth_func)\n\nCPU times: total: 2.55 s\nWall time: 2.54 s\nCPU times: total: 7.47 s\nWall time: 7.49 s\nCPU times: total: 11 s\nWall time: 887 ms\n\n\nFinally, we also provide functionality to use multiprocessing instead of multithreading.\nNOTE: There are some inherent limitation with the number of processes that Python can spawn. As such, no process Pool should use more than 50 processes.\n\nsource\n\n\nAlphaPool\n\n AlphaPool (process_count:int)\n\nCreate a multiprocessing.Pool object.\nArgs: process_count (int): The number of processes. If larger than available cores, it is trimmed to the available maximum.\nReturns: multiprocessing.Pool: A Pool object to parallelize functions with multiple processes."
  },
  {
    "objectID": "search.html",
    "href": "search.html",
    "title": "Search",
    "section": "",
    "text": "This notebook contains all functions related to searching and getting peptide-spectrum-matches (PSMs). When searching, we compare how similar an experimental spectrum is to a theoretical spectrum. As described in the FASTA notebook, we can calculate theoretical fragment masses for a given peptide sequence and get theoretical spectra. Typically, we calculate a database with all possible spectra, save it to disk, and then compare our experimental data. This allows re-using the database and saving time for this computational step. It could be that the database is too large to be saved on disc; in this case, generate the database on the fly, referring to only have a subset of all FASTA entries in memory and processing them."
  },
  {
    "objectID": "search.html#comparing-spectra",
    "href": "search.html#comparing-spectra",
    "title": "Search",
    "section": "Comparing spectra",
    "text": "Comparing spectra\nTo efficiently compare two spectra, we use a pointer based approach. We start with two sorted arrays, the query_frag that contains the m/z positions of the experimental query spectrum and the db_frag which contains the database fragment that is compared against to. The two pointers compare each m/z position with each other and check wheter they are within a certain tolerance frag_tol. Depending on their delta, either of the pointers is advanced. The function returns an arrray named hits that is the same length as the database spectrum and encodes the hit positions.\n\nsource\n\ncompare_frags\n\n compare_frags (query_frag:numpy.ndarray, db_frag:numpy.ndarray,\n                frag_tol:float, ppm:bool=False)\n\nCompare query and database frags and find hits\nArgs: query_frag (np.ndarray): Array with query fragments. db_frag (np.ndarray): Array with database fragments. frag_tol (float): Fragment tolerance for search. ppm (bool, optional): Use ppm as unit or Dalton. Defaults to False.\nReturns: np.ndarray: Array with reported hits.\n\nimport numpy as np\nquery_frag = np.array([100, 200, 300, 400])\ndb_frag = np.array([150, 200, 300, 450])\n\n# Hits: Query 2 -> Db 2 and Query 3 -> Db 3\n\ncompare_frags(query_frag, db_frag, frag_tol=1)\n\narray([0, 2, 3, 0], dtype=int16)\n\n\nThis function allows us to easily compare a query spectrum against a spectrum from a theoretical database.\n\nimport matplotlib.pyplot as plt\nfrom alphapept import constants\nfrom alphapept.fasta import get_frag_dict, parse\nimport alphapept.io\nimport numpy as np\n\n\npeptide = 'PEPTIDE'\n\n# Theoretical Spectrum\n\nfrag_dict = get_frag_dict(parse(peptide), constants.mass_dict)\ndb_frag = list(frag_dict.values())\ndb_frag.sort()\n\ndb_int = [100 for _ in db_frag]\n\n# Experimental Spectrum, dummy data\n\nquery_frag = np.array([98.06, 227.10, 263.08, 548.06, 653.31])\nquery_int = np.array([20, 80, 30, 30, 50])\n\nhits = compare_frags(query_frag, db_frag, frag_tol=1)\n\nhitpos = hits[hits > 0] - 1\nhit_x = query_frag[hitpos]\nhit_y = query_int[hitpos]\n\n\nplt.figure(figsize=(10,5))\nplt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\nplt.vlines(query_frag, 0, query_int, \"r\", label=\"Query\", alpha=0.5)\n\nplt.plot(hit_x, hit_y, \"ro\", label=\"Hit\", alpha=0.5)\n\nfor _ in frag_dict.keys():\n    plt.text(frag_dict[_], 104, _, fontsize=12, alpha = 0.8)\n    \nplt.title('Theoretical Spectrum for {}'.format(peptide))\nplt.xlabel('Mass')\nplt.ylabel('Intensity')\nplt.legend()\nplt.ylim([0,110])\nplt.show()"
  },
  {
    "objectID": "search.html#comparing-spectra-1",
    "href": "search.html#comparing-spectra-1",
    "title": "Search",
    "section": "Comparing Spectra",
    "text": "Comparing Spectra\nTo compare multiple spectra against a database, we first need some helper functions. First, we need a conversion function to convert from Dalton masses to ppm, which is implemented in the ppm_to_dalton function.\nTo minimize the search space, we typically only compare spectra with precursors in the same mass range as defined by prec_tol. To look up the limits for search, we define the function get_idxs, which is a wrapper to the fast searchsorted method from NumPy.\nThe actual search takes place in compare_spectrum_parallel, which utilizes the performance decorator from the performance notebook. Here we save the top matching spectra for each query spectrum. Note that for code compilation reasons, the code of the previously defined function compare_frags is duplicated in here.\n\nsource\n\nppm_to_dalton\n\n ppm_to_dalton (mass:float, prec_tol:int)\n\nFunction to convert ppm tolerances to Dalton.\nArgs: mass (float): Base mass. prec_tol (int): Tolerance.\nReturns: float: Tolerance in Dalton.\n\nsource\n\n\nget_idxs\n\n get_idxs (db_masses:numpy.ndarray, query_masses:numpy.ndarray,\n           prec_tol:float, ppm:bool)\n\nFunction to get upper and lower limits to define search range for a given precursor tolerance.\nArgs: db_masses (np.ndarray): Array containing database masses. query_masses (np.ndarray): Array containing query masses. prec_tol (float): Precursor tolerance for search. ppm: Flag to use ppm instead of Dalton.\nReturns: (np.ndarray, np.ndarray): Indices to lower and upper bounds.\n\nsource\n\n\ncompare_spectrum_parallel\n\n compare_spectrum_parallel (query_idx:int, query_masses:numpy.ndarray,\n                            idxs_lower:numpy.ndarray,\n                            idxs_higher:numpy.ndarray,\n                            query_indices:numpy.ndarray,\n                            query_frags:numpy.ndarray,\n                            query_ints:numpy.ndarray,\n                            db_indices:numpy.ndarray,\n                            db_frags:numpy.ndarray,\n                            best_hits:numpy.ndarray, score:numpy.ndarray,\n                            frag_tol:float, ppm:bool)\n\nCompares a spectrum and writes to the best_hits and score.\nArgs: query_idx (int): Integer to the query_spectrum that should be compared. query_masses (np.ndarray): Array with query masses. idxs_lower (np.ndarray): Array with indices for lower search boundary. idxs_higher (np.ndarray): Array with indices for upper search boundary. query_indices (np.ndarray): Array with indices to the query data. query_frags (np.ndarray): Array with frag types of the query data. query_ints (np.ndarray): Array with fragment intensities from the query. db_indices (np.ndarray): Array with indices to the database data. db_frags (np.ndarray): Array with frag types of the db data. best_hits (np.ndarray): Reporting array which stores indices to the best hits. score (np.ndarray): Reporting array that stores the scores of the best hits. frag_tol (float): Fragment tolerance for search. ppm (bool): Flag to use ppm instead of Dalton."
  },
  {
    "objectID": "search.html#wrapper",
    "href": "search.html#wrapper",
    "title": "Search",
    "section": "Wrapper",
    "text": "Wrapper\nTo conveniently perform peptide-spectrum matches on multiple datasets we define a wrapper get_psms that returns the PSMS when handing over query_data and db_data.\n\nsource\n\nquery_data_to_features\n\n query_data_to_features (query_data:dict)\n\nHelper function to extract features from query data. This is used when the feature finder will not be used.\nArgs: query_data (dict): Data structure containing the query data.\nReturns: pd.DataFrame: Pandas dataframe so that it can be used for subsequent processing.\n\nsource\n\n\nget_psms\n\n get_psms (query_data:dict, db_data:dict,\n           features:pandas.core.frame.DataFrame, parallel:bool,\n           frag_tol:float, prec_tol:float, ppm:bool, min_frag_hits:int,\n           callback:Callable=None, prec_tol_calibrated:float=None,\n           frag_tol_calibrated:float=None, **kwargs)\n\n[summary]\nArgs: query_data (dict): Data structure containing the query data. db_data (dict): Data structure containing the database data. features (pd.DataFrame): Pandas dataframe containing feature data. parallel (bool): Flag to use parallel processing. frag_tol (float): Fragment tolerance for search. prec_tol (float): Precursor tolerance for search. ppm (bool): Flag to use ppm instead of Dalton. min_frag_hits (int): Minimum number of frag hits to report a PSMs. callback (Callable, optional): Optional callback. Defaults to None. prec_tol_calibrated (float, optional): Precursor tolerance if calibration exists. Defaults to None. frag_tol_calibrated (float, optional): Fragment tolerance if calibration exists. Defaults to None.\nReturns: np.ndarray: Numpy recordarray storing the PSMs. int: 0"
  },
  {
    "objectID": "search.html#extracting-columns-for-scoring",
    "href": "search.html#extracting-columns-for-scoring",
    "title": "Search",
    "section": "Extracting columns for scoring",
    "text": "Extracting columns for scoring\nThe basic fragment comparison only counts the number of hits and matched intensity fraction when comparing a theoretical spectrum to an experimental one. Based on this metric, we can drastically reduce the number of candidates one wants to analyze for an in-depth comparison, which requires additional features. The following section describes several functions which extract parameters to compare spectrum matches better.\n\nFrag Delta\nfrag_delta substracts the experimental fragment masses from the theoretical fragment masses for each hit.\n\nsource\n\n\nfrag_delta\n\n frag_delta (query_frag:numpy.ndarray, db_frag:numpy.ndarray,\n             hits:numpy.ndarray)\n\nCalculates the mass difference for a given array of hits in Dalton and ppm.\nArgs: query_frag (np.ndarray): Array with query fragments. db_frag (np.ndarray): Array with database fragments. hits (np.ndarray): Array with reported hits.\nReturns: float: Fragment deltas in Dalton. float: Fragment deltas in ppm.\n\n\nIntensity Fraction\nintensity_fraction calculates the fraction of matched intensity. This refers to the intensity of all hits compared to the intensity of all peaks in the query spectrum.\n\nsource\n\n\nintensity_fraction\n\n intensity_fraction (query_int:numpy.ndarray, hits:numpy.ndarray)\n\nCalculate the fraction of matched intensity\nArgs: query_int (np.ndarray): Array with query intensities. hits (np.ndarray): Array with reported hits.\nReturns: float: Fraction of the matched intensity to the total intensity.\n\n\nFile Format\nTo have an efficient data format to store PSMs in the search. We use numpy-recarrays and define the utility functions add_column and remove_column to append and remove data.\n\nsource\n\n\nremove_column\n\n remove_column (recarray:numpy.ndarray, name:str)\n\nFunction to remove a column from a recarray.\nArgs: recarray (np.ndarray): NumPy record array. name (str): Column name of the column to be removed.\nReturns: np.ndarray: NumPy record array with removed column.\n\nsource\n\n\nadd_column\n\n add_column (recarray:numpy.ndarray, column:numpy.ndarray, name:str)\n\nFunction to add a column with given name to recarray\nArgs: recarray (np.ndarray): NumPy record array. column (np.ndarray): Data column that should be added to the record array. name (str): Name of the column in the new recordarray.\nReturns: np.ndarray: NumPy recordarray with new field."
  },
  {
    "objectID": "search.html#extracting-features-for-scoring",
    "href": "search.html#extracting-features-for-scoring",
    "title": "Search",
    "section": "Extracting features for scoring",
    "text": "Extracting features for scoring\n\nIndices\nWhen performing a database search, we need to know which experimental spectrum we compare with what database entry. We distinguish three indices:\n\nquery_idx\nraw_idx\nfeature_idx\n\nInitially, the get_psms function accepts experimental data in the form of query_data. Here, the query_idx refers to the index to query_data. However, this might not be the same index as the raw data. This is due to the implementation of the matching of MS1-features to MS2 spectra. Here we allow multiple matches and implement this by repeating the respective spectrum.\nWe then add the two columns feature_idx and raw_idx to the PSMs to later be able to distinguish where the match originated. In this case, raw_idx refers to the original spectrum.\nWhen not applying feature finding, raw_idx and query_idx are equivalent.\n\n\nFeatures\nIn the score-function we use the pre-filtered PSMs to extract additional columns for scoring such as the offset from theoretical to experimental precursor or the number of b- and y-ion hits.\n\nsource\n\n\nget_hits\n\n get_hits (query_frag:numpy.ndarray, query_int:numpy.ndarray,\n           db_frag:numpy.ndarray, db_int:numpy.ndarray,\n           frag_type:numpy.ndarray, mtol:float, ppm:bool, losses:list)\n\nFunction to extract the types of hits based on a single PSMs.\nThe reporting array stores information about the matched fragment_ions in a record array.\nArgs: query_frag (np.ndarray): Array with query fragments. query_int (np.ndarray): Array with query intensities. db_frag (np.ndarray): Array with database fragments. db_int (np.ndarray): Array with database intensities. frag_type (np.ndarray): Array with fragment types. mtol (float): Mass tolerance. ppm (bool): Flag to use ppm instead of Dalton. losses (list): List of losses.\nReturns: np.ndarray: NumPy array that stores ion information.\n\nsource\n\n\nscore\n\n score (psms:numpy.recarray, query_masses:numpy.ndarray,\n        query_masses_raw:numpy.ndarray, query_frags:numpy.ndarray,\n        query_ints:numpy.ndarray, query_indices:numpy.ndarray,\n        db_masses:numpy.ndarray, db_frags:numpy.ndarray,\n        frag_types:numpy.ndarray, mtol:float, db_indices:numpy.ndarray,\n        ppm:bool, psms_dtype:list, db_ints:numpy.ndarray=None,\n        parallel:bool=False)\n\nFunction to extract score columns when giving a recordarray with PSMs.\nArgs: psms (np.recarray): Recordarray containing PSMs. query_masses (np.ndarray): Array with query masses. query_masses_raw (np.ndarray): Array with raw query masses. query_frags (np.ndarray): Array with frag types of the query data. query_ints (np.ndarray): Array with fragment intensities from the query. query_indices (np.ndarray): Array with indices to the query data. db_masses (np.ndarray): Array with database masses. db_frags (np.ndarray): Array with fragment masses. frag_types (np.ndarray): Array with fragment types. mtol (float): Mass tolerance. db_indices (np.ndarray): Array with indices to the database array. ppm (bool): Flag to use ppm instead of Dalton. psms_dtype (list): List describing the dtype of the PSMs record array. db_ints (np.ndarray, optional): Array with database intensities. Defaults to None. parallel (bool, optional): Flag to use parallel processing. Defaults to False.\nReturns: np.recarray: Recordarray containing PSMs with additional columns. np.ndarray: NumPy array containing ion information.\n\nsource\n\n\nget_sequences\n\n get_sequences (psms:numpy.recarray, db_seqs:numpy.ndarray)\n\nGet sequences to add them to a recarray\nArgs: psms (np.recarray): Recordarray containing PSMs. db_seqs (np.ndarray): NumPy array containing sequences.\nReturns: np.ndarray: NumPy array containing a subset of sequences.\n\nsource\n\n\nget_score_columns\n\n get_score_columns (psms:numpy.recarray, query_data:dict,\n                    db_data:Union[dict,str],\n                    features:pandas.core.frame.DataFrame, parallel:bool,\n                    frag_tol:float, prec_tol:float, ppm:bool,\n                    prec_tol_calibrated:Union[NoneType,float]=None,\n                    frag_tol_calibrated:float=None, **kwargs)\n\nWrapper function to extract score columns.\nArgs: psms (np.recarray): Recordarray containing PSMs. query_data (dict): Data structure containing the query data. db_data: Union[dict, str]: Data structure containing the database data or path to database. features (pd.DataFrame): Pandas dataframe containing feature data. parallel (bool): Flag to use parallel processing. frag_tol (float): Fragment tolerance for search. prec_tol (float): Precursor tolerance for search. ppm (bool): Flag to use ppm instead of Dalton. prec_tol_calibrated (Union[None, float], optional): Calibrated offset mass. Defaults to None. frag_tol_calibrated (float, optional): Fragment tolerance if calibration exists. Defaults to None.\nReturns: np.recarray: Recordarray containing PSMs with additional columns. np.ndarray: NumPy array containing ion information."
  },
  {
    "objectID": "search.html#plot",
    "href": "search.html#plot",
    "title": "Search",
    "section": "Plot",
    "text": "Plot\n\nsource\n\nplot_psms\n\n plot_psms (index, ms_file)\n\n\n# Example usage to plot a psms\nms_file = alphapept.io.MS_Data_File('../testfiles/test.ms_data.hdf')\nplot_psms(0, ms_file)"
  },
  {
    "objectID": "search.html#searching-with-database",
    "href": "search.html#searching-with-database",
    "title": "Search",
    "section": "Searching with database",
    "text": "Searching with database\nWe save intermediate results to hdf5 files\n\nsource\n\nsearch_db\n\n search_db (to_process:tuple, callback:Callable=None, parallel:bool=False,\n            first_search:bool=True)\n\nWrapper function to perform database search to be used by a parallel pool.\nArgs: to_process (tuple): Tuple containing an index to the file and the experiment settings. callback (Callable, optional): Callback function to indicate progress. Defaults to None. parallel (bool, optional): Flag to use parallel processing. Defaults to False. first_search (bool, optional): Flag to indicate this is the first search. Defaults to True.\nReturns: Union[bool, str]: Returns True if the search was successfull, otherwise returns a string containing the Exception.\n\nsource\n\n\nstore_hdf\n\n store_hdf (df:pandas.core.frame.DataFrame, path:str, key:str,\n            replace:bool=False, swmr:bool=False)\n\nWrapper function to store a DataFrame in an hdf.\nArgs: df (pd.DataFrame): DataFrame to be stored. path (str): Target path of the hdf file. key (str): Name of the field to be saved. replace (bool, optional): Flag whether the field should be replaced.. Defaults to False. swmr (bool, optional): Flag to use swmr(single write multiple read)-mode. Defaults to False."
  },
  {
    "objectID": "search.html#searching-large-fasta-and-or-search-space",
    "href": "search.html#searching-large-fasta-and-or-search-space",
    "title": "Search",
    "section": "Searching Large Fasta and or Search Space",
    "text": "Searching Large Fasta and or Search Space\n\nsource\n\nsearch_fasta_block\n\n search_fasta_block (to_process:tuple)\n\nSearch fasta block. This file digests per block and does not use a saved database. For searches with big fasta files or unspecific searches.\nArgs: to_process (tuple): Tuple containing a fasta_index, fasta_block, a list of files and a list of experimental settings.\nReturns: list: A list of dataframes when searching the respective file. int: Number of new peptides that were generated in this iteration.\n\nsource\n\n\nfilter_top_n\n\n filter_top_n (temp:pandas.core.frame.DataFrame, top_n:int=10)\n\nTakes a dataframe and keeps only the top n entries (based on hits). Combines fasta indices for sequences.\nArgs: temp (pd.DataFrame): Pandas DataFrame containing PSMs. top_n (int, optional): Number of top-n entries to be kept. Defaults to 10.\nReturns: pd.DataFrame: Filtered DataFrame.\n\nsource\n\n\nsearch_parallel\n\n search_parallel (settings:dict, calibration:Union[list,NoneType]=None,\n                  fragment_calibration:Union[list,NoneType]=None,\n                  callback:Union[Callable,NoneType]=None)\n\nFunction to search multiple ms_data files in parallel. This function will additionally calculate fragments and precursor masses from a given FASTA file.\nArgs: settings (dict): Settings file containg the experimental definitions. calibration (Union[list, None], optional): List of calibrated offsets. Defaults to None. fragment_calibration (Union[list, None], optional): List of calibrated fragment offsets. Defaults to None. callback (Union[Callable, None], optional): Callback function. Defaults to None.\nReturns: dict: FASTA dictionary.\n\nsource\n\n\nion_extractor\n\n ion_extractor (df:pandas.core.frame.DataFrame, ms_file, frag_tol:float,\n                ppm:bool)\n\nExtracts the matched hits (fragment_ions) from a dataframe.\nArgs: df (pd.DataFrame): Pandas dataframe containing the results of the first search. ms_file : MsFile frag_tol (float): Fragment tolerance for search. ppm (bool): Flag to use ppm instead of Dalton.\nReturns: np.ndarray: Numpy recordarray storing the PSMs. np.ndarray: Numpy recordarray storing the fragment_ions."
  },
  {
    "objectID": "label.html",
    "href": "label.html",
    "title": "Label",
    "section": "",
    "text": "The label search is implemented based on the compare_frags from the search. We have a fixed number of reporter channels and check if we find a respective peak within the search tolerance.\nUseful resources:\n\nIsobaricAnalyzer\nTMT Talk from Hupo 2015\n\n\nsource\n\n\n\n label_search (query_frag:numpy.ndarray, query_int:numpy.ndarray,\n               label:numpy.ndarray, reporter_frag_tol:float, ppm:bool)\n\nFunction to search for a label for a given spectrum.\nArgs: query_frag (np.ndarray): Array with query fragments. query_int (np.ndarray): Array with query intensities. label (np.ndarray): Array with label masses. reporter_frag_tol (float): Fragment tolerance for search. ppm (bool): Flag to use ppm instead of Dalton.\nReturns: np.ndarray: Array with intensities for the respective label channel. np.ndarray: Array with offset masses.\n\ndef test_label_search():\n    query_frag = np.array([1,2,3,4,5])\n    query_int = np.array([1,2,3,4,5])\n    label = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    frag_tolerance = 0.1\n    ppm= False\n\n    assert np.allclose(label_search(query_frag, query_int, label, frag_tolerance, ppm)[0], query_int)\n\n    query_frag = np.array([1,2,3,4,6])\n    query_int = np.array([1,2,3,4,5])\n\n    assert np.allclose(label_search(query_frag, query_int, label, frag_tolerance, ppm)[0], np.array([1,2,3,4,0]))\n    \n    query_frag = np.array([1,2,3,4,6])\n    query_int = np.array([5,4,3,2,1])\n\n    assert np.allclose(label_search(query_frag, query_int, label, frag_tolerance, ppm)[0], np.array([5,4,3,2,0]))\n    \n    query_frag = np.array([1.1, 2.2, 3.3, 4.4, 6.6])\n    query_int = np.array([1,2,3,4,5])\n    \n    frag_tolerance = 0.5\n    ppm= False\n    \n    assert np.allclose(label_search(query_frag, query_int, label, frag_tolerance, ppm)[1], np.array([0.1, 0.2, 0.3, 0.4, 0.0]))\n    \ntest_label_search()\n\n\n#Example usage\n\nquery_frag = np.array([127, 128, 129.1, 132])\nquery_int = np.array([100, 200, 300, 400, 500])\n\nlabel = np.array([127.0, 128.0, 129.0, 130.0])\n\nfrag_tolerance = 0.1\nppm = False\n\nreport, offset = label_search(query_frag, query_int, label, frag_tolerance, ppm)\n\nprint(f'Reported intensities {report}, Offset {offset}')\n\nReported intensities [100. 200. 300.   0.], Offset [0.  0.  0.1 0. ]"
  },
  {
    "objectID": "label.html#ms2-search",
    "href": "label.html#ms2-search",
    "title": "Label",
    "section": "MS2 Search",
    "text": "MS2 Search\n\nsource\n\nsearch_label_on_ms_file\n\n search_label_on_ms_file (file_name:str, label:<class'NamedTuple'>,\n                          reporter_frag_tol:float, ppm:bool)\n\nWrapper function to search labels on an ms_file and write results to the peptide_fdr of the file.\nArgs: file_name (str): Path to ms_file: label (NamedTuple): Label with channels, mod_name and masses. reporter_frag_tol (float): Fragment tolerance for search. ppm (bool): Flag to use ppm instead of Dalton.\n\nsource\n\n\nfind_labels\n\n find_labels (to_process:dict, callback:<built-infunctioncallable>=None,\n              parallel:bool=False)\n\nWrapper function to search for labels.\nArgs: to_process (dict): A dictionary with settings indicating which files are to be processed and how. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None. parallel (bool): If True, process multiple files in parallel. This is not implemented yet! Defaults to False.\nReturns: bool: True if and only if the label finding was succesful."
  },
  {
    "objectID": "settings.html",
    "href": "settings.html",
    "title": "Settings",
    "section": "",
    "text": "AlphaPept stores all settings in *.yaml-files. This notebook contains functions to load, save, and print settings. Additionally, a settings template is defined. Here we define parameters, default values, and a range and what kind of parameter this is (e.g., float value, list, etc.). The idea here is to have definitions to automatically create graphical user interfaces for the settings."
  },
  {
    "objectID": "settings.html#settings",
    "href": "settings.html#settings",
    "title": "Settings",
    "section": "Settings",
    "text": "Settings\n\nSaving and Loading\nThe default scheme for saving settings are *.yaml-files. These files can be easily modified when opening with a text editor.\n\nsource\n\n\nsave_settings\n\n save_settings (settings:dict, path:str)\n\nSave settings file to path.\nArgs: settings (dict): A yaml dictionary. path (str): Path to the settings file.\n\nsource\n\n\nload_settings_as_template\n\n load_settings_as_template (path:str)\n\nLoads settings but removes fields that contain summary information.\nArgs: path (str): Path to the settings file.\n\nsource\n\n\nload_settings\n\n load_settings (path:str)\n\nLoad a yaml settings file.\nArgs: path (str): Path to the settings file.\n\nsource\n\n\nprint_settings\n\n print_settings (settings:dict)\n\nPrint a yaml settings file\nArgs: settings (dict): A yaml dictionary.\n\nsettings = {'field1': 0,'summary':123}\ndummy_path = 'to_delete.yaml'\n\nprint('--- print_settings ---')\nprint_settings(settings)\n\nsave_settings(settings, dummy_path)\n\nprint('--- load_settings ---')\n\nprint_settings(load_settings(dummy_path))\n\nprint('--- load_settings_as_template ---')\n\nprint_settings(load_settings_as_template(dummy_path))\n\n--- print_settings ---\nfield1: 0\nsummary: 123\n\n--- load_settings ---\nfield1: 0\nsummary: 123\n\n--- load_settings_as_template ---\nfield1: 0"
  },
  {
    "objectID": "settings.html#settings-template",
    "href": "settings.html#settings-template",
    "title": "Settings",
    "section": "Settings Template",
    "text": "Settings Template\nThe settings template defines individual settings. The idea is to provide a template so that a graphical user interface can be automatically generated. The list below represents what each item would be when using streamlit. This could be adapted for any kind of GUI library.\nEach entry has a type, default values, and a description.\n\nspinbox -> st.range, range with minimum and maximum values (int)\ndoublespinbox -> st.range, range with minimum and maximum values (float)\npath -> st.button, clickable button to select a path to save / load files.\ncombobox -> st.selectbox, dropdown menu with values to choose from\ncheckbox -> st.checkbox, checkbox that can be selected\ncheckgroup -> st.multiselect, creates a list of options that can be selected\nstring -> st.text_input, generic string input\nlist -> Creates a list that is displayed\nplaceholder -> This just prints the parameter and cannot be changed\n\n\nWorfklow settings\nWorkflow settings regarding the workflow - which algorithmic steps should be performed.\n\nprint(yaml.dump(SETTINGS_TEMPLATE['workflow']))\n\nalign:\n  default: false\n  description: Flag to align the data.\n  type: checkbox\ncontinue_runs:\n  default: false\n  description: Flag to continue previously computated runs. If False existing ms_data\n    will be deleted.\n  type: checkbox\ncreate_database:\n  default: true\n  description: Flag to create a database.\n  type: checkbox\nfind_features:\n  default: true\n  description: Flag to perform feature finding.\n  type: checkbox\nimport_raw_data:\n  default: true\n  description: Flag to import the raw data.\n  type: checkbox\nlfq_quantification:\n  default: true\n  description: Flag to perfrom lfq normalization.\n  type: checkbox\nmatch:\n  default: false\n  description: Flag to perform match-between runs.\n  type: checkbox\nrecalibrate_data:\n  default: true\n  description: Flag to perform recalibration.\n  type: checkbox\nsearch_data:\n  default: true\n  description: Flag to perform search.\n  type: checkbox\n\n\n\n\nprint(yaml.dump(SETTINGS_TEMPLATE['general']))\n\nn_processes:\n  default: 60\n  description: Maximum number of processes for multiprocessing. If larger than number\n    of processors it will be capped.\n  max: 60\n  min: 1\n  type: spinbox\n\n\n\n\n\nExperimental Settings\nCore defintions of the experiment, regarding the filepaths..\n\nprint(yaml.dump(SETTINGS_TEMPLATE['experiment']))\n\ndatabase_path:\n  default: null\n  description: Path to library file (.hdf).\n  filetype:\n  - hdf\n  folder: false\n  type: path\nfasta_paths:\n  default: []\n  description: List of paths for FASTA files.\n  type: list\nfile_paths:\n  default: []\n  description: Filepaths of the experiments.\n  type: list\nfraction:\n  default: []\n  description: List of fraction numbers for fractionated samples.\n  type: list\nmatching_group:\n  default: []\n  description: List of macthing groups for the raw files. This only allows match-between-runs\n    of files within the same groups.\n  type: list\nresults_path:\n  default: null\n  description: Path where the results should be stored.\n  filetype:\n  - hdf\n  folder: false\n  type: path\nsample_group:\n  default: []\n  description: Sample group, for raw files that should be quanted together.\n  type: list\nshortnames:\n  default: []\n  description: List of shortnames for the raw files.\n  type: list\n\n\n\n\n\nRaw file handling\n\nprint(yaml.dump(SETTINGS_TEMPLATE['raw']))\n\nn_most_abundant:\n  default: 400\n  description: Number of most abundant peaks to be isolated from raw spectra.\n  max: 1000\n  min: -1\n  type: spinbox\nuse_profile_ms1:\n  default: false\n  description: Use profile data for MS1 and perform own centroiding.\n  type: checkbox\n\n\n\n\n\nFASTA settings\n\nprint(yaml.dump(SETTINGS_TEMPLATE['fasta']))\n\nAL_swap:\n  default: false\n  description: Swap A and L for decoy generation.\n  type: checkbox\nKR_swap:\n  default: false\n  description: Swap K and R (only if terminal) for decoy generation.\n  type: checkbox\nfasta_block:\n  default: 1000\n  description: Number of fasta entries to be processed in one block.\n  max: 10000\n  min: 100\n  type: spinbox\nfasta_size_max:\n  default: 100\n  description: Maximum size of FASTA (MB) when switching on-the-fly.\n  max: 1000000\n  min: 1\n  type: spinbox\nisoforms_max:\n  default: 1024\n  description: Maximum number of isoforms per peptide.\n  max: 4096\n  min: 1\n  type: spinbox\nmods_fixed:\n  default:\n  - cC\n  description: Fixed modifications.\n  type: checkgroup\n  value:\n    aK: acetylation of lysine\n    cC: carbamidomethylation of C\n    deamN: deamidation of N\n    deamQ: deamidation of Q\n    eK: EASItag 6-plex on K\n    itraq4K: iTRAQ 4-plex on K\n    itraq4Y: iTRAQ 4-plex on Y\n    itraq8K: iTRAQ 8-plex on K\n    itraq8Y: iTRAQ 8-plex on Y\n    oxM: oxidation of M\n    pS: phosphorylation of S\n    pT: phosphorylation of T\n    pY: phosphorylation of Y\n    tmt0K: TMT zero on K\n    tmt0Y: TMT zero on Y\n    tmt2K: TMT duplex on K\n    tmt2Y: TMT duplex on Y\n    tmt6K: TMT sixplex/tenplex on K\n    tmt6Y: TMT sixplex/tenplex on Y\nmods_fixed_terminal:\n  default: []\n  description: Fixed terminal modifications.\n  type: checkgroup\n  value:\n    arg10>R: Arg 10 on peptide C-terminus\n    arg6>R: Arg 6 on peptide C-terminus\n    cm<C: pyro-cmC\n    e<^: EASItag 6-plex on peptide N-terminus\n    itraq4K<^: iTRAQ 4-plex on peptide N-terminus\n    itraq8K<^: iTRAQ 8-plex on peptide N-terminus\n    lys8>K: Lys 8 on peptide C-terminus\n    pg<E: pyro-E\n    pg<Q: pyro-Q\n    tmt0<^: TMT zero on peptide N-terminus\n    tmt2<^: TMT duplex on peptide N-terminus\n    tmt6<^: TMT sixplex/tenplex on peptide N-terminus\nmods_fixed_terminal_prot:\n  default: []\n  description: Fixed terminal modifications on proteins.\n  type: checkgroup\n  value:\n    a<^: acetylation of protein N-terminus\n    am>^: amidation of protein C-terminus\nmods_variable:\n  default:\n  - oxM\n  description: Variable modifications.\n  type: checkgroup\n  value:\n    aK: acetylation of lysine\n    cC: carbamidomethylation of C\n    deamN: deamidation of N\n    deamQ: deamidation of Q\n    eK: EASItag 6-plex on K\n    itraq4K: iTRAQ 4-plex on K\n    itraq4Y: iTRAQ 4-plex on Y\n    itraq8K: iTRAQ 8-plex on K\n    itraq8Y: iTRAQ 8-plex on Y\n    oxM: oxidation of M\n    pS: phosphorylation of S\n    pT: phosphorylation of T\n    pY: phosphorylation of Y\n    tmt0K: TMT zero on K\n    tmt0Y: TMT zero on Y\n    tmt2K: TMT duplex on K\n    tmt2Y: TMT duplex on Y\n    tmt6K: TMT sixplex/tenplex on K\n    tmt6Y: TMT sixplex/tenplex on Y\nmods_variable_terminal:\n  default: []\n  description: Varibale terminal modifications.\n  type: checkgroup\n  value:\n    arg10>R: Arg 10 on peptide C-terminus\n    arg6>R: Arg 6 on peptide C-terminus\n    cm<C: pyro-cmC\n    e<^: EASItag 6-plex on peptide N-terminus\n    itraq4K<^: iTRAQ 4-plex on peptide N-terminus\n    itraq8K<^: iTRAQ 8-plex on peptide N-terminus\n    lys8>K: Lys 8 on peptide C-terminus\n    pg<E: pyro-E\n    pg<Q: pyro-Q\n    tmt0<^: TMT zero on peptide N-terminus\n    tmt2<^: TMT duplex on peptide N-terminus\n    tmt6<^: TMT sixplex/tenplex on peptide N-terminus\nmods_variable_terminal_prot:\n  default:\n  - a<^\n  description: Varibale terminal modifications on proteins.\n  type: checkgroup\n  value:\n    a<^: acetylation of protein N-terminus\n    am>^: amidation of protein C-terminus\nn_missed_cleavages:\n  default: 2\n  description: Number of missed cleavages.\n  max: 99\n  min: 0\n  type: spinbox\nn_modifications_max:\n  default: 3\n  description: Limit the number of modifications per peptide.\n  max: 10\n  min: 1\n  type: spinbox\npep_length_max:\n  default: 27\n  description: Maximum peptide length.\n  max: 99\n  min: 7\n  type: spinbox\npep_length_min:\n  default: 7\n  description: Minimum peptide length.\n  max: 99\n  min: 7\n  type: spinbox\nprotease:\n  default: trypsin\n  description: Protease for digestions.\n  type: combobox\n  value:\n  - arg-c\n  - asp-n\n  - bnps-skatole\n  - caspase 1\n  - caspase 2\n  - caspase 3\n  - caspase 4\n  - caspase 5\n  - caspase 6\n  - caspase 7\n  - caspase 8\n  - caspase 9\n  - caspase 10\n  - chymotrypsin high specificity\n  - chymotrypsin low specificity\n  - clostripain\n  - cnbr\n  - enterokinase\n  - factor xa\n  - formic acid\n  - glutamyl endopeptidase\n  - granzyme b\n  - hydroxylamine\n  - iodosobenzoic acid\n  - lys_c\n  - lys_c/p\n  - lys_n\n  - ntcb\n  - pepsin ph1.3\n  - pepsin ph2.0\n  - proline endopeptidase\n  - proteinase k\n  - staphylococcal peptidase i\n  - thermolysin\n  - thrombin\n  - trypsin_full\n  - trypsin_exception\n  - non-specific\n  - trypsin\npseudo_reverse:\n  default: true\n  description: Use pseudo-reverse strategy instead of reverse.\n  type: checkbox\nsave_db:\n  default: true\n  description: Save DB or create on the fly.\n  type: checkbox\nspectra_block:\n  default: 100000\n  description: Maximum number of sequences to be collected before theoretical spectra\n    are generated.\n  max: 1000000\n  min: 1000\n  type: spinbox\n\n\n\n\n\nFeature Finding\n\nprint(yaml.dump(SETTINGS_TEMPLATE['features']))\n\ncentroid_tol:\n  default: 8\n  max: 25\n  min: 1\n  type: spinbox\nhill_check_large:\n  default: 40\n  max: 100\n  min: 1\n  type: spinbox\nhill_length_min:\n  default: 3\n  max: 10\n  min: 1\n  type: spinbox\nhill_nboot:\n  default: 150\n  max: 500\n  min: 1\n  type: spinbox\nhill_nboot_max:\n  default: 300\n  max: 500\n  min: 1\n  type: spinbox\nhill_smoothing:\n  default: 1\n  max: 10\n  min: 1\n  type: spinbox\nhill_split_level:\n  default: 1.3\n  max: 10.0\n  min: 0.1\n  type: doublespinbox\niso_charge_max:\n  default: 6\n  max: 6\n  min: 1\n  type: spinbox\niso_charge_min:\n  default: 1\n  max: 6\n  min: 1\n  type: spinbox\niso_corr_min:\n  default: 0.6\n  max: 1\n  min: 0.1\n  type: doublespinbox\niso_mass_range:\n  default: 5\n  max: 10\n  min: 1\n  type: spinbox\niso_n_seeds:\n  default: 100\n  max: 500\n  min: 1\n  type: spinbox\niso_split_level:\n  default: 1.3\n  max: 10.0\n  min: 0.1\n  type: doublespinbox\nmap_mob_range:\n  default: 0.3\n  max: 1\n  min: 0.1\n  type: doublespinbox\nmap_mz_range:\n  default: 1.5\n  max: 2\n  min: 0.1\n  type: doublespinbox\nmap_n_neighbors:\n  default: 5\n  max: 10\n  min: 1\n  type: spinbox\nmap_rt_range:\n  default: 0.5\n  max: 1\n  min: 0.1\n  type: doublespinbox\nmax_gap:\n  default: 2\n  max: 10\n  min: 1\n  type: spinbox\nsearch_unidentified:\n  default: false\n  description: Search MSMS w/o feature.\n  type: checkbox\n\n\n\n\n\nSearch\n\nprint(yaml.dump(SETTINGS_TEMPLATE['search']))\n\ncalibrate:\n  default: true\n  description: Recalibrate masses.\n  type: checkbox\ncalibration_std_frag:\n  default: 5\n  description: Std range for fragment tolerance after calibration.\n  max: 10\n  min: 1\n  type: spinbox\ncalibration_std_prec:\n  default: 5\n  description: Std range for precursor tolerance after calibration.\n  max: 10\n  min: 1\n  type: spinbox\nfrag_tol:\n  default: 50\n  description: Maximum fragment mass tolerance.\n  max: 500\n  min: 1\n  type: spinbox\nmin_frag_hits:\n  default: 7\n  description: Minimum number of fragment hits.\n  max: 99\n  min: 1\n  type: spinbox\nparallel:\n  default: true\n  description: Use parallel processing.\n  type: checkbox\npeptide_fdr:\n  default: 0.01\n  description: FDR level for peptides.\n  max: 1.0\n  min: 0.0\n  type: doublespinbox\nppm:\n  default: true\n  description: Use ppm instead of Dalton.\n  type: checkbox\nprec_tol:\n  default: 20\n  description: Maximum allowed precursor mass offset.\n  max: 500\n  min: 1\n  type: spinbox\nprotein_fdr:\n  default: 0.01\n  description: FDR level for proteins.\n  max: 1.0\n  min: 0.0\n  type: doublespinbox\nrecalibration_min:\n  default: 100\n  description: Minimum number of datapoints to perform calibration.\n  max: 10000\n  min: 100\n  type: spinbox\n\n\n\n\n\nScore\n\nprint(yaml.dump(SETTINGS_TEMPLATE['score']))\n\nmethod:\n  default: random_forest\n  description: Scoring method.\n  type: combobox\n  value:\n  - x_tandem\n  - random_forest\n\n\n\n\n\nCalibration\n\nprint(yaml.dump(SETTINGS_TEMPLATE['calibration']))\n\ncalib_mob_range:\n  default: 0.3\n  description: Scaling factor for mobility axis.\n  max: 1.0\n  min: 0.0\n  type: doublespinbox\ncalib_mz_range:\n  default: 2000\n  description: Scaling factor for mz axis in ppm.\n  max: 10000\n  min: 1\n  type: spinbox\ncalib_n_neighbors:\n  default: 100\n  description: Number of neighbors that are used for offset interpolation.\n  max: 1000\n  min: 1\n  type: spinbox\ncalib_rt_range:\n  default: 0.5\n  description: Scaling factor for rt axis.\n  max: 10\n  min: 0.0\n  type: doublespinbox\noutlier_std:\n  default: 3\n  description: Number of std. deviations to filter outliers in psms.\n  max: 5\n  min: 1\n  type: spinbox\n\n\n\n\n\nMatching\n\nprint(yaml.dump(SETTINGS_TEMPLATE['matching']))\n\nmatch_d_min:\n  default: 3\n  description: Minimum distance cutoff for matching.\n  max: 10.0\n  min: 0.001\n  type: doublespinbox\nmatch_group_tol:\n  default: 0\n  description: When having matching groups, match neighboring groups.\n  max: 100\n  min: 0\n  type: spinbox\nmatch_p_min:\n  default: 0.05\n  description: Minimum probability cutoff for matching.\n  max: 1.0\n  min: 0.001\n  type: doublespinbox\n\n\n\n\n\nIsobaric Labeling\n\nisobaric_label = {}\n\nisobaric_label[\"label\"] = {'type':'combobox', 'value':['None','TMT10plex'], 'default':'None', 'description':\"Type of isobaric label present.\"}\nisobaric_label[\"reporter_frag_tolerance\"] = {'type':'spinbox', 'min':1, 'max':500, 'default':15, 'description':\"Maximum fragment mass tolerance for a reporter.\"}\nisobaric_label[\"reporter_frag_tolerance_ppm\"] = {'type':'checkbox', 'default':True, 'description':\"Use ppm instead of Dalton.\"}\n\nSETTINGS_TEMPLATE[\"isobaric_label\"] = isobaric_label\n\n\n\nQuantification\n\nprint(yaml.dump(SETTINGS_TEMPLATE['quantification']))\n\nlfq_ratio_min:\n  default: 1\n  description: Minimum number of ratios for LFQ.\n  max: 10\n  min: 1\n  type: spinbox\nmax_lfq:\n  default: true\n  description: Perform max lfq type quantification.\n  type: checkbox\nmode:\n  default: ms1_int_sum_apex\n  description: Column to perform quantification on.\n  type: combobox\n  value:\n  - ms1_int_sum_apex\n\n\n\n\nsource\n\n\nhash_file\n\n hash_file (path)\n\nHelper function to hash a file Taken from https://stackoverflow.com/questions/22058048/hashing-a-file-in-python\n\nsource\n\n\ncreate_default_settings\n\n create_default_settings ()"
  },
  {
    "objectID": "additional_code.html",
    "href": "additional_code.html",
    "title": "Additional code",
    "section": "",
    "text": "The GUI code in alphapept/gui is not covered by notebooks. The reason for this is that a key feature of streamlit is that code changes can be directly displayed in the browser application. streamlit watches the *.py-files and automatically updates accordingly. Having an intermediate notebook that first needs to be run before the code changes are propagated would limit the coding experience with having live feedback of code changes.\nRegardless of that, complex algorithmic code that requires more documentation should always reside in the notebooks.\n\n\nThe central entry point of the GUI is webui.py. When the AlphaPept webpage launches, the user can choose from a list of options in the sidebar (e.g. Start, Status and New experiment).\nThe options that are displayed are all elements in the sidebar dictionary in webui.py. In order to add a new page, simply import your page as a module and add it to the dictionary."
  },
  {
    "objectID": "additional_code.html#ci-test",
    "href": "additional_code.html#ci-test",
    "title": "Additional code",
    "section": "CI test",
    "text": "CI test\nThe files test_ci.py and test_gpu_.py are part of the continous integration pipeline. As indicated in the contributing-page they can be run on a local machines.\n\n\n\n\n\n\nNote\n\n\n\nUnless you are a core developer, there should not be the need to add integration tests. Feel free to create a PR in case."
  },
  {
    "objectID": "additional_code.html#sandbox",
    "href": "additional_code.html#sandbox",
    "title": "Additional code",
    "section": "Sandbox",
    "text": "Sandbox\nThe sandbox folder contains multiple sample notebooks that show how AlphaPept can be interfaced. Furthermore, the underlying notebooks for the paper figures are present.\nThe folder /figures contains the output of the figure notebooks, which are indicated with a P. These rely on a helper file named test_files.py. The folder /timings contains timing documents from different machines. They were created running timing_test.py.\nSome notebooks create/download files, which are stored in /temp\n\nMisc Notebooks\n\nTest_Run: Notebook that runs a small thermo_IRT-file. It shows how the pipeline can be run from within a notebook, how the data can be accessed and how spectra can be plotted\nParameterScreen: The settings of AlphaPept are defined in the settings_template.yaml. This can be used to screen to parameters and check their influence on performance.\nSystems_check: This notebooks contains all larger tests from the continuous integration. It is meant to have more in-depth access to the data of the test runs. This notebook relies on the test_helper.py file."
  },
  {
    "objectID": "additional_code.html#installer",
    "href": "additional_code.html#installer",
    "title": "Additional code",
    "section": "Installer",
    "text": "Installer\nThe installer-folder has contains a batch script to freeze the current installation and create a one-click installer. In order to create the installer, we use the tool Inno Setup."
  },
  {
    "objectID": "io.html",
    "href": "io.html",
    "title": "Input / Output",
    "section": "",
    "text": "Input can be read from three different files formats:\n\n.raw files describing Thermo raw data.\n.d folders describing Bruker raw data.\n.mzML files describing generic input.\n\nAll reading functions return a query_data dictionary similar to the following structure:\nquery_data = {\n    \"scan_list_ms1\":   np.array(...)\n    \"rt_list_ms1\":     np.array(...)\n    \"mass_list_ms1\":   np.array(...)\n    \"int_list_ms1\":    np.array(...)\n    \"ms_list_ms1\":     np.array(...)\n    \"scan_list_ms2\":   np.array(...)\n    \"rt_list_ms2\":     np.array(...)\n    \"mass_list_ms2\":   mass_list_ms2\n    \"int_list_ms2\":    int_list_ms2\n    \"ms_list_ms2\":     np.array(...)\n    \"prec_mass_list2\": np.array(...)\n    \"mono_mzs2\":       np.array(...)\n    \"charge2\":         np.array(...)\n}\n\n\nTo read Thermo files, AlphaPept uses the pyrawfilereader package, a Python implementation of the commonly used rawfilereader tool. By using the custom python version, Thermo files can be read without having to install MSFileReader.\nThe user can pass an additional flag use_profile_ms1. This will then use the profile data which is not centroided already an peform centroiding. Note that this will lead to slightly different intensities, as the centroided data uses the apex and the centroid algorithm the summed intensity.\n\nsource\n\n\n\n\n load_thermo_raw (raw_file_name:str, n_most_abundant:int,\n                  use_profile_ms1:bool=False, callback:<built-\n                  infunctioncallable>=None)\n\nLoad raw thermo data as a dictionary.\nArgs: raw_file_name (str): The name of a Thermo .raw file. n_most_abundant (int): The maximum number of peaks to retain per MS2 spectrum. use_profile_ms1 (bool): Use profile data or centroid it beforehand. Defaults to False. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: tuple: A dictionary with all the raw data and a string with the acquisition_date_time\n\n\n\nTo access Bruker files, AlphaPept relies on the external timsdata library from Bruker (available in the alphatims\\ext folder, licenses are applicable). Unfortunately, these libraries are only available on Windows and Linux. As a result, the reading of raw data is not available on macOS. However, once raw data is converted to .ms_data.hdf output, other workflow steps (besides feature feating) are possible without problems on macOS.\n\nsource\n\n\n\n\n load_bruker_raw (d_folder_name:str, n_most_abundant:int, callback:<built-\n                  infunctioncallable>=None, **kwargs)\n\nLoad raw Bruker data as a dictionary.\nArgs: d_folder_name (str): The name of a Bruker .d folder. n_most_abundant (int): The maximum number of peaks to retain per MS2 spectrum. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: tuple: A dictionary with all the raw data and a string with the acquisition_date_time\nFor ccs (i.e., ion mobility) values, we need additional functions from the Bruker library. As the live feature-finder might not be able to determine some charge values, it is intended to perform this calculation at a later stage once we have charge values from the post-processing feature finder.\n\nsource\n\n\n\n\n one_over_k0_to_CCS (one_over_k0s:numpy.ndarray, charges:numpy.ndarray,\n                     mzs:numpy.ndarray)\n\nRetrieve collisional cross section (CCS) values from (mobility, charge, mz) arrays.\nArgs: one_over_k0s (np.ndarray): The ion mobilities (1D-np.float). charges (np.ndarray): The charges (1D-np.int). mzs (np.ndarray): The mz values (1D-np.float).\nReturns: np.ndarray: The CCS values.\nDue to availbility of Bruker libraries, this can only be tested on Windows and Linux.\n\n\n\nTo access .mzML files, we rely on the pyteomics package. For using an mzml format for performing a search, Peak Picking (data centroiding) should be applied to all MS levels of the data.\n\nsource\n\n\n\n\n load_mzml_data (filename:str, n_most_abundant:int, callback:<built-\n                 infunctioncallable>=None, **kwargs)\n\nLoad data from an mzml file as a dictionary.\nArgs: filename (str): The name of a .mzml file. n_most_abundant (int): The maximum number of peaks to retain per MS2 spectrum. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.\nReturns: tuple: A dictionary with all the raw data, a string with the acquisition_date_time and a string with the vendor.\n\nsource\n\n\n\n\n extract_mzml_info (input_dict:dict)\n\nExtract basic MS coordinate arrays from a dictionary.\nArgs: input_dict (dict): A dictionary obtained by iterating over a Pyteomics mzml.read function.\nReturns: tuple: The rt, masses, intensities, ms_order, prec_mass, mono_mz, charge arrays retrieved from the input_dict. If the ms level in the input dict does not equal 2, the charge, mono_mz and prec_mass will be equal to 0.\n\nsource\n\n\n\n\n check_sanity (mass_list:numpy.ndarray)\n\nSanity check for mass list to make sure the masses are sorted.\nArgs: mass_list (np.ndarray): The mz values (1D-np.float).\nRaises: ValueError: When the mz values are not sorted."
  },
  {
    "objectID": "io.html#reading-other-files",
    "href": "io.html#reading-other-files",
    "title": "Input / Output",
    "section": "Reading other files",
    "text": "Reading other files\nBenchmarking proteomics software against each other is not straightforward as various naming conventions exist and different algorithms are implemented. In this section, we define some helper functions that allow us to read results from other tools and facilitate the comparison of different tools against AlphaPept.\n\nReading MaxQuant .xml settings file\nOne of the most commonly used tools to analyze MS data is MaxQuant. AlphaPept reads MaxQuant .xml files as follows:\n\nsource\n\n\nextract_mq_settings\n\n extract_mq_settings (path:str)\n\nFunction to return MaxQuant values as a dictionary for a given xml file.\nArgs: path (str): File name of an xml file.\nReturns: dict: A dictionary with MaxQuant info.\nRaises: ValueError: When path is not a valid xml file.\n\nmq_dict = extract_mq_settings('../testfiles/test_mqpar.xml')\nmq_dict['fastaFiles']\n\n{'FastaFileInfo': {'fastaFilePath': 'testfile.fasta',\n  'identifierParseRule': '>([^\\\\s]*)',\n  'descriptionParseRule': '>(.*)',\n  'taxonomyParseRule': None,\n  'variationParseRule': None,\n  'modificationParseRule': None,\n  'taxonomyId': None}}\n\n\nAlphaPept incorporates PTMs directly in amino acid sequences with lower case identifiers. Parsing MaxQuant sequences with PTMs is done with:\n\nsource\n\n\nparse_mq_seq\n\n parse_mq_seq (peptide:str)\n\nReplaces maxquant convention to alphapept convention.\nToDo: include more sequences\nArgs: peptide (str): A peptide sequence from MaxQuant.\nReturns: str: A parsed peptide sequence compatible with AlphaPept.\nA quick test shows the results are correct:\n\nparse_mq_seq('_AFQPFFVELTM(Oxidation (M))PYSVIR_')\n\n'AFQPFFVELToxMPYSVIR'"
  },
  {
    "objectID": "io.html#preprocessing-data",
    "href": "io.html#preprocessing-data",
    "title": "Input / Output",
    "section": "Preprocessing data",
    "text": "Preprocessing data\nRaw data needs to be preprocessed to simplify it for AlphaPept analysis. Currently, this requires:\n\nProfiling of the data\nTrimming of spectra to retain only the ‘n’-most intense peaks\n\n\nCentroiding profile data\nImporting raw data frequently results in profile data. When having profile data, Alphapept first needs to perform centroiding to use this data properly. For this, it needs to search local maxima (“peaks”) of the intensity as a function of m/z. For this AlphaPept uses the function get_peaks. A peak is described by three points, the start of the peak, the center, and the end. The function accepts an intensity array and calculates the delta (gradient) between consecutive data points to determine the start, center, and end.\n\nsource\n\n\nget_peaks\n\n get_peaks (int_array:numpy.ndarray)\n\nDetects peaks in an array.\nArgs: int_array (np.ndarray): An array with intensity values.\nReturns: list: A regular Python list with all peaks. A peak is a triplet of the form (start, center, end)\nA quick test to illustrate the function:\n\nint_array = np.array([0,0,0,1,2,3,3,2,1,0,0,1,2,3,2,1,0])\nget_peaks(int_array)\n\n[(2, 6, 9), (9, 13, 16)]\n\n\nTo determine the center of the peak, we distinguish based on the number of raw data points that are contained in the peak:\n\nOne data point: m/z value of this point is taken as the center\nTwo datapoints: Average of the two m/z values, weighted by the intensities\nThree or more data points: Gaussian estimation of the center position.\n\nFor the Gaussian estimation, only the three central points are used to fit a Gaussian Peak shape. The Gaussian is then approximated with the logarithm.\nThe gaussian estimator is defined in gaussian_estimator and is used by the wrapper get_centroid.\n\nsource\n\n\ngaussian_estimator\n\n gaussian_estimator (peak:tuple, mz_array:numpy.ndarray,\n                     int_array:numpy.ndarray)\n\nThree-point gaussian estimator.\nArgs: peak (tuple): A triplet of the form (start, center, end) mz_array (np.ndarray): An array with mz values. int_array (np.ndarray): An array with intensity values.\nReturns: float: The gaussian estimate of the center.\n\nsource\n\n\nget_centroid\n\n get_centroid (peak:tuple, mz_array:numpy.ndarray,\n               int_array:numpy.ndarray)\n\nWrapper to estimate centroid center positions.\nArgs: peak (tuple): A triplet of the form (start, center, end) mz_array (np.ndarray): An array with mz values. int_array (np.ndarray): An array with intensity values.\nReturns: tuple: A tuple of the form (center, intensity)\nThe function performs as expected:\n\nint_array = np.array([0,0,0,1,2,3,3,2,1,0,0,1,2,3,2,1,0])\nmz_array = np.array([0,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\npeak = (2, 6, 9)\n\nprint(gaussian_estimator(peak, mz_array, int_array))\nprint(get_centroid(peak, mz_array, int_array))\n\n4.499999999999999\n(4.499999999999999, 12)\n\n\nThe detected centroid can also easily be visualized:\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_centroid(peak, mz_array, int_array):\n\n    plt.figure(figsize=(3, 3))\n\n    start, center, end = peak\n\n    centroid = get_centroid((start, center, end), mz_array, int_array)\n\n    plt.plot(mz_array[start - 2 : end + 2], int_array[start - 2 : end + 2])\n    plt.plot(mz_array[start - 2 : end + 2], int_array[start - 2 : end + 2], \"b.\")\n    plt.axvline(mz_array[start], color=\"r\")\n    plt.axvline(mz_array[end], color=\"r\")\n\n    plt.axvline(centroid[0], color=\"g\", label='Center')\n    plt.title(\"Centroid\")\n    plt.xlabel(\"m/z\")\n    plt.ylabel(\"Intensity\")\n    plt.legend()\n\n    plt.show()\n    \nint_array = np.array([0,0,0,1,2,3,3,2,1,0,0,1,2,3,2,1,0])\nmz_array = np.array([0,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n\npeak = (2, 6, 9)\n    \nplot_centroid(peak, mz_array, int_array)\n\n\n\n\n\nsource\n\n\ncentroid_data\n\n centroid_data (mz_array:numpy.ndarray, int_array:numpy.ndarray)\n\nEstimate centroids and intensities from profile data.\nArgs: mz_array (np.ndarray): An array with mz values. int_array (np.ndarray): An array with intensity values.\nReturns: tuple: A tuple of the form (mz_array_centroided, int_array_centroided)\n\n\nTrimming spectra to retain the n most intense peaks\nget_most_abundant: In order to save spectra in a more memory-efficient form, we only keep the n most abundant peaks. This allows us to save data in a fast, accessible matrix format.\nget_local_intensity: This calculates the local intensity to get local maxima.\n\nsource\n\n\nget_most_abundant\n\n get_most_abundant (mass:numpy.ndarray, intensity:numpy.ndarray,\n                    n_max:int, window:int=10)\n\nReturns the n_max most abundant peaks of a spectrum.\nArgs: mass (np.ndarray): An array with mz values. intensity (np.ndarray): An array with intensity values. n_max (int): The maximum number of peaks to retain. Setting n_max to -1 returns all peaks. window (int): Use local maximum in a window\nReturns: tuple: the filtered mass and intensity arrays.\n\nsource\n\n\nget_local_intensity\n\n get_local_intensity (intensity, window=10)\n\nCalculate the local intensity for a spectrum.\nArgs: intensity (np.ndarray): An array with intensity values. window (int): Window Size Returns: nop.ndarray: local intensity\nFor saving, we are currently relying on the hdf-container (see below).\nWhile we could, in principle, store the mz and int arrays as a list of variable length, this will come at a performance decrease. We, therefore, create an array of the dimensions of the n most abundant peaks and the number of spectra with the function list_to_numpy_f32 and fill the unoccupied cells with -1. This allows an increase in accessing times at the cost of additional disk space.\n\nsource\n\n\nlist_to_numpy_f32\n\n list_to_numpy_f32 (long_list:list)\n\nFunction to convert a list to np.float32 array.\nArgs: long_list (list): A regular Python list with values that can be converted to floats.\nReturns: np.ndarray: A np.float32 array."
  },
  {
    "objectID": "io.html#saving-output",
    "href": "io.html#saving-output",
    "title": "Input / Output",
    "section": "Saving output",
    "text": "Saving output\nAs MS hardware has continued to improve over the years, MS data has become more complex. To deal with this complexity, the MS community has already used many different data formats to store and access data. HDF containers are one option, but they have not yet gained widespread support.\n\nHDF containers in general\nIn general, an HDF container can be viewed as a compressed folder with metadata (i.e., attributes) associated to each single subfolder or file (i.e., data arrays of various types and sizes) within this container. A container might, for instance have contents that look like, e.g.:\nHDF_Container\n    {\n        meta_data_1: \"Some string\",\n        meta_data_2: 1234567890,\n        ...\n    }\n    array_1\n        {\n            meta_data_of_array1_1: \"Some other string\",\n            ...\n        },\n        100x2 int8\n    array_2\n        1000x2 float64\n    subfolder_1\n        {\n            meta_data_of_subfolder_1_1: \"Really any string of any length\",\n            ...\n        }\n        array_1_of_subfolder_1\n        subfolder_1_1\n        ...\n    subfolder_n\n    ...\nA few of the advantages of HDF are, e.g.:\n\nIt has no upper limit with regards to file size\nIt can be used on disk without consuming RAM memory\nIt is fully portable on different machines and systems\nIt is capable of fast IO operations\nIt allows data to be structured very transparent while still providing a flexible way to store metadata\n…\n\nFor these reasons, HDF containers have gained popularity in several scientific fields, including (astro)physics and geology. It is, therefore, no surprise that Python has excellent support for HDF containers. The two most used packages are h5py and tables, where the former has a generic API and the second is frequently used with pandas dataframes. An excellent viewer for HDF files is HDF Compass.\n\n\nUsing HDF containers in AlphaPept\nAlphaPept uses the python h5py package to store MS data in HDF containers, inspired by the ion_networks repository.\n\nFirst, a generic class is defined that will serve as an API for HDF containers. To ensure full transparency, this will include immutable metadata such as creation time, original_file_name and version.\nThe constructor of an HDF_File will be passed the file_name of an HDF container, an is_read_only flag, an is_overwritable flag and is_new_file flag.\nTo compare HDF_Files, several (magic) functions need to be defined.\nTraceability and reproducibility are ensured by storing a last_updated and a check function to warn users about potential compatibility issues.\n\n\nsource\n\n\nHDF_File\n\n HDF_File (file_name:str, is_read_only:bool=True, is_new_file:bool=False,\n           is_overwritable:bool=False)\n\nA generic class to store and retrieve on-disk data with an HDF container.\nContents of HDF containers come in three variants:\n\nGroups: folders\nDatasets: arrays\nAttributes: metadata associated with individual datasets or groups (with the root folder also considered as a normal group)\n\nThese contents can be accessed with read and write functions.\n\nsource\n\n\nHDF_File.write\n\n HDF_File.write (value, group_name:str=None, dataset_name:str=None,\n                 attr_name:str=None, overwrite:bool=None,\n                 dataset_compression:str=None, swmr:bool=False)\n\nWrite a value to an HDF_File.\nArgs: value (type): The name of the data to write. If the value is pd.DataFrame, a dataset_name must be provided. group_name (str): The group where to write data. If no group_name is provided, write directly to the root group. Defaults to None. dataset_name (str): If no dataset_name is provided, create a new group with value as name. The dataset where to write data. Defaults to None. attr_name (str): The attr where to write data. Defaults to None. overwrite (bool): Overwrite pre-existing data and truncate existing groups. If the False, ignore the is_overwritable flag of this HDF_File. Defaults to None. dataset_compression (str): The compression type to use for datasets. Defaults to None. swmr (bool): Open files in swmr mode. Defaults to False.\nRaises: IOError: When the object is read-only. KeyError: When the group_name or attr_name does not exist. ValueError: When trying to overwrite something while overwiting is disabled.\n\nsource\n\n\nHDF_File.read\n\n HDF_File.read (group_name:str=None, dataset_name:str=None,\n                attr_name:str=None, return_dataset_shape:bool=False,\n                return_dataset_dtype:bool=False,\n                return_dataset_slice:slice=slice(None, None, None),\n                swmr:bool=False)\n\nRead contents of an HDF_File.\nArgs: group_name (str): The group_name from where to read data. If no group_name has been provided, read directly from the root group. Defaults to None. dataset_name (str): The dataset to read. If no dataset_name has been provided, read directly from the group. If the dataset_name refers to a group, it is assumed to be pd.DataFrame and returned as such. Defaults to None. attr_name (str): The attribute to read. If attr_name is not None, read the attribute value instead of the contents of a group or dataset. If attr_name == ““, read all attributes as a dict. Defaults to None. return_dataset_shape (bool): Do not read complete dataset to minimize RAM and IO usage. Defaults to False. return_dataset_dtype (bool): Do not read complete dataset to minimize RAM and IO usage. Defaults to False. return_dataset_slice (slice): Do not read complete dataset to minimize RAM and IO usage. Defaults to slice(None). swmr (bool): Use swmr mode to read data. Defaults to False.\nReturns: type: Depending on what is requested, a dict, value, np.ndarray or pd.dataframe is returned.\nRaises: KeyError: When the group_name does not exist. KeyError: When the attr_name does not exist in the group or dataset. KeyError: When the dataset_name does not exist in the group. ValueError: When the requested dataset is not a np.ndarray or pd.dataframe.\nUnit tests for this generic HDF class include:\n\nCreation and truncation of files with various access.\nWriting and reading data from the container.\n\n\n\nThe final .ms_data.hdf file\nBased on the generic HDF_File, a subclass that acts as an MS data container can be implemented. This class should contain all (centroided) fragment ions, all their coordinates, and all the metadata.\n\nsource\n\n\nMS_Data_File\n\n MS_Data_File (file_name:str, is_read_only:bool=True,\n               is_new_file:bool=False, is_overwritable:bool=False)\n\nA class to store and retrieve on-disk MS data with an HDF container.\nA single generic function should allow to read raw data and store spectra. Different arguments allow different vendor formats.\n\nsource\n\n\nindex_ragged_list\n\n index_ragged_list (ragged_list:list)\n\nCreate lookup indices for a list of arrays for concatenation.\nArgs: value (list): Input list of arrays.\nReturns: indices: A numpy array with indices.\n\nsource\n\n\nMS_Data_File.import_raw_DDA_data\n\n MS_Data_File.import_raw_DDA_data (file_name:str, n_most_abundant:int=-1,\n                                   callback:<built-\n                                   infunctioncallable>=None,\n                                   query_data:dict=None, vendor:str=None)\n\nLoad centroided data and save it to this object.\nArgs: file_name (str): The file name with raw data (Thermo, Bruker or mzml). n_most_abundant (int): The maximum number of peaks to retain per MS2 spectrum. Defaults to -1. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None. query_data (dict): A dictionary with raw data. If this is not None, data will only be saved and not imported. Defaults to None. vendor (str): The vendor name, must be Thermo or Bruker if provided. Defaults to None.\nTesting of the MS_Data_File container includes reading and writing from different file formats.\nWhile that HDF data structure could be used directly, it is often easier to read it and return a query_data dictionary similar to those that are returned by the readers of Thermo, Bruker, mzML and mzXML raw data.\n\nsource\n\n\nMS_Data_File.read_DDA_query_data\n\n MS_Data_File.read_DDA_query_data (calibrated_fragments:bool=False,\n                                   force_recalibrate:bool=False,\n                                   swmr:bool=False, **kwargs)\n\nRead query data from this ms_data object and return it as a query_dict.\nArgs: calibrated_fragments (bool): If True, calibrated fragments are retrieved. Calibration offsets can already be present in the ms_data or recalculated. Defaults to False. force_recalibrate (bool): If calibrated fragments is True, recalibrate mzs values even if a recalibration is already provided. Defaults to False. swmr (bool): Open the file in swmr mode. Defaults to False. **kwargs (type): Can contain a database file name that was used for recalibration.\nReturns: dict: A query_dict with data for MS1 and MS2 scans."
  },
  {
    "objectID": "io.html#usage-in-workflows",
    "href": "io.html#usage-in-workflows",
    "title": "Input / Output",
    "section": "Usage in workflows",
    "text": "Usage in workflows\nTo use all the above functionality from a workflow with several parameters, the following functions are defined. These functions also allow parallel processing.\n\nsource\n\nraw_conversion\n\n raw_conversion (to_process:dict, callback:<built-\n                 infunctioncallable>=None, parallel:bool=False)\n\nWrapper function to convert raw to ms_data_file hdf.\nArgs: to_process (dict): A dictionary with settings indicating which files are to be processed and how. callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None. parallel (bool): If True, process multiple files in parallel. This is not implemented yet! Defaults to False.\nReturns: bool: True if and only if the conversion was succesful.\nThe following commands are bookkeeping to make sure this and other notebooks are properly parsed to python modules."
  },
  {
    "objectID": "file_formats.html",
    "href": "file_formats.html",
    "title": "AlphaPept workflow and files",
    "section": "",
    "text": "The core function of Alphapept is interface.run_complete_workflow(). This function requires a settings file (a dictionary containing the settings and file paths). Filewise, we store settings files as *.yaml file. When calling the core function, it will run a complete workflow based on the settings given."
  },
  {
    "objectID": "file_formats.html#gui",
    "href": "file_formats.html#gui",
    "title": "AlphaPept workflow and files",
    "section": "GUI",
    "text": "GUI\nWhen starting the AlphaPept GUI via the shortcut that the one-click installer created or via python (python -m alphapept gui), the AlphaPept server will be started. It can be accessed via a browser and provides a graphical user interface (GUI) to the AlphaPept functionality. The server extends the core function to a processing framework. The server is centered around three folders, Queue, Failed, and Finished, which will be created in the .alphapept-folder in the user’s home directory. Whenever a new *.yaml-file is found in the Queue-folder, the server will start handing this over to the core function and start processing. There are three ways to add files to the Queue-folder: 1. Via the New experiment-tab in the GUI 2. Manually copying a *.yaml-file into the Queue-folder 3. Automatically via the File watcher.\nThe File watcher can be set up to monitor a folder; whenever a new file matching pre-defined settings is copied to the folder, it will create a *.yaml-file and add it to the Queue-folder.\nWhenever an experiment succeeds, the *.yaml-file will be appended by summary information of the experiment and moved to the Finished-folder. As the *.yaml-file is only very small in size (~kB), it is intended to serve as a history of processed files.\nWhenever an experiment fails, the *.yaml-file will be moved to the Failed-folder. It can be moved from there to the Queue-folder for reprocessing.\n\nFASTA files\nWhen FASTA files are downloaded via the AlphaPept GUI, the files are stored in the FASTA folder in the alphapept home directory (e.g. C:\\Users\\username\\.alphapept\\fasta).\n\n\nHistory and Results\nAlphaPept screens all *.yaml-files in the finished folder and plots a run history based on the summary information. This is especially useful for QC or comparison purposes. Additionally, the *.yaml-files can be used to investigate the results of a run."
  },
  {
    "objectID": "file_formats.html#output-files",
    "href": "file_formats.html#output-files",
    "title": "AlphaPept workflow and files",
    "section": "Output Files",
    "text": "Output Files\nFor each run, AlphaPept creates several output files: - For each raw file, there will a .ms_data.hdf-file with raw-specific data, such as feature_table, first_search, second_search, peptide_fdr and identifications. Note that the table peptide_fdr does not apply the set fdr cutoff. - For the entire experiment, there will be a results.hdf (name can be defined in the settings), which contains experiment-specific data, such as protein_fdr and the protein_table (containing quantified proteins over all files). - Additionally to the results.hdf, there will be a *.yaml-file which contains the run settings and summary information of the run. This *.yaml can be used to serve as a template to rerun other files with the same settings. - If a database is created from FASTA-files there will be a database.hdf (name can be defined in the settings). This contains theoretical spectra and can be reused for other experiments (and speedup total analysis time)\nThe ms_data.hdf, results.hdf and database containers can be accessed via the alphapept.io library. The GUI also allows to explore these files. Additionally, the results.hdf can be directly loaded via the pandas-package (e.g. pd.read_hdf('results.hdf', 'protein_table').\nFor easier access, AlphaPept directly exports the most relevant tables as *.csv: - for each raw file: a _ids.csv-file with the best peptide-spectrum match per sepctrum. - for each raw file: if calibration was successfull, a _calibration.png to show the fragment calibration. - results_peptides.csv: The identified peptides after protein FDR. - results_proteins.csv: A table containing quantified proteins per file. Each column that additionally ends with _LFQ has the lfq intensity. This is after delayed normalization and extraction of optimal protein ratios (see the MaxLFQ paper). The column w/o _LFQ. Has the protein intensity after delayed normalization. Note: When LFQ is disabled, there is only one column per File and this is w/o delayed normalization. This leads to different intensities when comparing results w/ and w/o LFQ enabled and checking the non-LFQ-table. - results_protein_summary.csv: A table containing quantified proteins per file. This contains additional summary, e.g. the number of sequences that were found to identify the protein."
  },
  {
    "objectID": "file_formats.html#column-headers",
    "href": "file_formats.html#column-headers",
    "title": "AlphaPept workflow and files",
    "section": "Column headers",
    "text": "Column headers\nBelow is a description of the column headers in the output files.\n\nprotein_fdr\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndelta_m_ppm_abs\nabsolute value of delta_m in ppm\n\n\nb-H2O_hits\nb-ion hit with a water loss\n\n\nb-NH3 hits\nb-ion hit with a NH3 loss\n\n\nhits_b\nnumber of b ion hits\n\n\ncharge\ncharge of the peptide\n\n\ndb_idx\nindex to the theoretical database\n\n\ndecoy\nis the sequence a decoy or a hit (Yes / No)\n\n\ndecoys_cum\ncumulative number of decoys in table (used for FDR calculation)\n\n\ndelta_m\nmean mass delta when comparing experimental fragments to theoretical fragments when searching\n\n\ndelta_m_ppm\ndelta_m in ppm\n\n\ndist\na metric used to measure the distance of an MS1 feature (quantification) to a matching MS2 spectrum (identification). This is important in the mapping of MS1 features to MS2 spectra\n\n\nfasta_index\nindex to the fasta file that you use for searching\n\n\nfdr\ncalculated false discovery rate value for this peptide in the table. As the PSM score decreases more decoys will be found the FDR score increases until you reach your FDR threshold cutoff, below which we don’t count any more hits.\n\n\nfeature_idx\nindex to feature table from feature finding\n\n\nfeature_rank\nmultiple ms1 features will be mapped to a single ms2 spectra. The score_rank indicates the how close the feature was to the spectrum in comparison to other features in close distance.\n\n\nfwhm\nfwhm of the feature\n\n\nhits\ntotal number of b- and y-ion hits. A hit occurs when a theoretical fragment can be found within the frag_tol of an experimentally recorded fragment\n\n\nms1_int_apex\nintensity at feature apex\n\n\nfragments_int_ratio\nmean intensity ratio: experimental fragment intensity divided by theoretical intensity (if no db intensity is available db intensity is set to 1) for each matched ion\n\n\nms1_int_sum\nsummed intensity of the MS1-feature\n\n\nfragment_ion_idx\nindex to ion dataframe for this PSM\n\n\nfragment_ion_int\nintensity of each matched ion\n\n\nfragment_ion_type\ntype of the matched ion\n\n\nmass\nmass\n\n\nfragments_matched_int_sum\nsum of the intensity of fragments found in the PSM\n\n\nfragments_matched_int_ratio\nratio of the fragments_matched_int_sum to the total intensity in a spectrum\n\n\nfragments_matched_n_ratio\nnumber of ions that were found (hits) divided by the number of AAs in the matched sequence\n\n\nmz\nmz\n\n\nn_AA\nnumber of AAs in the sequence\n\n\nn_internal\nnumber of internal cleavage sites\n\n\nn_fragments_matched\nnumber of matched ions\n\n\nn_missed\nnumber of missed cleavages\n\n\nsequence_naked\nnaked sequence (i.e. withouth modifications)\n\n\nprec_offset\nmass offset from theoretical precursor mass to actual precursor mass\n\n\nprec_offset_ppm\nprec_offset in ppm\n\n\nprec_offset_raw_ppm\nprec_offset_raw in ppm\n\n\nprec_offset_raw\nmass offset from theoretical precursor mass to actual precursor mass (before recalibration)\n\n\nprecursor\npeptide + charge\n\n\nq_value\nq_value for FDR estiamtion\n\n\nquery_idx\nindex to query (for the spectrum index use raw_idx)\n\n\nscore_rank\none query can have multiple matching PSMs. This is the score_rank of the PSM to the query with respect to the score. See also raw_rank\n\n\nprecursor_rank\nif a precursor is measured multiple times, this is the score_rank with respect to the score\n\n\nraw_idx\nindex to the experimental spectrum\n\n\nraw_rank\none spectrum can have multiple matching PSMs. This is the score_rank of the PSM to the spectrum with respect to the score.\n\n\nrt\nretention time of the feature\n\n\nrt_apex\nretention time of the feature (at maximum intensity)\n\n\nrt_end\nend timepoint of feature\n\n\nrt_start\nstart timepoint of feature\n\n\nscan_no\nscan number\n\n\nscore\nscore used for FDR estiamtion\n\n\nsequence\nmatched sequence\n\n\ntarget\nboolean flag if sequence is target or decoy\n\n\ntarget_cum\ncumulative number of targets (used for FDR estiamtion)\n\n\nfragments_int_sum\nsummed intensity of all fragments in experimental spectrum\n\n\nx_tandem\nx_tandem score\n\n\ny-H2O_hits\nnumber of y-ion hits with water loss\n\n\ny-NH3_hits\nnumber of y-ion hits with ammonia loss\n\n\nhits_y\nnumber of y-ion hits\n\n\nfilename\nfull filename of raw data\n\n\nshortname\nshort filename of raw data (w/o folder path)\n\n\nprotein\nidentified protein\n\n\nprotein_group\nidentified protein_group\n\n\nrazor\nflag if peptide was used for protein grouping with razor approach\n\n\nprotein_idx\nindex to protein (with respect to FASTA)\n\n\ndecoy_protein\nflag if protein is decoy or not\n\n\nn_possible_proteins\nnumber of possible proteins for this peptide\n\n\nscore_protein_group\nscore of the protein group"
  },
  {
    "objectID": "file_formats.html#downstream-analysis",
    "href": "file_formats.html#downstream-analysis",
    "title": "AlphaPept workflow and files",
    "section": "Downstream analysis",
    "text": "Downstream analysis\nAlphaPept offers some basic plots in the results section (e.g., volcano, heatmap, and PCA). The *.csv-format should be generic to use with multiple other tools. Feel free to reach out in case you have ideas for plots or find that the output format not supported or has required columns missing. To reach out, report an issue here or send an email to opensource@alphapept.com.\n\nUsing with Perseus\nPerseus offers a generic table import, so you can directly use the results_proteins.csv.\n\nExample: Volcano-Plot\nAn excellent tutorial for creating volcano-plots with Perseus can be found here.\nBelow a quickstart to use AlphaPept with Perseus (tested with 1.6.15.0) The file used here is PXD006109 from the test runner (multi-species quantification test) with six files (three each group).\n\nOpen Perseus. \nDrag and drop the results_proteins.csv in the central pane of Perseus. The Generic matrix upload-window will open. \nSelect the appropriate columns (e.g., LFQ for LFQ-intensities) and select them for Main with the >-Button. The first row is empty. Assign this for text. Click OK, and the table should be loaded. \nClick on the f(x)-button and press OK on the window that opens to apply a log2(x)-transformation. \nClick on Annot. rows > Categorical annotation rows to assign a group for each file. Select multiple entries and click on the checkmark to assign multiple groups at the same time. Click OK to close the window. \nClick on the Volcano plot-symbol in the upper right Analysis-column. For the tutorial, we keep the standard settings and press OK.\nYou can double-click on the small volcano plot to show the plot. \n\nEnjoy your volcano-plot."
  },
  {
    "objectID": "chem.html",
    "href": "chem.html",
    "title": "Chem",
    "section": "",
    "text": "This notebook contains all chemistry-related functionality. Here, a major part is functionality to generate isotope patterns and the averagine model. This is required for feature finding, where an isotope pattern is validated by being compared to its averagine model. We use the data structure Isotopes from constants to handle Isotopes. Next, we define the class IsotopeDistribution to calculate isotope distributions for a given Isotope. The core of the calculation is the function fast_add, which allows the fast estimation of isotope distributions.  To calculate the isotope distribution for the averagine model, we define the function get_average_formula, which calculates the amino acid composition of the averagine molecule for the given mass."
  },
  {
    "objectID": "chem.html#isotopedistributions",
    "href": "chem.html#isotopedistributions",
    "title": "Chem",
    "section": "IsotopeDistributions",
    "text": "IsotopeDistributions\nThe calculation of isotope distributions is based on the algorithm introduced in the paper Calculation of isotope distributions in mass spectrometry. A trivial solution for a non-trivial problem by Hugo Kubinyi. A more detailed description of the algorithm can be found in the paper.\nThe implementation is a Python port of the Java version of the proteomicore - implementation from Johan Teleman\nIn brief, the approach avoids expanding polynomial expressions by combining precalculated patterns of hypothetical atom clusters and pruning low-intensity peaks.  To calculate the isotope distribution for a given isotope, we define the function dict_to_dist, which accepts a dictionary of amino acids and returns the isotope distribution.\n\nsource\n\ndict_to_dist\n\n dict_to_dist (counted_AA:numba.typed.typeddict.Dict,\n               isotopes:numba.typed.typeddict.Dict)\n\nFunction to convert a dictionary with counts of atoms to an isotope distribution.\nArgs: counted_AA (Dict): Numba-typed dict with counts of atoms. isotopes (Dict): Numba-typed lookup dict with isotopes.\nReturns: IsotopeDistribution: The calculated isotope distribution for the chemical compound.\n\nsource\n\n\nnumba_bin\n\n numba_bin (decimal:int)\n\nNumba compatible function to convert a decimal number to a binary (list).\nArgs: decimal (int): Decimal number.\nReturns: list: Number in binary.\n\nsource\n\n\nfast_add\n\n fast_add (m0:int, dm0:int, int0:numpy.ndarray, m1:int, dm1:int,\n           int1:numpy.ndarray, prune_level:float=1e-06)\n\nHelper function to quickly add isotope patterns.\nArgs: m0 (float): the mono-isotopic mass of the first isotope distribution. dm0 (int): the number of isotope intensities in the first isotope distribution. int0 (np.ndarray:float): the intensity pattern of the first isotope distribution. m1 (float): the mono-isotopic mass of the second isotope distribution. dm1 (int): the number of isotope intensities in the second isotope distribution. int1 (np.ndarray:float): the intensity pattern of the second isotope distribution. prune_level (float): Precision threshold. Defaults to 0.000001.\nReturns: int: Mass of new pattern. int: Number of isotopes in new pattern. np.ndarray: Intensity of new pattern.\n\n\n\nIsotopeDistribution\n\n IsotopeDistribution (*args, **kwargs)\n\nClass to represent isotope distributions.\nAttributes: m0 (int): the mono-isotopic mass. dm (int): number of isotopes. intensities (np.ndarray): isotope intensities.\nMethods: add: add another isotope distribution. copy: create a copy of the current isotope distribution. mult: multiply the current isotope distribution.\nOne can compare the implementation to the algorithm with plotting values from the paper against the own results. Example \\(K_{23}I_{22}\\) is Table 2 from the paper.\n\nimport matplotlib.pyplot as plt\n \nfrom alphapept.constants import isotopes\nimport numpy as np\nfrom numba.typed import Dict\nfrom numba import types\n\n# Example from paper:\nmass = 3688.1\nabundances = np.array([58.83,0.18,100,0.29,81.29,0.22,42.05,0.11,15.54,0.04,4.36,0.01,0.97,0.17,0.03])/100\nmasses = [mass + i for i in range(len(abundances))]\n\n# Calculation with own function\ncounted_AA = Dict.empty(key_type=types.unicode_type, value_type=types.int64)\ncounted_AA[\"K\"] = 23\ncounted_AA[\"I\"] = 22\ndist = dict_to_dist(counted_AA, isotopes)\nmasses_ = [dist.m0 + i for i in range(len(dist.intensities))]\n\n# Plot \nmasses = [mass + i for i in range(len(abundances))]\nplt.bar(masses, abundances*100, alpha=0.5, label='Paper')\nplt.bar(masses_, dist.intensities*100, alpha=0.5, label='Implementation')\nplt.xlabel('Mass')\nplt.ylabel('Relative Intensity (%)')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "chem.html#averagine",
    "href": "chem.html#averagine",
    "title": "Chem",
    "section": "Averagine",
    "text": "Averagine\nThe averagine model is based on Senko et al.. We define the function get_average_formula to calculate a dictionary with averagine masses.\n\nsource\n\nget_average_formula\n\n get_average_formula (molecule_mass:float,\n                      averagine_aa:numba.typed.typeddict.Dict,\n                      isotopes:numba.typed.typeddict.Dict,\n                      sulphur:bool=True)\n\nFunction to calculate the averagine formula for a molecule mass.\nArgs: molecule_mass (float): Input molecule mass for which the averagine model should be calculated. averagine_aa (Dict): Numba-typed dictionary with averagine masses. See alphapept.constants.averagine_aa. isotopes (Dict): Numba-typed lookup dict with isotopes. sulphur (bool, optional): Flag to consider sulphur. Defaults to True.\nRaises: NotImplementedError: If mode w/o sulphur is selected.\nReturns: Dict: Numba-typed dict with averagine composition.\n\nfrom alphapept.constants import averagine_aa, averagine_avg\n\n\nmolecule_mass = 300\nprint(get_average_formula(molecule_mass, averagine_aa, isotopes, sulphur=True))\n\n{C: 13, H: 24, N: 4, O: 4, S: 0}\n\n\nTo directly calculate the isotope distribution of a molecule mass based on the averagine model, we define the wrapper function mass_to_dist.\n\nsource\n\n\nmass_to_dist\n\n mass_to_dist (molecule_mass:float,\n               averagine_aa:numba.typed.typeddict.Dict,\n               isotopes:numba.typed.typeddict.Dict)\n\nFunction to calculate an isotope distribution from a molecule mass using the averagine model.\nArgs: molecule_mass (float, averagine_aa): input molecule mass. averagine_aa (Dict): Numba-typed dictionary with averagine masses. isotopes (Dict): Numba-typed lookup dict with isotopes.\nReturns: np.ndarray: isotope masses. np.ndarray: isotope intensity.\n\nprint(mass_to_dist(300, averagine_aa, isotopes))\n\n(array([300.18252563, 301.18539427, 302.18826291, 303.19113155,\n       304.19400019, 305.19686883]), array([1.00000000e+00, 1.59501684e-01, 2.01508986e-02, 1.86370995e-03,\n       1.39934810e-04, 8.55900106e-06]))\n\n\nWe can use this to plot isotope distributions for a given mass.\n\ndef plot_averagine(mass, averagine_aa, isotopes):\n\n    masses, intensity = mass_to_dist(mass, averagine_aa, isotopes)\n    plt.title(\"Averagine Isotopic Distribution for Mass {}\".format(mass))\n    plt.stem(masses, intensity*100, use_line_collection=True)\n    plt.ylabel(\"Relative Intensity (%)\")\n    plt.xlabel(\"Molecular Weight\")\n    plt.show()\n\nplot_averagine(500, averagine_aa, isotopes)\nplot_averagine(1000, averagine_aa, isotopes)"
  },
  {
    "objectID": "chem.html#mass-calculations",
    "href": "chem.html#mass-calculations",
    "title": "Chem",
    "section": "Mass Calculations",
    "text": "Mass Calculations\ncalculate_mass: This function allows to calculate the precursor mass from the monoisotopic m/z and the charge.\n\nsource\n\ncalculate_mass\n\n calculate_mass (mono_mz:float, charge:int)\n\nCalculate the precursor mass from mono mz and charge.\nArgs: mono_mz (float): mono m/z. charge (int): charge.\nReturns: float: precursor mass.\n\nmz = 300\ncharge = 2\n\nprint(calculate_mass(mz, charge))\n\n597.98544706626"
  },
  {
    "objectID": "export.html",
    "href": "export.html",
    "title": "Export",
    "section": "",
    "text": "Read MaxQuant folder and return the files."
  },
  {
    "objectID": "export.html#sequence-notation",
    "href": "export.html#sequence-notation",
    "title": "Export",
    "section": "Sequence Notation",
    "text": "Sequence Notation\nDictionary to be able to convert AlphaPept sequence notaiton to MaxQuant\n\nsource\n\nap_to_mq_sequence\n\n ap_to_mq_sequence (sequence, mod_translation)\n\nConverts AlphaPept sequence format to MaxQuant Format returns sequence_naked, len_sequence, modifications_, mq_sequence\n\nsource\n\n\nremove_mods\n\n remove_mods (sequence)"
  },
  {
    "objectID": "export.html#evidence.txt",
    "href": "export.html#evidence.txt",
    "title": "Export",
    "section": "evidence.txt",
    "text": "evidence.txt\nApparently, the columns for evidence.txt are dynamic. As an example, when including Oxidation of M as modification, the following columns will be added:\n'Oxidation (M)',\n'Oxidation (M) Probabilities',\n'Oxidation (M) Score Diffs',\n'Oxidation (M) site IDs'\nExample code to load and print the columns\nimport pandas as pd\nevidence = pd.read_csv('F:/TESTDATA/DEFAULT/THERMO_IRT_MANY_MOD/combined/txt/evidence.txt', sep = '\\t')\nfor _ in evidence.columns:\n    print(f\"mq_dict_evidence['{_}'] =\")\n\nsource\n\nprepare_ap_results\n\n prepare_ap_results (ref_ap)\n\n\nmq_dict_evidence = {}\n\nmq_dict_evidence['Sequence'] = 'sequence_naked'\nmq_dict_evidence['Length'] = 'n_AA'\nmq_dict_evidence['Modifications'] = 'mq_modifications'\nmq_dict_evidence['Modified sequence'] = 'mq_sequence'\nmq_dict_evidence['Missed cleavages'] = 'n_missed'\nmq_dict_evidence['Proteins'] = 'protein_group'#it is not entirely clear what the difference between Leading Proteins and Proteins is\nmq_dict_evidence['Leading proteins'] = 'protein_group'\nmq_dict_evidence['Leading razor protein'] = 'protein'\nmq_dict_evidence['Type'] = 'type'\nmq_dict_evidence['Raw file'] = 'mq_rawfile'\nmq_dict_evidence['MS/MS m/z'] = 'undefined'\nmq_dict_evidence['Charge'] = 'charge'\nmq_dict_evidence['m/z'] = 'mz'\nmq_dict_evidence['Mass'] = 'mass'\nmq_dict_evidence['Uncalibrated - Calibrated m/z [ppm]'] = 'undefined'\nmq_dict_evidence['Uncalibrated - Calibrated m/z [Da]'] = 'undefined'\nmq_dict_evidence['Mass error [ppm]'] = 'undefined'\nmq_dict_evidence['Mass error [Da]'] = 'undefined'\nmq_dict_evidence['Uncalibrated mass error [ppm]'] = 'undefined'\nmq_dict_evidence['Uncalibrated mass error [Da]'] = 'undefined'\nmq_dict_evidence['Max intensity m/z 0'] = 'undefined'\nmq_dict_evidence['Retention time'] = 'rt'\nmq_dict_evidence['Retention length'] = 'undefined'\nmq_dict_evidence['Calibrated retention time'] = 'undefined'\nmq_dict_evidence['Calibrated retention time start'] = 'undefined'\nmq_dict_evidence['Calibrated retention time finish'] = 'undefined'\nmq_dict_evidence['Retention time calibration'] = 'undefined'\nmq_dict_evidence['Match time difference'] = 'undefined'\nmq_dict_evidence['Match m/z difference'] = 'undefined'\nmq_dict_evidence['Match q-value'] = 'undefined'\nmq_dict_evidence['Match score'] = 'undefined'\nmq_dict_evidence['Number of data points'] = 'undefined'\nmq_dict_evidence['Number of scans'] = 'undefined'\nmq_dict_evidence['Number of isotopic peaks'] = 'n_isotopes'\nmq_dict_evidence['PIF'] = 'undefined'\nmq_dict_evidence['Fraction of total spectrum'] = 'fragments_matched_int_ratio'\nmq_dict_evidence['Base peak fraction'] = 'undefined'\nmq_dict_evidence['PEP'] = 'undefined'\nmq_dict_evidence['MS/MS count'] = 'undefined'\nmq_dict_evidence['MS/MS scan number'] = 'undefined'\nmq_dict_evidence['Score'] = 'score'\nmq_dict_evidence['Delta score'] = 'undefined'\nmq_dict_evidence['Combinatorics'] = 'undefined'\nmq_dict_evidence['Intensity'] = 'ms1_int_sum'\nmq_dict_evidence['Reverse'] = 'reverse'\nmq_dict_evidence['Potential contaminant'] = 'contaminant'\nmq_dict_evidence['id'] = 'id'\nmq_dict_evidence['Protein group IDs'] = 'undefined'\nmq_dict_evidence['Peptide ID'] = 'undefined'\nmq_dict_evidence['Mod. peptide ID'] = 'undefined'\nmq_dict_evidence['MS/MS IDs'] = 'undefined'\nmq_dict_evidence['Best MS/MS'] = 'undefined'\nmq_dict_evidence['Taxonomy IDs'] = 'undefined'\n\nimport pandas as pd\n\nref_ap = pd.read_csv('E:/test_temp/results.csv')\nref_ap = prepare_ap_results(ref_ap)\nmq_evidence = pd.DataFrame.from_dict({k: ref_ap[mq_dict_evidence[k]] for k in mq_dict_evidence.keys()})\nmq_evidence.to_csv('mq_evidence.txt', sep = '\\t', index=None)"
  },
  {
    "objectID": "export.html#proteingroups",
    "href": "export.html#proteingroups",
    "title": "Export",
    "section": "ProteinGroups",
    "text": "ProteinGroups\n#export \nmq_dict_proteinGroups = {}\n\nmq_dict_proteinGroups['Protein IDs'] =\nmq_dict_proteinGroups['Majority protein IDs'] =\nmq_dict_proteinGroups['Peptide counts (all)'] =\nmq_dict_proteinGroups['Peptide counts (razor+unique)'] =\nmq_dict_proteinGroups['Peptide counts (unique)'] =\nmq_dict_proteinGroups['Fasta headers'] =\nmq_dict_proteinGroups['Number of proteins'] =\nmq_dict_proteinGroups['Peptides'] =\nmq_dict_proteinGroups['Razor + unique peptides'] =\nmq_dict_proteinGroups['Unique peptides'] =\nmq_dict_proteinGroups['Sequence coverage [%]'] =\nmq_dict_proteinGroups['Unique + razor sequence coverage [%]'] =\nmq_dict_proteinGroups['Unique sequence coverage [%]'] =\nmq_dict_proteinGroups['Mol. weight [kDa]'] =\nmq_dict_proteinGroups['Sequence length'] =\nmq_dict_proteinGroups['Sequence lengths'] =\nmq_dict_proteinGroups['Q-value'] =\nmq_dict_proteinGroups['Score'] =\nmq_dict_proteinGroups['Intensity'] =\nmq_dict_proteinGroups['MS/MS count'] =\nmq_dict_proteinGroups['Only identified by site'] =\nmq_dict_proteinGroups['Reverse'] =\nmq_dict_proteinGroups['Potential contaminant'] =\nmq_dict_proteinGroups['id'] =\nmq_dict_proteinGroups['Peptide IDs'] =\nmq_dict_proteinGroups['Peptide is razor'] =\nmq_dict_proteinGroups['Mod. peptide IDs'] =\nmq_dict_proteinGroups['Evidence IDs'] =\nmq_dict_proteinGroups['MS/MS IDs'] =\nmq_dict_proteinGroups['Best MS/MS'] =\nmq_dict_proteinGroups['Oxidation (M) site IDs'] =\nmq_dict_proteinGroups['Oxidation (M) site positions'] =\nmq_dict_proteinGroups['Taxonomy IDs'] ="
  },
  {
    "objectID": "feature_finding.html",
    "href": "feature_finding.html",
    "title": "Feature Finding",
    "section": "",
    "text": "This part describes the implementation of the feature-finding algorithm. The core of the algorithm is described in the MaxQuant-Paper. The supplementary material explains the underlying methodology in great detail and is the foundation of the theoretical background that is described here. A refined version of the algorithm was presented with Dinosaur, which was also used as a reference for the Python implementation.\nFor the algorithm, we need serval modules:"
  },
  {
    "objectID": "feature_finding.html#loading-data",
    "href": "feature_finding.html#loading-data",
    "title": "Feature Finding",
    "section": "Loading Data",
    "text": "Loading Data\nFrom the IO library, we already have an *.ms_data.hdf container that contains centroided data. To use it in feature finding, we directly load the data."
  },
  {
    "objectID": "feature_finding.html#connecting-centroids-to-hills",
    "href": "feature_finding.html#connecting-centroids-to-hills",
    "title": "Feature Finding",
    "section": "Connecting Centroids to Hills",
    "text": "Connecting Centroids to Hills\n\n\n\n\n\n\nNote\n\n\n\nFeature finding relies heavily on the performance function decorator from the performance notebook: @alphapept.performance.performance_function. Part of this is that the functions will not have return values to be GPU compatible. Please check out this notebook for further information.\n\n\n\nConnecting centroids\nFeature finding starts with connecting centroids. For this we look at subsequent scans and compare peaks that are withing a defined mass tolerance (centroid_tol).\nImagine you have three scans with the following centroids:\n\nScan 0: 10, 20, 30\nScan 1: 10.2, 40.1\nScan 2: 40, 50, 60\n\nWhen comparing consecutive scans and defining the maximum delta mass to be 0.5 find the following connections: (Scan No, Centroid No) -> (Scan No, Centroid No). As we cannot easily store tuples in the matrix, we convert tuple containing the position of the connected centroid to an integer. * (0,0) -> (1,0) -> (3): 10 & 10.2 -> delta = 0.2 * (1,1) -> (2,0) -> (6): 40.1 & 40 -> delta = 0.1\nFinally, we store this in the results matrix:\n\\(\\begin{bmatrix} 3 & -1 & -1 \\\\ -1 & 6 & -1\\\\ -1 & -1 & -1 \\end{bmatrix}\\)\nThe coressponding scores matrix will look as follows:\n\\(\\begin{bmatrix} 0.2 & -1 & -1 \\\\ -1 & 0.1 & -1\\\\ -1 & -1 & -1 \\end{bmatrix}\\)\nThis allows us to not only easily store connections between centroids but also perform a quick lookup for the delta of an existing connection. Note that it also only stores the best connection for each centroid. To extract the connected centroids, we can use np.where(results >= 0). This implementation allows getting millions of connections within seconds.\nAs we are also allowing gaps, refering to that we can have connections between Scan 0 and Scan 2, we make the aforementioned matrix multdimensional, so that e.g. a first matrix stores the conncetions for no gap, the second matrix the connections with a gap of 1.\nThe functionality for this step is implemented in connect_centroids_unidirection and the wrapper find_centroid_connections.\n\nsource\n\n\nfind_centroid_connections\n\n find_centroid_connections (rowwise_peaks:numpy.ndarray,\n                            row_borders:numpy.ndarray,\n                            centroids:numpy.ndarray, max_gap:int,\n                            centroid_tol:float)\n\nWrapper function to call connect_centroids_unidirection\nArgs: rowwise_peaks (np.ndarray): Length of centroids with respect to the row borders. row_borders (np.ndarray): Row borders of the centroids array. centroids (np.ndarray): Array containing the centroids data. max_gap (int): Maximum gap when connecting centroids. centroid_tol (float): Centroid tolerance.\n\nsource\n\n\nconnect_centroids_unidirection\n\n connect_centroids_unidirection (x:numpy.ndarray,\n                                 row_borders:numpy.ndarray,\n                                 connections:numpy.ndarray,\n                                 scores:numpy.ndarray,\n                                 centroids:numpy.ndarray, max_gap:int,\n                                 centroid_tol:float)\n\nConnect centroids.\nArgs: x (np.ndarray): Index to datapoint. Note that this using the performance_function, so one passes an ndarray. row_borders (np.ndarray): Row borders of the centroids array. connections (np.ndarray): Connections matrix to store the connections scores (np.ndarray): Score matrix to store the connections centroids (np.ndarray): 1D Array containing the masses of the centroids data. max_gap (int): Maximum gap when connecting centroids. centroid_tol (float): Centroid tolerance.\nWe wrap the centroid connections in the function connect_centroids. This function converts the connections into an usable array.\n\nsource\n\n\nconnect_centroids\n\n connect_centroids (rowwise_peaks:numpy.ndarray,\n                    row_borders:numpy.ndarray, centroids:numpy.ndarray,\n                    max_gap:int, centroid_tol:float)\n\nFunction to connect centroids.\nArgs: rowwise_peaks (np.ndarray): Indexes for centroids. row_borders (np.ndarray): Row borders (for indexing). centroids (np.ndarray): Centroid data. max_gap: Maximum gap. centroid_tol: Centroid tol for matching centroids. Returns: np.ndarray: From index. np.ndarray: To index. float: Median score. float: Std deviation of the score.\n\nsource\n\n\neliminate_overarching_vertex\n\n eliminate_overarching_vertex (x:numpy.ndarray, from_idx:numpy.ndarray,\n                               to_idx:numpy.ndarray)\n\nEliminate overacrhing vertex.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. from_idx (np.ndarray): From index. to_idx (np.ndarray): To index.\n\nsource\n\n\nconvert_connections_to_array\n\n convert_connections_to_array (x:numpy.ndarray, from_r:numpy.ndarray,\n                               from_c:numpy.ndarray, to_r:numpy.ndarray,\n                               to_c:numpy.ndarray,\n                               row_borders:numpy.ndarray,\n                               out_from_idx:numpy.ndarray,\n                               out_to_idx:numpy.ndarray)\n\nConvert integer indices of a matrix to coordinates.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. from_r (np.ndarray): From array with row coordinates. from_c (np.ndarray): From array with column coordinates. to_r (np.ndarray): To array with row coordinates. to_c (np.ndarray): To array with column coordinates. row_borders (np.ndarray): Row borders (for indexing). out_from_idx (np.ndarray): Reporting array: 1D index from. out_to_idx (np.ndarray): Reporting array: 1D index to.\n\n#Sample snippet to show centroid conncetions\n\nimport matplotlib.pyplot as plt\n\nrow_borders = np.array([3, 6, 9])\nrowwise_peaks = np.array([3, 3, 3])\nmax_gap = 2\n\nscore = np.full((3,3, max_gap), np.inf)\nconnections = np.full((3,3, max_gap), -1)\n\ncentroids = np.array([10, 20, 30, 10.2, 20, 10, 30, 40])\ncentroid_tol = 0.5*1e5\n\nfrom_idx, to_idx, score_median, score_std = connect_centroids(rowwise_peaks, row_borders, centroids, max_gap, centroid_tol)\n\nscan_no = np.array([0, 0, 0, 1, 1, 2, 2, 2])\n\nplt.figure(figsize=(5,5))\nfor i, _ in enumerate(row_borders):\n    ctrd = centroids[_-rowwise_peaks[i]:_]\n    plt.plot(ctrd, np.ones_like(ctrd)*i, 'o')\n    \nfor i, _ in enumerate(from_idx):\n    from_ = _\n    to_ = to_idx[i]\n    plt.plot([centroids[from_], centroids[to_]], [scan_no[from_], scan_no[to_]], 'k:')\n    \nplt.ylabel('scan')\nplt.xlabel('m/z')\nplt.ylim(len(row_borders)+0.5, -1.5)\nplt.title('Peak connections')\nplt.show()\n\n\n\n\n\n\nExtracting hills.\nTo extract hills we extract connected components from the connections.\n\nsource\n\n\nremove_duplicate_hills\n\n remove_duplicate_hills (hill_ptrs, hill_data, path_node_cnt)\n\nRemoves hills that share datapoints. Starts from the largest hills.\n\nsource\n\n\nextract_hills\n\n extract_hills (query_data:dict, max_gap:int, centroid_tol:float)\n\n[summary]\nArgs: query_data (dict): Data structure containing the query data. max_gap (int): Maximum gap when connecting centroids. centroid_tol (float): Centroid tolerance.\nReturns: hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. path_node_cnt (int): Number of elements in this path. score_median (float): Median score. score_std (float): Std deviation of the score.\n\nsource\n\n\nget_hills\n\n get_hills (centroids:numpy.ndarray, from_idx:numpy.ndarray,\n            to_idx:numpy.ndarray, hill_length_min:int=3)\n\nFunction to get hills from centroid connections.\nArgs: centroids (np.ndarray): 1D Array containing the masses of the centroids. from_idx (np.ndarray): From index. to_idx (np.ndarray): To index. hill_length_min (int): Minimum hill length:\nReturns: hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. path_node_cnt (int): Number of elements in this path.\n\nsource\n\n\nfill_path_matrix\n\n fill_path_matrix (x:numpy.ndarray, path_start:numpy.ndarray,\n                   forwards:numpy.ndarray, out_hill_data:numpy.ndarray,\n                   out_hill_ptr:numpy.ndarray)\n\nFunction to fill the path matrix.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. path_starts (np.ndarray): Array that stores the starts of the paths. forwards (np.ndarray): Forward array. out_hill_data (np.ndarray): Array containing the indices to hills. out_hill_ptr (np.ndarray): Array containing the bounds to out_hill_data.\n\nsource\n\n\nfind_path_length\n\n find_path_length (x:numpy.ndarray, path_starts:numpy.ndarray,\n                   forward:numpy.ndarray, path_cnt:numpy.ndarray)\n\nFunction to extract the length of a path.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. path_starts (np.ndarray): Array that stores the starts of the paths. forward (np.ndarray): Array that stores forward information. path_cnt (np.ndarray): Reporting array to count the paths.\n\nsource\n\n\nfind_path_start\n\n find_path_start (x:numpy.ndarray, forward:numpy.ndarray,\n                  backward:numpy.ndarray, path_starts:numpy.ndarray)\n\nFunction to find the start of a path.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. forward (np.ndarray): Array to report forward connection. backward (np.ndarray): Array to report backward connection. path_starts (np.ndarray): Array to report path starts.\n\nsource\n\n\npath_finder\n\n path_finder (x:numpy.ndarray, from_idx:numpy.ndarray,\n              to_idx:numpy.ndarray, forward:numpy.ndarray,\n              backward:numpy.ndarray)\n\nExtracts path information and writes to path matrix.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. from_idx (np.ndarray): Array containing from indices. to_idx (np.ndarray): Array containing to indices. forward (np.ndarray): Array to report forward connection. backward (np.ndarray): Array to report backward connection.\n\n\nHill Splitting\nWhen having a hill with two or more maxima, we would like to split it at the minimum position. For this, we use a recursive approach. First, the minimum of a hill is detected. A hill is split at this minimum if the smaller of the surrounding maxima is at least the factor hill_split_level larger than the minimum. For each split, the process is repeated.\n\nsource\n\n\nfast_minima\n\n fast_minima (y:numpy.ndarray)\n\nFunction to calculate the local minimas of an array.\nArgs: y (np.ndarray): Input array.\nReturns: np.ndarray: Array containing minima positions.\n\nsource\n\n\nsplit_hills\n\n split_hills (hill_ptrs:numpy.ndarray, hill_data:numpy.ndarray,\n              int_data:numpy.ndarray, hill_split_level:float, window:int)\n\nWrapper function to split hills\nArgs: hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. hill_split_level (float): Split level for hills. window (int): Smoothing window.\nReturns: np.ndarray: Array containing the bounds to the hill_data with splits.\n\nsource\n\n\nsplit\n\n split (k:numpy.ndarray, hill_ptrs:numpy.ndarray, int_data:numpy.ndarray,\n        hill_data:numpy.ndarray, splits:numpy.ndarray,\n        hill_split_level:float, window:int)\n\nFunction to split hills.\nArgs: k (np.ndarray): Input index. Note that we are using the performance function so this is a range. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. int_data (np.ndarray): Array containing the intensity to each centroid. hill_data (np.ndarray): Array containing the indices to hills. splits (np.ndarray): Array containing splits. hill_split_level (float): Split level for hills. window (int): Smoothing window.\n\n\nFilter Hills\nTo filter hills, we define a minimum length hill_min_length. All peaks below the threshold hill_peak_min_length are accepted as is. For longer hills, the intensity at the start and the end are compared to the maximum intensity. If the ratio of the maximum raw intensity to the smoothed intensity and the beginning and end are larger than hill_peak_factor the hills are accepted.\n\nsource\n\n\nfilter_hills\n\n filter_hills (hill_data:numpy.ndarray, hill_ptrs:numpy.ndarray,\n               int_data:numpy.ndarray, hill_check_large:int=40,\n               window:int=1)\n\nFilters large hills.\nArgs: hill_data (np.ndarray): Array containing the indices to hills. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. int_data (np.ndarray): Array containing the intensity to each centroid. hill_check_large (int, optional): Length criterion when a hill is considered large.. Defaults to 40. window (int, optional): Smoothing window. Defaults to 1.\nReturns: np.ndarray: Filtered hill data. np.ndarray: Filtered hill points.\n\nsource\n\n\ncheck_large_hills\n\n check_large_hills (idx:numpy.ndarray, large_peaks:numpy.ndarray,\n                    hill_ptrs:numpy.ndarray, hill_data:numpy.ndarray,\n                    int_data:numpy.ndarray, to_remove:numpy.ndarray,\n                    large_peak:int=40, hill_peak_factor:float=2,\n                    window:int=1)\n\nFunction to check large hills and flag them for removal.\nArgs: idx (np.ndarray): Input index. Note that we are using the performance function so this is a range. large_peaks (np.ndarray): Array containing large peaks. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. to_remove (np.ndarray): Array with indexes to remove. large_peak (int, optional): Length criterion when a peak is large. Defaults to 40. hill_peak_factor (float, optional): Hill maximum criterion. Defaults to 2. window (int, optional): Smoothing window.. Defaults to 1.\nSince the mass estimate min the equation above is more complicated than just an average of the mj, a standard deviation based estimate of the error would not be appropriate. Therefore we calculate the error as a bootstrap2 estimate over B=150 bootstrap replications"
  },
  {
    "objectID": "feature_finding.html#calculating-hill-statistics",
    "href": "feature_finding.html#calculating-hill-statistics",
    "title": "Feature Finding",
    "section": "Calculating Hill Statistics",
    "text": "Calculating Hill Statistics\nNext, we calculate summary statistics for the connected centroids. We can obtain a high precision mass estimate for each hill by taking the average of the the masses and weighting this by their intensiteis:\n\\[\n\\overline{m} = \\frac{\\sum_{j=1}^nm_jI_j}{\\sum_{j=1}^nI_j}\n\\]\nTo estimate the mass error, we calculate the error as a boostrap estimate:\n\\[\\Delta \\overline{m} = \\sqrt{\\frac{\\sum_{b=1}^{B}(\\overline{m}_b - \\overline{m} )}{(B-1)}}\\]\nThe calculation of hill statistics for a single hill is implemented in get_hill_stats. To calculate the hill stats for a list of hills, we can call the wrapper get_hill_data.\n\nsource\n\nget_hill_data\n\n get_hill_data (query_data:dict, hill_ptrs:numpy.ndarray,\n                hill_data:numpy.ndarray, hill_nboot_max:int=300,\n                hill_nboot:int=150)\n\nWrapper function to get the hill data.\nArgs: query_data (dict): Data structure containing the query data. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. hill_nboot_max (int): Maximum number of bootstrap comparisons. hill_nboot (int): Number of bootstrap comparisons\nReturns: np.ndarray: Hill stats. np.ndarray: Sortindex. np.ndarray: Upper index. np.ndarray: Scan index. np.ndarray: Hill data. np.ndarray: Hill points.\n\nsource\n\n\nremove_duplicates\n\n remove_duplicates (stats:numpy.ndarray, hill_data:numpy.ndarray,\n                    hill_ptrs:numpy.ndarray)\n\nRemove duplicate hills.\nArgs: stats (np.ndarray): Stats array that contains summary statistics of hills. hill_data (np.ndarray): Array containing the indices to hills. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data.\nReturns: np.ndarray: Filtered hill data. np.ndarray: Filtered hill points. np.ndarray: Filtered hill stats.\n\nsource\n\n\nhill_stats\n\n hill_stats (idx:numpy.ndarray, hill_range:numpy.ndarray,\n             hill_ptrs:numpy.ndarray, hill_data:numpy.ndarray,\n             int_data:numpy.ndarray, mass_data:numpy.ndarray,\n             rt_:numpy.ndarray, rt_idx:numpy.ndarray, stats:numpy.ndarray,\n             hill_nboot_max:int, hill_nboot:int)\n\nFunction to calculate hill stats.\nArgs: idx (np.ndarray): Input index. Note that we are using the performance function so this is a range. hill_range (np.ndarray): Hill range. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. mass_data (np.ndarray): Array containing mass data. rt_ (np.ndarray): Array with retention time information for each scan. rt_idx (np.ndarray): Lookup array to match centroid idx to rt. stats (np.ndarray): Stats array that contains summary statistics of hills. hill_nboot_max (int): Maximum number of bootstrap comparisons. hill_nboot (int): Number of bootstrap comparisons"
  },
  {
    "objectID": "feature_finding.html#combining-hills-to-isotope-patterns",
    "href": "feature_finding.html#combining-hills-to-isotope-patterns",
    "title": "Feature Finding",
    "section": "Combining Hills to Isotope Patterns",
    "text": "Combining Hills to Isotope Patterns\nAfter obtaining summary statistics of hills, the next step is to check whether they belong together to form an isotope pattern. For this, we check wheter it is possible that they are neighbors in an isotope pattern, e.g. one having a 12C atom that has been replaced by a 13C version. The detailed criterion for the check is implemented in check_isotope_pattern and is as follows:\n\\[\\left | \\Delta m-\\frac{\\Delta M}{z} \\right |\\leq \\sqrt{\\left ( \\frac{\\Delta S}{z}  \\right )^{2}+\\Delta {m_{1}}^{2} +\\Delta {m_{2}}^{2}}\\]\nThe left side contains \\(\\Delta m\\), being the delta of the precise mass estimates from the summary statistics and \\(\\Delta M = 1.00286864\\), which is the mass difference ebtween the 13C peak and the monoisotopic peak in an averagine molecule of 1500 Da mass divided by the charge \\(z\\).\nThe right side contains \\(\\Delta S = 0.0109135\\), which is the maximum shift that a sulphur atom can cause (\\(\\Delta S = 2m(^{13}C) - 2m(^{12}C) - m(^{34}S) + m(^{32}S)\\)) and \\(\\Delta {m_{1}}\\) and \\(\\Delta {m_{2}}\\), which are the bootstrapped mass standard deviations.\n\nsource\n\ncheck_isotope_pattern\n\n check_isotope_pattern (mass1:float, mass2:float, delta_mass1:float,\n                        delta_mass2:float, charge:int,\n                        iso_mass_range:int=5)\n\nCheck if two masses could belong to the same isotope pattern.\nArgs: mass1 (float): Mass of the first pattern. mass2 (float): Mass of the second pattern. delta_mass1 (float): Delta mass of the first pattern. delta_mass2 (float): Delta mass of the second pattern. charge (int): Charge. iso_mass_range (int, optional): Mass range. Defaults to 5.\nReturns: bool: Flag to see if pattern belongs to the same pattern.\n\ncharge = 1\n\nmass1, delta_mass1 = 100, 0.1\nmass2, delta_mass2 = 101.1, 0.05\n\nprint(check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge))\n\nmass2, delta_mass2 = 102.1, 0.05\n\nprint(check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge))\n\nTrue\nFalse\n\n\n\n\nCosine Correlation of two hills\nAn additional criterion that is being checked is that the intensity profiles have sufficient overalp in retention time. This is validated by ensuring that two hills have a cosine correlation of at least 0.6.\n\\[\\frac{\\sum_{s=s_{min}}^{s_{max}}I_sJ_s}{\\sum_{s=s_{min}}^{s_{max}}I_s^{2} \\sum_{s=s_{min}}^{s_{max}}J_s^{2}} \\geq 0.6\\]\nThe intensities of two hills are only compared if both have an intensity value in a particular scan. Otherwise, the intensity is set to zero. Additionally, an overlap of at least three elements is required.\n\nsource\n\n\ncorrelate\n\n correlate (scans_:numpy.ndarray, scans_2:numpy.ndarray,\n            int_:numpy.ndarray, int_2:numpy.ndarray)\n\nCorrelate two scans.\nArgs: scans_ (np.ndarray): Masses of the first scan. scans_2 (np.ndarray): Masses of the second scan. int_ (np.ndarray): Intensity of the first scan. int_2 (np.ndarray): Intensity of the second scan.\nReturns: float: Correlation.\n\n\nExtracting pre-Isotope Patterns\nNow having two criteria to check whether hills could, in principle, belong together, we define the wrapper functions extract_edge and get_edges to extract the connected hills. To minimize the number of comparisons we need to perform, we only compare the hills that overlap in time (i.e., the start of one hill rt_min needs to be before the end of the other hill rt_max) and are less than the sum of \\(\\Delta M\\) and \\(\\Delta S\\) apart.\nTo extract all hills that belong together, we again rely on the NetworkX-package to extract the connected components.\n\nsource\n\n\nedge_correlation\n\n edge_correlation (idx:numpy.ndarray, to_keep:numpy.ndarray,\n                   sortindex_:numpy.ndarray, pre_edges:numpy.ndarray,\n                   hill_ptrs:numpy.ndarray, hill_data:numpy.ndarray,\n                   int_data:numpy.ndarray, scan_idx:numpy.ndarray,\n                   cc_cutoff:float)\n\nCorrelates two edges and flag them it they should be kept.\nArgs: idx (np.ndarray): Input index. Note that we are using the performance function so this is a range. to_keep (np.ndarray): Array with indices which edges should be kept. sortindex_ (np.ndarray): Sortindex to access the hills from stats. pre_edges (np.ndarray): Array with pre edges. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. scan_idx (np.ndarray): Array containing the scan index for a centroid. cc_cutoff (float): Cutoff value for what is considered correlating.\n\nsource\n\n\nextract_edge\n\n extract_edge (stats:numpy.ndarray, idxs_upper:numpy.ndarray, runner:int,\n               max_index:int, maximum_offset:float, iso_charge_min:int=1,\n               iso_charge_max:int=6, iso_mass_range:int=5)\n\nExtract edges.\nArgs: stats (np.ndarray): Stats array that contains summary statistics of hills. idxs_upper (np.ndarray): Upper index for comparing. runner (int): Index. max_index (int): Unused. maximum_offset (float): Maximum offset when comparing edges. iso_charge_min (int, optional): Minimum isotope charge. Defaults to 1. iso_charge_max (int, optional): Maximum isotope charge. Defaults to 6. iso_mass_range (float, optional): Mass search range. Defaults to 5.\nReturns: list: List of edges.\n\nsource\n\n\nget_pre_isotope_patterns\n\n get_pre_isotope_patterns (stats:numpy.ndarray, idxs_upper:numpy.ndarray,\n                           sortindex_:numpy.ndarray,\n                           hill_ptrs:numpy.ndarray,\n                           hill_data:numpy.ndarray,\n                           int_data:numpy.ndarray, scan_idx:numpy.ndarray,\n                           maximum_offset:float, iso_charge_min:int=1,\n                           iso_charge_max:int=6, iso_mass_range:float=5,\n                           cc_cutoff:float=0.6)\n\nFunction to extract pre isotope patterns.\nArgs: stats (np.ndarray): Stats array that contains summary statistics of hills. idxs_upper (np.ndarray): Upper index for comparison. sortindex_ (np.ndarray): Sortindex to access the hills from stats. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. scan_idx (np.ndarray): Array containing the scan index for a centroid. maximum_offset (float): Maximum offset when matching. iso_charge_min (int, optional): Minimum isotope charge. Defaults to 1. iso_charge_max (int, optional): Maximum isotope charge. Defaults to 6. iso_mass_range (float, optional): Mass search range. Defaults to 5. cc_cutoff (float, optional): Correlation cutoff. Defaults to 0.6.\nReturns: list: List of pre isotope patterns.\n\n\nExtracting Isotope Patterns\nThe extracted pre-isotope patterns may not be consistent because their pair-wise mass differences may not correspond to the same charge. To extract isotope patterns from pre-isotope patterns, we need to ensure that they are consistent for a single charge.\nTo do this, we start with the 100 most intense peaks from a pre-isotope pattern to be used as a seed. For each seed and charge we then try to extract the longest consistent isotope pattern. To check wheter a hill is consistent with the seed we employ a modified checking criterion (check_isotope_pattern_directed) to be as follows:\n\\[\\left | m-m_j-\\frac{j\\Delta M}{z} \\right |\\leq \\sqrt{\\left ( \\frac{\\Delta S}{z}  \\right )^{2}+\\Delta {m}^{2} +\\Delta {m_{j}}^{2}}\\]\nHere \\(m\\) is the mass of a seed peak, and \\(m_{j}\\) refers to a peak relative to the seed. \\(j\\) refers to the peaks to the left or right (negative or positive index) within the pattern. \\(j\\) needs to run over consecutive values so that gaps are not allowed. Besides this consistency check, two hills are also checked to have a cosine correlation of at least 0.6.\nProgrammatically, this is implemented in grow_trail and grow. These function uses a recursive approach that adds matching hills to the seed on the left and right side until no more hills can be added.\n\nsource\n\n\nget_trails\n\n get_trails (seed:int, pattern:numpy.ndarray, stats:numpy.ndarray,\n             charge_range:numba.typed.typedlist.List,\n             iso_mass_range:float, sortindex_:numpy.ndarray,\n             hill_ptrs:numpy.ndarray, hill_data:numpy.ndarray,\n             int_data:numpy.ndarray, scan_idx:numpy.ndarray,\n             cc_cutoff:float)\n\nWrapper to extract trails for a given charge range.\nArgs: seed (int): Seed index. pattern (np.ndarray): Pre isotope pattern. stats (np.ndarray): Stats array that contains summary statistics of hills. charge_range (List): Charge range. iso_mass_range (float): Mass range for checking isotope patterns. sortindex_ (np.ndarray): Sortindex to access the hills from stats. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. scan_idx (np.ndarray): Array containing the scan index for a centroid. cc_cutoff (float): Cutoff value for what is considered correlating.\nReturns: List: Trail of consistent hills.\n\nsource\n\n\ngrow_trail\n\n grow_trail (seed:int, pattern:numpy.ndarray, stats:numpy.ndarray,\n             charge:int, iso_mass_range:float, sortindex_:numpy.ndarray,\n             hill_ptrs:numpy.ndarray, hill_data:numpy.ndarray,\n             int_data:numpy.ndarray, scan_idx:numpy.ndarray,\n             cc_cutoff:float)\n\nWrapper to grow an isotope pattern to the left and right side.\nArgs: seed (int): Seed position. pattern (np.ndarray): Isotope pattern. stats (np.ndarray): Stats array that contains summary statistics of hills. charge (int): Charge. iso_mass_range (float): Mass range for checking isotope patterns. sortindex_ (np.ndarray): Sortindex to access the hills from stats. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. scan_idx (np.ndarray): Array containing the scan index for a centroid. cc_cutoff (float): Cutoff value for what is considered correlating.\nReturns: List: Isotope pattern.\n\nsource\n\n\ngrow\n\n grow (trail:numba.typed.typedlist.List, seed:int, direction:int,\n       relative_pos:int, index:int, stats:numpy.ndarray,\n       pattern:numpy.ndarray, charge:int, iso_mass_range:float,\n       sortindex_:numpy.ndarray, hill_ptrs:numpy.ndarray,\n       hill_data:numpy.ndarray, int_data:numpy.ndarray,\n       scan_idx:numpy.ndarray, cc_cutoff:float)\n\nGrows isotope pattern based on a seed and direction.\nArgs: trail (List): List of hills belonging to a pattern. seed (int): Seed position. direction (int): Direction in which to grow the trail relative_pos (int): Relative position. index (int): Index. stats (np.ndarray): Stats array that contains summary statistics of hills. pattern (np.ndarray): Isotope pattern. charge (int): Charge. iso_mass_range (float): Mass range for checking isotope patterns. sortindex_ (np.ndarray): Sortindex to access the hills from stats. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. scan_idx (np.ndarray): Array containing the scan index for a centroid. cc_cutoff (float): Cutoff value for what is considered correlating.\nReturns: List: List of hills belonging to a pattern.\n\nsource\n\n\ncheck_isotope_pattern_directed\n\n check_isotope_pattern_directed (mass1:float, mass2:float,\n                                 delta_mass1:float, delta_mass2:float,\n                                 charge:int, index:int,\n                                 iso_mass_range:float)\n\nCheck if two masses could belong to the same isotope pattern.\nArgs: mass1 (float): Mass of the first pattern. mass2 (float): Mass of the second pattern. delta_mass1 (float): Delta mass of the first pattern. delta_mass2 (float): Delta mass of the second pattern. charge (int): Charge. index (int): Index (unused). iso_mass_range (float): Isotope mass ranges. Returns: bool: Flag if two isotope patterns belong together.\n\nsource\n\n\nplot_pattern\n\n plot_pattern (pattern:numpy.ndarray, sorted_hills:numpy.ndarray,\n               centroids:numpy.ndarray, hill_data:numpy.ndarray)\n\nHelper function to plot a pattern.\nArgs: pattern (np.ndarray): Pre isotope pattern. sorted_hills (np.ndarray): Hills, sorted. centroids (np.ndarray): 1D Array containing the masses of the centroids. hill_data (np.ndarray): Array containing the indices to hills.\n\nsource\n\n\ntruncate\n\n truncate (array:numpy.ndarray, intensity_profile:numpy.ndarray,\n           seedpos:int, iso_split_level:float)\n\nFunction to truncate an intensity profile around its seedposition.\nArgs: array (np.ndarray): Input array. intensity_profile (np.ndarray): Intensities for the input array. seedpos (int): Seedposition. iso_split_level (float): Split level.\nReturns: np.ndarray: Truncated array.\n\nsource\n\n\nis_local_minima\n\n is_local_minima (y:numpy.ndarray, i:int)\n\nCheck if position is a local minima.\nArgs: y (np.ndarray): Input array. i (int): Position to check.\nReturns: bool: Flag if position is minima or not.\n\nsource\n\n\nget_local_minima\n\n get_local_minima (y:numpy.ndarray)\n\nFunction to return all local minima of a array\nArgs: y (np.ndarray): Input array.\nReturns: List: List with indices to minima.\n\nsource\n\n\nget_minpos\n\n get_minpos (y:numpy.ndarray, iso_split_level:float)\n\nFunction to get a list of minima in a trace. A minimum is returned if the ratio of lower of the surrounding maxima to the minimum is larger than the splitting factor.\nArgs: y (np.ndarray): Input array. iso_split_level (float): Isotope split level.\nReturns: List: List with min positions."
  },
  {
    "objectID": "feature_finding.html#isolating-isotope_patterns",
    "href": "feature_finding.html#isolating-isotope_patterns",
    "title": "Feature Finding",
    "section": "Isolating Isotope_patterns",
    "text": "Isolating Isotope_patterns\nThe extraction of the longest consistent isotope pattern is implemented in isolate_isotope_pattern. Here, three additional checks for an isotope pattern are implemented.\nThe first one is truncate. Here, one checks the seed position, whether it has a minimum to its left or right side. If a minimum is found, the isotope pattern is cut off at this position.\nThe second one is a mass filter. If the seed has a mass of smaller than 1000, the intensity maximum is detected, and all smaller masses are discarded. This reflects the averagine distribution for small masses where no minimum on the left side can be found.\nThe third one is check_averagine that relies on pattern_to_mz and cosine_averagine. It is used to ensure that the extracted isotope pattern has a cosine correlation of the averagine isotope pattern of the same mass of at least 0.6.\nAfter the longest consistent isotope pattern is found, the hills are removed from the pre-isotope pattern, and the process is repeated until no more isotope patterns can be extracted from the pre-isotope patterns.\n\nsource\n\nmz_to_mass\n\n mz_to_mass (mz:float, charge:int)\n\nFunction to calculate the mass from a mz value.\nArgs: mz (float): M/z charge (int): Charge.\nRaises: NotImplementedError: When a negative charge is used.\nReturns: float: mass\n\nsource\n\n\nint_list_to_array\n\n int_list_to_array (numba_list:numba.typed.typedlist.List)\n\nNumba compatbilte function to convert a numba list with integers to a numpy array\nArgs: numba_list (List): Input numba-typed List.\nReturns: np.ndarray: Output numpy array.\n\nsource\n\n\ncosine_averagine\n\n cosine_averagine (int_one:numpy.ndarray, int_two:numpy.ndarray,\n                   spec_one:numpy.ndarray, spec_two:numpy.ndarray)\n\nCalculate the cosine correlation of two hills.\nArgs: int_one (np.ndarray): Intensity of the first hill. int_two (np.ndarray): Intensity of the second hill. spec_one (np.ndarray): Scan numbers of the first hill. spec_two (np.ndarray): Scan numbers of the second hill.\nReturns: float: Cosine\n\nsource\n\n\npattern_to_mz\n\n pattern_to_mz (stats:numpy.ndarray, pattern:numpy.ndarray, charge:int)\n\nFunction to calculate masses and intensities from pattern for a given charge.\nArgs: stats (np.ndarray): Stats array that contains summary statistics of hills. pattern (np.ndarray): Isotope pattern. charge (int): Charge of the pattern.\nReturns: np.ndarray: masses np.ndarray: intensity\n\nsource\n\n\ncheck_averagine\n\n check_averagine (stats:numpy.ndarray, pattern:numpy.ndarray, charge:int,\n                  averagine_aa:numba.typed.typeddict.Dict,\n                  isotopes:numba.typed.typeddict.Dict)\n\nFunction to compare a pattern to an averagine model.\nArgs: stats (np.ndarray): Stats array that contains summary statistics of hills. pattern (np.ndarray): Isotope pattern. charge (int): Charge. averagine_aa (Dict): Dict containing averagine masses. isotopes (Dict): Dict containing isotopes.\nReturns: float: Averagine correlation."
  },
  {
    "objectID": "feature_finding.html#isotope-patterns",
    "href": "feature_finding.html#isotope-patterns",
    "title": "Feature Finding",
    "section": "Isotope Patterns",
    "text": "Isotope Patterns\nThe wrapper function get_isotope_patterns iterates over all pre_isotope_patterns.\n\nsource\n\nisolate_isotope_pattern\n\n isolate_isotope_pattern (pre_pattern:numpy.ndarray,\n                          hill_ptrs:numpy.ndarray,\n                          hill_data:numpy.ndarray, int_data:numpy.ndarray,\n                          scan_idx:numpy.ndarray, stats:numpy.ndarray,\n                          sortindex_:numpy.ndarray, iso_mass_range:float,\n                          charge_range:numba.typed.typedlist.List,\n                          averagine_aa:numba.typed.typeddict.Dict,\n                          isotopes:numba.typed.typeddict.Dict,\n                          iso_n_seeds:int, cc_cutoff:float,\n                          iso_split_level:float)\n\nIsolate isotope patterns.\nArgs: pre_pattern (np.ndarray): Pre isotope pattern. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. scan_idx (np.ndarray): Array containing the scan index for a centroid. stats (np.ndarray): Stats array that contains summary statistics of hills. sortindex_ (np.ndarray): Sortindex to access the hills from stats. iso_mass_range (float): Mass range for checking isotope patterns. charge_range (List): Charge range. averagine_aa (Dict): Dict containing averagine masses. isotopes (Dict): Dict containing isotopes. iso_n_seeds (int): Number of seeds. cc_cutoff (float): Cutoff value for what is considered correlating. iso_split_level (float): Split level when isotopes are split.\nReturns: np.ndarray: Array with the best pattern. int: Charge of the best pattern.\n\nsource\n\n\nget_isotope_patterns\n\n get_isotope_patterns (pre_isotope_patterns:list, hill_ptrs:numpy.ndarray,\n                       hill_data:numpy.ndarray, int_data:numpy.ndarray,\n                       scan_idx:numpy.ndarray, stats:numpy.ndarray,\n                       sortindex_:numpy.ndarray,\n                       averagine_aa:numba.typed.typeddict.Dict,\n                       isotopes:numba.typed.typeddict.Dict,\n                       iso_charge_min:int=1, iso_charge_max:int=6,\n                       iso_mass_range:float=5, iso_n_seeds:int=100,\n                       cc_cutoff:float=0.6, iso_split_level:float=1.3,\n                       callback:Union[Callable,NoneType]=None)\n\nWrapper function to iterate over pre_isotope_patterns.\nArgs: pre_isotope_patterns (list): List of pre-isotope patterns. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. scan_idx (np.ndarray): Array containing the scan index for a centroid. stats (np.ndarray): Stats array that contains summary statistics of hills. sortindex_ (np.ndarray): Sortindex to access the hills from stats. averagine_aa (Dict): Dict containing averagine masses. isotopes (Dict): Dict containing isotopes. iso_charge_min (int, optional): Minimum isotope charge. Defaults to 1. iso_charge_max (int, optional): Maximum isotope charge. Defaults to 6. iso_mass_range (float, optional): Mass search range. Defaults to 5. iso_n_seeds (int, optional): Number of isotope seeds. Defaults to 100. cc_cutoff (float, optional): Cuttoff for correlation.. Defaults to 0.6. iso_split_level (float, optional): Isotope split level.. Defaults to 1.3. callback (Union[Callable, None], optional): Callback function for progress. Defaults to None. Returns: list: List of isotope patterns. np.ndarray: Iso idx. np.ndarray: Array containing isotope charges.\n\nsource\n\n\nreport_\n\n report_ (idx:numpy.ndarray, isotope_charges:list, isotope_patterns:list,\n          iso_idx:numpy.ndarray, stats:numpy.ndarray,\n          sortindex_:numpy.ndarray, hill_ptrs:numpy.ndarray,\n          hill_data:numpy.ndarray, int_data:numpy.ndarray,\n          rt_:numpy.ndarray, rt_idx:numpy.ndarray, results:numpy.ndarray,\n          lookup_idx:numpy.ndarray)\n\nFunction to extract summary statstics from a list of isotope patterns and charges.\nMS1 feature intensity estimation. For each isotope envelope we interpolate the signal over the retention time range. All isotope enevelopes are summed up together to estimate the peak sahpe\nLastly, we report three estimates for the intensity:\n\nms1_int_sum_apex: The intensity at the peak of the summed signal.\nms1_int_sum_area: The area of the summed signal\nms1_int_max_apex: The intensity at the peak of the most intense isotope envelope\n\nms1_int_max_area: The area of the the most intense isotope envelope\n\nArgs: idx (np.ndarray): Input index. Note that we are using the performance function so this is a range. isotope_patterns (list): List containing isotope patterns (indices to hills). isotope_charges (list): List with charges assigned to the isotope patterns. iso_idx (np.ndarray): Index to isotope pattern. stats (np.ndarray): Stats array that contains summary statistics of hills. sortindex_ (np.ndarray): Sortindex to access the hills from stats. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. rt_ (np.ndarray): Array with retention time information for each scan. rt_idx (np.ndarray): Lookup array to match centroid idx to rt. results (np.ndarray): Recordarray with isotope pattern summary statistics. lookup_idx (np.ndarray): Lookup array for each centroid.\n\nsource\n\n\nfeature_finder_report\n\n feature_finder_report (query_data:dict, isotope_patterns:list,\n                        isotope_charges:list, iso_idx:numpy.ndarray,\n                        stats:numpy.ndarray, sortindex_:numpy.ndarray,\n                        hill_ptrs:numpy.ndarray, hill_data:numpy.ndarray)\n\nCreates a report dataframe with summary statistics of the found isotope patterns.\nArgs: query_data (dict): Data structure containing the query data. isotope_patterns (list): List containing isotope patterns (indices to hills). isotope_charges (list): List with charges assigned to the isotope patterns. iso_idx (np.ndarray): Index to the isotope pattern. stats (np.ndarray): Stats array that contains summary statistics of hills. sortindex_ (np.ndarray): Sortindex to access the hills from stats. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills.\nReturns: pd.DataFrame: DataFrame with isotope pattern summary statistics."
  },
  {
    "objectID": "feature_finding.html#data-output",
    "href": "feature_finding.html#data-output",
    "title": "Feature Finding",
    "section": "Data Output",
    "text": "Data Output\nFor each feature that is found we extract summary statistics and put it in tabular form to be used as as pandas dataframe."
  },
  {
    "objectID": "feature_finding.html#plotting",
    "href": "feature_finding.html#plotting",
    "title": "Feature Finding",
    "section": "Plotting",
    "text": "Plotting\nFor quality control reasons we also employ a function to plot a feature in its local environment."
  },
  {
    "objectID": "feature_finding.html#external-feature-finder",
    "href": "feature_finding.html#external-feature-finder",
    "title": "Feature Finding",
    "section": "External Feature Finder",
    "text": "External Feature Finder\nTo utilize the command-line Feature Finder from Bruker 4DFF-3.13 - uff-cmdline2.exe, we call it via a subprocess and wait until completion.\n\nsource\n\nmap_bruker\n\n map_bruker (feature_path:str, feature_table:pandas.core.frame.DataFrame,\n             query_data:dict)\n\nMap Ms1 to Ms2 via Table FeaturePrecursorMapping from Bruker FF.\nArgs: feature_path (str): Path to the feature file from Bruker FF (.features-file). feature_table (pd.DataFrame): Pandas DataFrame containing the features. query_data (dict): Data structure containing the query data.\nReturns: pd.DataFrame: DataFrame containing features information.\n\nsource\n\n\nconvert_bruker\n\n convert_bruker (feature_path:str)\n\nReads feature table and converts to feature table to be used with AlphaPept.\nArgs: feature_path (str): Path to the feature file from Bruker FF (.features-file).\nReturns: pd.DataFrame: DataFrame containing features information.\n\nsource\n\n\nextract_bruker\n\n extract_bruker (file:str, base_dir:str='ext/bruker/FF',\n                 config:str='proteomics_4d.config')\n\nCall Bruker Feautre Finder via subprocess.\nArgs: file (str): Filename for feature finding. base_dir (str, optional): Base dir where the feature finder is stored.. Defaults to “ext/bruker/FF”. config (str, optional): Config file for feature finder. Defaults to “proteomics_4d.config”.\nRaises: NotImplementedError: Unsupported operating system. FileNotFoundError: Feature finder not found. FileNotFoundError: Config file not found. FileNotFoundError: Feature file not found."
  },
  {
    "objectID": "feature_finding.html#isotope-export",
    "href": "feature_finding.html#isotope-export",
    "title": "Feature Finding",
    "section": "Isotope Export",
    "text": "Isotope Export\n\nsource\n\nget_stats\n\n get_stats (isotope_patterns, iso_idx, stats)"
  },
  {
    "objectID": "feature_finding.html#wrapper",
    "href": "feature_finding.html#wrapper",
    "title": "Feature Finding",
    "section": "Wrapper",
    "text": "Wrapper\n\nsource\n\nfind_features\n\n find_features (to_process:tuple, callback:Union[Callable,NoneType]=None,\n                parallel:bool=False)\n\nWrapper for feature finding.\nArgs: to_process (tuple): to_process tuple, to be used from a proces spool. callback (Union[Callable, None], optional): Optional callback function. Defaults to None. parallel (bool, optional): Flag to use parallel processing. Currently unused. Defaults to False.\nRaises: NotImplementedError: Error if the file extension is not understood.\nReturns: Union[str, bool]: Returns true if function was sucessfull, otherwise the exception as string."
  },
  {
    "objectID": "feature_finding.html#mapping",
    "href": "feature_finding.html#mapping",
    "title": "Feature Finding",
    "section": "Mapping",
    "text": "Mapping\nMapping MS1 to MS2\n\nsource\n\nmap_ms2\n\n map_ms2 (feature_table:pandas.core.frame.DataFrame, query_data:dict,\n          map_mz_range:float=1, map_rt_range:float=0.5,\n          map_mob_range:float=0.3, map_n_neighbors:int=5,\n          search_unidentified:bool=False, **kwargs)\n\nMap MS1 features to MS2 based on rt and mz. If ccs is included also add. Args: feature_table (pd.DataFrame): Pandas DataFrame with features. query_data (dict): Data structure containing the query data. map_mz_range (float, optional): Mapping range for mz (Da). Defaults to 1. map_rt_range (float, optional): Mapping range for rt (min). Defaults to 0.5. map_mob_range (float, optional): Mapping range for mobility (%). Defaults to 0.3. map_n_neighbors (int, optional): Maximum number of neighbors to be extracted. Defaults to 5. search_unidentified (bool, optional): Flag to perform search on features that have no isotope pattern. Defaults to False.\nReturns: pd.DataFrame: Table with features.\n\nsource\n\n\nreplace_infs\n\n replace_infs (array:numpy.ndarray)\n\nReplace nans and infs with 0\nArgs: array (np.ndarray): Input array.\nReturns: np.ndarray: Output array without nans and infs."
  },
  {
    "objectID": "score.html",
    "href": "score.html",
    "title": "Score",
    "section": "",
    "text": "This notebook contains all functions related to the scoring of peptide-spectrum-matches (PSMS).\nIn brief, this notebook includes the following:"
  },
  {
    "objectID": "score.html#filtering",
    "href": "score.html#filtering",
    "title": "Score",
    "section": "Filtering",
    "text": "Filtering\nThe filtering functions are essential base functions for scoring in AlphaPept. They make sure that only the ‘best precursor per spectum’ and the ‘best spectrum per precursor’ is used.\nRecall from the search that when having feautres, raw_idx refers to the actual index from the raw data. Otherwise it isquery_data.\nFor filtering, we have several functions. When applying for a score, we first use filter_score and then filter_precursor. filter_score is keeping the best score per experimental spectrum. First we rank by score for each query_idx. As we have multiple hits for each experimental spectrum from the search we only want to keep the best one.\nWhen performing feature finding, we assign multiple possible features to each experimental spectrum. The idea here is that a spectrum could originate from various precursors. To disentangle these psms we can use the following modes:\n\nsingle: This mode will only keep one feature per experimental spectrum (the one with the highest score and the closest distance). Each feature can only occur once.\nmultiple: Allow multiple features per experimental spectrum. Each feature can only occur once.\n\nfilter_precursor is intended for the case that a precursor (charge + sequence) occurs more than once. Only the one with the highest score will be kept.\n\nsource\n\nfilter_score\n\n filter_score (df:pandas.core.frame.DataFrame, mode:str='multiple')\n\nFilter psms feature table by keeping only the best scoring psm per experimental spectrum.\nTODO: psms could still have the same score when having modifications at multiple positions that are not distinguishable. Only keep one.\nArgs: df (pd.DataFrame): psms table of search results from alphapept. mode (str, optional): string specifying which mode to use for psms filtering. The two options are ‘single’ and ‘multiple’. ‘single’ will only keep one feature per experimental spectrum. ‘multiple’ will allow multiple features per experimental spectrum. In either option, each feature can only occur once. Defaults to ‘multiple’.\nReturns: pd.DataFrame: table containing the filtered psms results.\n\nsource\n\n\nfilter_precursor\n\n filter_precursor (df:pandas.core.frame.DataFrame)\n\nFilter psms feature table by precursor. Allow each precursor only once.\nArgs: df (pd.DataFrame): psms table of search results from alphapept.\nReturns: pd.DataFrame: table containing the filtered psms results."
  },
  {
    "objectID": "score.html#q-values",
    "href": "score.html#q-values",
    "title": "Score",
    "section": "Q-Values",
    "text": "Q-Values\nget_q_values is used to calculate q-values from FDR values. The direct relationship is illustrated further down in the notebook.\n\nsource\n\nget_q_values\n\n get_q_values (fdr_values:numpy.ndarray)\n\nCalculate q-values from fdr_values.\nArgs: fdr_values (np.ndarray): np.ndarray of fdr values.\nReturns: np.ndarray: np.ndarray of q-values."
  },
  {
    "objectID": "score.html#fdr",
    "href": "score.html#fdr",
    "title": "Score",
    "section": "FDR",
    "text": "FDR\nThe employed FDR strategy is based on a classical target-decoy competition approach. The procedure works as follows: 1. Consider only the best scoring target or decoy PSM per spectrum. 2. Sort all PSMs by decreasing scores. 3. Estimate the FDR as #decoys / #targets, where #targets (#decoys) is the number of positive target (decoy) PSMs at a given score threshold t (i.e. PSMs with scores higher than t). 4. Convert the estimated FDR to q-values by selecting the minimum FDR at which the identification could be made, i.e. the lowest score threshold t that could be set to include an identification without increasing the number of false positives. 5. Report the set of target PSMs with q-values smaller or equal to the selected fdr_level.\nInformative literature describing and discussing different FDR estimation approaches for shotgun proteomics can be found here (the implemented strategy in alphapept is referred to as T-TDC in this article): > Keich, Uri et al. “Improved False Discovery Rate Estimation Procedure for Shotgun Proteomics.” Journal of proteome research vol. 14,8 (2015): 3148-61. https://pubs.acs.org/doi/10.1021/acs.jproteome.5b00081\n\nsource\n\ncut_fdr\n\n cut_fdr (df:pandas.core.frame.DataFrame, fdr_level:float=0.01,\n          plot:bool=True, cut:bool=True)\n\nCuts a dataframe with a given fdr level\nArgs: df (pd.DataFrame): psms table of search results from alphapept. fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01. plot (bool, optional): flag to enable plot. Defaults to ‘True’. cut (bool, optional): flag to cut above fdr threshold. Defaults to ‘True’.\nReturns: float: numerical value of the applied score cutoff pd.DataFrame: df with psms within fdr\nNote that the test function for cut_fdr is further down in the notebook to also test protein-level FDR.\nSimulation of random scores for 50’000 measurements (corresponding to spectra). Simulated are decoys, true targets and false targets. We assume a false traget raio (pi0) of 0.8 and a mean score difference of 3.5.\nSimulated score distribution for a separate target and decoy database search:\nSimulated score distribution for a corresponding concatinated target-decoy database search with target-decoy-competition:\nApplication of the cut_fdr function to the simulated target-decoy competition dataset saved in TDC:\n\ncval, cut_TDC = cut_fdr(TDC, fdr_level=0.01)\n\n\n\n\n\n\n\nEvaluation of the FDR estimated by the target-decoy approach versus the true FDR confirms accurate FDR estimation by our approach. The true FDR is capped by the selected fraction of false targets (pi0 = 0.8) and by the effect of target decoy competition. Similarly, the true positive rate (TPR) is limited by the effect of target decoy competition and can only reach 1 in cases where not a single decoy scores higher than a true target.\nThe following figure illustrates the difference between fdr and q_value.\nPlease be aware that the shown simulations are not an accurate model for PSMS scoring and they were designed only for illustrative purposes and to test the implemeted functions."
  },
  {
    "objectID": "score.html#global-fdr",
    "href": "score.html#global-fdr",
    "title": "Score",
    "section": "Global FDR",
    "text": "Global FDR\nThe cut_global_fdr function has two specific applications: 1. Estimate q-values on the peptide and protein level  The concept here is based on selecting the best scoring precursor per peptide (or protein) to then estimate the FDR by target-decoy competition using the cut_fdr function. 2. Estimate q-values across an entire dataset on either precursor, peptide or protein level  The concept here is based on selecting the best scoring precursor, peptide or protein signal across an entire dataset to then estimate the FDR by target-decoy competition using the cut_fdr function.\nThis strategy was extensively tested and discussed in the following publications:\n\nNesvizhskii, Alexey I. “A survey of computational methods and error rate estimation procedures for peptide and protein identification in shotgun proteomics.” Journal of proteomics vol. 73,11 (2010): 2092-123. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2956504/\nSavitski, Mikhail M et al. “A Scalable Approach for Protein False Discovery Rate Estimation in Large Proteomic Data Sets.” Molecular & cellular proteomics : MCP vol. 14,9 (2015): 2394-404. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4563723/\nThe, Matthew et al. “Fast and Accurate Protein False Discovery Rates on Large-Scale Proteomics Data Sets with Percolator 3.0.” Journal of the American Society for Mass Spectrometry vol. 27,11 (2016): 1719-1727. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5059416/\nGupta, Nitin, and Pavel A Pevzner. “False discovery rates of protein identifications: a strike against the two-peptide rule.” Journal of proteome research vol. 8,9 (2009): 4173-81. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3398614/\n\n\nsource\n\ncut_global_fdr\n\n cut_global_fdr (data:pandas.core.frame.DataFrame,\n                 analyte_level:str='sequence', fdr_level:float=0.01,\n                 plot:bool=True, **kwargs)\n\nFunction to estimate and filter by global peptide or protein fdr\nArgs: data (pd.DataFrame): psms table of search results from alphapept. analyte_level (str, optional): string specifying the analyte level to apply the fdr threshold. Options include: ‘precursor’, ‘sequence’, ‘protein_group’ and ‘protein’. Defaults to ‘sequence’. fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01. plot (bool, optional): flag to enable plot. Defaults to ‘True’.\nReturns: pd.DataFrame: df with filtered results\nSimilar to the sequence level simulations we can simulatae score distributions for peptides beloning to proteins. In our simulation we assumed a poisson distribution for the number of peptides for each protein centered at 4 peptides.\nApplication of the cut_global_fdr function to the simulated protein-level target-decoy competition dataset saved in TDC_prot:\n\ncut_TDC_prot = cut_global_fdr(TDC_prot, fdr_level=0.01, analyte_level='protein')\n\n\n\n\n\n\n\nEvaluation of the protein-level FDR estimated by the target-decoy approach versus the true FDR confirms accurate FDR estimation by our approach:\nInvestigating the peptide-level statistics after protein-level FDR filtering shows a conservative pattern.\nPlease be aware that the shown simulations are not an accurate model for PSMS scoring and they were designed only for illustrative purposes and to test the implemeted functions."
  },
  {
    "objectID": "score.html#scoring",
    "href": "score.html#scoring",
    "title": "Score",
    "section": "Scoring",
    "text": "Scoring\n\nX!tandem scoring\n\nget_x_tandem_score performs scoring of PSMs according to the X!tandem strategy:\nscore_x_tandem first calls get_x_tandem_score and and subsequently applies the cut_fdr function to filter PSMs at the specified fdr_level.\n\n\nX!Tandem, Craig,R. and Beavis,R.C. (2003) Rapid Commun. Mass Spectrom., 17, 2310-2316\n\n\nsource\n\n\nfilter_with_score\n\n filter_with_score (df:pandas.core.frame.DataFrame)\n\nFilters the psms table by using the score column, no fdr filter. TODO: Remove redundancy with score functions, see issue: #275\nArgs: df (pd.DataFrame): psms table of search results from alphapept.\nReturns: pd.DataFrame: psms table filtered for no feature or precursor to be assigned multiple times.\n\nsource\n\n\nfilter_with_x_tandem\n\n filter_with_x_tandem (df:pandas.core.frame.DataFrame)\n\nFilters the psms table by using the x_tandem score, no fdr filter. TODO: Remove redundancy with score functions, see issue: #275\nArgs: df (pd.DataFrame): psms table of search results from alphapept.\nReturns: pd.DataFrame: psms table with an extra ‘score’ column for x_tandem, filtered for no feature or precursor to be assigned multiple times.\n\nsource\n\n\nscore_generic\n\n score_generic (df:pandas.core.frame.DataFrame, fdr_level:float=0.01,\n                plot:bool=True, **kwargs)\n\nFilters the psms table by using a generic score and filtering the results for fdr_level.\nArgs: df (pd.DataFrame): psms table of search results from alphapept. fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01.\nReturns: pd.DataFrame: psms table with an extra ‘score’ column for the generic score, filtered for no feature or precursor to be assigned multiple times.\n\nsource\n\n\nget_generic_score\n\n get_generic_score (df:pandas.core.frame.DataFrame)\n\nFunction to calculate the a generic score\nArgs: df (pd.DataFrame): psms table of search results from alphapept.\nReturns: np.ndarray: np.ndarray with x_tandem scores\n\nsource\n\n\nscore_x_tandem\n\n score_x_tandem (df:pandas.core.frame.DataFrame, fdr_level:float=0.01,\n                 plot:bool=True, **kwargs)\n\nFilters the psms table by using the x_tandem score and filtering the results for fdr_level.\nArgs: df (pd.DataFrame): psms table of search results from alphapept. fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01.\nReturns: pd.DataFrame: psms table with an extra ‘score’ column for x_tandem, filtered for no feature or precursor to be assigned multiple times.\n\nsource\n\n\nget_x_tandem_score\n\n get_x_tandem_score (df:pandas.core.frame.DataFrame)\n\nFunction to calculate the x tandem score\nArgs: df (pd.DataFrame): psms table of search results from alphapept.\nReturns: np.ndarray: np.ndarray with x_tandem scores\n\n\nScore and filter PSMs by any specified score\nscore_psms uses the specified score and applies the cut_fdr function to filter PSMs at the specified fdr_level. filter_score and filter_precursor are applied to only report the best PSM per acquired spectrum and the best signal per precursor (i.e. sequence + charge combination).\n\nsource\n\n\nscore_psms\n\n score_psms (df:pandas.core.frame.DataFrame, score:str='hits_y',\n             fdr_level:float=0.01, plot:bool=True, **kwargs)\n\nUses the specified score in df to filter psms and to apply the fdr_level threshold.\nArgs: df (pd.DataFrame): psms table of search results from alphapept. score (str, optional): string specifying the column in df to use as score. Defaults to ‘hits_y’. fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01. plot (bool, optional): flag to enable plot. Defaults to ‘True’.\nReturns: pd.DataFrame: filtered df with psms within fdr"
  },
  {
    "objectID": "score.html#machine-learning-based-scoring-of-psms",
    "href": "score.html#machine-learning-based-scoring-of-psms",
    "title": "Score",
    "section": "Machine learning based scoring of PSMs",
    "text": "Machine learning based scoring of PSMs\n\nget_ML_features extracts additional scoring metrics for the machine learning, including the number of amino acids per precursor, the number of missed cleavages and the logarithmic number of times the same peptide occurs in the set of PSMs\ntrain_RF trains a random forest classifier for scoring all PSMs. For this, we use the scikit-learn library.\n\nFirst, a machine learning pipeline is created including the sklearn StandardScaler and RandomForestClassifier. The StandardScaler is used to standardize all features by removing the mean and scaling to unit variance. For details on the RandomForestClassifier see: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html.\nNext, a grid search is initialized for testing the hyperparameter space (max_depth and max_leaf_nodes) of the random forest classifier by a 5-fold cross-validation using GridSearchCV.\nTo train the classifier, we first select a suitable set of PSMSs. This is achieved by an initial scoring and FDR estimation of the input PSMs based on the ini_score. Only targets below the train_fdr_level cutoff are considered for training the classifier. To ensure a balanced dataset for training (i.e. same number of targets and decoys), the number of PSMs per category is selected to be the minimum of either the number of high scoring targets below the train_fdr_level cutoff or the overall number of decoys among the PSMs. min_train specifies the minimum number of targets and decoys that should be available.\nOnce a balanced set of PSMs is established, the PSMs are split into a training and test set accoring to the test_size argument using train_test_split from sklearn while ensuring the PSMs are split in a stratified fashion (i.e. equal number of targets and decoys in both the training and test sets).\nThe grid search and training of the random forest classifier is performed on the training set of PSMs. The GridSearchCV returns the classifier which performed best across all cross-validation hold-out sets according to the scoring function (classification ‘accuracy’ is set as default score). The grid search is parallelize dinto n_jobs.\nNext, the trained classifier is applied to the testing set of PSMs and the test score is reported.\nIf plot is enabled, a figure illustrating the weights of each feature is produced.\nFinally the function returns the trained random forest classifier for subsequent application to the entire set of PSMs or for transfering to a different dataset.\n\nscore_ML applies a classifier trained by train_RF to a complete set of PSMs. It calls the cut_fdr function and filters for the specified fdr_level. filter_score and filter_precursor are applied to only report the best PSM per acquired spectrum and the best signal per precursor (i.e. sequence + charge combination).\n\n\nsource\n\nfilter_with_ML\n\n filter_with_ML (df:pandas.core.frame.DataFrame,\n                 trained_classifier:sklearn.model_selection._search.GridSe\n                 archCV, features:list=None, **kwargs)\n\nFilters the psms table by using the x_tandem score, no fdr filter. TODO: Remove redundancy with score functions, see issue: #275\nArgs: df (pd.DataFrame): psms table of search results from alphapept. trained_classifier (GridSearchCV): GridSearchCV object returned by train_RF. features (list): list with features returned by train_RF. Defaults to ‘None’.\nReturns: pd.DataFrame: psms table with an extra ‘score’ column from the trained_classifier by ML, filtered for no feature or precursor to be assigned multiple times.\n\nsource\n\n\nscore_ML\n\n score_ML (df:pandas.core.frame.DataFrame,\n           trained_classifier:sklearn.model_selection._search.GridSearchCV\n           , features:list=None, fdr_level:float=0.01, plot:bool=True,\n           **kwargs)\n\nApplies a trained ML classifier to df and uses the ML score to filter psms and to apply the fdr_level threshold.\nArgs: df (pd.DataFrame): psms table of search results from alphapept. trained_classifier (GridSearchCV): GridSearchCV object returned by train_RF. features (list): list with features returned by train_RF. Defaults to None. fdr_level (float, optional): fdr level that should be used for filtering. The value should lie between 0 and 1. Defaults to 0.01. plot (bool, optional): flag to enable plot. Defaults to True.\nReturns: pd.DataFrame: filtered df with psms within fdr\n\nsource\n\n\ntrain_RF\n\n train_RF (df:pandas.core.frame.DataFrame,\n           exclude_features:list=['precursor_idx', 'fragment_ion_idx',\n           'fasta_index', 'feature_rank', 'raw_rank', 'score_rank',\n           'db_idx', 'feature_idx', 'precursor', 'query_idx', 'raw_idx',\n           'sequence', 'decoy', 'sequence_naked', 'target'],\n           train_fdr_level:float=0.1, ini_score:str='x_tandem',\n           min_train:int=1000, test_size:float=0.8, max_depth:list=[5, 25,\n           50], max_leaf_nodes:list=[150, 200, 250], n_jobs:int=-1,\n           scoring:str='accuracy', plot:bool=False, random_state:int=42,\n           **kwargs)\n\nFunction to train a random forest classifier to separate targets from decoys via semi-supervised learning.\nArgs: df (pd.DataFrame): psms table of search results from alphapept. exclude_features (list, optional): list with column names of features to exclude for ML. Defaults to [‘precursor_idx’,‘fragment_ion_idx’,‘fasta_index’,‘feature_rank’,‘raw_rank’,‘score_rank’,‘db_idx’, ‘feature_idx’, ‘precursor’, ‘query_idx’, ‘raw_idx’,‘sequence’,‘decoy’,‘sequence_naked’,‘target’]. train_fdr_level (float, optional): Only targets below the train_fdr_level cutoff are considered for training the classifier. Defaults to 0.1. ini_score (str, optional): Initial score to select psms set for semi-supervised learning. Defaults to ‘x_tandem’. min_train (int, optional): Minimum number of psms in the training set. Defaults to 1000. test_size (float, optional): Fraction of psms used for testing. Defaults to 0.8. max_depth (list, optional): List of clf__max_depth parameters to test in the grid search. Defaults to [5,25,50]. max_leaf_nodes (list, optional): List of clf__max_leaf_nodes parameters to test in the grid search. Defaults to [150,200,250]. n_jobs (int, optional): Number of jobs to use for parallelizing the gridsearch. Defaults to -1, which in GridSearchCV corresponds to ‘use all available cores’. scoring (str, optional): Scoring method for the gridsearch. Defaults to ‘accuracy’. plot (bool, optional): flag to enable plot. Defaults to ‘False’. random_state (int, optional): Random state for initializing the RandomForestClassifier. Defaults to 42.\nReturns: [GridSearchCV, list]: GridSearchCV: GridSearchCV object with trained RandomForestClassifier. list: list of features used for training the classifier.\n\nsource\n\n\nget_ML_features\n\n get_ML_features (df:pandas.core.frame.DataFrame, protease:str='trypsin',\n                  **kwargs)\n\nUses the specified score in df to filter psms and to apply the fdr_level threshold.\nArgs: df (pd.DataFrame): psms table of search results from alphapept. protease (str, optional): string specifying the protease that was used for proteolytic digestion. Defaults to ‘trypsin’.\nReturns: pd.DataFrame: df including additional scores for subsequent ML."
  },
  {
    "objectID": "score.html#protein-grouping",
    "href": "score.html#protein-grouping",
    "title": "Score",
    "section": "Protein grouping",
    "text": "Protein grouping\nWhat is a protein group? A introduction and explanation can be found here [1]:\nThe proteome is characterized by a relatively high sequence redundancy. This results from different evolutionary processes and the presence of isoforms. In bottom-up proteomics, this situation leads to the problem that often a peptide cannot be uniquely associated with one protein of origin, which makes it impossible to unambiguously claim the presence of one protein over another. MaxQuant resolves this issue by collapsing all proteins that cannot be distinguished based on the identified peptides into protein groups.\nThe rule is that if all peptides of a given protein are a subset of the peptides used for identification of another protein, these proteins will be merged in a protein group. A more complex situation arises when two protein groups are identified with distinct peptides, except for one that is shared between the two. In this case, the two groups cannot be combined, as they contain group-unique peptides and will thus be reported separately in the MaxQuant output table. Depending on the user-defined setting, the shared peptide will not be used for quantification (unique peptides only), or it will be used for the quantification of the protein group with a larger number of associated peptides (unique + razor peptides).\n\nIn AlphaPept we employ the following strategy: First, we check whether a peptide is proteotypic, meaning that the peptide can only belong to one protein. For peptides that are shared between multiple proteins, we employ a razor approach.\nWe create a network and add all connections between the peptides and proteins. Then, we extract all connected components, referring to all peptides and proteins that are connected. For a cluster of connected components, we then iterate over all proteins and count the number of peptides that are connected to the particular protein. The protein with the most peptides will then be the razor protein.\nWe remove this protein and the respective peptides and continue with the extraction from the cluster until no more peptides are present.\nFor efficient implementation, the proteins and peptides are encoded as indexes. To distinguish proteins from peptides, proteins have a leading ‘p’.\n\n[1] Tyanova, S., Temu, T. & Cox, J. The MaxQuant computational platform for mass spectrometry-based shotgun proteomics. Nat Protoc 11, 2301–2319 (2016). https://doi.org/10.1038/nprot.2016.136\n\n\nsource\n\nperform_protein_grouping\n\n perform_protein_grouping (data:pandas.core.frame.DataFrame,\n                           pept_dict:dict, fasta_dict:dict, **kwargs)\n\nWrapper function to perform protein grouping by razor approach\nArgs: data (pd.DataFrame): psms table of scored and filtered search results from alphapept. pept_dict (dict): A dictionary mapping peptide indices to the originating proteins as a list. fasta_dict (dict): A dictionary with fasta sequences.\nReturns: pd.DataFrame: alphapept results table now including protein level information.\n\nsource\n\n\nget_protein_groups\n\n get_protein_groups (data:pandas.core.frame.DataFrame, pept_dict:dict,\n                     fasta_dict:dict, decoy=False, callback=None,\n                     **kwargs)\n\nFunction to perform protein grouping by razor approach. This function calls assign_proteins and get_shared_proteins. ToDo: implement callback for solving Each protein is indicated with a p -> protein index\nArgs: data (pd.DataFrame): psms table of scored and filtered search results from alphapept. pept_dict (dict): A dictionary mapping peptide indices to the originating proteins as a list. fasta_dict (dict): A dictionary with fasta sequences. decoy (bool, optional): Defaults to False. callback (bool, optional): Defaults to None.\nReturns: pd.DataFrame: alphapept results table now including protein level information.\n\nsource\n\n\nget_shared_proteins\n\n get_shared_proteins (data:pandas.core.frame.DataFrame,\n                      found_proteins:dict, pept_dict:dict)\n\nAssign peptides to razor proteins.\nArgs: data (pd.DataFrame): psms table of scored and filtered search results from alphapept, appended with n_possible_proteins. found_proteins (dict): dictionary mapping psms indices to proteins pept_dict (dict): dictionary mapping peptide indices to the originating proteins as a list\nReturns: dict: dictionary mapping peptides to razor proteins\n\nsource\n\n\nassign_proteins\n\n assign_proteins (data:pandas.core.frame.DataFrame, pept_dict:dict)\n\nAssign psms to proteins. This function appends the dataframe with a column ‘n_possible_proteins’ which indicates how many proteins a psm could be matched to. It returns the appended dataframe and a dictionary found_proteins where each protein is mapped to the psms indices.\nArgs: data (pd.DataFrame): psms table of scored and filtered search results from alphapept. pept_dict (dict): dictionary that matches peptide sequences to proteins\nReturns: pd.DataFrame: psms table of search results from alphapept appended with the number of matched proteins. dict: dictionary mapping psms indices to proteins.\n\nsource\n\n\nget_ion\n\n get_ion (i:int, df:pandas.core.frame.DataFrame,\n          fragment_ions:pandas.core.frame.DataFrame)\n\nHelper function to extract the ion-hits for a given DataFrame index. This function extracts the hit type and the intensities. E.g.: [‘b1’,‘y1’], np.array([10,20]).\nArgs: i (int): Row index for the DataFrame df (pd.DataFrame): DataFrame with PSMs fragment_ions (pd.DataFrame): DataFrame with ion hits\nReturns: list: List with strings that describe the ion type. np.ndarray: Array with intensity information\n\nsource\n\n\necdf\n\n ecdf (data:numpy.ndarray)\n\nCompute ECDF. Helper function to calculate the ECDF of a score distribution. This is later used to normalize the score from an arbitrary range to [0,1].\nArgs: data (np.ndarray): Array containting the score.\nReturns: np.ndarray: Array containg the score, sorted. np.ndarray: Noramalized counts."
  },
  {
    "objectID": "score.html#helper-functions",
    "href": "score.html#helper-functions",
    "title": "Score",
    "section": "Helper functions",
    "text": "Helper functions\nTo call the functions from the interface with a process pool, we define the helper functions score_hdf and protein_grouping_all.\n\nsource\n\nprotein_grouping_all\n\n protein_grouping_all (settings:dict, pept_dict:dict, fasta_dict:dict,\n                       callback=None)\n\nApply protein grouping on all files in an experiment. This function will load all dataframes (peptide_fdr level) and perform protein grouping.\nArgs: settings: (dict): Settings file for the experiment pept_dict: (dict): A peptide dictionary. fast_dict: (dict): A FASTA dictionary. callback: (Callable): Optional callback.\n\nsource\n\n\nscore_hdf\n\n score_hdf (to_process:tuple, callback:Callable=None, parallel:bool=False)\n\nApply scoring on an hdf file to be called from a parallel pool. This function does not raise errors but returns the exception as a string. Args: to_process: (int, dict): Tuple containg a file index and the settings. callback: (Callable): Optional callback parallel: (bool): Parallel flag (unused).\nReturns: Union[bool, str]: True if no eo exception occured, the exception if things failed."
  },
  {
    "objectID": "fasta.html",
    "href": "fasta.html",
    "title": "FASTA",
    "section": "",
    "text": "This notebook contains all functions related to creating spectra from FASTA files. In brief, what we are doing is the following:\nCurrently, numba has only limited string support. A lot of the functions are therefore Python-native."
  },
  {
    "objectID": "fasta.html#cleaving",
    "href": "fasta.html#cleaving",
    "title": "FASTA",
    "section": "Cleaving",
    "text": "Cleaving\nWe use regular expressions to find potential cleavage sites for cleaving and write the wrapper cleave_sequence to use it.\n\nsource\n\ncleave_sequence\n\n cleave_sequence (sequence:str='', n_missed_cleavages:int=0,\n                  protease:str='trypsin', pep_length_min:int=6,\n                  pep_length_max:int=65, **kwargs)\n\nCleave a sequence with a given protease. Filters to have a minimum and maximum length. Args: sequence (str): the given (protein) sequence. n_missed_cleavages (int): the number of max missed cleavages. protease (str): the protease/enzyme name, the regular expression can be found in alphapept.constants.protease_dict. pep_length_min (int): min peptide length. pep_length_max (int): max peptide length. Returns: list (of str): cleaved peptide sequences with missed cleavages.\n\nsource\n\n\nget_missed_cleavages\n\n get_missed_cleavages (sequences:list, n_missed_cleavages:int)\n\nCombine cleaved sequences to get sequences with missed cleavages Args: seqeuences (list of str): the list of cleaved sequences, no missed cleavages are there. n_missed_cleavages (int): the number of miss cleavage sites. Returns: list (of str): the sequences with missed cleavages.\n\nprotease = \"trypsin\"\nn_missed_cleavages = 0\npep_length_min, pep_length_max = 6, 65\n\ncleave_sequence('ABCDEFGHIJKLMNOPQRST', n_missed_cleavages, protease, pep_length_min, pep_length_max)\n\n['ABCDEFGHIJK', 'LMNOPQR']"
  },
  {
    "objectID": "fasta.html#counting-missed-and-internal-cleavages",
    "href": "fasta.html#counting-missed-and-internal-cleavages",
    "title": "FASTA",
    "section": "Counting missed and internal cleavages",
    "text": "Counting missed and internal cleavages\nThe following are helper functions to retrieve the number of missed cleavages and internal cleavage sites for each sequence.\n\nsource\n\ncount_internal_cleavages\n\n count_internal_cleavages (sequence:str='', protease:str='trypsin',\n                           **kwargs)\n\nCounts the number of internal cleavage sites for a given sequence and protease Args: sequence (str): the given (peptide) sequence. protease (str): the protease/enzyme name, the regular expression can be found in alphapept.constants.protease_dict. Returns: int (0 or 1): if the sequence is from internal cleavage.\n\nsource\n\n\ncount_missed_cleavages\n\n count_missed_cleavages (sequence:str='', protease:str='trypsin',\n                         **kwargs)\n\nCounts the number of missed cleavages for a given sequence and protease Args: sequence (str): the given (peptide) sequence. protease (str): the protease/enzyme name, the regular expression can be found in alphapept.constants.protease_dict. Returns: int: the number of miss cleavages\n\nprotease = \"trypsin\"\nprint(count_missed_cleavages('ABCDEFGHIJKLMNOPQRST', protease))\n\nprotease = \"trypsin\"\nprint(count_internal_cleavages('ABCDEFGHIJKLMNOPQRST', protease))\n\n2\n1"
  },
  {
    "objectID": "fasta.html#parsing",
    "href": "fasta.html#parsing",
    "title": "FASTA",
    "section": "Parsing",
    "text": "Parsing\nPeptides are composed out of amino acids that are written in capital letters - PEPTIDE. To distinguish modifications, they are written in lowercase such as PEPTIoxDE and can be of arbitrary length. For a modified amino acid (AA), the modification precedes the letter of the amino acid. Decoys are indicated with an underscore. Therefore, the parse function splits after _. When parsing, the peptide string is converted into a numba-compatible list, like so: PEPoxTIDE -> [P, E, P, oxT, I, D, E]. This allows that we can use the mass_dict from alphapept.constants to directly determine the masses for the corresponding amino acids.\n\nsource\n\nlist_to_numba\n\n list_to_numba (a_list)\n\nConvert Python list to numba.typed.List Args: a_list (list): Python list. Return: List (numba.typed.List): Numba typed list.\n\nsource\n\n\nparse\n\n parse (peptide:str)\n\nParser to parse peptide strings Args: peptide (str): modified peptide sequence. Return: List (numba.typed.List): a list of animo acids and modified amono acids\n\nprint(parse('PEPTIDE'))\nprint(parse('PEPoxTIDE'))\n\n[P, E, P, T, I, D, E, ...]\n[P, E, P, oxT, I, D, E, ...]"
  },
  {
    "objectID": "fasta.html#decoy",
    "href": "fasta.html#decoy",
    "title": "FASTA",
    "section": "Decoy",
    "text": "Decoy\nThe decoy strategy employed is a pseudo-reversal of the peptide sequence, keeping only the terminal amino acid and reversing the rest. Additionally, we can call the functions swap_KR and and swap_AL that will swap the respective AAs. The function swap_KR will only swap terminal AAs. The swapping functions only work if the AA is not modified.\n\nsource\n\nadd_decoy_tag\n\n add_decoy_tag (peptides)\n\nAdds a ’_decoy’ tag to a list of peptides\n\nsource\n\n\nget_decoys\n\n get_decoys (peptide_list, pseudo_reverse=False, AL_swap=False,\n             KR_swap=False, **kwargs)\n\nWrapper to get decoys for lists of peptides Args: peptide_list (list): the list of peptides to be reversed. pseudo_reverse (bool): If True, reverse the peptide bug keep the C-terminal amino acid; otherwise reverse the whole peptide. (Default: False) AL_swap (bool): replace A with L, and vice versa. (Default: False) KR_swap (bool): replace K with R at the C-terminal, and vice versa. (Default: False) Returns: list (of str): a list of decoy peptides\n\nsource\n\n\nswap_AL\n\n swap_AL (peptide:str)\n\nSwaps a A with L. Note: Only if AA is not modified. Args: peptide (str): peptide.\nReturns: str: peptide with swapped ALs.\n\nsource\n\n\nswap_KR\n\n swap_KR (peptide:str)\n\nSwaps a terminal K or R. Note: Only if AA is not modified.\nArgs: peptide (str): peptide.\nReturns: str: peptide with swapped KRs.\n\nsource\n\n\nget_decoy_sequence\n\n get_decoy_sequence (peptide:str, pseudo_reverse:bool=False,\n                     AL_swap:bool=False, KR_swap:bool=False)\n\nReverses a sequence and adds the ’_decoy’ tag. Args: peptide (str): modified peptide to be reversed. pseudo_reverse (bool): If True, reverse the peptide bug keep the C-terminal amino acid; otherwise reverse the whole peptide. (Default: False) AL_swap (bool): replace A with L, and vice versa. (Default: False) KR_swap (bool): replace K with R at the C-terminal, and vice versa. (Default: False) Returns: str: reversed peptide ending with the ’_decoy’ tag.\n\nprint(swap_AL(parse('KKKALKKK')))\nprint(swap_KR(parse('AAAKRAAA')))\n\n[K, K, K, L, A, K, K, K, ...]\n[A, A, A, K, R, A, A, A, ...]\n\n\n\nprint(get_decoy_sequence('PEPTIDE'))\n\nEDITPEP\n\n\n\nprint(get_decoys(['ABC','DEF','GHI']))\n\n['CBA', 'FED', 'IHG']"
  },
  {
    "objectID": "fasta.html#modifications",
    "href": "fasta.html#modifications",
    "title": "FASTA",
    "section": "Modifications",
    "text": "Modifications\nTo add modifications to the peptides, we distinguish fixed and variable modifications. Additionally, we make a distinction between whether the modification is only terminal or not.\n\nFixed Modifications\nFixed modifications are implemented by passing a list with modified AAs that should be replaced. As a AA is only one letter, the remainder is the modification.\n\nsource\n\n\nadd_fixed_mods\n\n add_fixed_mods (seqs:list, mods_fixed:list, **kwargs)\n\nAdds fixed modifications to sequences. Args: seqs (list of str): sequences to add fixed modifications mods_fixed (list of str): the string list of fixed modifications. Each modification string must be in lower case, except for that the last letter must be the modified amino acid (e.g. oxidation on M should be oxM). Returns: list (of str): the list of the modified sequences. ‘ABCDEF’ with fixed mod ‘cC’ will be ‘ABcCDEF’.\n\nmods_fixed = ['cC','bB']\npeptide_list = ['ABCDEF']\n\nadd_fixed_mods(peptide_list, mods_fixed)\n\n['AbBcCDEF']\n\n\n\nmods_fixed = ['aA','cC','bB']\npeptide_list = ['cABCDEF']\n\nadd_fixed_mods(peptide_list, mods_fixed)\n\n['caAbBcCDEF']\n\n\n\n\nVariable Modifications\nTo employ variable modifications, we loop through each variable modification and each position of the peptide and add them to the peptide list. For each iteration in get_isoforms, one more variable modification will be added.\n\nsource\n\n\nget_isoforms\n\n get_isoforms (mods_variable_dict:dict, peptide:str, isoforms_max:int,\n               n_modifications_max:int=None)\n\nFunction to generate modified forms (with variable modifications) for a given peptide - returns a list of modified forms. The original sequence is included in the list Args: mods_variable_dict (dict): Dicitionary with modifications. The key is AA, and value is the modified form (e.g. oxM). peptide (str): the peptide sequence to generate modified forms. isoforms_max (int): max number of modified forms to generate per peptide. n_modifications_max (int, optional): max number of variable modifications per peptide. Returns: list (of str): the list of peptide forms for the given peptide\n\nsource\n\n\nadd_variable_mod\n\n add_variable_mod (peps:list, mods_variable_dict:dict)\n\nFunction to add variable modification to a list of peptides. Args: peps (list): List of peptides. mods_variable_dict (dict): Dicitionary with modifications. The key is AA, and value is the modified form (e.g. oxM). Returns: list : the list of peptide forms for the given peptide.\n\nmods_variable_dict = {'S':'pS','P':'pP','M':'oxM'}\nisoforms_max = 1024\n\nprint(get_isoforms(mods_variable_dict, 'PEPTIDE', isoforms_max))\nprint(get_isoforms(mods_variable_dict, 'AMAMA', isoforms_max))\nprint(get_isoforms(mods_variable_dict, 'AMAMA', isoforms_max, n_modifications_max=1))\n\n['PEPTIDE', 'pPEPTIDE', 'PEpPTIDE', 'pPEpPTIDE']\n['AMAMA', 'AoxMAMA', 'AMAoxMA', 'AoxMAoxMA']\n['AMAMA', 'AoxMAMA', 'AMAoxMA']\n\n\nLastly, we define the wrapper add_variable_mods so that the functions can be called for lists of peptides and a list of variable modifications.\n\nsource\n\n\nadd_variable_mods\n\n add_variable_mods (peptide_list:list, mods_variable:list,\n                    isoforms_max:int, n_modifications_max:int, **kwargs)\n\nAdd variable modifications to the peptide list Args: peptide_list (list of str): peptide list. mods_variable (list of str): modification list. isoforms_max (int): max number of modified forms per peptide sequence. n_modifications_max (int): max number of variable modifications per peptide. Returns: list (of str): list of modified sequences for the given peptide list.\n\npeptide_list = ['AMA', 'AAC']\nmods_variable = ['oxM','amC']\nisoforms_max = 1024\nn_modifications_max = 10\n\nadd_variable_mods(peptide_list, mods_variable, isoforms_max, n_modifications_max)\n\n['AMA', 'AoxMA', 'AAC', 'AAamC']\n\n\n\n\nTerminal Modifications - Fixed\nTo handle terminal modifications, we use the following convention:\n\n< for the left side (N-terminal)\n> for the right side (C-Terminal)\n\nAdditionally, if we want to have a terminal modification on any AA we indicate this ^.\n\nsource\n\n\nadd_fixed_mods_terminal\n\n add_fixed_mods_terminal (peptides:list, mods_fixed_terminal:list,\n                          **kwargs)\n\nWrapper to add fixed mods on sequences and lists of mods Args: peptides (list of str): peptide list. mods_fixed_terminal (list of str): list of fixed terminal mods. Raises: “Invalid fixed terminal modification {mod}” exception for the given mod. Returns: list (of str): list of peptides with modification added.\n\nsource\n\n\nadd_fixed_mod_terminal\n\n add_fixed_mod_terminal (peptides:list, mod:str)\n\nAdds fixed terminal modifications Args: peptides (list of str): peptide list. mod (str): n-term mod contains ‘<^’ (e.g. a<^ for Acetyl@N-term); c-term mod contains ‘>^’. Raises: “Invalid fixed terminal modification ‘mod name’” for the given mod. Returns: list (of str): list of peptides with modification added.\n\npeptide = ['AMAMA']\n\nprint(f'Starting with peptide {peptide}')\nprint('Any n-term modified with x (x<^):', add_fixed_mods_terminal(peptide, ['x<^']))\nprint('Any c-term modified with x (x>^):', add_fixed_mods_terminal(peptide, ['x>^']))\nprint('Only A on n-term modified with x (x<A):', add_fixed_mods_terminal(peptide, ['x<A']))\nprint('Only A on c-term modified with x (x<A):', add_fixed_mods_terminal(peptide, ['x>A']))\n\nStarting with peptide ['AMAMA']\nAny n-term modified with x (x<^): ['xAMAMA']\nAny c-term modified with x (x>^): ['AMAMxA']\nOnly A on n-term modified with x (x<A): ['xAMAMA']\nOnly A on c-term modified with x (x<A): ['AMAMxA']\n\n\n\n\nTerminal Modifications - Variable\nLastly, to handle terminal variable modifications, we use the function add_variable_mods_terminal. As the modification can only be at the terminal end, this function only adds a peptide where the terminal end is modified.\n\nsource\n\n\nget_unique_peptides\n\n get_unique_peptides (peptides:list)\n\nFunction to return unique elements from list. Args: peptides (list of str): peptide list. Returns: list (of str): list of peptides (unique).\n\nsource\n\n\nadd_variable_mods_terminal\n\n add_variable_mods_terminal (peptides:list, mods_variable_terminal:list,\n                             **kwargs)\n\nFunction to add variable terminal modifications. Args: peptides (list of str): peptide list. mods_variable_terminal (list of str): list of variable terminal mods. Returns: list (of str): list of peptides with modification added.\n\npeptide_list = ['AMAMA']\nadd_variable_mods_terminal(peptide_list, ['x<^'])\n\n['xAMAMA', 'AMAMA']\n\n\n\n\nGenerating Peptides\nLastly, we put all the functions into a wrapper generate_peptides. It will accept a peptide and a dictionary with settings so that we can get all modified peptides.\n\nsource\n\n\ncheck_peptide\n\n check_peptide (peptide:str, AAs:set)\n\nCheck if the peptide contains non-AA letters. Args: peptide (str): peptide sequence. AAs (set): the set of legal amino acids. See alphapept.constants.AAs Returns: bool: True if all letters in the peptide is the subset of AAs, otherwise False\n\nsource\n\n\ngenerate_peptides\n\n generate_peptides (peptide:str, **kwargs)\n\nWrapper to get modified peptides (fixed and variable mods) from a peptide.\nArgs: peptide (str): the given peptide sequence. Returns: list (of str): all modified peptides.\nTODO: There can be some edge-cases which are not defined yet. Example: Setting the same fixed modification - once for all peptides and once for only terminal for the protein. The modification will then be applied twice.\n\nkwargs = {}\n\nkwargs[\"protease\"] = \"trypsin\"\nkwargs[\"n_missed_cleavages\"] = 2\nkwargs[\"pep_length_min\"] = 6\nkwargs[\"pep_length_max\"] = 27\nkwargs[\"mods_variable\"] = [\"oxM\"]\nkwargs[\"mods_variable_terminal\"] = []\nkwargs[\"mods_fixed\"] = [\"cC\"]\nkwargs[\"mods_fixed_terminal\"] = []\nkwargs[\"mods_fixed_terminal_prot\"] = []\nkwargs[\"mods_variable_terminal_prot\"]  = ['a<^']\nkwargs[\"isoforms_max\"] = 1024\nkwargs[\"n_modifications_max\"] = None\n\ngenerate_peptides('PEPTIDEM', **kwargs)\n\n['aPEPTIDEM',\n 'aPEPTIDEoxM',\n 'MEDITPEaP_decoy',\n 'oxMEDITPEaP_decoy',\n 'PEPTIDEM',\n 'PEPTIDEoxM',\n 'MEDITPEP_decoy',\n 'oxMEDITPEP_decoy']"
  },
  {
    "objectID": "fasta.html#mass-calculations",
    "href": "fasta.html#mass-calculations",
    "title": "FASTA",
    "section": "Mass Calculations",
    "text": "Mass Calculations\nUsing the mass_dict from constants and being able to parse sequences with parse, one can simply look up the masses for each modified or unmodified amino acid and add everything up.\n\nPrecursor\nTo calculate the mass of the neutral precursor, we start with the mass of an \\(H_2O\\) and add the masses of all amino acids of the sequence.\n\nsource\n\n\nget_precmass\n\n get_precmass (parsed_pep:list, mass_dict:numba.typed.typeddict.Dict)\n\nCalculate the mass of the neutral precursor Args: parsed_pep (list or numba.typed.List of str): the list of amino acids and modified amono acids. mass_dict (numba.typed.Dict): key is the amino acid or the modified amino acid, and the value is the mass. Returns: float: the peptide neutral mass.\n\nget_precmass(parse('PEPTIDE'), constants.mass_dict)\n\n799.35996420346\n\n\n\n\nFragments\nLikewise, we can calculate the masses of the fragment ions. We employ two functions: get_fragmass and get_frag_dict.\nget_fragmass is a fast, numba-compatible function that calculates the fragment masses and returns an array indicating whether the ion-type was b or y.\nget_frag_dict instead is not numba-compatible and hence a bit slower. It returns a dictionary with the respective ion and can be used for plotting theoretical spectra.\n\nsource\n\n\nget_fragmass\n\n get_fragmass (parsed_pep:list, mass_dict:numba.typed.typeddict.Dict)\n\nCalculate the masses of the fragment ions Args: parsed_pep (numba.typed.List of str): the list of amino acids and modified amono acids. mass_dict (numba.typed.Dict): key is the amino acid or the modified amino acid, and the value is the mass. Returns: Tuple[np.ndarray(np.float64), np.ndarray(np.int8)]: the fragment masses and the fragment types (represented as np.int8). For a fragment type, positive value means the b-ion, the value indicates the position (b1, b2, b3…); the negative value means the y-ion, the absolute value indicates the position (y1, y2, …).\n\nget_fragmass(parse('PEPTIDE'), constants.mass_dict)\n\n(array([ 98.06004033, 227.10263343, 324.15539729, 425.20307579,\n        538.28713979, 653.31408289, 148.06043425, 263.08737735,\n        376.17144135, 477.21911985, 574.27188371, 703.31447681]),\n array([ 1,  2,  3,  4,  5,  6, -1, -2, -3, -4, -5, -6], dtype=int8))\n\n\n\nsource\n\n\nget_frag_dict\n\n get_frag_dict (parsed_pep:list, mass_dict:dict)\n\nCalculate the masses of the fragment ions Args: parsed_pep (list or numba.typed.List of str): the list of amino acids and modified amono acids. mass_dict (numba.typed.Dict): key is the amino acid or the modified amino acid, and the value is the mass. Returns: dict{str:float}: key is the fragment type (b1, b2, …, y1, y2, …), value is fragment mass.\n\nget_frag_dict(parse('PEPTIDE'), constants.mass_dict)\n\n{'b1': 98.06004032687,\n 'b2': 227.10263342687,\n 'b3': 324.15539728686997,\n 'b4': 425.20307578687,\n 'b5': 538.28713978687,\n 'b6': 653.31408288687,\n 'y1': 148.06043425033,\n 'y2': 263.08737735033,\n 'y3': 376.17144135033,\n 'y4': 477.21911985033,\n 'y5': 574.27188371033,\n 'y6': 703.31447681033}\n\n\nThis allows us also to generate the theoretical isotopes for a fragment:\n\nimport matplotlib.pyplot as plt\n\npeptide = 'PEPTIDE'\n\nfrag_dict = get_frag_dict(parse(peptide), constants.mass_dict)\n\ndb_frag = list(frag_dict.values())\ndb_int = [100 for _ in db_frag]\n\nplt.figure(figsize=(10,5))\nplt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n\nfor _ in frag_dict.keys():\n    plt.text(frag_dict[_], 104, _, fontsize=12, alpha = 0.8)\n    \nplt.title('Theoretical Spectrum for {}'.format(peptide))\nplt.xlabel('Mass')\nplt.ylabel('Intensity')\nplt.ylim([0,110])\nplt.show()\n\n\n\n\n\n\nSpectra\nThe function get_spectrum returns a tuple with the following content:\n\nprecursor mass\npeptide sequence\nfragmasses\nfragtypes\n\nLikewise, get_spectra returns a list of tuples. We employ a list of tuples here as this way, we can sort them easily by precursor mass.\n\nsource\n\n\nget_spectra\n\n get_spectra (peptides:numba.typed.typedlist.List,\n              mass_dict:numba.typed.typeddict.Dict)\n\nGet neutral peptide mass, fragment masses and fragment types for a list of peptides Args: peptides (list of str): the (modified) peptide list. mass_dict (numba.typed.Dict): key is the amino acid or modified amino acid, and the value is the mass. Raises: Unknown exception and pass. Returns: list of Tuple[float, str, np.ndarray(np.float64), np.ndarray(np.int8)]: See get_spectrum.\n\nsource\n\n\nget_spectrum\n\n get_spectrum (peptide:str, mass_dict:numba.typed.typeddict.Dict)\n\nGet neutral peptide mass, fragment masses and fragment types for a peptide Args: peptide (str): the (modified) peptide. mass_dict (numba.typed.Dict): key is the amino acid or modified amino acid, and the value is the mass. Returns: Tuple[float, str, np.ndarray(np.float64), np.ndarray(np.int8)]: (peptide mass, peptide, fragment masses, fragment_types), for fragment types, see get_fragmass.\n\nprint(get_spectra(List(['PEPTIDE']), constants.mass_dict))\n\n[(799.35996420346, 'PEPTIDE', array([ 98.06004033, 148.06043425, 227.10263343, 263.08737735,\n       324.15539729, 376.17144135, 425.20307579, 477.21911985,\n       538.28713979, 574.27188371, 653.31408289, 703.31447681]), array([ 1, -1,  2, -2,  3, -3,  4, -4,  5, -5,  6, -6], dtype=int8)), ...]"
  },
  {
    "objectID": "fasta.html#reading-fasta",
    "href": "fasta.html#reading-fasta",
    "title": "FASTA",
    "section": "Reading FASTA",
    "text": "Reading FASTA\nTo read FASTA files, we use the SeqIO module from the Biopython library. This is a generator expression so that we read one FASTA entry after another until the StopIteration is reached, which is implemented in read_fasta_file. Additionally, we define the function read_fasta_file_entries that simply counts the number of FASTA entries.\nAll FASTA entries that contain AAs which are not in the mass_dict can be checked with check_sequence and will be ignored.\n\nsource\n\ncheck_sequence\n\n check_sequence (element:dict, AAs:set, verbose:bool=False)\n\nChecks wheter a sequence from a FASTA entry contains valid AAs Args: element (dict): fasta entry of the protein information. AAs (set): a set of amino acid letters. verbose (bool): logging the invalid amino acids. Returns: bool: False if the protein sequence contains non-AA letters, otherwise True.\n\nsource\n\n\nread_fasta_file_entries\n\n read_fasta_file_entries (fasta_filename='')\n\nFunction to count entries in fasta file Args: fasta_filename (str): fasta. Returns: int: number of entries.\n\nsource\n\n\nread_fasta_file\n\n read_fasta_file (fasta_filename:str='')\n\nRead a FASTA file line by line Args: fasta_filename (str): fasta. Yields: dict {id:str, name:str, description:str, sequence:str}: protein information.\n\n#load example fasta file\n\nfasta_path = '../testfiles/test.fasta'\n\nlist(read_fasta_file(fasta_path))[0]\n\n{'id': 'A0PJZ0',\n 'name': 'sp|A0PJZ0|A20A5_HUMAN',\n 'description': 'sp|A0PJZ0|A20A5_HUMAN Putative ankyrin repeat domain-containing protein 20A5 OS=Homo sapiens OX=9606 GN=ANKRD20A5P PE=5 SV=1',\n 'sequence': 'MKLFGFRSRRGQTVLGSIDHLYTGSGYRIRYSELQKIHKAAVKGDAAEMERCLARRSGDLDALDKQHRTALHLACASGHVKVVTLLVNRKCQIDIYDKENRTPLIQAVHCQEEACAVILLEHGANPNLKDIYGNTALHYAVYSESTSLAEKLLFHGENIEALDKV'}"
  },
  {
    "objectID": "fasta.html#peptide-dictionary",
    "href": "fasta.html#peptide-dictionary",
    "title": "FASTA",
    "section": "Peptide Dictionary",
    "text": "Peptide Dictionary\nIn order to efficiently store peptides, we rely on the Python dictionary. The idea is to have a dictionary with peptides as keys and indices to proteins as values. This way, one can quickly look up to which protein a peptide belongs to. The function add_to_pept_dict uses a regular python dictionary and allows to add peptides and stores indices to the originating proteins as a list. If a peptide is already present in the dictionary, the list is appended. The function returns a list of added_peptides, which were not present in the dictionary yet. One can use the function merge_pept_dicts to merge multiple peptide dicts.\n\nsource\n\nadd_to_pept_dict\n\n add_to_pept_dict (pept_dict:dict, new_peptides:list, i:int)\n\nAdd peptides to the peptide dictionary Args: pept_dict (dict): the key is peptide sequence, and the value is protein id list indicating where the peptide is from. new_peptides (list): the list of peptides to be added to pept_dict. i (int): the protein id where new_peptides are from. Returns: dict: same as the pept_dict in the arguments. list (of str): the peptides from new_peptides that not in the pept_dict.\n\npept_dict = {}\nnew_peptides = ['ABC','DEF']\n\npept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 0)\n\nnew_peptides = ['DEF','GHI']\n\npept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 1)\n\nprint(pept_dict)\n\n{'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n\n\n\nsource\n\n\nmerge_pept_dicts\n\n merge_pept_dicts (list_of_pept_dicts:list)\n\nMerge a list of peptide dict into a single dict. Args: list_of_pept_dicts (list of dict): the key of the pept_dict is peptide sequence, and the value is protein id list indicating where the peptide is from. Returns: dict: the key is peptide sequence, and the value is protein id list indicating where the peptide is from.\n\npept_dict_1 = {'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\npept_dict_2 = {'ABC': [3,4], 'JKL': [5, 6], 'MNO': [7]}\n\nmerge_pept_dicts([pept_dict_1, pept_dict_2])\n\n{'ABC': [0, 3, 4], 'DEF': [0, 1], 'GHI': [1], 'JKL': [5, 6], 'MNO': [7]}"
  },
  {
    "objectID": "fasta.html#generating-a-database",
    "href": "fasta.html#generating-a-database",
    "title": "FASTA",
    "section": "Generating a database",
    "text": "Generating a database\nTo wrap everything up, we employ two functions, generate_database and generate_spectra. The first one reads a FASTA file and generates a list of peptides, as well as the peptide dictionary and an ordered FASTA dictionary to be able to look up the protein indices later. For the callback we first read the whole FASTA file to determine the total number of entries in the FASTA file. For a typical FASTA file of 30 Mb with 40k entries, this should take less than a second. The progress of the digestion is monitored by processing the FASTA file one by one. The function generate_spectra then calculates precursor masses and fragment ions. Here, we split the total_number of sequences in 1000 steps to be able to track progress with the callback.\n\nsource\n\ngenerate_fasta_list\n\n generate_fasta_list (fasta_paths:list, callback=None, **kwargs)\n\nFunction to generate a database from a fasta file Args: fasta_paths (str or list of str): fasta path or a list of fasta paths. callback (function, optional): callback function. Returns: fasta_list (list of dict): list of protein entry dict {id:str, name:str, description:str, sequence:str}. fasta_dict (dict{int:dict}): the key is the protein id, the value is the protein entry dict.\n\nsource\n\n\ngenerate_database\n\n generate_database (mass_dict:dict, fasta_paths:list, callback=None,\n                    **kwargs)\n\nFunction to generate a database from a fasta file Args: mass_dict (dict): not used, will be removed in the future. fasta_paths (str or list of str): fasta path or a list of fasta paths. callback (function, optional): callback function. Returns: to_add (list of str): non-redundant (modified) peptides to be added. pept_dict (dict{str:list of int}): the key is peptide sequence, and the value is protein id list indicating where the peptide is from. fasta_dict (dict{int:dict}): the key is the protein id, the value is the protein entry dict {id:str, name:str, description:str, sequence:str}.\n\nsource\n\n\ngenerate_spectra\n\n generate_spectra (to_add:list, mass_dict:dict, callback=None)\n\nFunction to generate spectra list database from a fasta file Args: to_add (list): mass_dict (dict{str:float}): amino acid mass dict. callback (function, optional): callback function. (Default: None) Returns: list (of tuple): list of (peptide mass, peptide, fragment masses, fragment_types), see get_fragmass."
  },
  {
    "objectID": "fasta.html#parallelized-version",
    "href": "fasta.html#parallelized-version",
    "title": "FASTA",
    "section": "Parallelized version",
    "text": "Parallelized version\nTo speed up spectra generated, one can use the parallelized version. The function generate_database_parallel reads an entire FASTA file and splits it into multiple blocks. Each block will be processed, and the generated pept_dicts will be merged.\n\nsource\n\nblocks\n\n blocks (l:int, n:int)\n\nHelper function to create blocks from a given list Args: l (list): the list n (int): size per block Returns: Generator: List with splitted elements\n\nsource\n\n\nblock_idx\n\n block_idx (len_list:int, block_size:int=1000)\n\nHelper function to split length into blocks Args: len_list (int): list length. block_size (int, optional, default 1000): size per block. Returns: list[(int, int)]: list of (start, end) positions of blocks.\n\nsource\n\n\ngenerate_database_parallel\n\n generate_database_parallel (settings:dict, callback=None)\n\nFunction to generate a database from a fasta file in parallel. Args: settings: alphapept settings. Returns: list: theoretical spectra. See generate_spectra() dict: peptide dict. See add_to_pept_dict() dict: fasta_dict. See generate_fasta_list()\n\nsource\n\n\ndigest_fasta_block\n\n digest_fasta_block (to_process:tuple)\n\nDigest and create spectra for a whole fasta_block for multiprocessing. See generate_database_parallel.\n\n\nParallel search on large files\nIn some cases (e.g., a lot of modifications or very large FASTA files), it will not be useful to save the database as it will consume too much memory. Here, we use the function search_parallel from search. It creates theoretical spectra on the fly and directly searches against them. As we cannot create a pept_dict here, we need to create one from the search results. For this, we group peptides by their FASTA index and generate a lookup dictionary that can be used as a pept_dict.\n\nNote that we are passing the settings argument here. Search results should be stored in the corresponding path in the *.hdf file.\n\n\nsource\n\n\npept_dict_from_search\n\n pept_dict_from_search (settings:dict)\n\nGenerates a peptide dict from a large search."
  },
  {
    "objectID": "fasta.html#saving",
    "href": "fasta.html#saving",
    "title": "FASTA",
    "section": "Saving",
    "text": "Saving\nTo save the generated spectra, we rely on the HDF format. For this, we create a dictionary and save all the generated elements. The container will contain the following elements:\n\nprecursors: An array containing the precursor masses\nseqs: An array containing the peptide sequences for the precursor masses\npept_dict: A peptide dictionary to look up the peptides and return their FASTA index\nfasta_dict: A FASTA dictionary to look up the FASTA entry based on a pept_dict index\nfragmasses: An array containing the fragment masses. Unoccupied cells are filled with -1\nfragtypes:: An array containing the fragment types. 0 equals b-ions, and 1 equals y-ions. Unoccupied cells are filled with -1\nbounds: An integer array containing the upper bounds for the fragment masses/types array. This is needed to quickly slice the data.\n\nAll arrays are sorted according to the precursor mass.\n\n\n\n\n\n\nNote\n\n\n\nTo access the dictionaries such as pept_dict or fasta_dict, one needs to extract them using the .item() method like so: container[\"pept_dict\"].item().\n\n\n\nsource\n\nsave_database\n\n save_database (spectra:list, pept_dict:dict, fasta_dict:dict,\n                database_path:str, **kwargs)\n\nFunction to save a database to the *.hdf format. Write the database into hdf.\nArgs: spectra (list): list: theoretical spectra. See generate_spectra(). pept_dict (dict): peptide dict. See add_to_pept_dict(). fasta_dict (dict): fasta_dict. See generate_fasta_list(). database_path (str): Path to database.\n\nsource\n\n\nread_database\n\n read_database (database_path:str, array_name:str=None)\n\nRead database from hdf file. Args: database_path (str): hdf database file generate by alphapept. array_name (str): the dataset name to read return: dict: key is the dataset_name in hdf file, value is the python object read from the dataset_name"
  },
  {
    "objectID": "constants.html",
    "href": "constants.html",
    "title": "Constants",
    "section": "",
    "text": "This notebook stores all constants."
  },
  {
    "objectID": "constants.html#amino-acids",
    "href": "constants.html#amino-acids",
    "title": "Constants",
    "section": "Amino Acids",
    "text": "Amino Acids\nA set of valid amino acids.\n\nprint(AAs)\n\n{'U', 'R', 'S', 'M', 'P', 'I', 'E', 'H', 'L', 'N', 'C', 'G', 'D', 'W', 'K', 'F', 'T', 'V', 'A', 'Q', 'Y'}"
  },
  {
    "objectID": "constants.html#mass-dict",
    "href": "constants.html#mass-dict",
    "title": "Constants",
    "section": "Mass dict",
    "text": "Mass dict\nA numba compatible mass dict dictionary. This is created from the modifications.tsv. Change to allow custom modifications.\n\nsource\n\nget_mass_dict\n\n get_mass_dict (modfile:str, aasfile:str, verbose:bool=True)\n\nFunction to create a mass dict based on tsv files. This is used to create the hardcoded dict in the constants notebook. The dict needs to be hardcoded because of importing restrictions when using numba. More specifically, a global needs to be typed at runtime.\nArgs: modfile (str): Filename of modifications file. aasfile (str): Filename of AAs file. verbose (bool, optional): Flag to print dict.\nReturns: Returns a numba compatible dictionary with masses.\nRaises: FileNotFoundError: If files are not found.\n\nfor _ in mass_dict:\n    print(f\"{_+':': <12}{mass_dict[_]}\")\n\nA:          71.0371138\nC:          103.0091845\nD:          115.0269431\nE:          129.0425931\nF:          147.0684139\nG:          57.02146373\nH:          137.0589119\nI:          113.084064\nK:          128.094963\nL:          113.084064\nM:          131.0404846\nN:          114.0429275\nP:          97.05276386\nQ:          128.0585775\nR:          156.101111\nS:          87.03202843\nT:          101.0476785\nU:          150.9536333957\nV:          99.06841392\nW:          186.079313\nY:          163.0633286\ncC:         160.03064823\noxM:        147.03539923000002\naA:         113.04767849000001\naC:         145.01974919\naD:         157.03750779\naE:         171.05315779\naF:         189.07897859\naG:         99.03202842\naH:         179.06947659\naI:         155.09462869\naK:         170.10552769\naL:         155.09462869\naM:         173.05104929\naN:         156.05349219000001\naP:         139.06332855\naQ:         170.06914219\naR:         198.11167569\naS:         129.04259312\naT:         143.05824319\naU:         192.9641980857\naV:         141.07897861\naW:         228.08987769\naY:         205.07389329\namA:        70.053098207\namC:        102.02516890700001\namD:        114.042927507\namE:        128.058577507\namF:        146.084398307\namG:        56.037448137\namH:        136.074896307\namI:        112.100048407\namK:        127.11094740700001\namL:        112.100048407\namM:        130.056469007\namN:        113.05891190700001\namP:        96.068748267\namQ:        127.07456190700002\namR:        155.117095407\namS:        86.048012837\namT:        100.06366290700001\namU:        149.9696178027\namV:        98.084398327\namW:        185.095297407\namY:        162.079313007\npS:         166.99835935\npT:         181.01400942\npY:         243.02965952\ndeamN:      115.026943093\ndeamQ:      129.04259309300002\ncmC:        85.9826354\npgE:        111.03202841000001\npgQ:        111.03202840000002\ntmt0A:      295.1895917\ntmt0C:      327.1616624\ntmt0D:      339.179421\ntmt0E:      353.195071\ntmt0F:      371.2208918\ntmt0G:      281.17394163\ntmt0H:      361.2113898\ntmt0I:      337.2365419\ntmt0K:      352.2474409\ntmt0L:      337.2365419\ntmt0M:      355.1929625\ntmt0N:      338.1954054\ntmt0P:      321.20524176000004\ntmt0Q:      352.2110554\ntmt0R:      380.2535889\ntmt0S:      311.18450633\ntmt0T:      325.2001564\ntmt0U:      375.1061112957\ntmt0V:      323.22089182\ntmt0W:      410.2317909\ntmt0Y:      387.2158065\ntmt2A:      296.1929466\ntmt2C:      328.16501730000004\ntmt2D:      340.1827759\ntmt2E:      354.1984259\ntmt2F:      372.2242467\ntmt2G:      282.17729653000004\ntmt2H:      362.2147447\ntmt2I:      338.2398968\ntmt2K:      353.2507958\ntmt2L:      338.2398968\ntmt2M:      356.1963174\ntmt2N:      339.1987603\ntmt2P:      322.20859666\ntmt2Q:      353.21441030000005\ntmt2R:      381.25694380000004\ntmt2S:      312.18786123\ntmt2T:      326.2035113\ntmt2U:      376.1094661957\ntmt2V:      324.22424672\ntmt2W:      411.23514580000005\ntmt2Y:      388.2191614\ntmt6A:      300.200046\ntmt6C:      332.1721167\ntmt6D:      344.1898753\ntmt6E:      358.2055253\ntmt6F:      376.2313461\ntmt6G:      286.18439593\ntmt6H:      366.2218441\ntmt6I:      342.2469962\ntmt6K:      357.2578952\ntmt6L:      342.2469962\ntmt6M:      360.2034168\ntmt6N:      343.2058597\ntmt6P:      326.21569606\ntmt6Q:      357.2215097\ntmt6R:      385.2640432\ntmt6S:      316.19496062999997\ntmt6T:      330.2106107\ntmt6U:      380.11656559569997\ntmt6V:      328.23134612\ntmt6W:      415.2422452\ntmt6Y:      392.2262608\nitraq4KA:   215.13952419999998\nitraq4KC:   247.1115949\nitraq4KD:   259.1293535\nitraq4KE:   273.14500350000003\nitraq4KF:   291.1708243\nitraq4KG:   201.12387413\nitraq4KH:   281.1613223\nitraq4KI:   257.1864744\nitraq4KK:   272.1973734\nitraq4KL:   257.1864744\nitraq4KM:   275.142895\nitraq4KN:   258.1453379\nitraq4KP:   241.15517426\nitraq4KQ:   272.1609879\nitraq4KR:   300.2035214\nitraq4KS:   231.13443883\nitraq4KT:   245.15008890000001\nitraq4KU:   295.0560437957\nitraq4KV:   243.17082432\nitraq4KW:   330.1817234\nitraq4KY:   307.16573900000003\nitraq4K:    272.1973734\nitraq4Y:    307.16573900000003\nitraq8KA:   375.2393133\nitraq8KC:   407.211384\nitraq8KD:   419.2291426\nitraq8KE:   433.2447926\nitraq8KF:   451.2706134\nitraq8KG:   361.22366323\nitraq8KH:   441.2611114\nitraq8KI:   417.2862635\nitraq8KK:   432.2971625\nitraq8KL:   417.2862635\nitraq8KM:   435.2426841\nitraq8KN:   418.245127\nitraq8KP:   401.25496336000003\nitraq8KQ:   432.260777\nitraq8KR:   460.3033105\nitraq8KS:   391.23422793\nitraq8KT:   405.249878\nitraq8KU:   455.1558328957\nitraq8KV:   403.27061342\nitraq8KW:   490.2815125\nitraq8KY:   467.2655281\nitraq8K:    432.2971625\nitraq8Y:    467.2655281\neA:         337.1209813\neC:         369.093052\neD:         381.1108106\neE:         395.1264606\neF:         413.1522814\neG:         323.10533123\neH:         403.1427794\neI:         379.1679315\neK:         394.1788305\neL:         379.1679315\neM:         397.1243521\neN:         380.126795\neP:         363.13663136\neQ:         394.142445\neR:         422.1849785\neS:         353.11589592999997\neT:         367.131546\neU:         417.03750089569996\neV:         365.15228142\neW:         452.1631805\neY:         429.1471961\narg10R:     166.10938057776002\narg6R:      162.121241\nlys8K:      136.10916278888\nElectron:   0.00054857990907\nProton:     1.00727646687\nHydrogen:   1.00782503223\nC13:        13.003354835\nOxygen:     15.994914619\nOH:         17.002739651229998\nH2O:        18.01056468346\nNH3:        17.02654910112\ndelta_M:    1.00286864\ndelta_S:    0.0109135\n\n\n\n# Test that there is an entry for each AA\nfor _ in AAs:\n    assert _ in mass_dict.keys()\n    \nprint(mass_dict['A'])\nprint(mass_dict['K'])\n\n71.0371138\n128.094963"
  },
  {
    "objectID": "constants.html#isotopes",
    "href": "constants.html#isotopes",
    "title": "Constants",
    "section": "Isotopes",
    "text": "Isotopes\nWe define a jitclass that stores the base mass, the number of isotopes, and their abundances. We create the typed dictionary isotopes that stores different default isotopes.\n\n\nIsotope\n\n Isotope (*args, **kwargs)\n\nJit-compatible class to store isotopes\nAttributes: m0 (int): Mass of pattern dm0 (int): dm of pattern (number of isotopes) int0 (np.float32[:]): Intensities of pattern\n\nfor _ in isotopes:\n    print(f'Element {_}: base mass {isotopes[_].m0:<20} intensities {isotopes[_].intensities}')\n\nElement C: base mass 12.0                 intensities [0.9893 0.0107 0.    ]\nElement H: base mass 1.0079400539398193   intensities [9.99885e-01 1.15000e-04 0.00000e+00]\nElement O: base mass 15.994915008544922   intensities [9.9757e-01 3.8000e-04 2.0500e-03]\nElement N: base mass 14.003073692321777   intensities [0.99636 0.00364]\nElement S: base mass 31.972070693969727   intensities [9.499e-01 7.500e-03 4.250e-02 1.000e-04]\nElement I: base mass 126.90447235107422   intensities [1.]\nElement K: base mass 38.963706970214844   intensities [9.32581e-01 1.17000e-04 6.73020e-02]"
  },
  {
    "objectID": "constants.html#averagine-model",
    "href": "constants.html#averagine-model",
    "title": "Constants",
    "section": "Averagine Model",
    "text": "Averagine Model\n\n#Masses of the averagine model\nfor _ in averagine_aa:\n    print(f\"{_} {averagine_aa[_]}\")\n\nC 4.9384\nH 7.7583\nN 1.3577\nO 1.4773\nS 0.0417"
  },
  {
    "objectID": "constants.html#protease-dict",
    "href": "constants.html#protease-dict",
    "title": "Constants",
    "section": "Protease dict",
    "text": "Protease dict\nA numba compatible dictionary that stores different regular expressions needed for digestion. The dictionary was largely taken from the Pyteomics website which in turn derived the rules are from expasy. Some entries (Lys-C/ Lys-N) were updated according to OpenMS. A useful resource for testing Regex can be found here.\n\n#Entries in the protease_dict:\nfor _ in protease_dict:\n    print(f\"{_:<35} {protease_dict[_]}\")\n\narg-c                               R\nasp-n                               \\w(?=D)\nbnps-skatole                        W\ncaspase 1                           (?<=[FWYL]\\w[HAT])D(?=[^PEDQKR])\ncaspase 2                           (?<=DVA)D(?=[^PEDQKR])\ncaspase 3                           (?<=DMQ)D(?=[^PEDQKR])\ncaspase 4                           (?<=LEV)D(?=[^PEDQKR])\ncaspase 5                           (?<=[LW]EH)D\ncaspase 6                           (?<=VE[HI])D(?=[^PEDQKR])\ncaspase 7                           (?<=DEV)D(?=[^PEDQKR])\ncaspase 8                           (?<=[IL]ET)D(?=[^PEDQKR])\ncaspase 9                           (?<=LEH)D\ncaspase 10                          (?<=IEA)D\nchymotrypsin high specificity       ([FY](?=[^P]))|(W(?=[^MP]))\nchymotrypsin low specificity        ([FLY](?=[^P]))|(W(?=[^MP]))|(M(?=[^PY]))|(H(?=[^DMPW]))\nclostripain                         R\ncnbr                                M\nenterokinase                        (?<=[DE]{3})K\nfactor xa                           (?<=[AFGILTVM][DE]G)R\nformic acid                         D\nglutamyl endopeptidase              E\ngranzyme b                          (?<=IEP)D\nhydroxylamine                       N(?=G)\niodosobenzoic acid                  W\nlys_c                               K(?!P)\nlys_c/p                             K\nlys_n                               .K\nntcb                                \\w(?=C)\npepsin ph1.3                        ((?<=[^HKR][^P])[^R](?=[FL][^P]))|((?<=[^HKR][^P])[FL](?=\\w[^P]))\npepsin ph2.0                        ((?<=[^HKR][^P])[^R](?=[FLWY][^P]))|((?<=[^HKR][^P])[FLWY](?=\\w[^P]))\nproline endopeptidase               (?<=[HKR])P(?=[^P])\nproteinase k                        [AEFILTVWY]\nstaphylococcal peptidase i          (?<=[^E])E\nthermolysin                         [^DE](?=[AFILMV])\nthrombin                            ((?<=G)R(?=G))|((?<=[AFGILTVM][AFGILTVWA]P)R(?=[^DE][^DE]))\ntrypsin_full                        ([KR](?=[^P]))|((?<=W)K(?=P))|((?<=M)R(?=P))\ntrypsin_exception                   ((?<=[CD])K(?=D))|((?<=C)K(?=[HY]))|((?<=C)R(?=K))|((?<=R)R(?=[HR]))\nnon-specific                        ()\ntrypsin                             ([KR](?=[^P]))"
  },
  {
    "objectID": "constants.html#losses",
    "href": "constants.html#losses",
    "title": "Constants",
    "section": "Losses",
    "text": "Losses\n\n#Entries in the loss_dict:\nfor _ in loss_dict:\n    print(f\"{_:<5} {loss_dict[_]}\")\n\n      0.0\n-H2O  18.01056468346\n-NH3  17.03052"
  },
  {
    "objectID": "constants.html#labels",
    "href": "constants.html#labels",
    "title": "Constants",
    "section": "Labels",
    "text": "Labels\n\nfor label in label_dict:\n    print(label_dict[label])\n\nlabel(mod_name='tmt6', channels=['tmt10-126', 'tmt10-127N', 'tmt10-127C', 'tmt10-128N', 'tmt10-128C', 'tmt10-129N', 'tmt10-129C', 'tmt10-130N', 'tmt10-130C', 'tmt10-131', 'tmt10-131C'], masses=array([126.127726, 127.124761, 127.131081, 128.128116, 128.134436,\n       129.131471, 129.13779 , 130.134825, 130.141145, 131.13818 ,\n       131.144499]), reference_channel='tmt10-126', mods_fixed_terminal=['tmt6<^'], mods_variable=['tmt6Y', 'tmt6K'])"
  },
  {
    "objectID": "quantification.html",
    "href": "quantification.html",
    "title": "Quantification",
    "section": "",
    "text": "Algorithms related to label-free quantifications are motivated by the MaxLFQ paper. The main goal is to derive relative protein intensities that can be used for downstream analyses. In a first step, constant normalization coefficients are derived for each run. In a second step, pseudointensities are derived for each protein, such that differing conditions can be compared."
  },
  {
    "objectID": "quantification.html#delayed-normalization",
    "href": "quantification.html#delayed-normalization",
    "title": "Quantification",
    "section": "Delayed Normalization",
    "text": "Delayed Normalization\nDelayed normalization describes the process of normalizing the differences that occur from prefractionation as well as from sample handling. For each sample, a constant scaling factor is derived by minimizing the term \\[H(\\vec{N}) = \\sum_{P \\in peptides} \\sum_{A,B \\in sample pairs} |\\frac{I(N_A, P, A)}{I(N_B, P, B)}|, \\] with peptide intensities \\(I\\), which are determined by the peptide \\(P\\) the sample \\(A\\) or \\(B\\) and the normalization factors \\(N_A\\), \\(N_B\\). In principle H(N) quantifies the variation of peptides over the samples. Minimizing this variation gives appropriate scaling factors under the assumption that most peptides do not change between the samples. Peptide intensities for fractionated samples are described as the sum of the intensities over the fractions, with fraction-specific normalization factors. Therefore, calculation of the summed intensities is delayed until the normalization is finished.\n\nIn Silico Test data\nTo test the delayed normalization approach we create an in silico test dataset with a known ground truth. We therefore know, which systematic changes are between the samples and we employ different solvers to recover the normalization parameters.\n\nsource\n\n\ngaussian\n\n gaussian (mu:float, sigma:float, grid:numpy.ndarray)\n\nCalculates normally distributed probability densities along an input array.\nArgs: mu (float): mean of ND. sigma (float): standard deviation of ND. grid (np.ndarray): input array np.int[:]. For each element of the array, the probability density is calculated.\nReturns: np.ndarray: probability density array, np.float[:].\n\nsource\n\n\nreturn_elution_profile\n\n return_elution_profile (timepoint:float, sigma:float, n_runs:int)\n\nSimulates a gaussian elution profile.\nArgs: timepoint (float): coordinate of the peak apex. sigma (float): standard deviation of the gaussian. n_runs (int): number of points along which the density is calculated.\nReturns: np.ndarray: probability density array, np.float[:].\n\nsource\n\n\nsimulate_sample_profiles\n\n simulate_sample_profiles (n_peptides:int, n_runs:int, n_samples:int,\n                           threshold:float=0.2, use_noise:bool=True)\n\nGenerates random profiles to serve as test_data.\nArgs: n_peptides (int): number of peptides to be simulated. n_runs (int): number of runs to be simulated. n_samples (int): number of samples to be simulated. threshold (float, optional): threshold below which a simulated intensity will be discarded. Defaults to 0.2. use_noise (bool, optional): add simulated noise to the profile values. Defaults to True.\nReturns: Tuple[np.ndarray, np.ndarray]: profiles: np.float[:,:,:] array containing the simulated profiles, true_normalization: np.float[:,:,:] array containing the ground truth."
  },
  {
    "objectID": "quantification.html#delayed-normalization-1",
    "href": "quantification.html#delayed-normalization-1",
    "title": "Quantification",
    "section": "Delayed Normalization",
    "text": "Delayed Normalization\n\nsource\n\nget_total_error\n\n get_total_error (normalization:numpy.ndarray, profiles:numpy.ndarray)\n\nComputes the summed peptide errors over the whole dataset.\nArgs: normalization (np.ndarray): per sample normalization factors. profiles (np.ndarray): peptide intensity profiles over the dataset.\nReturns: float: summed peptide error."
  },
  {
    "objectID": "quantification.html#benchmarking-different-optimiziers",
    "href": "quantification.html#benchmarking-different-optimiziers",
    "title": "Quantification",
    "section": "Benchmarking different optimiziers",
    "text": "Benchmarking different optimiziers\nThe normalization step is in principle a quadratic minimization of the normalization factors. Such minimization problems can be solved in various ways and a variety of approaches are realized in python community packages. We compare different solvers using our benchmarking set and uncover substantial differences in precision and runtime. We observe that the Sequential Least Squares Quadratic Programming (SLSQP) approach is a robust solution in our benchmarking, which gives substantial speed improvements.\n\nfrom scipy.optimize import minimize\nfrom time import time\nfrom scipy.optimize import least_squares\nimport pandas as pd\nimport warnings\n\nn_peptides = 100\nn_runs = 10\nn_samples = 3\n\nprofiles, true_normalization = simulate_sample_profiles(n_peptides, n_runs, n_samples)\n\nmethods = ['L-BFGS-B', 'TNC', 'SLSQP','trf']\n\nresults = []\n\nfor method in methods:\n    \n    start = time()\n    \n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n    \n        if method in ['trf']:\n            x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n            bounds = (x0*0.1, x0)\n            res = least_squares(get_total_error, args = [profiles], bounds = bounds, x0 = x0*0.5, verbose=0, method = method)\n\n        else:\n            x0 = np.ones(profiles.shape[0] * profiles.shape[1])\n            bounds = [(0.1, 1) for _ in x0]\n            res = minimize(get_total_error, args = profiles , x0 = x0*0.5, bounds=bounds, method=method)\n\n    solution = res.x/np.max(res.x)\n    solution = solution.reshape(profiles.shape[:2])\n    \n    end = time()\n    \n    time_elapsed_min = (end-start)/60\n\n    optimality = get_total_error(solution, profiles) /get_total_error(x0, profiles)\n    optimality_ = get_total_error(solution, profiles) / get_total_error(true_normalization, profiles)\n    \n    results.append((method, time_elapsed_min, optimality, optimality_))\n    \npd.DataFrame(results, columns=['Method', 'Time Elapsed (min)','Error / Baseline Error','Error / Ground Truth'])\n\n\n\n\n\n  \n    \n      \n      Method\n      Time Elapsed (min)\n      Error / Baseline Error\n      Error / Ground Truth\n    \n  \n  \n    \n      0\n      L-BFGS-B\n      0.004574\n      0.709227\n      0.584782\n    \n    \n      1\n      TNC\n      0.005655\n      0.723398\n      0.596467\n    \n    \n      2\n      SLSQP\n      0.000702\n      0.709346\n      0.584880\n    \n    \n      3\n      trf\n      0.058068\n      0.709508\n      0.585013\n    \n  \n\n\n\n\n\nsource\n\nnormalize_experiment_SLSQP\n\n normalize_experiment_SLSQP (profiles:numpy.ndarray)\n\nCalculates normalization with SLSQP approach.\nArgs: profiles (np.ndarray): peptide intensities.\nReturns: np.ndarray: normalization factors.\n\nsource\n\n\nnormalize_experiment_BFGS\n\n normalize_experiment_BFGS (profiles:numpy.ndarray)\n\nCalculates normalization with BFGS approach.\nArgs: profiles (np.ndarray): peptide intensities.\nReturns: np.ndarray: normalization factors.\n\nsource\n\n\ndelayed_normalization\n\n delayed_normalization (df:pandas.core.frame.DataFrame,\n                        field:str='ms1_int_sum',\n                        minimum_occurence:bool=None)\n\nReturns normalization factors for given peptide intensities. If the solver does not converge, the unnormalized data will be used.\nArgs: df (pd.DataFrame): alphapept quantified features table. field (str, optional): The column in df containing the quantitative peptide information (i.e. precursor intensities). minimum_occurence (bool, optional): minimum number of replicates the peptide must be observed in. Defaults to None.\nReturns: [pd.DataFrame, np.ndarray]: pd.DataFrame: alphapept quantified features table extended with the normalized intensities, np.ndarray: normalized intensities\n\nsample_data = {}\n\nsample_data['precursor'] = ['Prec_1'] * 6 + ['Prec_2'] * 6 + ['Prec_3'] * 6\nsample_data['fraction'] = [1,1,2]*6\nsample_data['sample_group'] = ['A','A','A', 'B','B','B'] * 3\nsample_data['ms1_int_sum'] = [0.6, 0.8, 0.6, 1.2, 1.6, 1.2] * 3\n\ntest_df = pd.DataFrame(sample_data)\ntest_df, normalization = delayed_normalization(test_df, field='ms1_int_sum', minimum_occurence=0)\n\ndisplay(pd.DataFrame(normalization))\ndisplay(test_df.head(6))\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      1.0\n      0.5\n    \n    \n      1\n      1.0\n      0.5\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      precursor\n      fraction\n      sample_group\n      ms1_int_sum\n      ms1_int_sum_dn\n    \n  \n  \n    \n      0\n      Prec_1\n      1\n      A\n      0.6\n      0.9\n    \n    \n      1\n      Prec_1\n      1\n      A\n      0.8\n      1.2\n    \n    \n      2\n      Prec_1\n      2\n      A\n      0.6\n      0.9\n    \n    \n      3\n      Prec_1\n      1\n      B\n      1.2\n      0.9\n    \n    \n      4\n      Prec_1\n      1\n      B\n      1.6\n      1.2\n    \n    \n      5\n      Prec_1\n      2\n      B\n      1.2\n      0.9"
  },
  {
    "objectID": "quantification.html#constructing-protein-intensity-profiles",
    "href": "quantification.html#constructing-protein-intensity-profiles",
    "title": "Quantification",
    "section": "Constructing protein intensity profiles",
    "text": "Constructing protein intensity profiles\nProtein intensity profiles are constructed for each protein individually. All possible protein fold changes between the samples are derived from the median peptide fold changes. Subsequently, pseudointensities are chosen such that the fold changes between the pseudointensities ideally reconstruct the actually observed fold changes. Similar to the delayed normalization, this is formulated as a quadratic minimization, which we solve with the SLSQP solver.\nCodewise, we start with simulating in-silico test data to serve as a ground-truth for assessing solvers for the optimization problem. For the algorithmic optimization, we define the function get_protein_ratios that allows to quickly calculate the protein ratios. Next, we define an error function triangle_error that we use for the optimization problem. Lastly, we have several wrapper functions to access the functions."
  },
  {
    "objectID": "quantification.html#in-silico-test-data-1",
    "href": "quantification.html#in-silico-test-data-1",
    "title": "Quantification",
    "section": "In-silico test data",
    "text": "In-silico test data\nCreate a simulated input dataset of peptide intensities.\n\nsource\n\ngenerate_dummy_data\n\n generate_dummy_data (n_sequences:int, n_samples:int, noise:bool=True,\n                      remove:bool=True, peptide_ratio:bool=True,\n                      abundance:bool=True, signal_level:int=100,\n                      noise_divider:int=10, keep:float=0.8)\n\nSimulate an input dataset of peptide intensities.\nArgs: n_sequences (int): number of peptides to simulate. n_samples (int): number of samples to simulate. noise (bool, optional): add random signal to distort the simulated intensity levels. Defaults to True. remove (bool, optional): remove intensities (i.e. add missing values). Defaults to True. peptide_ratio (bool, optional): simulate different peptide intensities. Defaults to True. abundance (bool, optional): simulate different abundances for each sample (i.e. systematic shifts). Defaults to True. signal_level (int, optional): signal level for simulated intensity. Defaults to 100. noise_divider (int, optional): the factor through which the noise is divided (higher factor -> higher signal to noise). Defaults to 10. keep (float, optional): aimed-at fraction of non-missing values, applies if ‘remove’ is set. Defaults to 0.8.\nReturns: [pd.DataFrame, list, np.ndarray]: pd.DataFrame: simulated dataset with peptide intensities, list: sample names: np.ndarray: shift factors of each sample"
  },
  {
    "objectID": "quantification.html#determine-pair-wise-intenisty-ratios",
    "href": "quantification.html#determine-pair-wise-intenisty-ratios",
    "title": "Quantification",
    "section": "Determine pair-wise intenisty ratios",
    "text": "Determine pair-wise intenisty ratios\nThe pair-wise protein ratios are determined from the median peptide ratio.\n\nsource\n\nget_protein_ratios\n\n get_protein_ratios (signal:numpy.ndarray, column_combinations:list,\n                     minimum_ratios:int=1)\n\nCalculates the protein ratios between samples for one protein.\nArgs: signal (np.ndarray): np.array[:,:] containing peptide intensities for each sample. column_combinations (list): list of all index combinations to compare (usually all sample combinations). minimum_ratios (int, optional): minimum number of peptide ratios necessary to calculate a protein ratio. Defaults to 1.\nReturns: np.ndarray: np.array[:,:] matrix comparing the ratios for all column combinations."
  },
  {
    "objectID": "quantification.html#error-function",
    "href": "quantification.html#error-function",
    "title": "Quantification",
    "section": "Error Function",
    "text": "Error Function\nThe error function evaluates the difference between the actual observed fold change and the fold change that is derived from the pseudointensities.\n\nsource\n\ntriangle_error\n\n triangle_error (normalization:numpy.ndarray, ratios:numpy.ndarray)\n\nCalculates the difference between calculated ratios and expected ratios.\nArgs: normalization (np.ndarray): Used normalization. ratios (np.ndarray): Peptide ratios.\nReturns: float: summed quadratic difference."
  },
  {
    "objectID": "quantification.html#solver-implementation",
    "href": "quantification.html#solver-implementation",
    "title": "Quantification",
    "section": "Solver implementation",
    "text": "Solver implementation\nAs with the delayed normalization we implement multiple solvers from scipy.\n\nsource\n\nsolve_profile\n\n solve_profile (ratios:numpy.ndarray, method:str)\n\nCalculates protein pseudointensities with a specified solver. Args: ratios (np.ndarray): np.array[:,:] matrix containing all estimated protein ratios between samples. method (str): string specifying which solver to use. Raises: NotImplementedError: if the solver is not implemented. Returns: [np.ndarray, bool]: np.ndarray: the protein pseudointensities, bool: wether the solver was successful."
  },
  {
    "objectID": "quantification.html#solving-single-profiles",
    "href": "quantification.html#solving-single-profiles",
    "title": "Quantification",
    "section": "Solving single profiles",
    "text": "Solving single profiles\n\nsource\n\nprotein_profile\n\n protein_profile (files:list, minimum_ratios:int, chunk:tuple)\n\nFunction to extract optimal protein ratios for a given input of peptides.\nNote for the chunk argument: This construction is needed to call this function from a parallel pool.\nArgs: files (list): A list of files for which the profile shall be extracted. minimum_ratios (int): A minimum number of peptide ratios to be considered for optimization. chunk: (tuple[pd.DataFrame, str]): A pandas dataframe with the peptide information and a string to identify the protein.\nReturns: np.ndarray: optimized profile np.ndarray: profile w/o optimization str: protein identifier\n\nimport matplotlib.pyplot as plt\n\nsample_data = {}\n\nsample_data['precursor'] = ['Prec_1'] * 3 + ['Prec_2'] * 3 + ['Prec_3'] * 3\nsample_data['sample_group'] = ['A','B','C'] * 3\nsample_data['protein_group'] = ['X'] * 9\nsample_data['ms1_int_sum'] = [0.6, 0.8, 1.0, 0.6, 1.2, 1.4, 1.6, 1.2, 1.8]\n\ntest_df = pd.DataFrame(sample_data)\n\ndisplay(test_df.head(6))\n\ngrouped = test_df.groupby(['protein_group','sample_group','precursor']).sum().loc['X']\nfiles = ['A','B','C']\nminimum_ratios = 1\nchunk = (grouped, 'X')\n\nif False: #TODO: this test seems to break the CI\n    profile, pre_lfq, protein, success = protein_profile(files, minimum_ratios, chunk)\n\n    plt.figure(figsize=(5,5))\n    plt.title('Protein ratio')\n    plt.plot(pre_lfq, 'o', label='before optimization')\n    plt.plot(profile, 'o', label='after optimization')\n    plt.legend()\n    plt.show()\n\n\n\n\n\n  \n    \n      \n      precursor\n      sample_group\n      protein_group\n      ms1_int_sum\n    \n  \n  \n    \n      0\n      Prec_1\n      A\n      X\n      0.6\n    \n    \n      1\n      Prec_1\n      B\n      X\n      0.8\n    \n    \n      2\n      Prec_1\n      C\n      X\n      1.0\n    \n    \n      3\n      Prec_2\n      A\n      X\n      0.6\n    \n    \n      4\n      Prec_2\n      B\n      X\n      1.2\n    \n    \n      5\n      Prec_2\n      C\n      X\n      1.4"
  },
  {
    "objectID": "quantification.html#wrapper-functions",
    "href": "quantification.html#wrapper-functions",
    "title": "Quantification",
    "section": "Wrapper functions",
    "text": "Wrapper functions\nTo be compatible with interface, we have three wrapper functions:\n\nprotein_profile_parallel: A wrapper that executes protein_profile in parallel\nprotein_profile_parallel_ap: A wrapper function to calculate protein ratios based on AlphaPept tabular data\nprotein_profile_prallalel_mq: A wrapper function to calculate protein ratios based on MaxQuant tabular data\n\n\nsource\n\nprotein_profile_parallel\n\n protein_profile_parallel (df:pandas.core.frame.DataFrame,\n                           minimum_ratios:int, field:str, callback=None)\n\nDerives LFQ intensities from the feature table.\nArgs: df (pd.DataFrame): Feature table by alphapept. minimum_ratios (int): Minimum number of peptide ratios necessary to derive a protein ratio. field (str): The field containing the quantitative peptide information (i.e. precursor intensities). callback ([type], optional): Callback function. Defaults to None.\nReturns: pd.DataFrame: table containing the LFQ intensities of each protein in each sample.\n\nsource\n\n\nprotein_profile_parallel_mq\n\n protein_profile_parallel_mq (evidence_path:str, protein_groups_path:str,\n                              minimum_ratios:int=1,\n                              minimum_occurence:bool=None,\n                              delayed:bool=True, callback=None)\n\nDerives protein LFQ intensities from Maxquant quantified features.\nArgs: evidence_path (str): path to the Maxquant standard output table evidence.txt. protein_groups_path (str): path to the Maxquant standard output table proteinGroups.txt. minimum_ratios (int): minimum ratios (LFQ parameter) minimum_occurence (int): minimum occurence (LFQ parameter) delayed (bool): toggle for delayed normalization (on/off) callback ([type], optional): [description]. Defaults to None.\nRaises: FileNotFoundError: if Maxquant files cannot be found.\nReturns: pd.DataFrame: table containing the LFQ intensities of each protein in each sample.\n\nsource\n\n\nprotein_profile_parallel_ap\n\n protein_profile_parallel_ap (settings:dict,\n                              df:pandas.core.frame.DataFrame,\n                              callback=None)\n\nDerives protein LFQ intensities from the alphapept quantified feature table\nArgs: settings (dict): alphapept settings dictionary. df (pd.DataFrame): alphapept feature table. callback ([type], optional): [description]. Defaults to None.\nRaises: ValueError: raised in case of observed negative intensities.\nReturns: pd.DataFrame: table containing the LFQ intensities of each protein in each sample."
  },
  {
    "objectID": "display.html",
    "href": "display.html",
    "title": "Display",
    "section": "",
    "text": "Calculate the coverage of a target protein sequence by peptides in a given list\n\nsource\n\n\n\n calculate_sequence_coverage (target_sequence:str, peptide_list:list)\n\nCalculate the percentage of a target protein covered by a list of peptides. Args: target_sequence (str): the protein sequence against which the peptide_list should be compared. peptide_list (List[str]): the list of peptides (str) to be compared against the target_sequence. return: int: number of residues in target_sequence. int: number of residues in target_sequence covered by peptides in peptide_list. float: percentage of residues in target_sequence covered by peptides in peptide_list. list (dict{str:bool}): list of dicts where keys are residue one-letter codes and values are bool (covered = True, not-covered = False)."
  },
  {
    "objectID": "index.html#preprint",
    "href": "index.html#preprint",
    "title": "AlphaPept",
    "section": "Preprint",
    "text": "Preprint\nOur preprint AlphaPept, a modern and open framework for MS-based proteomics is now available here.\nBe sure to check out other packages of our ecosystem: - alphatims: Fast access to TimsTOF data. - alphamap: Peptide level MS data exploration. - alphapeptdeep: Predicting properties from peptides. - alphaviz: Vizualization of MS data."
  },
  {
    "objectID": "index.html#windows-quickstart",
    "href": "index.html#windows-quickstart",
    "title": "AlphaPept",
    "section": "Windows Quickstart",
    "text": "Windows Quickstart\n\n\nDownload the latest installer here, install and click the shortcut on the desktop. A browser window with the AlphaPept interface should open. In the case of Windows Firewall asking for network access for AlphaPept, please allow.\nIn the New Experiment, select a folder with raw files and FASTA files.\nSpecify additional settings such as modifications with Settings.\nClick Start and run the analysis.\n\nSee also below for more detailed instructions."
  },
  {
    "objectID": "index.html#current-functionality",
    "href": "index.html#current-functionality",
    "title": "AlphaPept",
    "section": "Current functionality",
    "text": "Current functionality\n\n\n\nFeature\nImplemented\n\n\n\n\nType\nDDA\n\n\nFiletypes\nBruker, Thermo\n\n\nQuantification\nLFQ\n\n\nIsobaric labels\nNone\n\n\nPlatform\nWindows\n\n\n\nLinux and macOS should, in principle, work but are not heavily tested and might require additional work to set up (see detailed instructions below). To read Thermo files, we use Mono, which can be used on Mac and Linux. For Bruker files, we can use Linux but not yet macOS."
  },
  {
    "objectID": "index.html#python-installation-instructions",
    "href": "index.html#python-installation-instructions",
    "title": "AlphaPept",
    "section": "Python Installation Instructions",
    "text": "Python Installation Instructions\n\nRequirements\nWe highly recommend the Anaconda or Miniconda Python distribution, which comes with a powerful package manager. See below for additional instructions for Linux and Mac as they require additional installation of Mono to use the RawFileReader.\nAlphaPept can be used as an application as a whole or as a Python Package where individual modules are called. Depending on the use case, AlphaPept will need different requirements, and you might not want to install all of them.\nCurrently, we have the default requirements.txt, additional requirements to run the GUI gui and packages used for developing develop.\nTherefore, you can install AlphaPept in multiple ways:\n\nThe default alphapept\nWith GUI-packages alphapept[gui]\nWith pacakges for development alphapept[develop] (alphapept[develop,gui]) respectively\n\nThe requirements typically contain pinned versions and will be automatically upgraded and tested with dependabot. This stable version allows having a reproducible workflow. However, in order to avoid conflicts with package versions that are too strict, the requirements are not pinned when being installed. To use the strict version use the -stable-flag, e.g. alphapept[stable].\nFor end-users that want to set up a processing environment in Python, the \"alphapept[stable,gui-stable]\" is the batteries-included-version that you want to use.\n\n\nPython\nIt is strongly recommended to install AlphaPept in its own environment. 1. Open the console and create a new conda environment: conda create --name alphapept python=3.8 2. Activate the environment: conda activate alphapept 3. Install AlphaPept via pip: pip install \"alphapept[stable,gui-stable]\". If you want to use AlphaPept as a package without the GUI dependencies and without strict version dependencies, use pip install alphapept.\nIf AlphaPept is installed correctly, you should be able to import AlphaPept as a package within the environment; see below.\n\n\nLinux\n\nInstall the build-essentials: sudo apt-get install build-essential.\nInstall AlphaPept via pip: pip install \"alphapept[stable,gui-stable]\". If you want to use AlphaPept as a package withouth the GUI dependencies and strict version dependencies use pip install alphapept.\nInstall libgomp.1 with sudo apt-get install libgomp1.\n\n\nBruker Support\n\nCopy-paste the Bruker library for feature finding to your /usr/lib folder with sudo cp alphapept/ext/bruker/FF/linux64/alphapeptlibtbb.so.2 /usr/lib/libtbb.so.2.\n\n\n\nThermo Support\n\nInstall Mono from mono-project website Mono Linux. NOTE, the installed mono version should be at least 6.10, which requires you to add the ppa to your trusted sources!\nInstall pythonnet with pip install pythonnet==2.5.2\n\n\n\n\n\nMac\n\nInstall AlphaPept via pip: pip install \"alphapept[stable,gui-stable]\". If you want to use AlphaPept as a package withouth the GUI dependencies and strict version dependencies use pip install alphapept.\n\n\nBruker Support\n\nOnly supported for preprocessed files.\n\n\n\nThermo Support\n\nInstall brew and pkg-config: brew install pkg-config\nInstall Mono from mono-project website Mono Mac\nRegister the Mono-Path to your system: For macOS Catalina, open the configuration of zsh via the terminal:\n\n\nType in cd to navigate to the home directory.\nType nano ~/.zshrc to open the configuration of the terminal\nAdd the path to your mono installation: export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:/usr/lib/pkgconfig:/Library/Frameworks/Mono.framework/Versions/Current/lib/pkgconfig:$PKG_CONFIG_PATH. Make sure that the Path matches to your version (Here 6.12.0)\nSave everything and execute . ~/.zshrc\n\n\nInstall pythonnet with pip install pythonnet==2.5.2\n\n\n\n\n\nDeveloper\n\nRedirect to the folder of choice and clone the repository: git clone https://github.com/MannLabs/alphapept.git\nNavigate to the alphapept folder with cd alphapept and install the package with pip install . (default users) or with pip install -e . to enable developers mode. Note that you can use the different requirements here aswell (e.g. pip install \".[gui-stable]\")\n\n\n\nGPU Support\nSome functionality of AlphaPept is GPU optimized that uses Nvidia’s CUDA. To enable this, additional packages need to be installed.\n\nMake sure to have a working CUDA toolkit installation that is compatible with CuPy. To check type nvcc --version in your terminal.\nInstall cupy. Make sure to install the cupy version matching your CUDA toolkit (e.g. pip install cupy-cuda110 for CUDA toolkit 11.0.\n\n\n\n\nAdditional Notes\n\nTo access Thermo files, we have integrated RawFileReader into AlphaPept. We rely on Mono for Linux/Mac systems.\n\n\nTo access Bruker files, we rely on the timsdata-library. Currently, only Windows is supported. For feature finding, we use the Bruker Feature Finder, which can be found in the ext folder of this repository.\n\n\nNotes for NBDEV\n\nFor developing with the notebooks, install the nbdev package (see the development requirements)\nTo facilitate navigating the notebooks, use jupyter notebook extensions. They can be called from a running jupyter instance like so: http://localhost:8888/nbextensions. The extensions collapsible headings and toc2 are very beneficial."
  },
  {
    "objectID": "index.html#standalone-windows-installer",
    "href": "index.html#standalone-windows-installer",
    "title": "AlphaPept",
    "section": "Standalone Windows Installer",
    "text": "Standalone Windows Installer\nTo use AlphaPept as a stand-alone program for end-users, it can be installed on Windows machines via a one-click installer. Download the latest version here."
  },
  {
    "objectID": "index.html#additional-documentation",
    "href": "index.html#additional-documentation",
    "title": "AlphaPept",
    "section": "Additional Documentation",
    "text": "Additional Documentation\nThe documentation is automatically built based on the jupyter notebooks (nbs/index.ipynb) and can be found here:"
  },
  {
    "objectID": "index.html#version-performance",
    "href": "index.html#version-performance",
    "title": "AlphaPept",
    "section": "Version Performance",
    "text": "Version Performance\nAn overview of the performance of different versions can be found here. We re-run multiple tests on datasets for different versions so that users can assess what changes from version to version. Feel free to suggest a test set in case."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "AlphaPept",
    "section": "How to use",
    "text": "How to use\nAlphaPept is meant to be a framework to implement and test new ideas quickly but also to serve as a performant processing pipeline. In principle, there are three use-cases:\n\nGUI: Use the graphical user interface to select settings and process files manually.\nCMD: Use the command-line interface to process files. Useful when building automatic pipelines.\nPython: Use python modules to build individual workflows. Useful when building customized pipelines and using Python as a scripting language or when implementing new ideas.\n\n\nWindows Standalone Installation\nFor the windows installation, simply click on the shortcut after installation. The windows installation also installs the command-line tool so that you can call alphapept via alphapept in the command line.\n\n\n\nPython Package\nOnce AlphaPept is correctly installed, you can use it like any other python module.\n\nfrom alphapept.fasta import get_frag_dict, parse\nfrom alphapept import constants\n\npeptide = 'PEPT'\n\nget_frag_dict(parse(peptide), constants.mass_dict)\n\n{'b1': 98.06004032687,\n 'b2': 227.10263342687,\n 'b3': 324.15539728686997,\n 'y1': 120.06551965033,\n 'y2': 217.11828351033,\n 'y3': 346.16087661033}\n\n\n\n\nUsing as a tool\nIf alphapept is installed an a conda or virtual environment, launch this environment first.\nTo launch the command line interface use: * alphapept\nThis allows us to select different modules. To start the GUI use: * alphapept gui\nTo run a workflow, use: * alphapept workflow your_own_workflow.yaml An example workflow is easily generated by running the GUI once and saving the settings which can be modified on a per-project basis.\n\n\nCMD / Python\n\nCreate a settings-file. This can be done by changing the default_settings.yaml in the repository or using the GUI.\nRun the analysis with the new settings file. alphapept run new_settings.yaml\n\nWithin Python (i.e., Jupyter notebook) the following code would be required)\nfrom alphapept.settings import load_settings\nimport alphapept.interface\nsettings = load_settings('new_settings.yaml')\nr = alphapept.interface.run_complete_workflow(settings)\nThis also allows you to break the workflow down in indiviudal steps, e.g.:\nsettings = alphapept.interface.import_raw_data(settings)\nsettings = alphapept.interface.feature_finding(settings)"
  },
  {
    "objectID": "index.html#notebooks",
    "href": "index.html#notebooks",
    "title": "AlphaPept",
    "section": "Notebooks",
    "text": "Notebooks\nWithin the notebooks, we try to cover most aspects of a proteomics workflow:\n\nSettings: General settings to define a workflow\nChem: Chemistry related functions, e.g., for calculating isotope distributions\nInput / Output: Everything related to importing and exporting and the file formats used\nFASTA: Generating theoretical databases from FASTA files\nFeature Finding: How to extract MS1 features for quantification\nSearch: Comparing theoretical databases to experimental spectra and getting Peptide-Spectrum-Matches (PSMs)\nScore: Scoring PSMs\nRecalibration: Recalibration of data based on identified peptides\nQuantification: Functions for quantification, e.g., LFQ\nMatching: Functions for Match-between-runs\nConstants: A collection of constants\nInterface: Code that generates the command-line-interface (CLI) and makes workflow steps callable\nPerformance: Helper functions to speed up code with CPU / GPU\nExport: Helper functions to make exports compatbile to other Software tools\nLabel: Code for support isobaric label search\nDisplay: Code related to displaying in the streamlit gui\nAdditional code: Overview of additional code not covered by the notebooks\nHow to contribute: Contribution guidelines\nAlphaPept workflow and files: Overview of the worfklow, files and column names"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "AlphaPept",
    "section": "Contributing",
    "text": "Contributing\nIf you have a feature request or a bug report, please post it either as an idea in the discussions or as an issue on the GitHub issue tracker. Upvoting features in the discussions page will help to prioritize what to implement next. If you want to contribute, put a PR for it. You can find more guidelines for contributing and how to get started here. We will gladly guide you through the codebase and credit you accordingly. Additionally, you can check out the Projects page on GitHub. You can also contact us via opensource@alphapept.com.\nIf you like the project, consider starring it!"
  },
  {
    "objectID": "index.html#cite-us",
    "href": "index.html#cite-us",
    "title": "AlphaPept",
    "section": "Cite us",
    "text": "Cite us\n@article {Strauss2021.07.23.453379,\n    author = {Strauss, Maximilian T and Bludau, Isabell and Zeng, Wen-Feng and Voytik, Eugenia and Ammar, Constantin and Schessner, Julia and Ilango, Rajesh and Gill, Michelle and Meier, Florian and Willems, Sander and Mann, Matthias},\n    title = {AlphaPept, a modern and open framework for MS-based proteomics},\n    elocation-id = {2021.07.23.453379},\n    year = {2021},\n    doi = {10.1101/2021.07.23.453379},\n    publisher = {Cold Spring Harbor Laboratory},\n    URL = {https://www.biorxiv.org/content/early/2021/07/26/2021.07.23.453379},\n    eprint = {https://www.biorxiv.org/content/early/2021/07/26/2021.07.23.453379.full.pdf},\n    journal = {bioRxiv}\n}"
  },
  {
    "objectID": "recalibration.html",
    "href": "recalibration.html",
    "title": "Recalibration",
    "section": "",
    "text": "This notebook contains evertyhing related to recalibration of data."
  },
  {
    "objectID": "recalibration.html#recalibration-after-search",
    "href": "recalibration.html#recalibration-after-search",
    "title": "Recalibration",
    "section": "Recalibration after search",
    "text": "Recalibration after search\n\nPrecursor mass calibration\nRecalibration refers to the computational step where masses are recalibrated after a first search. The identified peptides are used to calculate the deviations of experimental masses to their theoretical masses. After recalibration, a second search with decreased precursor tolerance is performed.\nThe recalibration is largely motivated by the software lock mass paper:\nCox J, Michalski A, Mann M. Software lock mass by two-dimensional minimization of peptide mass errors. J Am Soc Mass Spectrom. 2011;22(8):1373-1380. doi:10.1007/s13361-011-0142-8\nHere, mass offsets are piecewise linearly approximated. The positions for approximation need to fulfill a number of criteria (e.g., a minimum number of samples and a minimum distance). The AlphaPept implementation is slightly modified by employing a more general KNeighborsRegressor-approach. In brief, the calibration is calculated for each point individually by estimating the deviation from its identified neighbors in n-dimensional space (e.g., retention time, mass, mobility).\nMore specifically, the algorithm consists of the following steps:\n\nOutlier removal: We remove outliers from the identified peptides by only accepting identifications with a mass offset that is within n (default 3) standard deviations to the mean.\nFor each point, we perform a neighbors lookup of the next n (default 100) neighbors. For the neighbor’s lookup we need to scale the axis, which is done with a transform function either absolute or relative.\nNext, we perform a regression based on the neighbors to determine the mass offset. The contribution of each neighbor is weighted by their distance.\n\n\n\nFragment mass calibration\nThe fragment mass calibration is based on the identified fragment_ions (i.e., b-hits and y-hits). For each hit, we calculate the offset to its theoretical mass. The correction is then applied by taking the median offset in ppm and applying it globally.\n\nsource\n\n\nremove_outliers\n\n remove_outliers (df:pandas.core.frame.DataFrame, outlier_std:float)\n\nHelper function to remove outliers from a dataframe. Outliers are removed based on the precursor offset mass (prec_offset). All values within x standard deviations to the median are kept.\nArgs: df (pd.DataFrame): Input dataframe that contains a prec_offset_ppm-column. outlier_std (float): Range of standard deviations to filter outliers\nRaises: ValueError: An error if the column is not present in the dataframe.\nReturns: pd.DataFrame: A dataframe w/o outliers.\n\nsource\n\n\ntransform\n\n transform (x:numpy.ndarray, column:str, scaling_dict:dict)\n\nHelper function to transform an input array for neighbors lookup used for calibration\nNote: The scaling_dict stores information about how scaling is applied and is defined in get_calibration\nRelative transformation: Compare distances relatively, for mz that is ppm, for mobility %. Absolute transformation: Compare distance absolute, for RT it is the timedelta.\nAn example definition is below:\nscaling_dict = {} scaling_dict[‘mz’] = (‘relative’, calib_mz_range/1e6) scaling_dict[‘rt’] = (‘absolute’, calib_rt_range) scaling_dict[‘mobility’] = (‘relative’, calib_mob_range)\nArgs: x (np.ndarray): Input array. column (str): String to lookup what scaling should be applied. scaling_dict (dict): Lookup dict to retrieve the scaling operation and factor for the column.\nRaises: KeyError: An error if the column is not present in the dict. NotImplementedError: An error if the column is not present in the dict.\nReturns: np.ndarray: A scaled array.\n\nsource\n\n\nkneighbors_calibration\n\n kneighbors_calibration (df:pandas.core.frame.DataFrame,\n                         features:pandas.core.frame.DataFrame, cols:list,\n                         target:str, scaling_dict:dict,\n                         calib_n_neighbors:int)\n\nCalibration using a KNeighborsRegressor. Input arrays from are transformed to be used with a nearest-neighbor approach. Based on neighboring points a calibration is calculated for each input point.\nArgs: df (pd.DataFrame): Input dataframe that contains identified peptides (w/o outliers). features (pd.DataFrame): Features dataframe for which the masses are calibrated. cols (list): List of input columns for the calibration. target (str): Target column on which offset is calculated. scaling_dict (dict): A dictionary that contains how scaling operations are applied. calib_n_neighbors (int): Number of neighbors for calibration.\nReturns: np.ndarray: A numpy array with calibrated masses.\n\nsource\n\n\nget_calibration\n\n get_calibration (df:pandas.core.frame.DataFrame,\n                  features:pandas.core.frame.DataFrame, file_name='',\n                  settings=None, outlier_std:float=3,\n                  calib_n_neighbors:int=100, calib_mz_range:int=100,\n                  calib_rt_range:float=0.5, calib_mob_range:float=0.3,\n                  **kwargs)\n\nWrapper function to get calibrated values for the precursor mass.\nArgs: df (pd.DataFrame): Input dataframe that contains identified peptides. features (pd.DataFrame): Features dataframe for which the masses are calibrated. outlier_std (float, optional): Range in standard deviations for outlier removal. Defaults to 3. calib_n_neighbors (int, optional): Number of neighbors used for regression. Defaults to 100. calib_mz_range (int, optional): Scaling factor for mz range. Defaults to 20. calib_rt_range (float, optional): Scaling factor for rt_range. Defaults to 0.5. calib_mob_range (float, optional): Scaling factor for mobility range. Defaults to 0.3. **kwargs: Arbitrary keyword arguments so that settings can be passes as whole.\nReturns: corrected_mass (np.ndarray): The calibrated mass y_hat_std (float): The standard deviation of the precursor offset after calibration\n\nsource\n\n\ncalibrate_fragments_nn\n\n calibrate_fragments_nn (ms_file_, file_name, settings)\n\n\nsource\n\n\nsave_precursor_calibration\n\n save_precursor_calibration (df, corrected, std_offset, file_name,\n                             settings)\n\n\nsource\n\n\nsave_fragment_calibration\n\n save_fragment_calibration (fragment_ions, corrected, std_offset,\n                            file_name, settings)\n\n\nsource\n\n\ndensity_scatter\n\n density_scatter (x, y, ax=None, sort=True, bins=20, **kwargs)\n\nScatter plot colored by 2d histogram Adapted from https://stackoverflow.com/questions/20105364/how-can-i-make-a-scatter-plot-colored-by-density-in-matplotlib\n\nsource\n\n\nchunks\n\n chunks (lst, n)\n\nYield successive n-sized chunks from lst.\n\nsource\n\n\ncalibrate_hdf\n\n calibrate_hdf (to_process:tuple, callback=None, parallel=True)\n\nWrapper function to get calibrate a hdf file when using the parallel executor. The function loads the respective dataframes from the hdf, calls the calibration function and applies the offset.\nArgs: to_process (tuple): Tuple that contains the file index and the settings dictionary. callback ([type], optional): Placeholder for callback (unused). parallel (bool, optional): Placeholder for parallel usage (unused).\nReturns: Union[str,bool]: Either True as boolean when calibration is successfull or the Error message as string.\n\nDatabase calibration\nAnother way to calibrate the fragment and precursor masses is by directly comparing them to a previously generated theoretical mass database. Here, peaks in the distribution of databases are used to align the experimental masses.\n\nsource\n\n\n\nget_db_targets\n\n get_db_targets (db_file_name:str, max_ppm:int=100,\n                 min_distance:float=0.5, ms_level:int=2)\n\nFunction to extract database targets for database-calibration. Based on the FASTA database it finds masses that occur often. These will be used for calibration.\nArgs: db_file_name (str): Path to the database. max_ppm (int, optional): Maximum distance in ppm between two peaks. Defaults to 100. min_distance (float, optional): Minimum distance between two calibration peaks. Defaults to 0.5. ms_level (int, optional): MS-Level used for calibration, either precursors (1) or fragmasses (2). Defaults to 2.\nRaises: ValueError: When ms_level is not valid.\nReturns: np.ndarray: Numpy array with calibration masses.\n\nsource\n\n\nalign_run_to_db\n\n align_run_to_db (ms_data_file_name:str, db_array:numpy.ndarray,\n                  max_ppm_distance:int=1000000, rt_step_size:float=0.1,\n                  plot_ppms:bool=False, ms_level:int=2)\n\nFunction align a run to it’s theoretical FASTA database.\nArgs: ms_data_file_name (str): Path to the run. db_array (np.ndarray): Numpy array containing the database targets. max_ppm_distance (int, optional): Maximum distance in ppm. Defaults to 1000000. rt_step_size (float, optional): Stepsize for rt calibration. Defaults to 0.1. plot_ppms (bool, optional): Flag to indicate plotting. Defaults to False. ms_level (int, optional): ms_level for calibration. Defaults to 2.\nRaises: ValueError: When ms_level is not valid.\nReturns: np.ndarray: Estimated errors\n\nsource\n\n\ncalibrate_fragments\n\n calibrate_fragments (db_file_name:str, ms_data_file_name:str,\n                      ms_level:int=2, write=True, plot_ppms=False)\n\nWrapper function to calibrate fragments. Calibrated values are saved to corrected_fragment_mzs\nArgs: db_file_name (str): Path to database ms_data_file_name (str): Path to ms_data file ms_level (int, optional): MS-level for calibration. Defaults to 2. write (bool, optional): Boolean flag for test purposes to avoid writing to testfile. Defaults to True. plot_ppms (bool, optional): Boolean flag to plot the calibration. Defaults to False."
  },
  {
    "objectID": "matching.html",
    "href": "matching.html",
    "title": "Matching",
    "section": "",
    "text": "For matching MS2-identifications to MS1-features, we first need to align the datasets on top of each other to be able to transfer identifications correctly. Datasets are aligned by comparing shared precursors and calculating the median offset. When comparing all files to each other, we get an overdetermined linear equation system. By solving this, we find offset parameters that minimize the shift of all files to each other. Offset is either applied relative (mz, mobility) or absolute (rt).\n\n\nFor some parameters, we would like to have a relative correction of values. Consider the case of different mz-values, e.g. 300 and 600. If we assume that the offset is larger for larger m/z values, we would not want an absolute correction of e.g. +0.5 Da (300.5 and 600.5) but rather a relative correction of e.g. +0.1% (300.3 and 600.6).\n\n\n\nIn contrast to the relative correction, sometimes absolute correction is more applicable. Consider the case of retention time. Here one would rather not expect a relative offset but rather an absolute offset. As an example, consider a lag time of 0.5 Minutes. This would be constant for all retention times and not differ e.g., for later retention times.\n\nsource\n\n\n\n\n calculate_distance (table_1:pandas.core.frame.DataFrame,\n                     table_2:pandas.core.frame.DataFrame,\n                     offset_dict:dict, calib:bool=False)\n\nCalculate the distance between two precursors for different columns Distance can either be relative or absolute.\nAn example for a minimal offset_dict is: offset_dict = {‘mass’:‘absolute’}\nArgs: table_1 (pd.DataFrame): Dataframe with precusor data. table_2 (pd.DataFrame): Dataframe with precusor data. offset_dict (dict): Dictionary with column names and how the distance should be calculated. calib (bool): Flag to indicate that distances should be calculated on calibrated columns. Defaults to False.\nRaises: KeyError: If either table_1 or table_2 is not indexed by precursor\n\nsource\n\n\n\n\n calib_table (table:pandas.core.frame.DataFrame,\n              delta:pandas.core.series.Series, offset_dict:dict)\n\nApply offset to a table. Different operations for offsets exist. Offsets will be saved with a ’_calib’-suffix. If this does not already exist, it will be created.\nArgs: table_1 (pd.DataFrame): Dataframe with data. delta (pd.Series): Series cotaining the offset. offset_dict (dict): Dictionary with column names and how the distance should be calculated.\nRaises: NotImplementedError: If the type of vonversion is not implemented.\n\nsource\n\n\n\n\n align (deltas:pandas.core.frame.DataFrame, filenames:list,\n        weights:numpy.ndarray=None, n_jobs=None)\n\nAlign multiple datasets. This function creates a matrix to represent the shifts from each dataset to another. This effectively is an overdetermined equation system and is solved with a linear regression.\nArgs: deltas (pd.DataFrame): Distances from each dataset to another. filenames (list): The filenames of the datasts that were compared. weights (np.ndarray, optional): Distances can be weighted by their number of shared elements. Defaults to None. n_jobs (optional): Number of processes to be used. Defaults to None (=1).\nReturns: np.ndarray: alignment values.\n\nsource\n\n\n\n\n calculate_deltas (combos:list, calib:bool=False, callback:Callable=None)\n\nWrapper function to calculate the distances of multiple files.\nIn here, we define the offset_dict to make a relative comparison for mz and mobility and absolute for rt.\nTODO: This function could be speed-up by parallelization\nArgs: combos (list): A list containing tuples of filenames that should be compared. calib (bool): Boolean flag to indicate distance should be calculated on calibrated data. callback (Callable): A callback function to track progress.\nReturns: pd.DataFrame: Dataframe containing the deltas of the files np.ndarray: Numpy array containing the weights of each comparison (i.e. number of shared elements) dict: Offset dictionary whicch was used for comparing.\n\nsource\n\n\n\n\n align_datasets (settings:dict, callback:<built-infunctioncallable>=None)\n\nWrapper function that aligns all experimental files specified a settings file.\nArgs: settings (dict): A list with raw file names. callback (Callable): Callback function to indicate progress.\n\nsource\n\n\n\n\n align_files (filenames:list, alignment:pandas.core.frame.DataFrame,\n              offset_dict:dict)\n\nWrapper function that aligns a list of files.\nArgs: filenames (list): A list with raw file names. alignment (pd.DataFrame): A pandas dataframe containing the alignment information. offset_dict (dict): Dictionary with column names and how the distance should be calculated."
  },
  {
    "objectID": "matching.html#matching",
    "href": "matching.html#matching",
    "title": "Matching",
    "section": "Matching",
    "text": "Matching\nTransfer MS2 identifications to similar MS1 features.\nFor “match-between-runs” we start with aligning datasets. To create a reference we use for matching, we combine all datasets of a matching group. When using the default settings, the matching group consists of all files. We then group the dataset by precursor and calculate it’s average properties (rt, mz, mobility). By combining several files we further are able to calculate a standard deviation. This allows us to know where and with what deviation we would expect an MS1 feature and have the corresponding identification. This is our matching reference. In the matching step, we go through each dataset individually and check if there are precursors in the reference that were not identified in this dataset. We then perform a nearest-neighbor lookup to find if any MS1 features exist that are in close proximity to the reference. The distance metric we use is normed by the median standard of the deviation. Lastly we assess the confidence in a transfered identifcation by using the Mahalanobis distance.\n\nsource\n\nget_probability\n\n get_probability (df:pandas.core.frame.DataFrame,\n                  ref:pandas.core.frame.DataFrame,\n                  sigma:pandas.core.frame.DataFrame, index:int)\n\nProbablity estimate of a transfered identification using the Mahalanobis distance.\nThe function calculates the probability that a feature is a reference feature. The reference features containing std deviations so that a probability can be estimated.\nIt is required that the data frames are matched, meaning that the first entry in df matches to the first entry in ref.\nArgs: df (pd.DataFrame): Dataset containing transferered features ref (pd.DataFrame): Dataset containing reference features sigma (pd.DataFrame): Dataset containing the standard deviations of the reference features index (int): Index to the datframes that should be compared\nReturns: float: Mahalanobis distance\n\n#Example usage\n\na = pd.DataFrame({'mass':[100,200,300],'rt':[1,2,3]})\nb = pd.DataFrame({'mass':[100,200,302],'rt':[1,2.5,3]})\nstd = pd.DataFrame({'mass':[0.1,0.1,0.1],'rt':[1,1,1]})\n\nprint(f\"First element: (ideal match): {get_probability(a, b, std, 0):.2f}\")\nprint(f\"Second element: (rt slightly off): {get_probability(a, b, std, 1):.2f}\")\nprint(f\"Third element: (mass completely off): {get_probability(a, b, std, 2):.2f}\")\n\nFirst element: (ideal match): 0.00\nSecond element: (rt slightly off): 0.12\nThird element: (mass completely off): 1.00\n\n\n\nsource\n\n\nmatch_datasets\n\n match_datasets (settings:dict, callback:Callable=None)\n\nMatch datasets: Wrapper function to match datasets based on a settings file. This implementation uses matching groups but not fractions.\nArgs: settings (dict): Dictionary containg specifications of the run callback (Callable): Callback function to indicate progress.\n\nsource\n\n\nconvert_decoy\n\n convert_decoy (float_)\n\nUtility function to convert type for decoy after grouping."
  }
]