{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTA\n",
    "\n",
    "> Functions related to generating spectra from FASTA files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all functions related to creating spectra from FASTA files. In brief, what we are doing is the following:\n",
    "\n",
    "1. Read a FASTA file and digest the proteins to generate peptides\n",
    "2. For each peptide, calculate a theoretical spectrum and precursor mass\n",
    "3. Save spectra\n",
    "\n",
    "Currently, `numba` has only limited string support. A lot of the functions are therefore Python-native."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaving\n",
    "\n",
    "For cleaving, we use regular expressions to find potential cleavages sites and write the wrapper `cleave_sequence` to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from alphapept import constants\n",
    "import re\n",
    "\n",
    "def get_missed_cleavages(sequences, n_missed_cleavages):\n",
    "    \"\"\"\n",
    "    Combine cleaved sequences to get sequences with missed cleavages\n",
    "    \"\"\"\n",
    "    missed = []\n",
    "    for k in range(len(sequences)-n_missed_cleavages):\n",
    "        missed.append(''.join(sequences[k-1:k+n_missed_cleavages]))\n",
    "\n",
    "    return missed\n",
    "\n",
    "\n",
    "def cleave_sequence(\n",
    "    sequence=\"\",\n",
    "    num_missed_cleavages=0,\n",
    "    protease=\"trypsin\",\n",
    "    min_length=6,\n",
    "    max_length=65,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleave a sequence with a given protease. Filters to have a minimum and maximum length.\n",
    "    \"\"\"\n",
    "\n",
    "    proteases = constants.protease_dict\n",
    "    pattern = proteases[protease]\n",
    "\n",
    "    p = re.compile(pattern)\n",
    "\n",
    "    cutpos = [m.start()+1 for m in p.finditer(sequence)]\n",
    "    cutpos.insert(0,0)\n",
    "    cutpos.append(len(sequence))\n",
    "\n",
    "    base_sequences = [sequence[cutpos[i]:cutpos[i+1]] for i in range(len(cutpos)-1)]\n",
    "\n",
    "    sequences = base_sequences.copy()\n",
    "\n",
    "    for i in range(1, num_missed_cleavages+1):\n",
    "        sequences.extend(get_missed_cleavages(base_sequences, i))\n",
    "\n",
    "    sequences = [_ for _ in sequences if len(_)>=min_length and len(_)<=max_length]\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABCDEFGHIJK', 'LMNOPQR']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protease = \"trypsin\"\n",
    "num_missed_cleavages = 0\n",
    "min_length, max_length = 6, 65\n",
    "\n",
    "cleave_sequence('ABCDEFGHIJKLMNOPQRST', num_missed_cleavages, protease, min_length, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_cleave_sequence():\n",
    "    \n",
    "    protease = \"trypsin\"\n",
    "    min_length, max_length = 6, 65\n",
    "\n",
    "    assert set(cleave_sequence('ABCDEFGHIJKLMNOPQRST', 0, protease, min_length, max_length)) == set(['ABCDEFGHIJK', 'LMNOPQR'])\n",
    "    assert set(cleave_sequence('ABCDEFGHIJKLMNOPQRST', 1, protease, min_length, max_length)) == set(['ABCDEFGHIJK', 'LMNOPQR', 'ABCDEFGHIJKLMNOPQR'])\n",
    "\n",
    "test_cleave_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting missed and internal cleavages\n",
    "The following are helper functions to retrieve the number of missed cleavages and internal cleavage sites for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "from alphapept import constants\n",
    "\n",
    "def count_missed_cleavages(sequence=\"\", protease=\"trypsin\", **kwargs):\n",
    "    \"\"\"\n",
    "    Counts the number of missed cleavages for a given sequence and protease\n",
    "    \"\"\"\n",
    "    proteases = constants.protease_dict\n",
    "    protease = proteases[protease]\n",
    "    p = re.compile(protease)\n",
    "    n_missed = len(p.findall(sequence))\n",
    "    return n_missed\n",
    "\n",
    "def count_internal_cleavages(sequence=\"\", protease=\"trypsin\", **kwargs):\n",
    "    \"\"\"\n",
    "    Counts the number of internal cleavage sites for a given sequence and protease\n",
    "    \"\"\"\n",
    "    proteases = constants.protease_dict\n",
    "    protease = proteases[protease]\n",
    "    match = re.search(protease,sequence[-1]+'_')\n",
    "    if match:\n",
    "        n_internal = 0\n",
    "    else:\n",
    "        n_internal = 1\n",
    "    return n_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "protease = \"trypsin\"\n",
    "print(count_missed_cleavages('ABCDEFGHIJKLMNOPQRST', protease))\n",
    "\n",
    "protease = \"trypsin\"\n",
    "print(count_internal_cleavages('ABCDEFGHIJKLMNOPQRST', protease))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_missed_cleavages():  \n",
    "    assert count_missed_cleavages('ABCDEFGHIJKLMNOPQRST', 'trypsin') == 2\n",
    "    assert count_missed_cleavages('ABCDEFGHIJKLMNOPQRST', 'clostripain') == 1\n",
    "    \n",
    "test_get_missed_cleavages()\n",
    "\n",
    "def test_get_internal_cleavages():\n",
    "    assert count_internal_cleavages('ABCDEFGHIJKLMNOPQRST', 'trypsin') == 1\n",
    "    assert count_internal_cleavages('ABCDEFGHIJKLMNOPQRSTK', 'trypsin') == 0\n",
    "\n",
    "test_get_internal_cleavages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "Peptides are composed out of amino acids that are written in capital letters - `PEPTIDE`. to distinguish modifications, they are written in lowercase such as `PEPTIoxDE` and can be of arbitrary length. For a modified amino acid, the modification precedes the letter of the amino acid. Decoys are indicated with an underscore. Therfore, the `parse` function splits after `_`. When parsing, the peptide string is converted into a numba-compatible list, like so: `PEPoxTIDE` -> `[P, E, P, oxT, I, D, E]`. This allows that we can use the `mass_dict` from `alphapept.constants` to directly determine the masses for the corresponding amino acids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "from numba.typed import List\n",
    "\n",
    "@njit\n",
    "def parse(peptide):\n",
    "    \"\"\"\n",
    "    Parser to parse peptide strings\n",
    "    \"\"\"\n",
    "    if \"_\" in peptide:\n",
    "        peptide = peptide.split(\"_\")[0]\n",
    "    parsed = List()\n",
    "    string = \"\"\n",
    "\n",
    "    for i in peptide:\n",
    "        string += i\n",
    "        if i.isupper():\n",
    "            parsed.append(string)\n",
    "            string = \"\"\n",
    "\n",
    "    return parsed\n",
    "\n",
    "def list_to_numba(a_list):\n",
    "    numba_list = List()\n",
    "\n",
    "    for element in a_list:\n",
    "        numba_list.append(element)\n",
    "\n",
    "    return numba_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P, E, P, T, I, D, E]\n",
      "[P, E, P, oxT, I, D, E]\n"
     ]
    }
   ],
   "source": [
    "print(parse('PEPTIDE'))\n",
    "print(parse('PEPoxTIDE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_parse():\n",
    "    peptide = \"PEPTIDE\"\n",
    "    assert parse(peptide) == list_to_numba([\"P\", \"E\", \"P\", \"T\", \"I\", \"D\", \"E\"])\n",
    "    peptide = \"PEPoxTIDE\"\n",
    "    assert parse(peptide) == list_to_numba([\"P\", \"E\", \"P\", \"oxT\", \"I\", \"D\", \"E\"])\n",
    "    peptide = \"PEPTIDE_decoy\"\n",
    "    assert parse(peptide) == list_to_numba([\"P\", \"E\", \"P\", \"T\", \"I\", \"D\", \"E\"])\n",
    "    \n",
    "test_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoy\n",
    "\n",
    "The decoy strategy employed is a pseudo-reversal of the peptide sequence, keeping only the terminal amino-acid and reversing the rest. Additionally, we can call the functions `swap_KR` and and `swap_AL` that will swap the respective AAs. The function `swap_KR` will only swap terminal AAs. The swapping functions only work if the AA is not modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def get_decoy_sequence(peptide, pseudo_reverse=False, AL_swap=False, KR_swap = False):\n",
    "    \"\"\"\n",
    "    Reverses a sequence and adds the '_decoy' tag.\n",
    "\n",
    "    \"\"\"\n",
    "    pep = parse(peptide)\n",
    "    if pseudo_reverse:\n",
    "        rev_pep = pep[:-1][::-1]\n",
    "        rev_pep.append(pep[-1])\n",
    "    else:\n",
    "        rev_pep = pep[::-1]\n",
    "\n",
    "    if AL_swap:\n",
    "        rev_pep = swap_AL(rev_pep)\n",
    "\n",
    "    if KR_swap:\n",
    "        rev_pep = swap_KR(rev_pep)\n",
    "\n",
    "    rev_pep = \"\".join(rev_pep)\n",
    "\n",
    "    return rev_pep\n",
    "\n",
    "\n",
    "@njit\n",
    "def swap_KR(peptide):\n",
    "    \"\"\"\n",
    "    Swaps a terminal K or R. Note: Only if AA is not modified.\n",
    "    \"\"\"\n",
    "    if peptide[-1] == 'K':\n",
    "        peptide[-1] = 'R'\n",
    "    elif peptide[-1] == 'R':\n",
    "        peptide[-1] = 'K'\n",
    "\n",
    "    return peptide\n",
    "\n",
    "\n",
    "@njit\n",
    "def swap_AL(peptide):\n",
    "    \"\"\"\n",
    "    Swaps a A with L. Note: Only if AA is not modified.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < len(range(len(peptide) - 1)):\n",
    "        if peptide[i] == \"A\":\n",
    "            peptide[i] = peptide[i + 1]\n",
    "            peptide[i + 1] = \"A\"\n",
    "            i += 1\n",
    "        elif peptide[i] == \"L\":\n",
    "            peptide[i] = peptide[i + 1]\n",
    "            peptide[i + 1] = \"L\"\n",
    "            i += 1\n",
    "        i += 1\n",
    "\n",
    "    #aa_table = \"GAVLIFMPWSCTYHKRQEND\"\n",
    "    #DiaNN_table  = \"LLLVVLLLLTSSSSLLNDQE\"\n",
    "\n",
    "    #idx = aa_table.find(peptide[-2])\n",
    "    #peptide[-2] = decoy_table[idx]\n",
    "\n",
    "    return peptide\n",
    "\n",
    "def get_decoys(peptide_list, pseudo_reverse=False, AL_swap=False, KR_swap = False, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper to get decoys for lists of peptides\n",
    "    \"\"\"\n",
    "    decoys = []\n",
    "    decoys.extend([get_decoy_sequence(peptide, pseudo_reverse, AL_swap, KR_swap) for peptide in peptide_list])\n",
    "    return decoys\n",
    "\n",
    "def add_decoy_tag(peptides):\n",
    "    \"\"\"\n",
    "    Adds a _decoy tag to a list of peptides\n",
    "    \"\"\"\n",
    "    return [peptide + \"_decoy\" for peptide in peptides]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K, K, K, L, A, K, K, K]\n",
      "[A, A, A, K, R, A, A, A]\n"
     ]
    }
   ],
   "source": [
    "print(swap_AL(parse('KKKALKKK')))\n",
    "print(swap_KR(parse('AAAKRAAA')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDITPEP\n"
     ]
    }
   ],
   "source": [
    "print(get_decoy_sequence('PEPTIDE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CBA', 'FED', 'IHG']\n"
     ]
    }
   ],
   "source": [
    "print(get_decoys(['ABC','DEF','GHI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_swap_AL():\n",
    "    assert swap_AL(parse(\"ABCDEF\")) == parse(\"BACDEF\")\n",
    "    assert swap_AL(parse(\"GHIKLM\")) == parse(\"GHIKML\")\n",
    "    assert swap_AL(parse(\"FEDCBA\")) == parse(\"FEDCBA\")\n",
    "    assert swap_AL(parse(\"GHIKL\")) == parse(\"GHIKL\")\n",
    "    assert swap_AL(parse(\"ABCDEFGHIKLM\")) == parse(\"BACDEFGHIKML\")\n",
    "    assert swap_AL(parse(\"BBAcCD\")) == parse(\"BBcCAD\")\n",
    "    assert swap_AL(parse(\"FEDCBA\")) == parse(\"FEDCBA\")\n",
    "\n",
    "test_swap_AL()\n",
    "\n",
    "def test_swapKR():\n",
    "    assert swap_KR(parse(\"ABCDEK\")) == parse(\"ABCDER\")\n",
    "    assert swap_KR(parse(\"ABCDER\")) == parse(\"ABCDEK\")\n",
    "    assert swap_KR(parse(\"ABCDEF\")) == parse(\"ABCDEF\")\n",
    "    assert swap_KR(parse(\"KABCDEF\")) == parse(\"KABCDEF\")\n",
    "    assert swap_KR(parse(\"KABCRDEF\")) == parse(\"KABCRDEF\")\n",
    "    assert swap_KR(parse(\"KABCKDEF\")) == parse(\"KABCKDEF\")\n",
    "\n",
    "test_swapKR()\n",
    "    \n",
    "def test_get_decoy_sequence():\n",
    "    peptide = \"PEPTIDER\"\n",
    "    assert get_decoy_sequence(peptide, pseudo_reverse=True) == \"EDITPEPR\"\n",
    "    assert get_decoy_sequence(peptide) == \"REDITPEP\"\n",
    "    assert get_decoy_sequence(peptide, KR_swap=True, pseudo_reverse=True) == \"EDITPEPK\"\n",
    "    \n",
    "test_get_decoy_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications\n",
    "\n",
    "To add modifications to the peptides, we distinguish fixed and variable modifications. Additionally, we make a distinction between whether the modification is only terminal or not. \n",
    "\n",
    "### Fixed Modifications\n",
    "Fixed modifications are implemented by passing a list with modified AAs that should be replaced. As a AA is only one letter, the remainder is the modification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_fixed_mods(seqs, mods_fixed, **kwargs):\n",
    "    \"\"\"\n",
    "    Adds fixed modifications to sequences.\n",
    "    \"\"\"\n",
    "    if not mods_fixed:\n",
    "        return seqs\n",
    "    else:\n",
    "        for mod_aa in mods_fixed:\n",
    "            seqs = [seq.replace(mod_aa[-1], mod_aa) for seq in seqs]\n",
    "        return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbBcCDEF']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods_fixed = ['cC','bB']\n",
    "peptide_list = ['ABCDEF']\n",
    "\n",
    "add_fixed_mods(peptide_list, mods_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_add_fixed_mods():\n",
    "    mods_fixed = ['cC']\n",
    "    peptide_list = ['ABCDEF']\n",
    "\n",
    "    peptides_new = add_fixed_mods(peptide_list, [])\n",
    "    assert peptides_new == peptide_list\n",
    "    \n",
    "    peptides_new = add_fixed_mods(peptide_list, mods_fixed)\n",
    "    assert peptides_new == ['ABcCDEF']\n",
    "    \n",
    "test_add_fixed_mods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Modifications\n",
    "\n",
    "To employ variable modifications, we use the function `get_mod_pos` that returns a list of tuples with all possible modifications when giving a dicitionary with variable modifications.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphapept.fasta import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_variable_mod(peps, mods_variable_dict):\n",
    "    peptides = []\n",
    "    for pep_ in peps:\n",
    "        pep, min_idx = pep_\n",
    "        for mod in mods_variable_dict:\n",
    "            for i in range(len(pep)):\n",
    "                if i >= min_idx:\n",
    "                    c = pep[i]\n",
    "                    if c == mod:\n",
    "                        peptides.append((pep[:i]+[mods_variable_dict[c]]+pep[i+1:], i))         \n",
    "    return peptides\n",
    "\n",
    "\n",
    "def get_isoforms(mods_variable_dict, peptide, max_isoforms):\n",
    "    \"\"\"\n",
    "    Function to generate isoforms for a given peptide - returns a list of isoforms.\n",
    "    The original sequence is included in the list\n",
    "    \"\"\"\n",
    "    pep = list(parse(peptide))\n",
    "\n",
    "    peptides = [pep]\n",
    "    new_peps = [(pep, 0)]\n",
    "    while len(peptides) < max_isoforms:\n",
    "        new_peps = add_variable_mod(new_peps, mods_variable_dict)\n",
    "\n",
    "        if len(new_peps) == 0:\n",
    "            break\n",
    "        if len(new_peps) > 1:\n",
    "            if new_peps[0][0] == new_peps[1][0]:\n",
    "                new_peps = new_peps[0:1]\n",
    "                \n",
    "        for _ in new_peps:\n",
    "            if len(peptides) < max_isoforms:\n",
    "                peptides.append(_[0])\n",
    "\n",
    "    peptides = [''.join(_) for _ in peptides]\n",
    "\n",
    "    return peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PEPTIDE', 'pPEPTIDE', 'PEpPTIDE', 'pPEpPTIDE']\n",
      "['AMAMA', 'AoxMAMA', 'AMAoxMA', 'AoxMAoxMA']\n"
     ]
    }
   ],
   "source": [
    "mods_variable_dict = {'S':'pS','P':'pP','M':'oxM'}\n",
    "max_isoforms = 1024\n",
    "\n",
    "print(get_isoforms(mods_variable_dict, 'PEPTIDE', max_isoforms))\n",
    "print(get_isoforms(mods_variable_dict, 'AMAMA', max_isoforms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define the wrapper `add_variable_mods` so that the functions can be called for lists of peptides and a list of variable modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_isoforms():\n",
    "\n",
    "    mods_variable_dict = {'S':'pS','P':'pP'}\n",
    "    peptide = 'PEPTIDE'\n",
    "    max_isoforms = 1024\n",
    "    get_isoforms(mods_variable_dict, peptide, max_isoforms)\n",
    "\n",
    "    assert len(get_isoforms(mods_variable_dict, peptide, max_isoforms)) == 4\n",
    "    \n",
    "test_get_isoforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from itertools import chain\n",
    "\n",
    "def add_variable_mods(peptide_list, mods_variable, max_isoforms, **kwargs):\n",
    "    #the peptide_list originates from one peptide already -> limit isoforms here\n",
    "    \n",
    "    max_ = max_isoforms - len(peptide_list) + 1\n",
    "    \n",
    "    if max_ < 0:\n",
    "        max_ = 0\n",
    "    \n",
    "    if not mods_variable:\n",
    "        return peptide_list\n",
    "    else:\n",
    "        mods_variable_r = {}\n",
    "        for _ in mods_variable:\n",
    "            mods_variable_r[_[-1]] = _\n",
    "\n",
    "        peptide_list = [get_isoforms(mods_variable_r, peptide, max_) for peptide in peptide_list]\n",
    "        return list(chain.from_iterable(peptide_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMA', 'AoxMA', 'AAC', 'AAamC']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptide_list = ['AMA', 'AAC']\n",
    "mods_variable = ['oxM','amC']\n",
    "max_isoforms = 1024\n",
    "\n",
    "add_variable_mods(peptide_list, mods_variable, max_isoforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_add_variable_mods():\n",
    "    mods_variable = ['oxM']\n",
    "    peptide = ['AMAMA']\n",
    "\n",
    "    peptides_new = add_variable_mods(peptide, [], 1024)\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    peptides_new = add_variable_mods(peptide, mods_variable, 1024)\n",
    "\n",
    "    assert set(['AMAMA', 'AMAoxMA', 'AoxMAMA', 'AoxMAoxMA']) == set(peptides_new)\n",
    "\n",
    "    # Check if number of isoforms is correct\n",
    "    peptides_new = add_variable_mods(peptide, mods_variable, 3)\n",
    "    assert len(peptides_new) == 3\n",
    "    \n",
    "    \n",
    "    peptide_list = ['PEPTIDE']\n",
    "    mods_variable = ['pP','pS']\n",
    "    max_isoforms = 1024\n",
    "\n",
    "    peptides_new = add_variable_mods(peptide_list, mods_variable, max_isoforms)\n",
    "    \n",
    "    assert len(peptides_new) == 4\n",
    "    \n",
    "test_add_variable_mods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal Modifications - Fixed\n",
    "\n",
    "To handle terminal modifications, we use the following convention:\n",
    "\n",
    "* `<` for the left side (N-terminal)\n",
    "* `>` for the right side (C-Terminal)\n",
    "\n",
    "Additionally, if we want to have a terminal modification on any AA we indicate this `^`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_fixed_mod_terminal(peptides, mod):\n",
    "    \"\"\"\n",
    "    Adds fixed terminal modifications\n",
    "    \"\"\"\n",
    "    # < for left side (N-Term), > for right side (C-Term)\n",
    "    if \"<^\" in mod: #Any n-term, e.g. a<^\n",
    "        peptides = [mod[:-2] + peptide for peptide in peptides]\n",
    "    elif \">^\" in mod: #Any c-term, e.g. a>^\n",
    "        peptides = [peptide[:-1] + mod[:-2] + peptide[-1] for peptide in peptides]\n",
    "    elif \"<\" in mod: #only if specific AA, e.g. ox<C\n",
    "        peptides = [peptide[0].replace(mod[-1], mod[:-2]+mod[-1]) + peptide[1:] for peptide in peptides]\n",
    "    elif \">\" in mod:\n",
    "        peptides = [peptide[:-1] + peptide[-1].replace(mod[-1], mod[:-2]+mod[-1]) for peptide in peptides]\n",
    "    else:\n",
    "        # This should not happen\n",
    "        raise (\"Invalid fixed terminal modification {}.\".format(key))\n",
    "    return peptides\n",
    "\n",
    "def add_fixed_mods_terminal(peptides, mods_fixed_terminal, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper to add fixed mods on sequences and lists of mods\n",
    "    \"\"\"\n",
    "    if mods_fixed_terminal == []:\n",
    "        return peptides\n",
    "    else:\n",
    "        # < for left side (N-Term), > for right side (C-Term)\n",
    "        for key in mods_fixed_terminal:\n",
    "            peptides = add_fixed_mod_terminal(peptides, key)\n",
    "        return peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any n-term modified with x (x<^): ['xAMAMA']\n",
      "Any c-term modified with x (x>^): ['AMAMxA']\n",
      "Only A on n-term modified with x (x<A): ['xAMAMA']\n",
      "Only A on c-term modified with x (x<A): ['AMAMxA']\n"
     ]
    }
   ],
   "source": [
    "peptide = ['AMAMA']\n",
    "\n",
    "print('Any n-term modified with x (x<^):', add_fixed_mods_terminal(peptide, ['x<^']))\n",
    "print('Any c-term modified with x (x>^):', add_fixed_mods_terminal(peptide, ['x>^']))\n",
    "print('Only A on n-term modified with x (x<A):', add_fixed_mods_terminal(peptide, ['x<A']))\n",
    "print('Only A on c-term modified with x (x<A):', add_fixed_mods_terminal(peptide, ['x>A']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_add_fixed_mods_terminal():\n",
    "    peptide = ['AMAMA']\n",
    "\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, [])\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    #Any N-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x<^'])\n",
    "    assert peptides_new == ['xAMAMA']\n",
    "\n",
    "    #Any C-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x>^'])\n",
    "    assert peptides_new == ['AMAMxA']\n",
    "\n",
    "    #Selected N-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x<A'])\n",
    "    assert peptides_new == ['xAMAMA']\n",
    "\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x<C'])\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    #Selected C-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x>A'])\n",
    "    assert peptides_new == ['AMAMxA']\n",
    "\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x>C'])\n",
    "    assert peptides_new == peptide\n",
    "    \n",
    "test_add_fixed_mods_terminal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal Modifications - Variable\n",
    "\n",
    "Lastly, to handle terminal variable modifications we use the function `add_variable_mods_terminal`. As the modifcation can only be at the terminal end this function only adds a peptide where the terminal end is modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_variable_mods_terminal(peptides, mods_variable_terminal, **kwargs):\n",
    "    \"Function to add variable terminal modifications\"\n",
    "    if not mods_variable_terminal:\n",
    "        return peptides\n",
    "    else:\n",
    "        new_peptides_n = peptides.copy()\n",
    "\n",
    "        for key in mods_variable_terminal:\n",
    "            if \"<\" in key:\n",
    "                # Only allow one variable mod on one end\n",
    "                new_peptides_n.extend(\n",
    "                    add_fixed_mod_terminal(peptides, key)\n",
    "                )\n",
    "        new_peptides_n = get_unique_peptides(new_peptides_n)\n",
    "        # N complete, let's go for c-terminal\n",
    "        new_peptides_c = new_peptides_n\n",
    "        for key in mods_variable_terminal:\n",
    "            if \">\" in key:\n",
    "                # Only allow one variable mod on one end\n",
    "                new_peptides_c.extend(\n",
    "                    add_fixed_mod_terminal(new_peptides_n, key)\n",
    "                )\n",
    "\n",
    "        return get_unique_peptides(new_peptides_c)\n",
    "\n",
    "def get_unique_peptides(peptides):\n",
    "    return list(set(peptides))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMAMA', 'xAMAMA']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptide_list = ['AMAMA']\n",
    "add_variable_mods_terminal(peptide_list, ['x<^'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_add_variable_mods_terminal():\n",
    "    peptide_list = ['AMAMA']\n",
    "\n",
    "    peptides_new = add_variable_mods_terminal(peptide_list, [])\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    #Any N-term\n",
    "    peptides_new = add_variable_mods_terminal(peptide_list, ['x<^'])\n",
    "    assert set(peptides_new) == set(['xAMAMA', 'AMAMA'])\n",
    "    \n",
    "test_add_variable_mods_terminal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Peptides\n",
    "\n",
    "Lastly we put all the functions into a wrapper `generate_peptides`. It will accept a peptide and a dictionary with settings so that we can get all modified peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate_peptides(peptide, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper to get modified peptides from a peptide\n",
    "    \"\"\"\n",
    "    mod_peptide = add_fixed_mods_terminal([peptide], kwargs['mods_fixed_terminal_prot'])\n",
    "    mod_peptide = add_variable_mods_terminal(mod_peptide, kwargs['mods_variable_terminal_prot'])\n",
    "\n",
    "    peptides = []\n",
    "    [peptides.extend(cleave_sequence(_, **kwargs)) for _ in mod_peptide]\n",
    "    \n",
    "    peptides = [_ for _ in peptides if check_peptide(_, constants.AAs)]\n",
    "    \n",
    "    max_isoforms = kwargs['max_isoforms']\n",
    "    \n",
    "    all_peptides = []\n",
    "    for peptide in peptides: #1 per, limit the number of isoforms\n",
    "        #Regular peptides\n",
    "        mod_peptides = add_fixed_mods([peptide], **kwargs)\n",
    "        mod_peptides = add_fixed_mods_terminal(mod_peptides, **kwargs)\n",
    "        mod_peptides = add_variable_mods_terminal(mod_peptides, **kwargs)\n",
    "        \n",
    "        kwargs['max_isoforms'] = max_isoforms - len(mod_peptides)\n",
    "        mod_peptides = add_variable_mods(mod_peptides, **kwargs)\n",
    "        \n",
    "        all_peptides.extend(mod_peptides)\n",
    "\n",
    "        #Decoys:\n",
    "        decoy_peptides = get_decoys([peptide], **kwargs)\n",
    "\n",
    "        mod_peptides_decoy = add_fixed_mods(decoy_peptides, **kwargs)\n",
    "        mod_peptides_decoy = add_fixed_mods_terminal(mod_peptides_decoy, **kwargs)\n",
    "        mod_peptides_decoy = add_variable_mods_terminal(mod_peptides_decoy, **kwargs)\n",
    "        \n",
    "        kwargs['max_isoforms'] = max_isoforms - len(mod_peptides_decoy)\n",
    "        \n",
    "        mod_peptides_decoy = add_variable_mods(mod_peptides_decoy, **kwargs)\n",
    "\n",
    "        mod_peptides_decoy = add_decoy_tag(mod_peptides_decoy)\n",
    "\n",
    "        \n",
    "        all_peptides.extend(mod_peptides_decoy)\n",
    "\n",
    "    return all_peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_peptide' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-f4957465d90c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"max_isoforms\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mgenerate_peptides\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PEPTIDEM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-e0f482e98cc5>\u001b[0m in \u001b[0;36mgenerate_peptides\u001b[1;34m(peptide, **kwargs)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mpeptides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleave_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmod_peptide\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mpeptides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpeptides\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_peptide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAAs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mmax_isoforms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_isoforms'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-e0f482e98cc5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mpeptides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleave_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmod_peptide\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mpeptides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpeptides\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_peptide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAAs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mmax_isoforms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_isoforms'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'check_peptide' is not defined"
     ]
    }
   ],
   "source": [
    "kwargs = {}\n",
    "\n",
    "kwargs[\"protease\"] = \"trypsin\"\n",
    "kwargs[\"num_missed_cleavages\"] = 2\n",
    "kwargs[\"min_length\"] = 6\n",
    "kwargs[\"max_length\"] = 27\n",
    "kwargs[\"mods_variable\"] = [\"oxM\"]\n",
    "kwargs[\"mods_variable_terminal\"] = []\n",
    "kwargs[\"mods_fixed\"] = [\"cC\"]\n",
    "kwargs[\"mods_fixed_terminal\"] = []\n",
    "kwargs[\"mods_fixed_terminal_prot\"] = []\n",
    "kwargs[\"mods_variable_terminal_prot\"]  = []\n",
    "kwargs[\"max_isoforms\"] = 1024\n",
    "\n",
    "generate_peptides('PEPTIDEM', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_generate_peptides():\n",
    "    kwargs = {}\n",
    "\n",
    "    kwargs[\"protease\"] = \"trypsin\"\n",
    "    kwargs[\"num_missed_cleavages\"] = 2\n",
    "    kwargs[\"min_length\"] = 6\n",
    "    kwargs[\"max_length\"] = 27\n",
    "    kwargs[\"mods_variable\"] = [\"oxM\"]\n",
    "    kwargs[\"mods_variable_terminal\"] = []\n",
    "    kwargs[\"mods_fixed\"] = [\"cC\"]\n",
    "    kwargs[\"mods_fixed_terminal\"] = []\n",
    "    kwargs[\"mods_fixed_terminal_prot\"] = []\n",
    "    kwargs[\"mods_variable_terminal_prot\"]  = []\n",
    "    kwargs[\"max_isoforms\"] = 1024\n",
    "    kwargs['pseudo_reverse'] = True\n",
    "\n",
    "    peps = generate_peptides('PEPTIDEM', **kwargs)\n",
    "    assert set(peps) == set(['PEPTIDEM', 'PEPTIDEoxM', 'EDITPEPM_decoy', 'EDITPEPoxM_decoy'])\n",
    "    \n",
    "test_generate_peptides()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mass Calculations\n",
    "\n",
    "Using the `mass_dict` from `constants` and being able to parse sequences with `parse` one can simply look up the masses for each modified or unmodified amino acid and add everything up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precursor\n",
    "\n",
    "To calculate the mass of the neutral precursor we start with the mass of an $H_2O$ and add the masses of all amino acids of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "from numba.typed import List\n",
    "import numpy as np\n",
    "\n",
    "@njit\n",
    "def get_precmass(parsed_pep, mass_dict):\n",
    "    \"\"\"\n",
    "    Calculate the mass of the neutral precursor\n",
    "    \"\"\"\n",
    "    tmass = mass_dict[\"H2O\"]\n",
    "    for _ in parsed_pep:\n",
    "        tmass += mass_dict[_]\n",
    "\n",
    "    return tmass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_precmass(parse('PEPTIDE'), constants.mass_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_precmass():\n",
    "    \n",
    "    precmass = get_precmass(parse('PEPTIDE'), constants.mass_dict)\n",
    "    \n",
    "    assert np.allclose(precmass, 799.3599642034599)\n",
    "    \n",
    "test_get_precmass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fragments\n",
    "\n",
    "Likewise, we can calculate the masses of the fragment ions. We employ two functions: `get_fragmass` and `get_frag_dict`. \n",
    "\n",
    "`get_fragmass` is a fast, `numba`-compatible function that calculates the fragmasses and returns an array indicating wheter the iontype was `b` or `y`. \n",
    "\n",
    "`get_frag_dict` instead is not `numba`-compatible and hence a bit slower. It returns a dictionary with the respective ion and can be used for plotting theoretical spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@njit\n",
    "def get_fragmass(parsed_pep, mass_dict):\n",
    "    \"\"\"\n",
    "    Calculate the masses of the fragment ions\n",
    "    \"\"\"\n",
    "    n_frags = (len(parsed_pep) - 1) * 2\n",
    "\n",
    "    frag_masses = np.zeros(n_frags, dtype=np.float64)\n",
    "    frag_type = np.zeros(n_frags, dtype=np.int8)\n",
    "\n",
    "    # b-ions > 0\n",
    "    n_frag = 0\n",
    "    \n",
    "    frag_m = mass_dict[\"Proton\"]\n",
    "    for idx, _ in enumerate(parsed_pep[:-1]):\n",
    "        frag_m += mass_dict[_]\n",
    "        frag_masses[n_frag] = frag_m\n",
    "        frag_type[n_frag] = (idx+1)\n",
    "        n_frag += 1\n",
    "        \n",
    "    # y-ions < 0\n",
    "    frag_m = mass_dict[\"Proton\"] + mass_dict[\"H2O\"]\n",
    "    for idx, _ in enumerate(parsed_pep[::-1][:-1]):\n",
    "        frag_m += mass_dict[_]\n",
    "        frag_masses[n_frag] = frag_m\n",
    "        frag_type[n_frag] = -(idx+1)\n",
    "        n_frag += 1\n",
    "\n",
    "    return frag_masses, frag_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fragmass(parse('PEPTIDE'), constants.mass_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_fragmass():\n",
    "    \n",
    "    frag_masses, frag_type = get_fragmass(parse('PEPTIDE'), constants.mass_dict)\n",
    "    \n",
    "    ref_masses = np.array([ 98.06004033, 227.10263343, 324.15539729, 425.20307579,\n",
    "        538.28713979, 653.31408289, 148.06043425, 263.08737735,\n",
    "        376.17144135, 477.21911985, 574.27188371, 703.31447681])\n",
    "    \n",
    "    assert np.allclose(frag_masses, ref_masses)\n",
    "                          \n",
    "test_get_fragmass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_frag_dict(parsed_pep, mass_dict):\n",
    "    \n",
    "    frag_dict = {}\n",
    "    frag_masses, frag_type = get_fragmass(parsed_pep, constants.mass_dict)\n",
    "    \n",
    "    for idx, _ in enumerate(frag_masses):\n",
    "        \n",
    "        cnt = frag_type[idx]\n",
    "        if cnt > 0:\n",
    "            identifier = 'b'\n",
    "        else:\n",
    "            identifier = 'y'\n",
    "            cnt = -cnt\n",
    "        frag_dict[identifier+str(cnt)] = _\n",
    "           \n",
    "    return frag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_frag_dict(parse('PEPTIDE'), constants.mass_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_frag_dict():\n",
    "    \n",
    "    refdict = {'b1': 98.06004032687,\n",
    " 'b2': 227.10263342686997,\n",
    " 'b3': 324.15539728686997,\n",
    " 'y1': 120.06551965033,\n",
    " 'y2': 217.11828351033,\n",
    " 'y3': 346.16087661033}\n",
    "    \n",
    "    newdict = get_frag_dict(parse('PEPT'), constants.mass_dict)\n",
    "    \n",
    "    for key in newdict.keys():\n",
    "        \n",
    "        assert np.allclose(refdict[key], newdict[key])\n",
    "        \n",
    "test_get_frag_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us also to generate the theorteical isotopes for a fragment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "peptide = 'PEPTIDE'\n",
    "\n",
    "frag_dict = get_frag_dict(parse(peptide), constants.mass_dict)\n",
    "\n",
    "db_frag = list(frag_dict.values())\n",
    "db_int = [100 for _ in db_frag]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "\n",
    "for _ in frag_dict.keys():\n",
    "    plt.text(frag_dict[_], 104, _, fontsize=12, alpha = 0.8)\n",
    "    \n",
    "plt.title('Theoretical Spectrum for {}'.format(peptide))\n",
    "plt.xlabel('Mass')\n",
    "plt.ylabel('Intensity')\n",
    "plt.ylim([0,110])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectra\n",
    "\n",
    "The function `get_spectrum` returns a tuple with the following content:\n",
    "\n",
    "* precursor mass\n",
    "* peptide sequence\n",
    "* fragmasses\n",
    "* fragtypes\n",
    "\n",
    "Likewise, `get_spectra` returns a list of tuples. We employ a list of tuples here as this way we can sort them easily by precursor mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def get_spectrum(peptide, mass_dict):\n",
    "    parsed_peptide = parse(peptide)\n",
    "\n",
    "    fragmasses, fragtypes = get_fragmass(parsed_peptide, mass_dict)\n",
    "    sortindex = np.argsort(fragmasses)\n",
    "    fragmasses = fragmasses[sortindex]\n",
    "    fragtypes = fragtypes[sortindex]\n",
    "\n",
    "    precmass = get_precmass(parsed_peptide, mass_dict)\n",
    "\n",
    "    return (precmass, peptide, fragmasses, fragtypes)\n",
    "\n",
    "@njit\n",
    "def get_spectra(peptides, mass_dict):\n",
    "    spectra = List()\n",
    "\n",
    "    for i in range(len(peptides)):\n",
    "        spectra.append(get_spectrum(peptides[i], mass_dict))\n",
    "\n",
    "    return spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_spectra(List(['PEPTIDE']), constants.mass_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_spectra():\n",
    "    \n",
    "    spectra = get_spectra(List(['PEPTIDE']), constants.mass_dict)\n",
    "    \n",
    "    precmass, peptide, frags, fragtypes = spectra[0]\n",
    "    \n",
    "    assert np.allclose(precmass, 799.3599642034599)\n",
    "    \n",
    "    assert peptide == 'PEPTIDE'\n",
    "    \n",
    "    assert np.allclose(frags, np.array([ 98.06004033, 148.06043425, 227.10263343, 263.08737735,\n",
    "       324.15539729, 376.17144135, 425.20307579, 477.21911985,\n",
    "       538.28713979, 574.27188371, 653.31408289, 703.31447681]))\n",
    "\n",
    "test_get_spectra()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading FASTA\n",
    "\n",
    "To read FASTA files we use the `SeqIO` module from the `Biopython` library. This is a generator expression so that we read one FASTA entry after another until the `StopIteration` is reached, which is implemented in `read_fasta_file`. Additionally, we define the funciton `read_fasta_file_entries` that simply counts the number of FASTA entries.\n",
    "\n",
    "All FASTA entries that contain AAs which are not in the mass_dict can be checked with `check_sequence` and will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "from glob import glob\n",
    "import logging\n",
    "\n",
    "def read_fasta_file(fasta_filename=\"\"):\n",
    "    \"\"\"\n",
    "    Read a FASTA file line by line\n",
    "    \"\"\"\n",
    "    with open(fasta_filename, \"rt\") as handle:\n",
    "        iterator = SeqIO.parse(handle, \"fasta\")\n",
    "        while iterator:\n",
    "            try:\n",
    "                record = next(iterator)\n",
    "                parts = record.id.split(\"|\")  # pipe char\n",
    "                if len(parts) > 1:\n",
    "                    id = parts[1]\n",
    "                else:\n",
    "                    id = record.name\n",
    "                sequence = str(record.seq)\n",
    "                entry = {\n",
    "                    \"id\": id,\n",
    "                    \"name\": record.name,\n",
    "                    \"description\": record.description,\n",
    "                    \"sequence\": sequence,\n",
    "                }\n",
    "\n",
    "                yield entry\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "\n",
    "def read_fasta_file_entries(fasta_filename=\"\"):\n",
    "    \"\"\"\n",
    "    Function to count entries in fasta file\n",
    "    \"\"\"\n",
    "    with open(fasta_filename, \"rt\") as handle:\n",
    "        iterator = SeqIO.parse(handle, \"fasta\")\n",
    "        count = 0\n",
    "        while iterator:\n",
    "            try:\n",
    "                record = next(iterator)\n",
    "                count+=1\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "        return count\n",
    "\n",
    "\n",
    "def check_sequence(element, AAs):\n",
    "    \"\"\"\n",
    "    Checks wheter a sequence from a FASTA entry contains valid AAs\n",
    "    \"\"\"\n",
    "    if not set(element['sequence']).issubset(AAs):\n",
    "        unknown = set(element['sequence']) - set(AAs)\n",
    "        logging.error(f'This FASTA entry contains unknown AAs {unknown} - Peptides with unknown AAs will be skipped: \\n {element}\\n')\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "def check_peptide(peptide, AAs):\n",
    "    \n",
    "    if set(peptide).issubset(AAs):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load example fasta file\n",
    "\n",
    "fasta_path = '../testfiles/test.fasta'\n",
    "\n",
    "list(read_fasta_file(fasta_path))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peptide Dictionary\n",
    "\n",
    "In order to efficiently store peptides, we rely on the Python dictionary. The idea is to have a dictionary with peptides as keys and indices to proteins as values. This way, one can quickly look up to which protein a peptide belongs to. The function `add_to_pept_dict` uses a regular python dictionary and allows to add peptides and stores indices to the originating proteins as a list. If a peptide is already present in the dictionary, the list is appended. The function returns a list of `added_peptides`, which were not present in the dictionary yet. One can use the function `merge_pept_dicts` to merge multiple peptide dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_to_pept_dict(pept_dict, new_peptides, i):\n",
    "    \"\"\"\n",
    "    Add peptides to the peptide dictionary\n",
    "    \"\"\"\n",
    "    added_peptides = List()\n",
    "    for peptide in new_peptides:\n",
    "        if peptide in pept_dict:\n",
    "            pept_dict[peptide].append(i)\n",
    "        else:\n",
    "            pept_dict[peptide] = [i]\n",
    "            added_peptides.append(peptide)\n",
    "\n",
    "    return pept_dict, added_peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pept_dict = {}\n",
    "new_peptides = ['ABC','DEF']\n",
    "\n",
    "pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 0)\n",
    "\n",
    "new_peptides = ['DEF','GHI']\n",
    "\n",
    "pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 1)\n",
    "\n",
    "print(pept_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_add_to_pept_dict():\n",
    "    pept_dict = {}\n",
    "    new_peptides = ['ABC','DEF']\n",
    "\n",
    "    pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 0)\n",
    "\n",
    "    new_peptides = ['DEF','GHI']\n",
    "\n",
    "    pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 1)\n",
    "    \n",
    "    assert pept_dict == {'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n",
    "    \n",
    "test_add_to_pept_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def merge_pept_dicts(list_of_pept_dicts):\n",
    "    \n",
    "    if len(list_of_pept_dicts) == 0:\n",
    "        raise ValueError('Need to pass at least 1 element.')\n",
    "    \n",
    "    new_pept_dict = list_of_pept_dicts[0]\n",
    "\n",
    "    for pept_dict in list_of_pept_dicts[1:]:\n",
    "\n",
    "        for key in pept_dict.keys():\n",
    "            if key in new_pept_dict:\n",
    "                for element in pept_dict[key]:\n",
    "                    new_pept_dict[key].append(element)\n",
    "            else:\n",
    "                new_pept_dict[key] = pept_dict[key]\n",
    "\n",
    "    return new_pept_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pept_dict_1 = {'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n",
    "pept_dict_2 = {'ABC': [3,4], 'JKL': [5, 6], 'MNO': [7]}\n",
    "\n",
    "merge_pept_dicts([pept_dict_1, pept_dict_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_merge_pept_dicts():\n",
    "    pept_dict_1 = {'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n",
    "    pept_dict_2 = {'ABC': [3,4], 'JKL': [5, 6], 'MNO': [7]}\n",
    "\n",
    "    assert merge_pept_dicts([pept_dict_1, pept_dict_2]) == {'ABC': [0, 3, 4], 'DEF': [0, 1], 'GHI': [1], 'JKL': [5, 6], 'MNO': [7]}\n",
    "    \n",
    "test_merge_pept_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a database\n",
    "\n",
    "To wrap everything up, we employ two functions `generate_database` and `generate_spectra`. The first one reads a FASTA file and generates a list of peptides, as well as the peptide dictionary and an ordered FASTA dictionary to be able to look up the protein indices laster. For the `callback` we first read the whole FASTA file to determine the total number of entries in the FASTA file.  For a typical FASTA file of 30 Mb with 40k entries, this should take less than a second. The progress of the digestion is monitored by processing the FASTA file one by one.\n",
    "The function `generate_spectra` then calculates precursor masses and fragment ions. Here, we split the total_number of sequences in `1000` steps to be able to track progress with the `callback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import OrderedDict\n",
    "\n",
    "def generate_fasta_list(fasta_paths, callback = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    fasta_list = []\n",
    "\n",
    "    fasta_dict = OrderedDict()\n",
    "\n",
    "    fasta_index = 0\n",
    "\n",
    "    if type(fasta_paths) is str:\n",
    "        fasta_paths = [fasta_paths]\n",
    "        n_fastas = 1\n",
    "\n",
    "    elif type(fasta_paths) is list:\n",
    "        n_fastas = len(fasta_paths)\n",
    "\n",
    "    for f_id, fasta_file in enumerate(fasta_paths):\n",
    "        n_entries = read_fasta_file_entries(fasta_file)\n",
    "\n",
    "        fasta_generator = read_fasta_file(fasta_file)\n",
    "\n",
    "        for element in fasta_generator:\n",
    "            check_sequence(element, constants.AAs)\n",
    "            fasta_list.append(element)\n",
    "            fasta_dict[fasta_index] = element\n",
    "            fasta_index += 1\n",
    "    return fasta_list, fasta_dict\n",
    "\n",
    "\n",
    "def generate_database(mass_dict, fasta_paths, callback = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    to_add = List()\n",
    "    fasta_dict = OrderedDict()\n",
    "    fasta_index = 0\n",
    "\n",
    "    pept_dict = {}\n",
    "\n",
    "    if type(fasta_paths) is str:\n",
    "        fasta_paths = [fasta_paths]\n",
    "        n_fastas = 1\n",
    "\n",
    "    elif type(fasta_paths) is list:\n",
    "        n_fastas = len(fasta_paths)\n",
    "\n",
    "    for f_id, fasta_file in enumerate(fasta_paths):\n",
    "        n_entries = read_fasta_file_entries(fasta_file)\n",
    "\n",
    "        fasta_generator = read_fasta_file(fasta_file)\n",
    "\n",
    "        for element in fasta_generator:\n",
    "            \n",
    "            fasta_dict[fasta_index] = element\n",
    "            mod_peptides = generate_peptides(element[\"sequence\"], **kwargs)\n",
    "            pept_dict, added_seqs = add_to_pept_dict(pept_dict, mod_peptides, fasta_index)\n",
    "            if len(added_seqs) > 0:\n",
    "                to_add.extend(added_seqs)\n",
    "\n",
    "            fasta_index += 1\n",
    "\n",
    "            if callback:\n",
    "                callback(fasta_index/n_entries/n_fastas+f_id)\n",
    "\n",
    "    return to_add, pept_dict, fasta_dict\n",
    "\n",
    "\n",
    "def generate_spectra(to_add, mass_dict, callback = None):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "\n",
    "    if len(to_add) > 0:\n",
    "\n",
    "        if callback: #Chunk the spectra to get a progress_bar\n",
    "            spectra = []\n",
    "\n",
    "            stepsize = int(np.ceil(len(to_add)/1000))\n",
    "\n",
    "            for i in range(0, len(to_add), stepsize):\n",
    "                sub = to_add[i:i + stepsize]\n",
    "                spectra.extend(get_spectra(sub, mass_dict))\n",
    "                callback((i+1)/len(to_add))\n",
    "\n",
    "        else:\n",
    "            spectra = get_spectra(to_add, mass_dict)\n",
    "    else:\n",
    "        raise ValueError(\"No spectra to generate.\")\n",
    "\n",
    "    return spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized version\n",
    "\n",
    "To speed up spectra generated, one can use the parallelized version. The function `generate_database_parallel` reads an entire FASTA file and splits it into multiple blocks. Each block will be processed, and the generated pept_dicts will be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from multiprocessing import Pool\n",
    "from alphapept import constants\n",
    "mass_dict = constants.mass_dict\n",
    "\n",
    "def block_idx(len_list, block_size = 1000):\n",
    "    \"\"\"\n",
    "    Create indices for a list of length len_list\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "\n",
    "    while end <= len_list:\n",
    "        end += block_size\n",
    "        blocks.append((start, end))\n",
    "        start = end\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def blocks(l, n):\n",
    "    \"\"\"\n",
    "    Create blocks from a given list\n",
    "    \"\"\"\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))\n",
    "\n",
    "def digest_fasta_block(to_process):\n",
    "    \"\"\"\n",
    "    Digest and create spectra for a whole fasta_block\n",
    "    \"\"\"\n",
    "\n",
    "    fasta_index, fasta_block, settings = to_process\n",
    "\n",
    "    to_add = List()\n",
    "\n",
    "    f_index = 0\n",
    "\n",
    "    pept_dict = {}\n",
    "    for element in fasta_block:\n",
    "        sequence = element[\"sequence\"]\n",
    "        mod_peptides = generate_peptides(sequence, **settings['fasta'])\n",
    "        pept_dict, added_peptides = add_to_pept_dict(pept_dict, mod_peptides, fasta_index+f_index)\n",
    "        if len(added_peptides) > 0:\n",
    "            to_add.extend(added_seqs)\n",
    "        f_index += 1\n",
    "\n",
    "    spectra = []\n",
    "    if len(to_add) > 0:\n",
    "        for specta_block in blocks(to_add, settings['fasta']['spectra_block']):\n",
    "            spectra.extend(generate_spectra(specta_block, mass_dict))\n",
    "\n",
    "    return (spectra, pept_dict)\n",
    "\n",
    "def generate_database_parallel(settings, callback = None):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    n_processes = settings['general']['n_processes']\n",
    "\n",
    "    fasta_list, fasta_dict = generate_fasta_list(**settings['fasta'])\n",
    "    \n",
    "    logging.info(f'FASTA contains {len(fasta_list):,} entries.')\n",
    "    \n",
    "    if len(fasta_list) > settings['fasta']['db_size']:\n",
    "        logging.info(f\"FASTA exceeds set db_size of {settings['fasta']['db_size']:,}. Shortening fasta.\")\n",
    "        fasta_list = fasta_list[:settings['fasta']['db_size']]\n",
    "        \n",
    "    blocks = block_idx(len(fasta_list), settings['fasta']['fasta_block'])\n",
    "\n",
    "    to_process = [(idx_start, fasta_list[idx_start:idx_end], settings) for idx_start, idx_end in  blocks]\n",
    "\n",
    "    spectra = []\n",
    "    pept_dicts = []\n",
    "    with Pool(n_processes) as p:\n",
    "        max_ = len(to_process)\n",
    "        for i, _ in enumerate(p.imap_unordered(digest_fasta_block, to_process)):\n",
    "            if callback:\n",
    "                callback((i+1)/max_)\n",
    "            spectra.extend(_[0])\n",
    "            pept_dicts.append(_[1])\n",
    "\n",
    "    spectra = sorted(spectra, key=lambda x: x[1])\n",
    "    spectra_set = [spectra[idx] for idx in range(len(spectra)-1) if spectra[idx][1] != spectra[idx+1][1]]\n",
    "    spectra_set.append(spectra[-1])\n",
    "\n",
    "    pept_dict = merge_pept_dicts(pept_dicts)\n",
    "\n",
    "    return spectra_set, pept_dict, fasta_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel search on large files\n",
    "\n",
    "In some cases (i.e., a lot of modifications), it will not be useful to save the database as it will consume too much memory. Here, we use the function `search_parallel` from search. It creates theoretical spectra on the fly and directly searches against them. As we cannot create a pept_dict here, we need to create one from the search results. For this, we group peptides by their FASTA index and generate a lookup dictionary that can be used as a pept_dict.\n",
    "\n",
    "> Note that we are passing the settings argument here. Search results should be stored in the corresponding path in the hdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pept_dict_from_search(settings):\n",
    "    \"\"\"\n",
    "    Generates a peptide dict from a large search\n",
    "    \"\"\"\n",
    "\n",
    "    paths = settings['experiment']['file_paths']\n",
    "    bases = [os.path.splitext(_)[0]+'.ms_data.hdf' for _ in paths]\n",
    "\n",
    "    all_dfs = []\n",
    "    for _ in bases:\n",
    "        try:\n",
    "            df = alphapept.io.MS_Data_File(_).read(dataset_name=\"peptide_fdr\")\n",
    "        except KeyError:\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "        if len(df) > 0:\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    if sum([len(_) for _ in all_dfs]) == 0:\n",
    "        raise ValueError(\"No sequences present to concatenate.\")\n",
    "\n",
    "    df = pd.concat(all_dfs)\n",
    "\n",
    "    df['fasta_index'] = df['fasta_index'].str.split(',')\n",
    "\n",
    "    lst_col = 'fasta_index'\n",
    "\n",
    "    df_ = pd.DataFrame({\n",
    "          col:np.repeat(df[col].values, df[lst_col].str.len())\n",
    "          for col in df.columns.drop(lst_col)}\n",
    "        ).assign(**{lst_col:np.concatenate(df[lst_col].values)})[df.columns]\n",
    "\n",
    "    df_['fasta_index'] = df_['fasta_index'].astype('int')\n",
    "    df_grouped = df_.groupby(['sequence'])['fasta_index'].unique()\n",
    "\n",
    "    pept_dict = {}\n",
    "    for keys, vals in zip(df_grouped.index, df_grouped.values):\n",
    "        pept_dict[keys] = vals.tolist()\n",
    "\n",
    "    return pept_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "To save the generated spectra, we rely on NumPy's NPZ format. For this, we create a dictionary and save all the generated elements. The container will contain the following elements:\n",
    "\n",
    "* `precursors`: An array containing the precursor masses\n",
    "* `seqs`: An array containing the peptide sequences for the precursor masses\n",
    "* `pept_dict`: A peptide dictionary to look up the peptides and return their FASTA index\n",
    "* `fasta_dict`: A fasta dictionary to look up the FASTA entry based on a pept_dict index\n",
    "* `fragmasses`: An array containing the fragment masses. Unoccupied cells are filled with -1\n",
    "* `fragtypes:`: An array containg the fragment types. 0 equals b-ions and 1 equals y-ions. Unoccupied cells are filled with -1\n",
    "* `bounds`: An integer array containing the upper bounds for the fragment masses / types array. This is needed to quickly slice the data.\n",
    "\n",
    "All arrays are sorted according to the precursor mass.\n",
    "\n",
    "> To access the dictionaries such as `pept_dict` or `fasta_dict`, one needs to extract them using the `.item()` method like so: `container['pept_dict'].item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphapept.io\n",
    "import pandas as pd\n",
    "\n",
    "def save_database(spectra, pept_dict, fasta_dict, database_path, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to save a database to the *.hdf format.\n",
    "    \"\"\"\n",
    "\n",
    "    precmasses, seqs, fragmasses, fragtypes = zip(*spectra)\n",
    "    sortindex = np.argsort(precmasses)\n",
    "    \n",
    "    to_save = {}\n",
    "    \n",
    "    to_save[\"precursors\"] = np.array(precmasses)[sortindex]\n",
    "    to_save[\"seqs\"] = np.array(seqs, dtype=object)[sortindex]\n",
    "    to_save[\"proteins\"] = pd.DataFrame(fasta_dict).T\n",
    "\n",
    "    to_save[\"fragmasses\"] = alphapept.io.list_to_numpy_f32(np.array(fragmasses, dtype='object')[sortindex])\n",
    "    to_save[\"fragtypes\"] = alphapept.io.list_to_numpy_f32(np.array(fragtypes, dtype='object')[sortindex])\n",
    "\n",
    "    to_save[\"bounds\"] = np.sum(to_save['fragmasses']>=0,axis=0).astype(np.int64)\n",
    "\n",
    "    db_file = alphapept.io.HDF_File(database_path, is_new_file=True)\n",
    "    for key, value in to_save.items():\n",
    "        db_file.write(value, dataset_name=key)\n",
    "    \n",
    "    peps = np.array(list(pept_dict), dtype=object)\n",
    "    indices = np.empty(len(peps) + 1, dtype=np.int64)\n",
    "    indices[0] = 0\n",
    "    indices[1:] = np.cumsum([len(pept_dict[i]) for i in peps])\n",
    "    proteins = np.concatenate([pept_dict[i] for i in peps])\n",
    "    \n",
    "    db_file.write(\"peptides\")\n",
    "    db_file.write(\n",
    "        peps,\n",
    "        dataset_name=\"sequences\",\n",
    "        group_name=\"peptides\"\n",
    "    )\n",
    "    db_file.write(\n",
    "        indices,\n",
    "        dataset_name=\"protein_indptr\",\n",
    "        group_name=\"peptides\"\n",
    "    )\n",
    "    db_file.write(\n",
    "        proteins,\n",
    "        dataset_name=\"protein_indices\",\n",
    "        group_name=\"peptides\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import collections\n",
    "\n",
    "def read_database(database_path:str, array_name:str=None):\n",
    "    db_file = alphapept.io.HDF_File(database_path)\n",
    "    if array_name is None:\n",
    "        db_data = {\n",
    "            key: db_file.read(\n",
    "                dataset_name=key\n",
    "            ) for key in db_file.read() if key not in (\n",
    "                \"proteins\",\n",
    "                \"peptides\"\n",
    "            )\n",
    "        }\n",
    "        db_data[\"fasta_dict\"] = np.array(\n",
    "            collections.OrderedDict(db_file.read(dataset_name=\"proteins\").T)\n",
    "        )\n",
    "        peps = db_file.read(dataset_name=\"sequences\", group_name=\"peptides\")\n",
    "        protein_indptr = db_file.read(\n",
    "            dataset_name=\"protein_indptr\",\n",
    "            group_name=\"peptides\"\n",
    "        )\n",
    "        protein_indices = db_file.read(\n",
    "            dataset_name=\"protein_indices\",\n",
    "            group_name=\"peptides\"\n",
    "        )\n",
    "        db_data[\"pept_dict\"] = np.array(\n",
    "            {\n",
    "                pep: (protein_indices[s: e]).tolist() for pep, s, e in zip(\n",
    "                    peps,\n",
    "                    protein_indptr[:-1],\n",
    "                    protein_indptr[1:],\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        db_data[\"seqs\"] = db_data[\"seqs\"].astype(str)\n",
    "    else:\n",
    "        db_data = db_file.read(dataset_name=array_name)\n",
    "    return db_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphapept] *",
   "language": "python",
   "name": "conda-env-alphapept-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
