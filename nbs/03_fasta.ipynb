{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTA\n",
    "\n",
    "> Functions related to generating spectra from FASTA files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all functions related to creating spectra from FASTA files. In brief, what we are doing is the following:\n",
    "\n",
    "1. Read a FASTA file and in silico digest the proteins to generate peptides\n",
    "2. For each peptide, calculate a theoretical spectrum and precursor mass\n",
    "3. Save spectra\n",
    "\n",
    "Currently, `numba` has only limited string support. A lot of the functions are therefore Python-native."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaving\n",
    "\n",
    "We use regular expressions to find potential cleavage sites for cleaving and write the wrapper `cleave_sequence` to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from alphapept import constants\n",
    "import re\n",
    "\n",
    "def get_missed_cleavages(sequences, n_missed_cleavages):\n",
    "    \"\"\"\n",
    "    Combine cleaved sequences to get sequences with missed cleavages\n",
    "    \"\"\"\n",
    "    missed = []\n",
    "    for k in range(len(sequences)-n_missed_cleavages):\n",
    "        missed.append(''.join(sequences[k-1:k+n_missed_cleavages]))\n",
    "\n",
    "    return missed\n",
    "\n",
    "\n",
    "def cleave_sequence(\n",
    "    sequence=\"\",\n",
    "    n_missed_cleavages=0,\n",
    "    protease=\"trypsin\",\n",
    "    pep_length_min=6,\n",
    "    pep_length_max=65,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleave a sequence with a given protease. Filters to have a minimum and maximum length.\n",
    "    \"\"\"\n",
    "\n",
    "    proteases = constants.protease_dict\n",
    "    pattern = proteases[protease]\n",
    "\n",
    "    p = re.compile(pattern)\n",
    "\n",
    "    cutpos = [m.start()+1 for m in p.finditer(sequence)]\n",
    "    cutpos.insert(0,0)\n",
    "    cutpos.append(len(sequence))\n",
    "\n",
    "    base_sequences = [sequence[cutpos[i]:cutpos[i+1]] for i in range(len(cutpos)-1)]\n",
    "\n",
    "    sequences = base_sequences.copy()\n",
    "\n",
    "    for i in range(1, n_missed_cleavages+1):\n",
    "        sequences.extend(get_missed_cleavages(base_sequences, i))\n",
    "\n",
    "    sequences = [_ for _ in sequences if len(_)>=pep_length_min and len(_)<=pep_length_max]\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABCDEFGHIJK', 'LMNOPQR']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protease = \"trypsin\"\n",
    "n_missed_cleavages = 0\n",
    "pep_length_min, pep_length_max = 6, 65\n",
    "\n",
    "cleave_sequence('ABCDEFGHIJKLMNOPQRST', n_missed_cleavages, protease, pep_length_min, pep_length_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_cleave_sequence():\n",
    "    \n",
    "    protease = \"trypsin\"\n",
    "    pep_length_min, pep_length_max = 6, 65\n",
    "\n",
    "    assert set(cleave_sequence('ABCDEFGHIJKLMNOPQRST', 0, protease, pep_length_min, pep_length_max)) == set(['ABCDEFGHIJK', 'LMNOPQR'])\n",
    "    assert set(cleave_sequence('ABCDEFGHIJKLMNOPQRST', 1, protease, pep_length_min, pep_length_max)) == set(['ABCDEFGHIJK', 'LMNOPQR', 'ABCDEFGHIJKLMNOPQR'])\n",
    "\n",
    "test_cleave_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting missed and internal cleavages\n",
    "The following are helper functions to retrieve the number of missed cleavages and internal cleavage sites for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "from alphapept import constants\n",
    "\n",
    "def count_missed_cleavages(sequence=\"\", protease=\"trypsin\", **kwargs):\n",
    "    \"\"\"\n",
    "    Counts the number of missed cleavages for a given sequence and protease\n",
    "    \"\"\"\n",
    "    proteases = constants.protease_dict\n",
    "    protease = proteases[protease]\n",
    "    p = re.compile(protease)\n",
    "    n_missed = len(p.findall(sequence))\n",
    "    return n_missed\n",
    "\n",
    "def count_internal_cleavages(sequence=\"\", protease=\"trypsin\", **kwargs):\n",
    "    \"\"\"\n",
    "    Counts the number of internal cleavage sites for a given sequence and protease\n",
    "    \"\"\"\n",
    "    proteases = constants.protease_dict\n",
    "    protease = proteases[protease]\n",
    "    match = re.search(protease,sequence[-1]+'_')\n",
    "    if match:\n",
    "        n_internal = 0\n",
    "    else:\n",
    "        n_internal = 1\n",
    "    return n_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "protease = \"trypsin\"\n",
    "print(count_missed_cleavages('ABCDEFGHIJKLMNOPQRST', protease))\n",
    "\n",
    "protease = \"trypsin\"\n",
    "print(count_internal_cleavages('ABCDEFGHIJKLMNOPQRST', protease))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_missed_cleavages():  \n",
    "    assert count_missed_cleavages('ABCDEFGHIJKLMNOPQRST', 'trypsin') == 2\n",
    "    assert count_missed_cleavages('ABCDEFGHIJKLMNOPQRST', 'clostripain') == 1\n",
    "    \n",
    "test_get_missed_cleavages()\n",
    "\n",
    "def test_get_internal_cleavages():\n",
    "    assert count_internal_cleavages('ABCDEFGHIJKLMNOPQRST', 'trypsin') == 1\n",
    "    assert count_internal_cleavages('ABCDEFGHIJKLMNOPQRSTK', 'trypsin') == 0\n",
    "\n",
    "test_get_internal_cleavages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "Peptides are composed out of amino acids that are written in capital letters - `PEPTIDE`. To distinguish modifications, they are written in lowercase such as `PEPTIoxDE` and can be of arbitrary length. For a modified amino acid (AA), the modification precedes the letter of the amino acid. Decoys are indicated with an underscore. Therefore, the `parse` function splits after `_`. When parsing, the peptide string is converted into a `numba`-compatible list, like so: `PEPoxTIDE` -> `[P, E, P, oxT, I, D, E]`. This allows that we can use the `mass_dict` from `alphapept.constants` to directly determine the masses for the corresponding amino acids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "from numba.typed import List\n",
    "\n",
    "@njit\n",
    "def parse(peptide):\n",
    "    \"\"\"\n",
    "    Parser to parse peptide strings\n",
    "    \"\"\"\n",
    "    if \"_\" in peptide:\n",
    "        peptide = peptide.split(\"_\")[0]\n",
    "    parsed = List()\n",
    "    string = \"\"\n",
    "\n",
    "    for i in peptide:\n",
    "        string += i\n",
    "        if i.isupper():\n",
    "            parsed.append(string)\n",
    "            string = \"\"\n",
    "\n",
    "    return parsed\n",
    "\n",
    "def list_to_numba(a_list):\n",
    "    numba_list = List()\n",
    "\n",
    "    for element in a_list:\n",
    "        numba_list.append(element)\n",
    "\n",
    "    return numba_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P, E, P, T, I, D, E, ...]\n",
      "[P, E, P, oxT, I, D, E, ...]\n"
     ]
    }
   ],
   "source": [
    "print(parse('PEPTIDE'))\n",
    "print(parse('PEPoxTIDE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_parse():\n",
    "    peptide = \"PEPTIDE\"\n",
    "    assert parse(peptide) == list_to_numba([\"P\", \"E\", \"P\", \"T\", \"I\", \"D\", \"E\"])\n",
    "    peptide = \"PEPoxTIDE\"\n",
    "    assert parse(peptide) == list_to_numba([\"P\", \"E\", \"P\", \"oxT\", \"I\", \"D\", \"E\"])\n",
    "    peptide = \"PEPTIDE_decoy\"\n",
    "    assert parse(peptide) == list_to_numba([\"P\", \"E\", \"P\", \"T\", \"I\", \"D\", \"E\"])\n",
    "    \n",
    "test_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoy\n",
    "\n",
    "The decoy strategy employed is a pseudo-reversal of the peptide sequence, keeping only the terminal amino acid and reversing the rest. Additionally, we can call the functions `swap_KR` and and `swap_AL` that will swap the respective AAs. The function `swap_KR` will only swap terminal AAs. The swapping functions only work if the AA is not modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def get_decoy_sequence(peptide, pseudo_reverse=False, AL_swap=False, KR_swap = False):\n",
    "    \"\"\"\n",
    "    Reverses a sequence and adds the '_decoy' tag.\n",
    "\n",
    "    \"\"\"\n",
    "    pep = parse(peptide)\n",
    "    if pseudo_reverse:\n",
    "        rev_pep = pep[:-1][::-1]\n",
    "        rev_pep.append(pep[-1])\n",
    "    else:\n",
    "        rev_pep = pep[::-1]\n",
    "\n",
    "    if AL_swap:\n",
    "        rev_pep = swap_AL(rev_pep)\n",
    "\n",
    "    if KR_swap:\n",
    "        rev_pep = swap_KR(rev_pep)\n",
    "\n",
    "    rev_pep = \"\".join(rev_pep)\n",
    "\n",
    "    return rev_pep\n",
    "\n",
    "\n",
    "@njit\n",
    "def swap_KR(peptide):\n",
    "    \"\"\"\n",
    "    Swaps a terminal K or R. Note: Only if AA is not modified.\n",
    "    \"\"\"\n",
    "    if peptide[-1] == 'K':\n",
    "        peptide[-1] = 'R'\n",
    "    elif peptide[-1] == 'R':\n",
    "        peptide[-1] = 'K'\n",
    "\n",
    "    return peptide\n",
    "\n",
    "\n",
    "@njit\n",
    "def swap_AL(peptide):\n",
    "    \"\"\"\n",
    "    Swaps a A with L. Note: Only if AA is not modified.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < len(range(len(peptide) - 1)):\n",
    "        if peptide[i] == \"A\":\n",
    "            peptide[i] = peptide[i + 1]\n",
    "            peptide[i + 1] = \"A\"\n",
    "            i += 1\n",
    "        elif peptide[i] == \"L\":\n",
    "            peptide[i] = peptide[i + 1]\n",
    "            peptide[i + 1] = \"L\"\n",
    "            i += 1\n",
    "        i += 1\n",
    "\n",
    "    return peptide\n",
    "\n",
    "def get_decoys(peptide_list, pseudo_reverse=False, AL_swap=False, KR_swap = False, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper to get decoys for lists of peptides\n",
    "    \"\"\"\n",
    "    decoys = []\n",
    "    decoys.extend([get_decoy_sequence(peptide, pseudo_reverse, AL_swap, KR_swap) for peptide in peptide_list])\n",
    "    return decoys\n",
    "\n",
    "def add_decoy_tag(peptides):\n",
    "    \"\"\"\n",
    "    Adds a _decoy tag to a list of peptides\n",
    "    \"\"\"\n",
    "    return [peptide + \"_decoy\" for peptide in peptides]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K, K, K, L, A, K, K, K, ...]\n",
      "[A, A, A, K, R, A, A, A, ...]\n"
     ]
    }
   ],
   "source": [
    "print(swap_AL(parse('KKKALKKK')))\n",
    "print(swap_KR(parse('AAAKRAAA')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDITPEP\n"
     ]
    }
   ],
   "source": [
    "print(get_decoy_sequence('PEPTIDE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CBA', 'FED', 'IHG']\n"
     ]
    }
   ],
   "source": [
    "print(get_decoys(['ABC','DEF','GHI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_swap_AL():\n",
    "    assert swap_AL(parse(\"ABCDEF\")) == parse(\"BACDEF\")\n",
    "    assert swap_AL(parse(\"GHIKLM\")) == parse(\"GHIKML\")\n",
    "    assert swap_AL(parse(\"FEDCBA\")) == parse(\"FEDCBA\")\n",
    "    assert swap_AL(parse(\"GHIKL\")) == parse(\"GHIKL\")\n",
    "    assert swap_AL(parse(\"ABCDEFGHIKLM\")) == parse(\"BACDEFGHIKML\")\n",
    "    assert swap_AL(parse(\"BBAcCD\")) == parse(\"BBcCAD\")\n",
    "    assert swap_AL(parse(\"FEDCBA\")) == parse(\"FEDCBA\")\n",
    "\n",
    "test_swap_AL()\n",
    "\n",
    "def test_swapKR():\n",
    "    assert swap_KR(parse(\"ABCDEK\")) == parse(\"ABCDER\")\n",
    "    assert swap_KR(parse(\"ABCDER\")) == parse(\"ABCDEK\")\n",
    "    assert swap_KR(parse(\"ABCDEF\")) == parse(\"ABCDEF\")\n",
    "    assert swap_KR(parse(\"KABCDEF\")) == parse(\"KABCDEF\")\n",
    "    assert swap_KR(parse(\"KABCRDEF\")) == parse(\"KABCRDEF\")\n",
    "    assert swap_KR(parse(\"KABCKDEF\")) == parse(\"KABCKDEF\")\n",
    "\n",
    "test_swapKR()\n",
    "    \n",
    "def test_get_decoy_sequence():\n",
    "    peptide = \"PEPTIDER\"\n",
    "    assert get_decoy_sequence(peptide, pseudo_reverse=True) == \"EDITPEPR\"\n",
    "    assert get_decoy_sequence(peptide) == \"REDITPEP\"\n",
    "    assert get_decoy_sequence(peptide, KR_swap=True, pseudo_reverse=True) == \"EDITPEPK\"\n",
    "    \n",
    "test_get_decoy_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications\n",
    "\n",
    "To add modifications to the peptides, we distinguish fixed and variable modifications. Additionally, we make a distinction between whether the modification is only terminal or not. \n",
    "\n",
    "### Fixed Modifications\n",
    "Fixed modifications are implemented by passing a list with modified AAs that should be replaced. As a AA is only one letter, the remainder is the modification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_fixed_mods(seqs, mods_fixed, **kwargs):\n",
    "    \"\"\"\n",
    "    Adds fixed modifications to sequences.\n",
    "    \"\"\"\n",
    "    if not mods_fixed:\n",
    "        return seqs\n",
    "    else:\n",
    "        for mod_aa in mods_fixed:\n",
    "            seqs = [seq.replace(mod_aa[-1], mod_aa) for seq in seqs]\n",
    "        return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbBcCDEF']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods_fixed = ['cC','bB']\n",
    "peptide_list = ['ABCDEF']\n",
    "\n",
    "add_fixed_mods(peptide_list, mods_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caAbBcCDEF']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods_fixed = ['aA','cC','bB']\n",
    "peptide_list = ['cABCDEF']\n",
    "\n",
    "add_fixed_mods(peptide_list, mods_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_add_fixed_mods():\n",
    "    mods_fixed = ['cC']\n",
    "    peptide_list = ['ABCDEF']\n",
    "\n",
    "    peptides_new = add_fixed_mods(peptide_list, [])\n",
    "    assert peptides_new == peptide_list\n",
    "    \n",
    "    peptides_new = add_fixed_mods(peptide_list, mods_fixed)\n",
    "    assert peptides_new == ['ABcCDEF']\n",
    "    \n",
    "test_add_fixed_mods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Modifications\n",
    "\n",
    "To employ variable modifications, we loop through each variable modification and each position of the peptide and add them to the peptide list. For each iteration in get_isoforms, one more variable modification will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_variable_mod(peps, mods_variable_dict):\n",
    "    peptides = []\n",
    "    for pep_ in peps:\n",
    "        pep, min_idx = pep_\n",
    "        for mod in mods_variable_dict:\n",
    "            for i in range(len(pep)):\n",
    "                if i >= min_idx:\n",
    "                    c = pep[i]\n",
    "                    if c == mod:\n",
    "                        peptides.append((pep[:i]+[mods_variable_dict[c]]+pep[i+1:], i))   \n",
    "    return peptides\n",
    "\n",
    "\n",
    "def get_isoforms(mods_variable_dict, peptide, isoforms_max, n_modifications_max=None):\n",
    "    \"\"\"\n",
    "    Function to generate isoforms for a given peptide - returns a list of isoforms.\n",
    "    The original sequence is included in the list\n",
    "    \"\"\"\n",
    "    pep = list(parse(peptide))\n",
    "\n",
    "    peptides = [pep]\n",
    "    new_peps = [(pep, 0)]\n",
    "    \n",
    "    iteration = 0\n",
    "    while len(peptides) < isoforms_max:\n",
    "        \n",
    "        \n",
    "        if n_modifications_max:\n",
    "            if iteration >= n_modifications_max:\n",
    "                break\n",
    "\n",
    "        new_peps = add_variable_mod(new_peps, mods_variable_dict)\n",
    "\n",
    "        if len(new_peps) == 0:\n",
    "            break\n",
    "        if len(new_peps) > 1:\n",
    "            if new_peps[0][0] == new_peps[1][0]:\n",
    "                new_peps = new_peps[0:1]\n",
    "                \n",
    "        for _ in new_peps:\n",
    "            if len(peptides) < isoforms_max:\n",
    "                peptides.append(_[0])\n",
    "                \n",
    "        iteration +=1 \n",
    "                \n",
    "        \n",
    "\n",
    "    peptides = [''.join(_) for _ in peptides]\n",
    "\n",
    "    return peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PEPTIDE', 'pPEPTIDE', 'PEpPTIDE', 'pPEpPTIDE']\n",
      "['AMAMA', 'AoxMAMA', 'AMAoxMA', 'AoxMAoxMA']\n",
      "['AMAMA', 'AoxMAMA', 'AMAoxMA']\n"
     ]
    }
   ],
   "source": [
    "mods_variable_dict = {'S':'pS','P':'pP','M':'oxM'}\n",
    "isoforms_max = 1024\n",
    "\n",
    "print(get_isoforms(mods_variable_dict, 'PEPTIDE', isoforms_max))\n",
    "print(get_isoforms(mods_variable_dict, 'AMAMA', isoforms_max))\n",
    "print(get_isoforms(mods_variable_dict, 'AMAMA', isoforms_max, n_modifications_max=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define the wrapper `add_variable_mods` so that the functions can be called for lists of peptides and a list of variable modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_isoforms():\n",
    "\n",
    "    mods_variable_dict = {'S':'pS','P':'pP'}\n",
    "    peptide = 'PEPTIDE'\n",
    "    isoforms_max = 1024\n",
    "    get_isoforms(mods_variable_dict, peptide, isoforms_max)\n",
    "\n",
    "    assert len(get_isoforms(mods_variable_dict, peptide, isoforms_max)) == 4\n",
    "    \n",
    "test_get_isoforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from itertools import chain\n",
    "\n",
    "def add_variable_mods(peptide_list, mods_variable, isoforms_max, n_modifications_max, **kwargs):\n",
    "    #the peptide_list originates from one peptide already -> limit isoforms here\n",
    "    \n",
    "    max_ = isoforms_max - len(peptide_list) + 1\n",
    "    \n",
    "    if max_ < 0:\n",
    "        max_ = 0\n",
    "    \n",
    "    if not mods_variable:\n",
    "        return peptide_list\n",
    "    else:\n",
    "        mods_variable_r = {}\n",
    "        for _ in mods_variable:\n",
    "            mods_variable_r[_[-1]] = _\n",
    "\n",
    "        peptide_list = [get_isoforms(mods_variable_r, peptide, max_, n_modifications_max) for peptide in peptide_list]\n",
    "        return list(chain.from_iterable(peptide_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMA', 'AoxMA', 'AAC', 'AAamC']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptide_list = ['AMA', 'AAC']\n",
    "mods_variable = ['oxM','amC']\n",
    "isoforms_max = 1024\n",
    "n_modifications_max = 10\n",
    "\n",
    "add_variable_mods(peptide_list, mods_variable, isoforms_max, n_modifications_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_add_variable_mods():\n",
    "    mods_variable = ['oxM']\n",
    "    peptide = ['AMAMA']\n",
    "\n",
    "    peptides_new = add_variable_mods(peptide, [], 1024, None)\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    peptides_new = add_variable_mods(peptide, mods_variable, 1024, None)\n",
    "\n",
    "    assert set(['AMAMA', 'AMAoxMA', 'AoxMAMA', 'AoxMAoxMA']) == set(peptides_new)\n",
    "\n",
    "    # Check if number of isoforms is correct\n",
    "    peptides_new = add_variable_mods(peptide, mods_variable, 3, None)\n",
    "    assert len(peptides_new) == 3\n",
    "    \n",
    "    \n",
    "    peptide_list = ['PEPTIDE']\n",
    "    mods_variable = ['pP','pS']\n",
    "    isoforms_max = 1024\n",
    "\n",
    "    peptides_new = add_variable_mods(peptide_list, mods_variable, isoforms_max, None)\n",
    "    \n",
    "    assert len(peptides_new) == 4\n",
    "    \n",
    "test_add_variable_mods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal Modifications - Fixed\n",
    "\n",
    "To handle terminal modifications, we use the following convention:\n",
    "\n",
    "* `<` for the left side (N-terminal)\n",
    "* `>` for the right side (C-Terminal)\n",
    "\n",
    "Additionally, if we want to have a terminal modification on any AA we indicate this `^`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_fixed_mod_terminal(peptides, mod):\n",
    "    \"\"\"\n",
    "    Adds fixed terminal modifications\n",
    "    \"\"\"\n",
    "    # < for left side (N-Term), > for right side (C-Term)\n",
    "    if \"<^\" in mod: #Any n-term, e.g. a<^\n",
    "        peptides = [mod[:-2] + peptide for peptide in peptides]\n",
    "    elif \">^\" in mod: #Any c-term, e.g. a>^\n",
    "        peptides = [peptide[:-1] + mod[:-2] + peptide[-1] for peptide in peptides]\n",
    "    elif \"<\" in mod: #only if specific AA, e.g. ox<C\n",
    "        peptides = [peptide[0].replace(mod[-1], mod[:-2]+mod[-1]) + peptide[1:] for peptide in peptides]\n",
    "    elif \">\" in mod:\n",
    "        peptides = [peptide[:-1] + peptide[-1].replace(mod[-1], mod[:-2]+mod[-1]) for peptide in peptides]\n",
    "    else:\n",
    "        # This should not happen\n",
    "        raise (\"Invalid fixed terminal modification {}.\".format(key))\n",
    "    return peptides\n",
    "\n",
    "def add_fixed_mods_terminal(peptides, mods_fixed_terminal, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper to add fixed mods on sequences and lists of mods\n",
    "    \"\"\"\n",
    "    if mods_fixed_terminal == []:\n",
    "        return peptides\n",
    "    else:\n",
    "        # < for left side (N-Term), > for right side (C-Term)\n",
    "        for key in mods_fixed_terminal:\n",
    "            peptides = add_fixed_mod_terminal(peptides, key)\n",
    "        return peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with peptide ['AMAMA']\n",
      "Any n-term modified with x (x<^): ['xAMAMA']\n",
      "Any c-term modified with x (x>^): ['AMAMxA']\n",
      "Only A on n-term modified with x (x<A): ['xAMAMA']\n",
      "Only A on c-term modified with x (x<A): ['AMAMxA']\n"
     ]
    }
   ],
   "source": [
    "peptide = ['AMAMA']\n",
    "\n",
    "print(f'Starting with peptide {peptide}')\n",
    "print('Any n-term modified with x (x<^):', add_fixed_mods_terminal(peptide, ['x<^']))\n",
    "print('Any c-term modified with x (x>^):', add_fixed_mods_terminal(peptide, ['x>^']))\n",
    "print('Only A on n-term modified with x (x<A):', add_fixed_mods_terminal(peptide, ['x<A']))\n",
    "print('Only A on c-term modified with x (x<A):', add_fixed_mods_terminal(peptide, ['x>A']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_add_fixed_mods_terminal():\n",
    "    peptide = ['AMAMA']\n",
    "\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, [])\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    #Any N-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x<^'])\n",
    "    assert peptides_new == ['xAMAMA']\n",
    "\n",
    "    #Any C-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x>^'])\n",
    "    assert peptides_new == ['AMAMxA']\n",
    "\n",
    "    #Selected N-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x<A'])\n",
    "    assert peptides_new == ['xAMAMA']\n",
    "\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x<C'])\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    #Selected C-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x>A'])\n",
    "    assert peptides_new == ['AMAMxA']\n",
    "\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x>C'])\n",
    "    assert peptides_new == peptide\n",
    "    \n",
    "test_add_fixed_mods_terminal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal Modifications - Variable\n",
    "\n",
    "Lastly, to handle terminal variable modifications, we use the function `add_variable_mods_terminal`. As the modification can only be at the terminal end, this function only adds a peptide where the terminal end is modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_variable_mods_terminal(peptides, mods_variable_terminal, **kwargs):\n",
    "    \"Function to add variable terminal modifications\"\n",
    "    if not mods_variable_terminal:\n",
    "        return peptides\n",
    "    else:\n",
    "        new_peptides_n = peptides.copy()\n",
    "\n",
    "        for key in mods_variable_terminal:\n",
    "            if \"<\" in key:\n",
    "                # Only allow one variable mod on one end\n",
    "                new_peptides_n.extend(\n",
    "                    add_fixed_mod_terminal(peptides, key)\n",
    "                )\n",
    "        new_peptides_n = get_unique_peptides(new_peptides_n)\n",
    "        # N complete, let's go for c-terminal\n",
    "        new_peptides_c = new_peptides_n\n",
    "        for key in mods_variable_terminal:\n",
    "            if \">\" in key:\n",
    "                # Only allow one variable mod on one end\n",
    "                new_peptides_c.extend(\n",
    "                    add_fixed_mod_terminal(new_peptides_n, key)\n",
    "                )\n",
    "\n",
    "        return get_unique_peptides(new_peptides_c)\n",
    "\n",
    "def get_unique_peptides(peptides):\n",
    "    return list(set(peptides))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMAMA', 'xAMAMA']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptide_list = ['AMAMA']\n",
    "add_variable_mods_terminal(peptide_list, ['x<^'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_add_variable_mods_terminal():\n",
    "    peptide_list = ['AMAMA']\n",
    "\n",
    "    peptides_new = add_variable_mods_terminal(peptide_list, [])\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    #Any N-term\n",
    "    peptides_new = add_variable_mods_terminal(peptide_list, ['x<^'])\n",
    "    assert set(peptides_new) == set(['xAMAMA', 'AMAMA'])\n",
    "    \n",
    "test_add_variable_mods_terminal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Peptides\n",
    "\n",
    "Lastly, we put all the functions into a wrapper `generate_peptides`. It will accept a peptide and a dictionary with settings so that we can get all modified peptides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate_peptides(peptide, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper to get modified peptides from a peptide\n",
    "    \n",
    "    TODO:\n",
    "    \n",
    "    There can be some edge-cases which are not defined yet.\n",
    "    Example:\n",
    "    Setting the same fixed modification - once for all peptides and once for only terminal for the protein.\n",
    "    The modification will thenbe applied twice.\n",
    "    \n",
    "    \"\"\"\n",
    "    mod_peptide = add_fixed_mods_terminal([peptide], kwargs['mods_fixed_terminal_prot'])\n",
    "    mod_peptide = add_variable_mods_terminal(mod_peptide, kwargs['mods_variable_terminal_prot'])\n",
    "    \n",
    "    peptides = []\n",
    "    [peptides.extend(cleave_sequence(_, **kwargs)) for _ in mod_peptide]\n",
    "\n",
    "    peptides = [_ for _ in peptides if check_peptide(_, constants.AAs)]\n",
    "    \n",
    "    isoforms_max = kwargs['isoforms_max']\n",
    "\n",
    "    all_peptides = []\n",
    "    for peptide in peptides: #1 per, limit the number of isoforms\n",
    "\n",
    "        #Regular peptides\n",
    "        mod_peptides = add_fixed_mods([peptide], **kwargs)\n",
    "        mod_peptides = add_fixed_mods_terminal(mod_peptides, **kwargs)\n",
    "        mod_peptides = add_variable_mods_terminal(mod_peptides, **kwargs)\n",
    "        \n",
    "        kwargs['isoforms_max'] = isoforms_max - len(mod_peptides)\n",
    "        mod_peptides = add_variable_mods(mod_peptides, **kwargs)\n",
    "        \n",
    "        all_peptides.extend(mod_peptides)\n",
    "\n",
    "        #Decoys:\n",
    "        decoy_peptides = get_decoys([peptide], **kwargs)\n",
    "\n",
    "        mod_peptides_decoy = add_fixed_mods(decoy_peptides, **kwargs)\n",
    "        mod_peptides_decoy = add_fixed_mods_terminal(mod_peptides_decoy, **kwargs)\n",
    "        mod_peptides_decoy = add_variable_mods_terminal(mod_peptides_decoy, **kwargs)\n",
    "        \n",
    "        kwargs['isoforms_max'] = isoforms_max - len(mod_peptides_decoy)\n",
    "        \n",
    "        mod_peptides_decoy = add_variable_mods(mod_peptides_decoy, **kwargs)\n",
    "\n",
    "        mod_peptides_decoy = add_decoy_tag(mod_peptides_decoy)\n",
    "\n",
    "        all_peptides.extend(mod_peptides_decoy)\n",
    "\n",
    "    return all_peptides\n",
    "\n",
    "def check_peptide(peptide, AAs):\n",
    "    \n",
    "    if set([_ for _ in peptide if _.isupper()]).issubset(AAs):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aPEPTIDEM',\n",
       " 'aPEPTIDEoxM',\n",
       " 'MEDITPEaP_decoy',\n",
       " 'oxMEDITPEaP_decoy',\n",
       " 'PEPTIDEM',\n",
       " 'PEPTIDEoxM',\n",
       " 'MEDITPEP_decoy',\n",
       " 'oxMEDITPEP_decoy']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {}\n",
    "\n",
    "kwargs[\"protease\"] = \"trypsin\"\n",
    "kwargs[\"n_missed_cleavages\"] = 2\n",
    "kwargs[\"pep_length_min\"] = 6\n",
    "kwargs[\"pep_length_max\"] = 27\n",
    "kwargs[\"mods_variable\"] = [\"oxM\"]\n",
    "kwargs[\"mods_variable_terminal\"] = []\n",
    "kwargs[\"mods_fixed\"] = [\"cC\"]\n",
    "kwargs[\"mods_fixed_terminal\"] = []\n",
    "kwargs[\"mods_fixed_terminal_prot\"] = []\n",
    "kwargs[\"mods_variable_terminal_prot\"]  = ['a<^']\n",
    "kwargs[\"isoforms_max\"] = 1024\n",
    "kwargs[\"n_modifications_max\"] = None\n",
    "\n",
    "generate_peptides('PEPTIDEM', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_generate_peptides():\n",
    "    kwargs = {}\n",
    "\n",
    "    kwargs[\"protease\"] = \"trypsin\"\n",
    "    kwargs[\"n_missed_cleavages\"] = 2\n",
    "    kwargs[\"pep_length_min\"] = 6\n",
    "    kwargs[\"pep_length_max\"] = 27\n",
    "    kwargs[\"mods_variable\"] = [\"oxM\"]\n",
    "    kwargs[\"mods_variable_terminal\"] = []\n",
    "    kwargs[\"mods_fixed\"] = [\"cC\"]\n",
    "    kwargs[\"mods_fixed_terminal\"] = []\n",
    "    kwargs[\"mods_fixed_terminal_prot\"] = []\n",
    "    kwargs[\"mods_variable_terminal_prot\"]  = []\n",
    "    kwargs[\"isoforms_max\"] = 1024\n",
    "    kwargs['pseudo_reverse'] = True\n",
    "    kwargs[\"n_modifications_max\"] = None\n",
    "\n",
    "    peps = generate_peptides('PEPTIDEM', **kwargs)\n",
    "    assert set(peps) == set(['PEPTIDEM', 'PEPTIDEoxM', 'EDITPEPM_decoy', 'EDITPEPoxM_decoy'])\n",
    "    \n",
    "test_generate_peptides()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mass Calculations\n",
    "\n",
    "Using the `mass_dict` from `constants` and being able to parse sequences with `parse`, one can simply look up the masses for each modified or unmodified amino acid and add everything up.\n",
    "\n",
    "### Precursor\n",
    "\n",
    "To calculate the mass of the neutral precursor, we start with the mass of an $H_2O$ and add the masses of all amino acids of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "from numba.typed import List\n",
    "import numpy as np\n",
    "\n",
    "@njit\n",
    "def get_precmass(parsed_pep, mass_dict):\n",
    "    \"\"\"\n",
    "    Calculate the mass of the neutral precursor\n",
    "    \"\"\"\n",
    "    tmass = mass_dict[\"H2O\"]\n",
    "    for _ in parsed_pep:\n",
    "        tmass += mass_dict[_]\n",
    "\n",
    "    return tmass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799.35996420346"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_precmass(parse('PEPTIDE'), constants.mass_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_precmass():\n",
    "    \n",
    "    precmass = get_precmass(parse('PEPTIDE'), constants.mass_dict)\n",
    "    \n",
    "    assert np.allclose(precmass, 799.3599642034599)\n",
    "    \n",
    "test_get_precmass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fragments\n",
    "\n",
    "Likewise, we can calculate the masses of the fragment ions. We employ two functions: `get_fragmass` and `get_frag_dict`. \n",
    "\n",
    "`get_fragmass` is a fast, `numba`-compatible function that calculates the fragment masses and returns an array indicating whether the ion-type was `b` or `y`. \n",
    "\n",
    "`get_frag_dict` instead is not `numba`-compatible and hence a bit slower. It returns a dictionary with the respective ion and can be used for plotting theoretical spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@njit\n",
    "def get_fragmass(parsed_pep, mass_dict):\n",
    "    \"\"\"\n",
    "    Calculate the masses of the fragment ions\n",
    "    \"\"\"\n",
    "    n_frags = (len(parsed_pep) - 1) * 2\n",
    "\n",
    "    frag_masses = np.zeros(n_frags, dtype=np.float64)\n",
    "    frag_type = np.zeros(n_frags, dtype=np.int8)\n",
    "\n",
    "    # b-ions > 0\n",
    "    n_frag = 0\n",
    "    \n",
    "    frag_m = mass_dict[\"Proton\"]\n",
    "    for idx, _ in enumerate(parsed_pep[:-1]):\n",
    "        frag_m += mass_dict[_]\n",
    "        frag_masses[n_frag] = frag_m\n",
    "        frag_type[n_frag] = (idx+1)\n",
    "        n_frag += 1\n",
    "        \n",
    "    # y-ions < 0\n",
    "    frag_m = mass_dict[\"Proton\"] + mass_dict[\"H2O\"]\n",
    "    for idx, _ in enumerate(parsed_pep[::-1][:-1]):\n",
    "        frag_m += mass_dict[_]\n",
    "        frag_masses[n_frag] = frag_m\n",
    "        frag_type[n_frag] = -(idx+1)\n",
    "        n_frag += 1\n",
    "\n",
    "    return frag_masses, frag_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 98.06004033, 227.10263343, 324.15539729, 425.20307579,\n",
       "        538.28713979, 653.31408289, 148.06043425, 263.08737735,\n",
       "        376.17144135, 477.21911985, 574.27188371, 703.31447681]),\n",
       " array([ 1,  2,  3,  4,  5,  6, -1, -2, -3, -4, -5, -6], dtype=int8))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fragmass(parse('PEPTIDE'), constants.mass_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_fragmass():\n",
    "    \n",
    "    frag_masses, frag_type = get_fragmass(parse('PEPTIDE'), constants.mass_dict)\n",
    "    \n",
    "    ref_masses = np.array([ 98.06004033, 227.10263343, 324.15539729, 425.20307579,\n",
    "        538.28713979, 653.31408289, 148.06043425, 263.08737735,\n",
    "        376.17144135, 477.21911985, 574.27188371, 703.31447681])\n",
    "    \n",
    "    assert np.allclose(frag_masses, ref_masses)\n",
    "                          \n",
    "test_get_fragmass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_frag_dict(parsed_pep, mass_dict):\n",
    "    \n",
    "    frag_dict = {}\n",
    "    frag_masses, frag_type = get_fragmass(parsed_pep, constants.mass_dict)\n",
    "    \n",
    "    for idx, _ in enumerate(frag_masses):\n",
    "        \n",
    "        cnt = frag_type[idx]\n",
    "        if cnt > 0:\n",
    "            identifier = 'b'\n",
    "        else:\n",
    "            identifier = 'y'\n",
    "            cnt = -cnt\n",
    "        frag_dict[identifier+str(cnt)] = _\n",
    "           \n",
    "    return frag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b1': 98.06004032687,\n",
       " 'b2': 227.10263342687,\n",
       " 'b3': 324.15539728686997,\n",
       " 'b4': 425.20307578687,\n",
       " 'b5': 538.28713978687,\n",
       " 'b6': 653.31408288687,\n",
       " 'y1': 148.06043425033,\n",
       " 'y2': 263.08737735033,\n",
       " 'y3': 376.17144135033,\n",
       " 'y4': 477.21911985033,\n",
       " 'y5': 574.27188371033,\n",
       " 'y6': 703.31447681033}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_frag_dict(parse('PEPTIDE'), constants.mass_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_frag_dict():\n",
    "    \n",
    "    refdict = {'b1': 98.06004032687,\n",
    " 'b2': 227.10263342686997,\n",
    " 'b3': 324.15539728686997,\n",
    " 'y1': 120.06551965033,\n",
    " 'y2': 217.11828351033,\n",
    " 'y3': 346.16087661033}\n",
    "    \n",
    "    newdict = get_frag_dict(parse('PEPT'), constants.mass_dict)\n",
    "    \n",
    "    for key in newdict.keys():\n",
    "        \n",
    "        assert np.allclose(refdict[key], newdict[key])\n",
    "        \n",
    "test_get_frag_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us also to generate the theoretical isotopes for a fragment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "peptide = 'PEPTIDE'\n",
    "\n",
    "frag_dict = get_frag_dict(parse(peptide), constants.mass_dict)\n",
    "\n",
    "db_frag = list(frag_dict.values())\n",
    "db_int = [100 for _ in db_frag]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "\n",
    "for _ in frag_dict.keys():\n",
    "    plt.text(frag_dict[_], 104, _, fontsize=12, alpha = 0.8)\n",
    "    \n",
    "plt.title('Theoretical Spectrum for {}'.format(peptide))\n",
    "plt.xlabel('Mass')\n",
    "plt.ylabel('Intensity')\n",
    "plt.ylim([0,110])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectra\n",
    "\n",
    "The function `get_spectrum` returns a tuple with the following content:\n",
    "\n",
    "* precursor mass\n",
    "* peptide sequence\n",
    "* fragmasses\n",
    "* fragtypes\n",
    "\n",
    "Likewise, `get_spectra` returns a list of tuples. We employ a list of tuples here as this way, we can sort them easily by precursor mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def get_spectrum(peptide, mass_dict):\n",
    "    parsed_peptide = parse(peptide)\n",
    "\n",
    "    fragmasses, fragtypes = get_fragmass(parsed_peptide, mass_dict)\n",
    "    sortindex = np.argsort(fragmasses)\n",
    "    fragmasses = fragmasses[sortindex]\n",
    "    fragtypes = fragtypes[sortindex]\n",
    "\n",
    "    precmass = get_precmass(parsed_peptide, mass_dict)\n",
    "\n",
    "    return (precmass, peptide, fragmasses, fragtypes)\n",
    "\n",
    "@njit\n",
    "def get_spectra(peptides, mass_dict):\n",
    "    spectra = List()\n",
    "\n",
    "    for i in range(len(peptides)):\n",
    "        try:\n",
    "            spectra.append(get_spectrum(peptides[i], mass_dict))\n",
    "        except Exception: #TODO: This is to fix edge cases when having multiple modifications on the same AA.  \n",
    "            pass\n",
    "\n",
    "    return spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(799.35996420346, 'PEPTIDE', array([ 98.06004033, 148.06043425, 227.10263343, 263.08737735,\n",
      "       324.15539729, 376.17144135, 425.20307579, 477.21911985,\n",
      "       538.28713979, 574.27188371, 653.31408289, 703.31447681]), array([ 1, -1,  2, -2,  3, -3,  4, -4,  5, -5,  6, -6], dtype=int8)), ...]\n"
     ]
    }
   ],
   "source": [
    "print(get_spectra(List(['PEPTIDE']), constants.mass_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_spectra():\n",
    "    \n",
    "    spectra = get_spectra(List(['PEPTIDE']), constants.mass_dict)\n",
    "    \n",
    "    precmass, peptide, frags, fragtypes = spectra[0]\n",
    "    \n",
    "    assert np.allclose(precmass, 799.3599642034599)\n",
    "    \n",
    "    assert peptide == 'PEPTIDE'\n",
    "    \n",
    "    assert np.allclose(frags, np.array([ 98.06004033, 148.06043425, 227.10263343, 263.08737735,\n",
    "       324.15539729, 376.17144135, 425.20307579, 477.21911985,\n",
    "       538.28713979, 574.27188371, 653.31408289, 703.31447681]))\n",
    "\n",
    "test_get_spectra()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading FASTA\n",
    "\n",
    "To read FASTA files, we use the `SeqIO` module from the `Biopython` library. This is a generator expression so that we read one FASTA entry after another until the `StopIteration` is reached, which is implemented in `read_fasta_file`. Additionally, we define the function `read_fasta_file_entries` that simply counts the number of FASTA entries.\n",
    "\n",
    "All FASTA entries that contain AAs which are not in the mass_dict can be checked with `check_sequence` and will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "from glob import glob\n",
    "import logging\n",
    "\n",
    "def read_fasta_file(fasta_filename=\"\"):\n",
    "    \"\"\"\n",
    "    Read a FASTA file line by line\n",
    "    \"\"\"\n",
    "    with open(fasta_filename, \"rt\") as handle:\n",
    "        iterator = SeqIO.parse(handle, \"fasta\")\n",
    "        while iterator:\n",
    "            try:\n",
    "                record = next(iterator)\n",
    "                parts = record.id.split(\"|\")  # pipe char\n",
    "                if len(parts) > 1:\n",
    "                    id = parts[1]\n",
    "                else:\n",
    "                    id = record.name\n",
    "                sequence = str(record.seq)\n",
    "                entry = {\n",
    "                    \"id\": id,\n",
    "                    \"name\": record.name,\n",
    "                    \"description\": record.description,\n",
    "                    \"sequence\": sequence,\n",
    "                }\n",
    "\n",
    "                yield entry\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "\n",
    "def read_fasta_file_entries(fasta_filename=\"\"):\n",
    "    \"\"\"\n",
    "    Function to count entries in fasta file\n",
    "    \"\"\"\n",
    "    with open(fasta_filename, \"rt\") as handle:\n",
    "        iterator = SeqIO.parse(handle, \"fasta\")\n",
    "        count = 0\n",
    "        while iterator:\n",
    "            try:\n",
    "                record = next(iterator)\n",
    "                count+=1\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "        return count\n",
    "\n",
    "\n",
    "def check_sequence(element, AAs, verbose = False):\n",
    "    \"\"\"\n",
    "    Checks wheter a sequence from a FASTA entry contains valid AAs\n",
    "    \"\"\"\n",
    "    if not set(element['sequence']).issubset(AAs):\n",
    "        unknown = set(element['sequence']) - set(AAs)\n",
    "        if verbose:\n",
    "            logging.error(f'This FASTA entry contains unknown AAs {unknown} - Peptides with unknown AAs will be skipped: \\n {element}\\n')\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'A0PJZ0',\n",
       " 'name': 'sp|A0PJZ0|A20A5_HUMAN',\n",
       " 'description': 'sp|A0PJZ0|A20A5_HUMAN Putative ankyrin repeat domain-containing protein 20A5 OS=Homo sapiens OX=9606 GN=ANKRD20A5P PE=5 SV=1',\n",
       " 'sequence': 'MKLFGFRSRRGQTVLGSIDHLYTGSGYRIRYSELQKIHKAAVKGDAAEMERCLARRSGDLDALDKQHRTALHLACASGHVKVVTLLVNRKCQIDIYDKENRTPLIQAVHCQEEACAVILLEHGANPNLKDIYGNTALHYAVYSESTSLAEKLLFHGENIEALDKV'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load example fasta file\n",
    "\n",
    "fasta_path = '../testfiles/test.fasta'\n",
    "\n",
    "list(read_fasta_file(fasta_path))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peptide Dictionary\n",
    "\n",
    "In order to efficiently store peptides, we rely on the Python dictionary. The idea is to have a dictionary with peptides as keys and indices to proteins as values. This way, one can quickly look up to which protein a peptide belongs to. The function `add_to_pept_dict` uses a regular python dictionary and allows to add peptides and stores indices to the originating proteins as a list. If a peptide is already present in the dictionary, the list is appended. The function returns a list of `added_peptides`, which were not present in the dictionary yet. One can use the function `merge_pept_dicts` to merge multiple peptide dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_to_pept_dict(pept_dict, new_peptides, i):\n",
    "    \"\"\"\n",
    "    Add peptides to the peptide dictionary\n",
    "    \"\"\"\n",
    "    added_peptides = List()\n",
    "    for peptide in new_peptides:\n",
    "        if peptide in pept_dict:\n",
    "            if i not in pept_dict[peptide]:\n",
    "                pept_dict[peptide].append(i)\n",
    "        else:\n",
    "            pept_dict[peptide] = [i]\n",
    "            added_peptides.append(peptide)\n",
    "\n",
    "    return pept_dict, added_peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n"
     ]
    }
   ],
   "source": [
    "pept_dict = {}\n",
    "new_peptides = ['ABC','DEF']\n",
    "\n",
    "pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 0)\n",
    "\n",
    "new_peptides = ['DEF','GHI']\n",
    "\n",
    "pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 1)\n",
    "\n",
    "print(pept_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_add_to_pept_dict():\n",
    "    pept_dict = {}\n",
    "    new_peptides = ['ABC','DEF']\n",
    "\n",
    "    pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 0)\n",
    "\n",
    "    new_peptides = ['DEF','GHI']\n",
    "\n",
    "    pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 1)\n",
    "    \n",
    "    assert pept_dict == {'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n",
    "    \n",
    "test_add_to_pept_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def merge_pept_dicts(list_of_pept_dicts):\n",
    "    \n",
    "    if len(list_of_pept_dicts) == 0:\n",
    "        raise ValueError('Need to pass at least 1 element.')\n",
    "    \n",
    "    new_pept_dict = list_of_pept_dicts[0]\n",
    "\n",
    "    for pept_dict in list_of_pept_dicts[1:]:\n",
    "\n",
    "        for key in pept_dict.keys():\n",
    "            if key in new_pept_dict:\n",
    "                for element in pept_dict[key]:\n",
    "                    new_pept_dict[key].append(element)\n",
    "            else:\n",
    "                new_pept_dict[key] = pept_dict[key]\n",
    "\n",
    "    return new_pept_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABC': [0, 3, 4], 'DEF': [0, 1], 'GHI': [1], 'JKL': [5, 6], 'MNO': [7]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pept_dict_1 = {'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n",
    "pept_dict_2 = {'ABC': [3,4], 'JKL': [5, 6], 'MNO': [7]}\n",
    "\n",
    "merge_pept_dicts([pept_dict_1, pept_dict_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_merge_pept_dicts():\n",
    "    pept_dict_1 = {'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n",
    "    pept_dict_2 = {'ABC': [3,4], 'JKL': [5, 6], 'MNO': [7]}\n",
    "\n",
    "    assert merge_pept_dicts([pept_dict_1, pept_dict_2]) == {'ABC': [0, 3, 4], 'DEF': [0, 1], 'GHI': [1], 'JKL': [5, 6], 'MNO': [7]}\n",
    "    \n",
    "test_merge_pept_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a database\n",
    "\n",
    "To wrap everything up, we employ two functions, `generate_database` and `generate_spectra`. The first one reads a FASTA file and generates a list of peptides, as well as the peptide dictionary and an ordered FASTA dictionary to be able to look up the protein indices later. For the `callback` we first read the whole FASTA file to determine the total number of entries in the FASTA file.  For a typical FASTA file of 30 Mb with 40k entries, this should take less than a second. The progress of the digestion is monitored by processing the FASTA file one by one.\n",
    "The function `generate_spectra` then calculates precursor masses and fragment ions. Here, we split the total_number of sequences in `1000` steps to be able to track progress with the `callback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import OrderedDict\n",
    "\n",
    "def generate_fasta_list(fasta_paths, callback = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    fasta_list = []\n",
    "\n",
    "    fasta_dict = OrderedDict()\n",
    "\n",
    "    fasta_index = 0\n",
    "\n",
    "    if type(fasta_paths) is str:\n",
    "        fasta_paths = [fasta_paths]\n",
    "        n_fastas = 1\n",
    "\n",
    "    elif type(fasta_paths) is list:\n",
    "        n_fastas = len(fasta_paths)\n",
    "\n",
    "    for f_id, fasta_file in enumerate(fasta_paths):\n",
    "        n_entries = read_fasta_file_entries(fasta_file)\n",
    "\n",
    "        fasta_generator = read_fasta_file(fasta_file)\n",
    "\n",
    "        for element in fasta_generator:\n",
    "            check_sequence(element, constants.AAs)\n",
    "            fasta_list.append(element)\n",
    "            fasta_dict[fasta_index] = element\n",
    "            fasta_index += 1\n",
    "            \n",
    "                \n",
    "    return fasta_list, fasta_dict\n",
    "\n",
    "\n",
    "def generate_database(mass_dict, fasta_paths, callback = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    to_add = List()\n",
    "    fasta_dict = OrderedDict()\n",
    "    fasta_index = 0\n",
    "\n",
    "    pept_dict = {}\n",
    "\n",
    "    if type(fasta_paths) is str:\n",
    "        fasta_paths = [fasta_paths]\n",
    "        n_fastas = 1\n",
    "\n",
    "    elif type(fasta_paths) is list:\n",
    "        n_fastas = len(fasta_paths)\n",
    "\n",
    "    for f_id, fasta_file in enumerate(fasta_paths):\n",
    "        n_entries = read_fasta_file_entries(fasta_file)\n",
    "\n",
    "        fasta_generator = read_fasta_file(fasta_file)\n",
    "\n",
    "        for element in fasta_generator:\n",
    "            \n",
    "            fasta_dict[fasta_index] = element\n",
    "            mod_peptides = generate_peptides(element[\"sequence\"], **kwargs)\n",
    "            pept_dict, added_seqs = add_to_pept_dict(pept_dict, mod_peptides, fasta_index)\n",
    "            if len(added_seqs) > 0:\n",
    "                to_add.extend(added_seqs)\n",
    "\n",
    "            fasta_index += 1\n",
    "\n",
    "            if callback:\n",
    "                callback(fasta_index/n_entries/n_fastas+f_id)\n",
    "\n",
    "    return to_add, pept_dict, fasta_dict\n",
    "\n",
    "\n",
    "def generate_spectra(to_add, mass_dict, callback = None):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "\n",
    "    if len(to_add) > 0:\n",
    "\n",
    "        if callback: #Chunk the spectra to get a progress_bar\n",
    "            spectra = []\n",
    "\n",
    "            stepsize = int(np.ceil(len(to_add)/1000))\n",
    "\n",
    "            for i in range(0, len(to_add), stepsize):\n",
    "                sub = to_add[i:i + stepsize]\n",
    "                spectra.extend(get_spectra(sub, mass_dict))\n",
    "                callback((i+1)/len(to_add))\n",
    "\n",
    "        else:\n",
    "            spectra = get_spectra(to_add, mass_dict)\n",
    "    else:\n",
    "        raise ValueError(\"No spectra to generate.\")\n",
    "\n",
    "    return spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized version\n",
    "\n",
    "To speed up spectra generated, one can use the parallelized version. The function `generate_database_parallel` reads an entire FASTA file and splits it into multiple blocks. Each block will be processed, and the generated pept_dicts will be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from multiprocessing import Pool\n",
    "from alphapept import constants\n",
    "mass_dict = constants.mass_dict\n",
    "\n",
    "def block_idx(len_list, block_size = 1000):\n",
    "    \"\"\"\n",
    "    Create indices for a list of length len_list\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "\n",
    "    while end <= len_list:\n",
    "        end += block_size\n",
    "        blocks.append((start, end))\n",
    "        start = end\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def blocks(l, n):\n",
    "    \"\"\"\n",
    "    Create blocks from a given list\n",
    "    \"\"\"\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))\n",
    "\n",
    "def digest_fasta_block(to_process):\n",
    "    \"\"\"\n",
    "    Digest and create spectra for a whole fasta_block\n",
    "    \"\"\"\n",
    "\n",
    "    fasta_index, fasta_block, settings = to_process\n",
    "\n",
    "    to_add = List()\n",
    "\n",
    "    f_index = 0\n",
    "\n",
    "    pept_dict = {}\n",
    "    for element in fasta_block:\n",
    "        sequence = element[\"sequence\"]\n",
    "        mod_peptides = generate_peptides(sequence, **settings['fasta'])\n",
    "        pept_dict, added_peptides = add_to_pept_dict(pept_dict, mod_peptides, fasta_index+f_index)\n",
    "        if len(added_peptides) > 0:\n",
    "            to_add.extend(added_peptides)\n",
    "        f_index += 1\n",
    "\n",
    "    spectra = []\n",
    "    if len(to_add) > 0:\n",
    "        for specta_block in blocks(to_add, settings['fasta']['spectra_block']):\n",
    "            spectra.extend(generate_spectra(specta_block, mass_dict))\n",
    "\n",
    "    return (spectra, pept_dict)\n",
    "\n",
    "import alphapept.performance\n",
    "\n",
    "def generate_database_parallel(settings, callback = None):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    \n",
    "    n_processes = alphapept.performance.set_worker_count(\n",
    "        worker_count=settings['general']['n_processes'],\n",
    "        set_global=False\n",
    "    )\n",
    "\n",
    "    fasta_list, fasta_dict = generate_fasta_list(fasta_paths = settings['experiment']['fasta_paths'], **settings['fasta'])\n",
    "    \n",
    "    logging.info(f'FASTA contains {len(fasta_list):,} entries.')\n",
    "        \n",
    "    blocks = block_idx(len(fasta_list), settings['fasta']['fasta_block'])\n",
    "\n",
    "    to_process = [(idx_start, fasta_list[idx_start:idx_end], settings) for idx_start, idx_end in  blocks]\n",
    "\n",
    "    spectra = []\n",
    "    pept_dicts = []\n",
    "    with Pool(n_processes) as p:\n",
    "        max_ = len(to_process)\n",
    "        for i, _ in enumerate(p.imap_unordered(digest_fasta_block, to_process)):\n",
    "            if callback:\n",
    "                callback((i+1)/max_)\n",
    "            spectra.extend(_[0])\n",
    "            pept_dicts.append(_[1])\n",
    "\n",
    "    spectra = sorted(spectra, key=lambda x: x[1])\n",
    "    spectra_set = [spectra[idx] for idx in range(len(spectra)-1) if spectra[idx][1] != spectra[idx+1][1]]\n",
    "    spectra_set.append(spectra[-1])\n",
    "\n",
    "    pept_dict = merge_pept_dicts(pept_dicts)\n",
    "\n",
    "    return spectra_set, pept_dict, fasta_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel search on large files\n",
    "\n",
    "In some cases (e.g., a lot of modifications or very large FASTA files), it will not be useful to save the database as it will consume too much memory. Here, we use the function `search_parallel` from search. It creates theoretical spectra on the fly and directly searches against them. As we cannot create a pept_dict here, we need to create one from the search results. For this, we group peptides by their FASTA index and generate a lookup dictionary that can be used as a pept_dict.\n",
    "\n",
    "> Note that we are passing the settings argument here. Search results should be stored in the corresponding path in the `*.hdf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pept_dict_from_search(settings):\n",
    "    \"\"\"\n",
    "    Generates a peptide dict from a large search\n",
    "    \"\"\"\n",
    "\n",
    "    paths = settings['experiment']['file_paths']\n",
    "    bases = [os.path.splitext(_)[0]+'.ms_data.hdf' for _ in paths]\n",
    "\n",
    "    all_dfs = []\n",
    "    for _ in bases:\n",
    "        try:\n",
    "            df = alphapept.io.MS_Data_File(_).read(dataset_name=\"peptide_fdr\")\n",
    "        except KeyError:\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "        if len(df) > 0:\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    if sum([len(_) for _ in all_dfs]) == 0:\n",
    "        raise ValueError(\"No sequences present to concatenate.\")\n",
    "\n",
    "    df = pd.concat(all_dfs)\n",
    "\n",
    "    df['fasta_index'] = df['fasta_index'].str.split(',')\n",
    "\n",
    "    lst_col = 'fasta_index'\n",
    "\n",
    "    df_ = pd.DataFrame({\n",
    "          col:np.repeat(df[col].values, df[lst_col].str.len())\n",
    "          for col in df.columns.drop(lst_col)}\n",
    "        ).assign(**{lst_col:np.concatenate(df[lst_col].values)})[df.columns]\n",
    "\n",
    "    df_['fasta_index'] = df_['fasta_index'].astype('int')\n",
    "    df_grouped = df_.groupby(['sequence'])['fasta_index'].unique()\n",
    "\n",
    "    pept_dict = {}\n",
    "    for keys, vals in zip(df_grouped.index, df_grouped.values):\n",
    "        pept_dict[keys] = vals.tolist()\n",
    "\n",
    "    return pept_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "To save the generated spectra, we rely on the HDF format. For this, we create a dictionary and save all the generated elements. The container will contain the following elements:\n",
    "\n",
    "* `precursors`: An array containing the precursor masses\n",
    "* `seqs`: An array containing the peptide sequences for the precursor masses\n",
    "* `pept_dict`: A peptide dictionary to look up the peptides and return their FASTA index\n",
    "* `fasta_dict`: A FASTA dictionary to look up the FASTA entry based on a pept_dict index\n",
    "* `fragmasses`: An array containing the fragment masses. Unoccupied cells are filled with -1\n",
    "* `fragtypes:`: An array containing the fragment types. 0 equals b-ions, and 1 equals y-ions. Unoccupied cells are filled with -1\n",
    "* `bounds`: An integer array containing the upper bounds for the fragment masses/types array. This is needed to quickly slice the data.\n",
    "\n",
    "All arrays are sorted according to the precursor mass.\n",
    "\n",
    "> To access the dictionaries such as `pept_dict` or `fasta_dict`, one needs to extract them using the `.item()` method like so: `container['pept_dict'].item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphapept.io\n",
    "import pandas as pd\n",
    "\n",
    "def save_database(spectra, pept_dict, fasta_dict, database_path, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to save a database to the *.hdf format.\n",
    "    \"\"\"\n",
    "    \n",
    "    precmasses, seqs, fragmasses, fragtypes = zip(*spectra)\n",
    "    sortindex = np.argsort(precmasses)\n",
    "    fragmasses = np.array(fragmasses, dtype=object)[sortindex]\n",
    "    fragtypes = np.array(fragtypes, dtype=object)[sortindex]\n",
    "\n",
    "    lens = [len(_) for _ in fragmasses]\n",
    "\n",
    "    n_frags = sum(lens)\n",
    "\n",
    "    frags = np.zeros(n_frags, dtype=fragmasses[0].dtype)\n",
    "    frag_types = np.zeros(n_frags, dtype=fragtypes[0].dtype)\n",
    "\n",
    "    indices = np.zeros(len(lens) + 1, np.int64)\n",
    "    indices[1:] = lens\n",
    "    indices = np.cumsum(indices)\n",
    "\n",
    "    #Fill data\n",
    "\n",
    "    for _ in range(len(indices)-1):\n",
    "\n",
    "        start = indices[_]\n",
    "        end = indices[_+1]\n",
    "        frags[start:end] = fragmasses[_]\n",
    "        frag_types[start:end] = fragtypes[_]\n",
    "    \n",
    "    to_save = {}\n",
    "    \n",
    "    to_save[\"precursors\"] = np.array(precmasses)[sortindex]\n",
    "    to_save[\"seqs\"] = np.array(seqs, dtype=object)[sortindex]\n",
    "    to_save[\"proteins\"] = pd.DataFrame(fasta_dict).T\n",
    "\n",
    "    to_save[\"fragmasses\"] = frags\n",
    "    to_save[\"fragtypes\"] = frag_types\n",
    "    to_save[\"indices\"] = indices\n",
    "\n",
    "    db_file = alphapept.io.HDF_File(database_path, is_new_file=True)\n",
    "    for key, value in to_save.items():\n",
    "        db_file.write(value, dataset_name=key)\n",
    "    \n",
    "    peps = np.array(list(pept_dict), dtype=object)\n",
    "    indices = np.empty(len(peps) + 1, dtype=np.int64)\n",
    "    indices[0] = 0\n",
    "    indices[1:] = np.cumsum([len(pept_dict[i]) for i in peps])\n",
    "    proteins = np.concatenate([pept_dict[i] for i in peps])\n",
    "    \n",
    "    db_file.write(\"peptides\")\n",
    "    db_file.write(\n",
    "        peps,\n",
    "        dataset_name=\"sequences\",\n",
    "        group_name=\"peptides\"\n",
    "    )\n",
    "    db_file.write(\n",
    "        indices,\n",
    "        dataset_name=\"protein_indptr\",\n",
    "        group_name=\"peptides\"\n",
    "    )\n",
    "    db_file.write(\n",
    "        proteins,\n",
    "        dataset_name=\"protein_indices\",\n",
    "        group_name=\"peptides\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import collections\n",
    "\n",
    "def read_database(database_path:str, array_name:str=None):\n",
    "    db_file = alphapept.io.HDF_File(database_path)\n",
    "    if array_name is None:\n",
    "        db_data = {\n",
    "            key: db_file.read(\n",
    "                dataset_name=key\n",
    "            ) for key in db_file.read() if key not in (\n",
    "                \"proteins\",\n",
    "                \"peptides\"\n",
    "            )\n",
    "        }\n",
    "        db_data[\"fasta_dict\"] = np.array(\n",
    "            collections.OrderedDict(db_file.read(dataset_name=\"proteins\").T)\n",
    "        )\n",
    "        peps = db_file.read(dataset_name=\"sequences\", group_name=\"peptides\")\n",
    "        protein_indptr = db_file.read(\n",
    "            dataset_name=\"protein_indptr\",\n",
    "            group_name=\"peptides\"\n",
    "        )\n",
    "        protein_indices = db_file.read(\n",
    "            dataset_name=\"protein_indices\",\n",
    "            group_name=\"peptides\"\n",
    "        )\n",
    "        db_data[\"pept_dict\"] = np.array(\n",
    "            {\n",
    "                pep: (protein_indices[s: e]).tolist() for pep, s, e in zip(\n",
    "                    peps,\n",
    "                    protein_indptr[:-1],\n",
    "                    protein_indptr[1:],\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        db_data[\"seqs\"] = db_data[\"seqs\"].astype(str)\n",
    "    else:\n",
    "        db_data = db_file.read(dataset_name=array_name)\n",
    "    return db_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted 12_speed.ipynb.\n",
      "Converted 13_export.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted XX_file_formats.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphapept]",
   "language": "python",
   "name": "conda-env-alphapept-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
