{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTA\n",
    "\n",
    "> Functions related to generating spectra from FASTA files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all functions related to creating spectra from FASTA files. In brief, what we are doing is the following:\n",
    "\n",
    "1. Read a FASTA file and digest the proteins to generate peptides\n",
    "2. For each peptide, calculate a theoretical spectrum and precursor mass\n",
    "3. Save spectra\n",
    "\n",
    "Currently, `numba` has only limited string support. A lot of the functions are therefore Python-native."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaving\n",
    "\n",
    "For cleaving, we use regular expressions to find potential cleavages sites and write the wrapper `cleave_sequence` to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from alphapept import constants\n",
    "import re\n",
    "\n",
    "def get_missed_cleavages(sequences, n_missed_cleavages):\n",
    "    \"\"\"\n",
    "    Combine cleaved sequences to get sequences with missed cleavages\n",
    "    \"\"\"\n",
    "    missed = []\n",
    "    for k in range(len(sequences)-n_missed_cleavages):\n",
    "        missed.append(''.join(sequences[k-1:k+n_missed_cleavages]))\n",
    "\n",
    "    return missed\n",
    "\n",
    "\n",
    "def cleave_sequence(\n",
    "    sequence=\"\",\n",
    "    num_missed_cleavages=0,\n",
    "    protease=\"trypsin\",\n",
    "    min_length=6,\n",
    "    max_length=65,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleave a sequence with a given protease. Filters to have a minimum and maximum length.\n",
    "    \"\"\"\n",
    "\n",
    "    proteases = constants.protease_dict\n",
    "    pattern = proteases[protease]\n",
    "\n",
    "    p = re.compile(pattern)\n",
    "\n",
    "    cutpos = [m.start()+1 for m in p.finditer(sequence)]\n",
    "    cutpos.insert(0,0)\n",
    "    cutpos.append(len(sequence))\n",
    "\n",
    "    base_sequences = [sequence[cutpos[i]:cutpos[i+1]] for i in range(len(cutpos)-1)]\n",
    "\n",
    "    sequences = base_sequences.copy()\n",
    "\n",
    "    for i in range(1, num_missed_cleavages+1):\n",
    "        sequences.extend(get_missed_cleavages(base_sequences, i))\n",
    "\n",
    "    sequences = [_ for _ in sequences if len(_)>=min_length and len(_)<=max_length]\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABCDEFGHIJK', 'LMNOPQR']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protease = \"trypsin\"\n",
    "num_missed_cleavages = 0\n",
    "min_length, max_length = 6, 65\n",
    "\n",
    "cleave_sequence('ABCDEFGHIJKLMNOPQRST', num_missed_cleavages, protease, min_length, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_cleave_sequence():\n",
    "    \n",
    "    protease = \"trypsin\"\n",
    "    min_length, max_length = 6, 65\n",
    "\n",
    "    assert set(cleave_sequence('ABCDEFGHIJKLMNOPQRST', 0, protease, min_length, max_length)) == set(['ABCDEFGHIJK', 'LMNOPQR'])\n",
    "    assert set(cleave_sequence('ABCDEFGHIJKLMNOPQRST', 1, protease, min_length, max_length)) == set(['ABCDEFGHIJK', 'LMNOPQR', 'ABCDEFGHIJKLMNOPQR'])\n",
    "\n",
    "test_cleave_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting missed and internal cleavages\n",
    "The following are helper functions to retrieve the number of missed cleavages and internal cleavage sites for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "from alphapept import constants\n",
    "\n",
    "def count_missed_cleavages(sequence=\"\", protease=\"trypsin\", **kwargs):\n",
    "    \"\"\"\n",
    "    Counts the number of missed cleavages for a given sequence and protease\n",
    "    \"\"\"\n",
    "    proteases = constants.protease_dict\n",
    "    protease = proteases[protease]\n",
    "    p = re.compile(protease)\n",
    "    n_missed = len(p.findall(sequence))\n",
    "    return n_missed\n",
    "\n",
    "def count_internal_cleavages(sequence=\"\", protease=\"trypsin\", **kwargs):\n",
    "    \"\"\"\n",
    "    Counts the number of internal cleavage sites for a given sequence and protease\n",
    "    \"\"\"\n",
    "    proteases = constants.protease_dict\n",
    "    protease = proteases[protease]\n",
    "    match = re.search(protease,sequence[-1]+'_')\n",
    "    if match:\n",
    "        n_internal = 0\n",
    "    else:\n",
    "        n_internal = 1\n",
    "    return n_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "protease = \"trypsin\"\n",
    "print(count_missed_cleavages('ABCDEFGHIJKLMNOPQRST', protease))\n",
    "\n",
    "protease = \"trypsin\"\n",
    "print(count_internal_cleavages('ABCDEFGHIJKLMNOPQRST', protease))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_missed_cleavages():  \n",
    "    assert count_missed_cleavages('ABCDEFGHIJKLMNOPQRST', 'trypsin') == 2\n",
    "    assert count_missed_cleavages('ABCDEFGHIJKLMNOPQRST', 'clostripain') == 1\n",
    "    \n",
    "test_get_missed_cleavages()\n",
    "\n",
    "def test_get_internal_cleavages():\n",
    "    assert count_internal_cleavages('ABCDEFGHIJKLMNOPQRST', 'trypsin') == 1\n",
    "    assert count_internal_cleavages('ABCDEFGHIJKLMNOPQRSTK', 'trypsin') == 0\n",
    "\n",
    "test_get_internal_cleavages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "Peptides are composed out of amino acids that are written in capital letters - `PEPTIDE`. to distinguish modifications, they are written in lowercase such as `PEPTIoxDE` and can be of arbitrary length. For a modified amino acid, the modification precedes the letter of the amino acid. Decoys are indicated with an underscore. Therfore, the `parse` function splits after `_`. When parsing, the peptide string is converted into a numba-compatible list, like so: `PEPoxTIDE` -> `[P, E, P, oxT, I, D, E]`. This allows that we can use the `mass_dict` from `alphapept.constants` to directly determine the masses for the corresponding amino acids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "from numba.typed import List\n",
    "\n",
    "@njit\n",
    "def parse(peptide):\n",
    "    \"\"\"\n",
    "    Parser to parse peptide strings\n",
    "    \"\"\"\n",
    "    if \"_\" in peptide:\n",
    "        peptide = peptide.split(\"_\")[0]\n",
    "    parsed = List()\n",
    "    string = \"\"\n",
    "\n",
    "    for i in peptide:\n",
    "        string += i\n",
    "        if i.isupper():\n",
    "            parsed.append(string)\n",
    "            string = \"\"\n",
    "\n",
    "    return parsed\n",
    "\n",
    "def list_to_numba(a_list):\n",
    "    numba_list = List()\n",
    "\n",
    "    for element in a_list:\n",
    "        numba_list.append(element)\n",
    "\n",
    "    return numba_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P, E, P, T, I, D, E]\n",
      "[P, E, P, oxT, I, D, E]\n"
     ]
    }
   ],
   "source": [
    "print(parse('PEPTIDE'))\n",
    "print(parse('PEPoxTIDE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_parse():\n",
    "    peptide = \"PEPTIDE\"\n",
    "    assert parse(peptide) == list_to_numba([\"P\", \"E\", \"P\", \"T\", \"I\", \"D\", \"E\"])\n",
    "    peptide = \"PEPoxTIDE\"\n",
    "    assert parse(peptide) == list_to_numba([\"P\", \"E\", \"P\", \"oxT\", \"I\", \"D\", \"E\"])\n",
    "    peptide = \"PEPTIDE_decoy\"\n",
    "    assert parse(peptide) == list_to_numba([\"P\", \"E\", \"P\", \"T\", \"I\", \"D\", \"E\"])\n",
    "    \n",
    "test_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoy\n",
    "\n",
    "The decoy strategy employed is a pseudo-reversal of the peptide sequence, keeping only the terminal amino-acid and reversing the rest. Additionally, we can call the functions `swap_KR` and and `swap_AL` that will swap the respective AAs. The function `swap_KR` will only swap terminal AAs. The swapping functions only work if the AA is not modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def get_decoy_sequence(peptide, pseudo_reverse=False, AL_swap=False, KR_swap = False):\n",
    "    \"\"\"\n",
    "    Reverses a sequence and adds the '_decoy' tag.\n",
    "\n",
    "    \"\"\"\n",
    "    pep = parse(peptide)\n",
    "    if pseudo_reverse:\n",
    "        rev_pep = pep[:-1][::-1]\n",
    "        rev_pep.append(pep[-1])\n",
    "    else:\n",
    "        rev_pep = pep[::-1]\n",
    "\n",
    "    if AL_swap:\n",
    "        rev_pep = swap_AL(rev_pep)\n",
    "\n",
    "    if KR_swap:\n",
    "        rev_pep = swap_KR(rev_pep)\n",
    "\n",
    "    rev_pep = \"\".join(rev_pep)\n",
    "\n",
    "    return rev_pep\n",
    "\n",
    "\n",
    "@njit\n",
    "def swap_KR(peptide):\n",
    "    \"\"\"\n",
    "    Swaps a terminal K or R. Note: Only if AA is not modified.\n",
    "    \"\"\"\n",
    "    if peptide[-1] == 'K':\n",
    "        peptide[-1] = 'R'\n",
    "    elif peptide[-1] == 'R':\n",
    "        peptide[-1] = 'K'\n",
    "\n",
    "    return peptide\n",
    "\n",
    "\n",
    "@njit\n",
    "def swap_AL(peptide):\n",
    "    \"\"\"\n",
    "    Swaps a A with L. Note: Only if AA is not modified.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < len(range(len(peptide) - 1)):\n",
    "        if peptide[i] == \"A\":\n",
    "            peptide[i] = peptide[i + 1]\n",
    "            peptide[i + 1] = \"A\"\n",
    "            i += 1\n",
    "        elif peptide[i] == \"L\":\n",
    "            peptide[i] = peptide[i + 1]\n",
    "            peptide[i + 1] = \"L\"\n",
    "            i += 1\n",
    "        i += 1\n",
    "\n",
    "    #aa_table = \"GAVLIFMPWSCTYHKRQEND\"\n",
    "    #DiaNN_table  = \"LLLVVLLLLTSSSSLLNDQE\"\n",
    "\n",
    "    #idx = aa_table.find(peptide[-2])\n",
    "    #peptide[-2] = decoy_table[idx]\n",
    "\n",
    "    return peptide\n",
    "\n",
    "def get_decoys(peptide_list, pseudo_reverse=False, AL_swap=False, KR_swap = False, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper to get decoys for lists of peptides\n",
    "    \"\"\"\n",
    "    decoys = []\n",
    "    decoys.extend([get_decoy_sequence(peptide, pseudo_reverse, AL_swap, KR_swap) for peptide in peptide_list])\n",
    "    return decoys\n",
    "\n",
    "def add_decoy_tag(peptides):\n",
    "    \"\"\"\n",
    "    Adds a _decoy tag to a list of peptides\n",
    "    \"\"\"\n",
    "    return [peptide + \"_decoy\" for peptide in peptides]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K, K, K, L, A, K, K, K]\n",
      "[A, A, A, K, R, A, A, A]\n"
     ]
    }
   ],
   "source": [
    "print(swap_AL(parse('KKKALKKK')))\n",
    "print(swap_KR(parse('AAAKRAAA')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDITPEP\n"
     ]
    }
   ],
   "source": [
    "print(get_decoy_sequence('PEPTIDE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CBA', 'FED', 'IHG']\n"
     ]
    }
   ],
   "source": [
    "print(get_decoys(['ABC','DEF','GHI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_swap_AL():\n",
    "    assert swap_AL(parse(\"ABCDEF\")) == parse(\"BACDEF\")\n",
    "    assert swap_AL(parse(\"GHIKLM\")) == parse(\"GHIKML\")\n",
    "    assert swap_AL(parse(\"FEDCBA\")) == parse(\"FEDCBA\")\n",
    "    assert swap_AL(parse(\"GHIKL\")) == parse(\"GHIKL\")\n",
    "    assert swap_AL(parse(\"ABCDEFGHIKLM\")) == parse(\"BACDEFGHIKML\")\n",
    "    assert swap_AL(parse(\"BBAcCD\")) == parse(\"BBcCAD\")\n",
    "    assert swap_AL(parse(\"FEDCBA\")) == parse(\"FEDCBA\")\n",
    "\n",
    "test_swap_AL()\n",
    "\n",
    "def test_swapKR():\n",
    "    assert swap_KR(parse(\"ABCDEK\")) == parse(\"ABCDER\")\n",
    "    assert swap_KR(parse(\"ABCDER\")) == parse(\"ABCDEK\")\n",
    "    assert swap_KR(parse(\"ABCDEF\")) == parse(\"ABCDEF\")\n",
    "    assert swap_KR(parse(\"KABCDEF\")) == parse(\"KABCDEF\")\n",
    "    assert swap_KR(parse(\"KABCRDEF\")) == parse(\"KABCRDEF\")\n",
    "    assert swap_KR(parse(\"KABCKDEF\")) == parse(\"KABCKDEF\")\n",
    "\n",
    "test_swapKR()\n",
    "    \n",
    "def test_get_decoy_sequence():\n",
    "    peptide = \"PEPTIDER\"\n",
    "    assert get_decoy_sequence(peptide, pseudo_reverse=True) == \"EDITPEPR\"\n",
    "    assert get_decoy_sequence(peptide) == \"REDITPEP\"\n",
    "    assert get_decoy_sequence(peptide, KR_swap=True, pseudo_reverse=True) == \"EDITPEPK\"\n",
    "    \n",
    "test_get_decoy_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications\n",
    "\n",
    "To add modifications to the peptides, we distinguish fixed and variable modifications. Additionally, we make a distinction between whether the modification is only terminal or not. \n",
    "\n",
    "### Fixed Modifications\n",
    "Fixed modifications are implemented by passing a list with modified AAs that should be replaced. As a AA is only one letter, the remainder is the modification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_fixed_mods(seqs, mods_fixed, **kwargs):\n",
    "    \"\"\"\n",
    "    Adds fixed modifications to sequences.\n",
    "    \"\"\"\n",
    "    if not mods_fixed:\n",
    "        return seqs\n",
    "    else:\n",
    "        for mod_aa in mods_fixed:\n",
    "            seqs = [seq.replace(mod_aa[-1], mod_aa) for seq in seqs]\n",
    "        return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbBcCDEF']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods_fixed = ['cC','bB']\n",
    "peptide_list = ['ABCDEF']\n",
    "\n",
    "add_fixed_mods(peptide_list, mods_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_add_fixed_mods():\n",
    "    mods_fixed = ['cC']\n",
    "    peptide_list = ['ABCDEF']\n",
    "\n",
    "    peptides_new = add_fixed_mods(peptide_list, [])\n",
    "    assert peptides_new == peptide_list\n",
    "    \n",
    "    peptides_new = add_fixed_mods(peptide_list, mods_fixed)\n",
    "    assert peptides_new == ['ABcCDEF']\n",
    "    \n",
    "test_add_fixed_mods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Modifications\n",
    "\n",
    "To employ variable modifications, we use the function `get_mod_pos` that returns a list of tuples with all possible modifications when giving a dicitionary with variable modifications.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphapept.fasta import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_variable_mod(peps, mods_variable_dict):\n",
    "    peptides = []\n",
    "    for pep in peps:\n",
    "        for mod in mods_variable_dict:\n",
    "            for i in range(len(pep)):\n",
    "                c = pep[i]\n",
    "                if c in mods_variable_dict:\n",
    "                    peptides.append(pep[:i]+[mods_variable_dict[c]]+pep[i+1:])\n",
    "                \n",
    "    return peptides\n",
    "\n",
    "\n",
    "def get_isoforms(mods_variable_dict, peptide, max_isoforms):\n",
    "    \"\"\"\n",
    "    Function to generate isoforms for a given peptide - returns a list of isoforms.\n",
    "    The original sequence is included in the list\n",
    "    \"\"\"\n",
    "    pep = list(parse(peptide))\n",
    "\n",
    "    peptides = [pep]\n",
    "    new_peps = peptides\n",
    "    while len(peptides) < max_isoforms:\n",
    "        new_peps = add_variable_mod(new_peps, mods_variable_dict)\n",
    "        if len(new_peps) == 0:\n",
    "            break\n",
    "        peptides.extend(new_peps)\n",
    "\n",
    "    peptides = [''.join(_) for _ in peptides]\n",
    "\n",
    "    return peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMAMA', 'AoxMAMA', 'AMAoxMA', 'AoxMAoxMA', 'AoxMAoxMA']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods_variable_dict = {'M':'oxM'}\n",
    "peptide = 'AMAMA'\n",
    "max_isoforms = 1024\n",
    "get_isoforms(mods_variable_dict, peptide, max_isoforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define the wrapper `add_variable_mods` so that the functions can be called for lists of peptides and a list of variable modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from itertools import chain\n",
    "\n",
    "def add_variable_mods(peptide_list, mods_variable, max_isoforms, **kwargs):\n",
    "    #the peptide_list originates from one peptide already -> limit isoforms here\n",
    "    \n",
    "    max_ = max_isoforms - len(peptide_list) + 1\n",
    "    \n",
    "    if max_ < 0:\n",
    "        max_ = 0\n",
    "    \n",
    "    if not mods_variable:\n",
    "        return peptide_list\n",
    "    else:\n",
    "        mods_variable_r = {}\n",
    "        for _ in mods_variable:\n",
    "            mods_variable_r[_[-1]] = _\n",
    "\n",
    "        peptide_list = [get_isoforms(mods_variable_r, peptide, max_) for peptide in peptide_list]\n",
    "        return list(chain.from_iterable(peptide_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMA', 'AoxMA', 'AoxMA', 'AAC', 'AAamC', 'AAamC']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptide_list = ['AMA', 'AAC']\n",
    "mods_variable = ['oxM','amC']\n",
    "max_isoforms = 1024\n",
    "\n",
    "add_variable_mods(peptide_list, mods_variable, max_isoforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_add_variable_mods():\n",
    "    mods_variable = ['oxM']\n",
    "    peptide = ['AMAMA']\n",
    "\n",
    "    peptides_new = add_variable_mods(peptide, [], 1024)\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    peptides_new = add_variable_mods(peptide, mods_variable, 1024)\n",
    "\n",
    "    assert set(['AMAMA', 'AMAoxMA', 'AoxMAMA', 'AoxMAoxMA']) == set(peptides_new)\n",
    "\n",
    "    # Check if number of isoforms is correct\n",
    "    peptides_new = add_variable_mods(peptide, mods_variable, 3)\n",
    "    assert len(peptides_new) == 3\n",
    "    \n",
    "test_add_variable_mods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal Modifications - Fixed\n",
    "\n",
    "To handle terminal modifications, we use the following convention:\n",
    "\n",
    "* `<` for the left side (N-terminal)\n",
    "* `>` for the right side (C-Terminal)\n",
    "\n",
    "Additionally, if we want to have a terminal modification on any AA we indicate this `^`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_fixed_mod_terminal(peptides, mod):\n",
    "    \"\"\"\n",
    "    Adds fixed terminal modifications\n",
    "    \"\"\"\n",
    "    # < for left side (N-Term), > for right side (C-Term)\n",
    "    if \"<^\" in mod: #Any n-term, e.g. a<^\n",
    "        peptides = [mod[:-2] + peptide for peptide in peptides]\n",
    "    elif \">^\" in mod: #Any c-term, e.g. a>^\n",
    "        peptides = [peptide[:-1] + mod[:-2] + peptide[-1] for peptide in peptides]\n",
    "    elif \"<\" in mod: #only if specific AA, e.g. ox<C\n",
    "        peptides = [peptide[0].replace(mod[-1], mod[:-2]+mod[-1]) + peptide[1:] for peptide in peptides]\n",
    "    elif \">\" in mod:\n",
    "        peptides = [peptide[:-1] + peptide[-1].replace(mod[-1], mod[:-2]+mod[-1]) for peptide in peptides]\n",
    "    else:\n",
    "        # This should not happen\n",
    "        raise (\"Invalid fixed terminal modification {}.\".format(key))\n",
    "    return peptides\n",
    "\n",
    "def add_fixed_mods_terminal(peptides, mods_fixed_terminal, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper to add fixed mods on sequences and lists of mods\n",
    "    \"\"\"\n",
    "    if mods_fixed_terminal == []:\n",
    "        return peptides\n",
    "    else:\n",
    "        # < for left side (N-Term), > for right side (C-Term)\n",
    "        for key in mods_fixed_terminal:\n",
    "            peptides = add_fixed_mod_terminal(peptides, key)\n",
    "        return peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any n-term modified with x (x<^): ['xAMAMA']\n",
      "Any c-term modified with x (x>^): ['AMAMxA']\n",
      "Only A on n-term modified with x (x<A): ['xAMAMA']\n",
      "Only A on c-term modified with x (x<A): ['AMAMxA']\n"
     ]
    }
   ],
   "source": [
    "peptide = ['AMAMA']\n",
    "\n",
    "print('Any n-term modified with x (x<^):', add_fixed_mods_terminal(peptide, ['x<^']))\n",
    "print('Any c-term modified with x (x>^):', add_fixed_mods_terminal(peptide, ['x>^']))\n",
    "print('Only A on n-term modified with x (x<A):', add_fixed_mods_terminal(peptide, ['x<A']))\n",
    "print('Only A on c-term modified with x (x<A):', add_fixed_mods_terminal(peptide, ['x>A']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_add_fixed_mods_terminal():\n",
    "    peptide = ['AMAMA']\n",
    "\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, [])\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    #Any N-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x<^'])\n",
    "    assert peptides_new == ['xAMAMA']\n",
    "\n",
    "    #Any C-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x>^'])\n",
    "    assert peptides_new == ['AMAMxA']\n",
    "\n",
    "    #Selected N-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x<A'])\n",
    "    assert peptides_new == ['xAMAMA']\n",
    "\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x<C'])\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    #Selected C-term\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x>A'])\n",
    "    assert peptides_new == ['AMAMxA']\n",
    "\n",
    "    peptides_new = add_fixed_mods_terminal(peptide, ['x>C'])\n",
    "    assert peptides_new == peptide\n",
    "    \n",
    "test_add_fixed_mods_terminal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal Modifications - Variable\n",
    "\n",
    "Lastly, to handle terminal variable modifications we use the function `add_variable_mods_terminal`. As the modifcation can only be at the terminal end this function only adds a peptide where the terminal end is modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_variable_mods_terminal(peptides, mods_variable_terminal, **kwargs):\n",
    "    \"Function to add variable terminal modifications\"\n",
    "    if not mods_variable_terminal:\n",
    "        return peptides\n",
    "    else:\n",
    "        new_peptides_n = peptides.copy()\n",
    "\n",
    "        for key in mods_variable_terminal:\n",
    "            if \"<\" in key:\n",
    "                # Only allow one variable mod on one end\n",
    "                new_peptides_n.extend(\n",
    "                    add_fixed_mod_terminal(peptides, key)\n",
    "                )\n",
    "        new_peptides_n = get_unique_peptides(new_peptides_n)\n",
    "        # N complete, let's go for c-terminal\n",
    "        new_peptides_c = new_peptides_n\n",
    "        for key in mods_variable_terminal:\n",
    "            if \">\" in key:\n",
    "                # Only allow one variable mod on one end\n",
    "                new_peptides_c.extend(\n",
    "                    add_fixed_mod_terminal(new_peptides_n, key)\n",
    "                )\n",
    "\n",
    "        return get_unique_peptides(new_peptides_c)\n",
    "\n",
    "def get_unique_peptides(peptides):\n",
    "    return list(set(peptides))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xAMAMA', 'AMAMA']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptide_list = ['AMAMA']\n",
    "add_variable_mods_terminal(peptide_list, ['x<^'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_add_variable_mods_terminal():\n",
    "    peptide_list = ['AMAMA']\n",
    "\n",
    "    peptides_new = add_variable_mods_terminal(peptide_list, [])\n",
    "    assert peptides_new == peptide\n",
    "\n",
    "    #Any N-term\n",
    "    peptides_new = add_variable_mods_terminal(peptide_list, ['x<^'])\n",
    "    assert set(peptides_new) == set(['xAMAMA', 'AMAMA'])\n",
    "    \n",
    "test_add_variable_mods_terminal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Peptides\n",
    "\n",
    "Lastly we put all the functions into a wrapper `generate_peptides`. It will accept a peptide and a dictionary with settings so that we can get all modified peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate_peptides(peptide, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper to get modified peptides from a peptide\n",
    "    \"\"\"\n",
    "    mod_peptide = add_fixed_mods_terminal([peptide], kwargs['mods_fixed_terminal_prot'])\n",
    "    mod_peptide = add_variable_mods_terminal(mod_peptide, kwargs['mods_variable_terminal_prot'])\n",
    "\n",
    "    peptides = []\n",
    "    [peptides.extend(cleave_sequence(_, **kwargs)) for _ in mod_peptide]\n",
    "    \n",
    "    all_peptides = []\n",
    "    for peptide in peptides:\n",
    "        #Regular peptides\n",
    "        mod_peptides = add_fixed_mods([peptide], **kwargs)\n",
    "        mod_peptides = add_fixed_mods_terminal(mod_peptides, **kwargs)\n",
    "        mod_peptides = add_variable_mods_terminal(mod_peptides, **kwargs)\n",
    "        mod_peptides = add_variable_mods(mod_peptides, **kwargs)\n",
    "        \n",
    "        all_peptides.extend(mod_peptides)\n",
    "\n",
    "        #Decoys:\n",
    "        decoy_peptides = get_decoys([peptide], **kwargs)\n",
    "\n",
    "        mod_peptides_decoy = add_fixed_mods(decoy_peptides, **kwargs)\n",
    "        mod_peptides_decoy = add_fixed_mods_terminal(mod_peptides_decoy, **kwargs)\n",
    "        mod_peptides_decoy = add_variable_mods_terminal(mod_peptides_decoy, **kwargs)\n",
    "        mod_peptides_decoy = add_variable_mods(mod_peptides_decoy, **kwargs)\n",
    "\n",
    "        mod_peptides_decoy = add_decoy_tag(mod_peptides_decoy)\n",
    "\n",
    "        \n",
    "        all_peptides.extend(mod_peptides_decoy)\n",
    "\n",
    "    return all_peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PEPTIDEM', 'PEPTIDEoxM', 'MEDITPEP_decoy', 'oxMEDITPEP_decoy']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {}\n",
    "\n",
    "kwargs[\"protease\"] = \"trypsin\"\n",
    "kwargs[\"num_missed_cleavages\"] = 2\n",
    "kwargs[\"min_length\"] = 6\n",
    "kwargs[\"max_length\"] = 27\n",
    "kwargs[\"mods_variable\"] = [\"oxM\"]\n",
    "kwargs[\"mods_variable_terminal\"] = []\n",
    "kwargs[\"mods_fixed\"] = [\"cC\"]\n",
    "kwargs[\"mods_fixed_terminal\"] = []\n",
    "kwargs[\"mods_fixed_terminal_prot\"] = []\n",
    "kwargs[\"mods_variable_terminal_prot\"]  = []\n",
    "kwargs[\"max_isoforms\"] = 1024\n",
    "\n",
    "generate_peptides('PEPTIDEM', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PEPTIDEM']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleave_sequence('PEPTIDEM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_generate_peptides():\n",
    "    kwargs = {}\n",
    "\n",
    "    kwargs[\"protease\"] = \"trypsin\"\n",
    "    kwargs[\"num_missed_cleavages\"] = 2\n",
    "    kwargs[\"min_length\"] = 6\n",
    "    kwargs[\"max_length\"] = 27\n",
    "    kwargs[\"mods_variable\"] = [\"oxM\"]\n",
    "    kwargs[\"mods_variable_terminal\"] = []\n",
    "    kwargs[\"mods_fixed\"] = [\"cC\"]\n",
    "    kwargs[\"mods_fixed_terminal\"] = []\n",
    "    kwargs[\"mods_fixed_terminal_prot\"] = []\n",
    "    kwargs[\"mods_variable_terminal_prot\"]  = []\n",
    "    kwargs[\"max_isoforms\"] = 1024\n",
    "    kwargs['pseudo_reverse'] = True\n",
    "\n",
    "    peps = generate_peptides('PEPTIDEM', **kwargs)\n",
    "    assert set(peps) == set(['PEPTIDEM', 'PEPTIDEoxM', 'EDITPEPM_decoy', 'EDITPEPoxM_decoy'])\n",
    "    \n",
    "test_generate_peptides()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mass Calculations\n",
    "\n",
    "Using the `mass_dict` from `constants` and being able to parse sequences with `parse` one can simply look up the masses for each modified or unmodified amino acid and add everything up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precursor\n",
    "\n",
    "To calculate the mass of the neutral precursor we start with the mass of an $H_2O$ and add the masses of all amino acids of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "from numba.typed import List\n",
    "import numpy as np\n",
    "\n",
    "@njit\n",
    "def get_precmass(parsed_pep, mass_dict):\n",
    "    \"\"\"\n",
    "    Calculate the mass of the neutral precursor\n",
    "    \"\"\"\n",
    "    tmass = mass_dict[\"H2O\"]\n",
    "    for _ in parsed_pep:\n",
    "        tmass += mass_dict[_]\n",
    "\n",
    "    return tmass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799.3599642034599"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_precmass(parse('PEPTIDE'), constants.mass_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_precmass():\n",
    "    \n",
    "    precmass = get_precmass(parse('PEPTIDE'), constants.mass_dict)\n",
    "    \n",
    "    assert np.allclose(precmass, 799.3599642034599)\n",
    "    \n",
    "test_get_precmass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fragments\n",
    "\n",
    "Likewise, we can calculate the masses of the fragment ions. We employ two functions: `get_fragmass` and `get_frag_dict`. \n",
    "\n",
    "`get_fragmass` is a fast, `numba`-compatible function that calculates the fragmasses and returns an array indicating wheter the iontype was `b` or `y`. \n",
    "\n",
    "`get_frag_dict` instead is not `numba`-compatible and hence a bit slower. It returns a dictionary with the respective ion and can be used for plotting theoretical spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@njit\n",
    "def get_fragmass(parsed_pep, mass_dict):\n",
    "    \"\"\"\n",
    "    Calculate the masses of the fragment ions\n",
    "    \"\"\"\n",
    "    n_frags = (len(parsed_pep) - 1) * 2\n",
    "\n",
    "    frag_masses = np.zeros(n_frags, dtype=np.float64)\n",
    "    frag_type = np.zeros(n_frags, dtype=np.int8)\n",
    "\n",
    "    # b-ions > 0\n",
    "    n_frag = 0\n",
    "    \n",
    "    frag_m = mass_dict[\"Proton\"]\n",
    "    for idx, _ in enumerate(parsed_pep[:-1]):\n",
    "        frag_m += mass_dict[_]\n",
    "        frag_masses[n_frag] = frag_m\n",
    "        frag_type[n_frag] = (idx+1)\n",
    "        n_frag += 1\n",
    "        \n",
    "    # y-ions < 0\n",
    "    frag_m = mass_dict[\"Proton\"] + mass_dict[\"H2O\"]\n",
    "    for idx, _ in enumerate(parsed_pep[::-1][:-1]):\n",
    "        frag_m += mass_dict[_]\n",
    "        frag_masses[n_frag] = frag_m\n",
    "        frag_type[n_frag] = -(idx+1)\n",
    "        n_frag += 1\n",
    "\n",
    "    return frag_masses, frag_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 98.06004033, 227.10263343, 324.15539729, 425.20307579,\n",
       "        538.28713979, 653.31408289, 148.06043425, 263.08737735,\n",
       "        376.17144135, 477.21911985, 574.27188371, 703.31447681]),\n",
       " array([ 1,  2,  3,  4,  5,  6, -1, -2, -3, -4, -5, -6], dtype=int8))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fragmass(parse('PEPTIDE'), constants.mass_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_fragmass():\n",
    "    \n",
    "    frag_masses, frag_type = get_fragmass(parse('PEPTIDE'), constants.mass_dict)\n",
    "    \n",
    "    ref_masses = np.array([ 98.06004033, 227.10263343, 324.15539729, 425.20307579,\n",
    "        538.28713979, 653.31408289, 148.06043425, 263.08737735,\n",
    "        376.17144135, 477.21911985, 574.27188371, 703.31447681])\n",
    "    \n",
    "    assert np.allclose(frag_masses, ref_masses)\n",
    "                          \n",
    "test_get_fragmass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_frag_dict(parsed_pep, mass_dict):\n",
    "    \n",
    "    frag_dict = {}\n",
    "    frag_masses, frag_type = get_fragmass(parsed_pep, constants.mass_dict)\n",
    "    \n",
    "    for idx, _ in enumerate(frag_masses):\n",
    "        \n",
    "        cnt = frag_type[idx]\n",
    "        if cnt > 0:\n",
    "            identifier = 'b'\n",
    "        else:\n",
    "            identifier = 'y'\n",
    "            cnt = -cnt\n",
    "        frag_dict[identifier+str(cnt)] = _\n",
    "           \n",
    "    return frag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b1': 98.06004032687,\n",
       " 'b2': 227.10263342686997,\n",
       " 'b3': 324.15539728686997,\n",
       " 'b4': 425.20307578687,\n",
       " 'b5': 538.28713978687,\n",
       " 'b6': 653.31408288687,\n",
       " 'y1': 148.06043425033,\n",
       " 'y2': 263.08737735033,\n",
       " 'y3': 376.1714413503299,\n",
       " 'y4': 477.21911985032995,\n",
       " 'y5': 574.2718837103299,\n",
       " 'y6': 703.3144768103299}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_frag_dict(parse('PEPTIDE'), constants.mass_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_frag_dict():\n",
    "    \n",
    "    refdict = {'b1': 98.06004032687,\n",
    " 'b2': 227.10263342686997,\n",
    " 'b3': 324.15539728686997,\n",
    " 'y1': 120.06551965033,\n",
    " 'y2': 217.11828351033,\n",
    " 'y3': 346.16087661033}\n",
    "    \n",
    "    newdict = get_frag_dict(parse('PEPT'), constants.mass_dict)\n",
    "    \n",
    "    for key in newdict.keys():\n",
    "        \n",
    "        assert np.allclose(refdict[key], newdict[key])\n",
    "        \n",
    "test_get_frag_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us also to generate the theorteical isotopes for a fragment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "peptide = 'PEPTIDE'\n",
    "\n",
    "frag_dict = get_frag_dict(parse(peptide), constants.mass_dict)\n",
    "\n",
    "db_frag = list(frag_dict.values())\n",
    "db_int = [100 for _ in db_frag]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.vlines(db_frag, 0, db_int, \"k\", label=\"DB\", alpha=0.2)\n",
    "\n",
    "for _ in frag_dict.keys():\n",
    "    plt.text(frag_dict[_], 104, _, fontsize=12, alpha = 0.8)\n",
    "    \n",
    "plt.title('Theoretical Spectrum for {}'.format(peptide))\n",
    "plt.xlabel('Mass')\n",
    "plt.ylabel('Intensity')\n",
    "plt.ylim([0,110])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectra\n",
    "\n",
    "The function `get_spectrum` returns a tuple with the following content:\n",
    "\n",
    "* precursor mass\n",
    "* peptide sequence\n",
    "* fragmasses\n",
    "* fragtypes\n",
    "\n",
    "Likewise, `get_spectra` returns a list of tuples. We employ a list of tuples here as this way we can sort them easily by precursor mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@njit\n",
    "def get_spectrum(peptide, mass_dict):\n",
    "    parsed_peptide = parse(peptide)\n",
    "\n",
    "    fragmasses, fragtypes = get_fragmass(parsed_peptide, mass_dict)\n",
    "    sortindex = np.argsort(fragmasses)\n",
    "    fragmasses = fragmasses[sortindex]\n",
    "    fragtypes = fragtypes[sortindex]\n",
    "\n",
    "    precmass = get_precmass(parsed_peptide, mass_dict)\n",
    "\n",
    "    return (precmass, peptide, fragmasses, fragtypes)\n",
    "\n",
    "@njit\n",
    "def get_spectra(peptides, mass_dict):\n",
    "    spectra = List()\n",
    "\n",
    "    for i in range(len(peptides)):\n",
    "        spectra.append(get_spectrum(peptides[i], mass_dict))\n",
    "\n",
    "    return spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(799.3599642034599, 'PEPTIDE', array([ 98.06004033, 148.06043425, 227.10263343, 263.08737735,\n",
      "       324.15539729, 376.17144135, 425.20307579, 477.21911985,\n",
      "       538.28713979, 574.27188371, 653.31408289, 703.31447681]), array([ 1, -1,  2, -2,  3, -3,  4, -4,  5, -5,  6, -6], dtype=int8))]\n"
     ]
    }
   ],
   "source": [
    "print(get_spectra(List(['PEPTIDE']), constants.mass_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_get_spectra():\n",
    "    \n",
    "    spectra = get_spectra(List(['PEPTIDE']), constants.mass_dict)\n",
    "    \n",
    "    precmass, peptide, frags, fragtypes = spectra[0]\n",
    "    \n",
    "    assert np.allclose(precmass, 799.3599642034599)\n",
    "    \n",
    "    assert peptide == 'PEPTIDE'\n",
    "    \n",
    "    assert np.allclose(frags, np.array([ 98.06004033, 148.06043425, 227.10263343, 263.08737735,\n",
    "       324.15539729, 376.17144135, 425.20307579, 477.21911985,\n",
    "       538.28713979, 574.27188371, 653.31408289, 703.31447681]))\n",
    "\n",
    "test_get_spectra()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading FASTA\n",
    "\n",
    "To read FASTA files we use the `SeqIO` module from the `Biopython` library. This is a generator expression so that we read one FASTA entry after another until the `StopIteration` is reached, which is implemented in `read_fasta_file`. Additionally, we define the funciton `read_fasta_file_entries` that simply counts the number of FASTA entries.\n",
    "\n",
    "All FASTA entries that contain AAs which are not in the mass_dict can be checked with `check_sequence` and will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "from glob import glob\n",
    "import logging\n",
    "\n",
    "def read_fasta_file(fasta_filename=\"\"):\n",
    "    \"\"\"\n",
    "    Read a FASTA file line by line\n",
    "    \"\"\"\n",
    "    with open(fasta_filename, \"rt\") as handle:\n",
    "        iterator = SeqIO.parse(handle, \"fasta\")\n",
    "        while iterator:\n",
    "            try:\n",
    "                record = next(iterator)\n",
    "                parts = record.id.split(\"|\")  # pipe char\n",
    "                if len(parts) > 1:\n",
    "                    id = parts[1]\n",
    "                else:\n",
    "                    id = record.name\n",
    "                sequence = str(record.seq)\n",
    "                entry = {\n",
    "                    \"id\": id,\n",
    "                    \"name\": record.name,\n",
    "                    \"description\": record.description,\n",
    "                    \"sequence\": sequence,\n",
    "                }\n",
    "\n",
    "                yield entry\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "\n",
    "def read_fasta_file_entries(fasta_filename=\"\"):\n",
    "    \"\"\"\n",
    "    Function to count entries in fasta file\n",
    "    \"\"\"\n",
    "    with open(fasta_filename, \"rt\") as handle:\n",
    "        iterator = SeqIO.parse(handle, \"fasta\")\n",
    "        count = 0\n",
    "        while iterator:\n",
    "            try:\n",
    "                record = next(iterator)\n",
    "                count+=1\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "        return count\n",
    "\n",
    "\n",
    "def check_sequence(element, AAs):\n",
    "    \"\"\"\n",
    "    Checks wheter a sequence from a FASTA entry contains valid AAs\n",
    "    \"\"\"\n",
    "    if not set(element['sequence']).issubset(AAs):\n",
    "        unknown = set(element['sequence']) - set(AAs)\n",
    "        logging.error(f'This FASTA entry contains unknown AAs {unknown} and will be skipped: \\n {element}\\n')\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'A0PJZ0',\n",
       " 'name': 'sp|A0PJZ0|A20A5_HUMAN',\n",
       " 'description': 'sp|A0PJZ0|A20A5_HUMAN Putative ankyrin repeat domain-containing protein 20A5 OS=Homo sapiens OX=9606 GN=ANKRD20A5P PE=5 SV=1',\n",
       " 'sequence': 'MKLFGFRSRRGQTVLGSIDHLYTGSGYRIRYSELQKIHKAAVKGDAAEMERCLARRSGDLDALDKQHRTALHLACASGHVKVVTLLVNRKCQIDIYDKENRTPLIQAVHCQEEACAVILLEHGANPNLKDIYGNTALHYAVYSESTSLAEKLLFHGENIEALDKV'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load example fasta file\n",
    "\n",
    "fasta_path = '../testfiles/test.fasta'\n",
    "\n",
    "list(read_fasta_file(fasta_path))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peptide Dictionary\n",
    "\n",
    "In order to efficiently store peptides, we rely on the Python dictionary. The idea is to have a dictionary with peptides as keys and indices to proteins as values. This way, one can quickly look up to which protein a peptide belongs to. The function `add_to_pept_dict` uses a regular python dictionary and allows to add peptides and stores indices to the originating proteins as a list. If a peptide is already present in the dictionary, the list is appended. The function returns a list of `added_peptides`, which were not present in the dictionary yet. One can use the function `merge_pept_dicts` to merge multiple peptide dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_to_pept_dict(pept_dict, new_peptides, i):\n",
    "    \"\"\"\n",
    "    Add peptides to the peptide dictionary\n",
    "    \"\"\"\n",
    "    added_peptides = List()\n",
    "    for peptide in new_peptides:\n",
    "        if peptide in pept_dict:\n",
    "            pept_dict[peptide].append(i)\n",
    "        else:\n",
    "            pept_dict[peptide] = [i]\n",
    "            added_peptides.append(peptide)\n",
    "\n",
    "    return pept_dict, added_peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n"
     ]
    }
   ],
   "source": [
    "pept_dict = {}\n",
    "new_peptides = ['ABC','DEF']\n",
    "\n",
    "pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 0)\n",
    "\n",
    "new_peptides = ['DEF','GHI']\n",
    "\n",
    "pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 1)\n",
    "\n",
    "print(pept_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_add_to_pept_dict():\n",
    "    pept_dict = {}\n",
    "    new_peptides = ['ABC','DEF']\n",
    "\n",
    "    pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 0)\n",
    "\n",
    "    new_peptides = ['DEF','GHI']\n",
    "\n",
    "    pept_dict, added_peptides = add_to_pept_dict(pept_dict, new_peptides, 1)\n",
    "    \n",
    "    assert pept_dict == {'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n",
    "    \n",
    "test_add_to_pept_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def merge_pept_dicts(list_of_pept_dicts):\n",
    "    \n",
    "    if len(list_of_pept_dicts) == 0:\n",
    "        raise ValueError('Need to pass at least 1 element.')\n",
    "    \n",
    "    new_pept_dict = list_of_pept_dicts[0]\n",
    "\n",
    "    for pept_dict in list_of_pept_dicts[1:]:\n",
    "\n",
    "        for key in pept_dict.keys():\n",
    "            if key in new_pept_dict:\n",
    "                for element in pept_dict[key]:\n",
    "                    new_pept_dict[key].append(element)\n",
    "            else:\n",
    "                new_pept_dict[key] = pept_dict[key]\n",
    "\n",
    "    return new_pept_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABC': [0, 3, 4], 'DEF': [0, 1], 'GHI': [1], 'JKL': [5, 6], 'MNO': [7]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pept_dict_1 = {'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n",
    "pept_dict_2 = {'ABC': [3,4], 'JKL': [5, 6], 'MNO': [7]}\n",
    "\n",
    "merge_pept_dicts([pept_dict_1, pept_dict_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_merge_pept_dicts():\n",
    "    pept_dict_1 = {'ABC': [0], 'DEF': [0, 1], 'GHI': [1]}\n",
    "    pept_dict_2 = {'ABC': [3,4], 'JKL': [5, 6], 'MNO': [7]}\n",
    "\n",
    "    assert merge_pept_dicts([pept_dict_1, pept_dict_2]) == {'ABC': [0, 3, 4], 'DEF': [0, 1], 'GHI': [1], 'JKL': [5, 6], 'MNO': [7]}\n",
    "    \n",
    "test_merge_pept_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a database\n",
    "\n",
    "To wrap everything up, we employ two functions `generate_database` and `generate_spectra`. The first one reads a FASTA file and generates a list of peptides, as well as the peptide dictionary and an ordered FASTA dictionary to be able to look up the protein indices laster. For the `callback` we first read the whole FASTA file to determine the total number of entries in the FASTA file.  For a typical FASTA file of 30 Mb with 40k entries, this should take less than a second. The progress of the digestion is monitored by processing the FASTA file one by one.\n",
    "The function `generate_spectra` then calculates precursor masses and fragment ions. Here, we split the total_number of sequences in `1000` steps to be able to track progress with the `callback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import OrderedDict\n",
    "\n",
    "def generate_fasta_list(fasta_paths, callback = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    fasta_list = []\n",
    "\n",
    "    fasta_dict = OrderedDict()\n",
    "\n",
    "    fasta_index = 0\n",
    "\n",
    "    if type(fasta_paths) is str:\n",
    "        fasta_paths = [fasta_paths]\n",
    "        n_fastas = 1\n",
    "\n",
    "    elif type(fasta_paths) is list:\n",
    "        n_fastas = len(fasta_paths)\n",
    "\n",
    "    for f_id, fasta_file in enumerate(fasta_paths):\n",
    "        n_entries = read_fasta_file_entries(fasta_file)\n",
    "\n",
    "        fasta_generator = read_fasta_file(fasta_file)\n",
    "\n",
    "        for element in fasta_generator:\n",
    "            if check_sequence(element, constants.AAs):\n",
    "                fasta_list.append(element)\n",
    "                fasta_dict[fasta_index] = element\n",
    "                fasta_index += 1\n",
    "\n",
    "    return fasta_list, fasta_dict\n",
    "\n",
    "\n",
    "def generate_database(mass_dict, fasta_paths, callback = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    to_add = List()\n",
    "    fasta_dict = OrderedDict()\n",
    "    fasta_index = 0\n",
    "\n",
    "    pept_dict = {}\n",
    "\n",
    "    if type(fasta_paths) is str:\n",
    "        fasta_paths = [fasta_paths]\n",
    "        n_fastas = 1\n",
    "\n",
    "    elif type(fasta_paths) is list:\n",
    "        n_fastas = len(fasta_paths)\n",
    "\n",
    "    for f_id, fasta_file in enumerate(fasta_paths):\n",
    "        n_entries = read_fasta_file_entries(fasta_file)\n",
    "\n",
    "        fasta_generator = read_fasta_file(fasta_file)\n",
    "\n",
    "        for element in fasta_generator:\n",
    "            if check_sequence(element, constants.AAs):\n",
    "                fasta_dict[fasta_index] = element\n",
    "                mod_peptides = generate_peptides(element[\"sequence\"], **kwargs)\n",
    "                pept_dict, added_seqs = add_to_pept_dict(pept_dict, mod_peptides, fasta_index)\n",
    "                if len(added_seqs) > 0:\n",
    "                    to_add.extend(added_seqs)\n",
    "\n",
    "            fasta_index += 1\n",
    "\n",
    "            if callback:\n",
    "                callback(fasta_index/n_entries/n_fastas+f_id)\n",
    "\n",
    "    return to_add, pept_dict, fasta_dict\n",
    "\n",
    "\n",
    "def generate_spectra(to_add, mass_dict, callback = None):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "\n",
    "    if len(to_add) > 0:\n",
    "\n",
    "        if callback: #Chunk the spectra to get a progress_bar\n",
    "            spectra = []\n",
    "\n",
    "            stepsize = int(np.ceil(len(to_add)/1000))\n",
    "\n",
    "            for i in range(0, len(to_add), stepsize):\n",
    "                sub = to_add[i:i + stepsize]\n",
    "                spectra.extend(get_spectra(sub, mass_dict))\n",
    "                callback((i+1)/len(to_add))\n",
    "\n",
    "        else:\n",
    "            spectra = get_spectra(to_add, mass_dict)\n",
    "    else:\n",
    "        raise ValueError(\"No spectra to generate.\")\n",
    "\n",
    "    return spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized version\n",
    "\n",
    "To speed up spectra generated, one can use the parallelized version. The function `generate_database_parallel` reads an entire FASTA file and splits it into multiple blocks. Each block will be processed, and the generated pept_dicts will be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from multiprocessing import Pool\n",
    "from alphapept import constants\n",
    "mass_dict = constants.mass_dict\n",
    "\n",
    "def block_idx(len_list, block_size = 1000):\n",
    "    \"\"\"\n",
    "    Create indices for a list of length len_list\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "\n",
    "    while end <= len_list:\n",
    "        end += block_size\n",
    "        blocks.append((start, end))\n",
    "        start = end\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def blocks(l, n):\n",
    "    \"\"\"\n",
    "    Create blocks from a given list\n",
    "    \"\"\"\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))\n",
    "\n",
    "def digest_fasta_block(to_process):\n",
    "    \"\"\"\n",
    "    Digest and create spectra for a whole fasta_block\n",
    "    \"\"\"\n",
    "\n",
    "    fasta_index, fasta_block, settings = to_process\n",
    "\n",
    "    to_add = List()\n",
    "\n",
    "    f_index = 0\n",
    "\n",
    "    pept_dict = {}\n",
    "    for element in fasta_block:\n",
    "        sequence = element[\"sequence\"]\n",
    "        mod_peptides = generate_peptides(sequence, **settings['fasta'])\n",
    "        pept_dict, added_peptides = add_to_pept_dict(pept_dict, mod_peptides, fasta_index+f_index)\n",
    "\n",
    "        if len(added_peptides) > 0:\n",
    "            to_add.extend(added_peptides)\n",
    "        f_index += 1\n",
    "\n",
    "    spectra = []\n",
    "    if len(to_add) > 0:\n",
    "        for specta_block in blocks(to_add, settings['fasta']['spectra_block']):\n",
    "            spectra.extend(generate_spectra(specta_block, mass_dict))\n",
    "\n",
    "    return (spectra, pept_dict)\n",
    "\n",
    "def generate_database_parallel(settings, callback = None):\n",
    "    \"\"\"\n",
    "    Function to generate a database from a fasta file\n",
    "    \"\"\"\n",
    "    n_processes = settings['general']['n_processes']\n",
    "\n",
    "    fasta_list, fasta_dict = generate_fasta_list(**settings['fasta'])\n",
    "\n",
    "    blocks = block_idx(len(fasta_list), settings['fasta']['fasta_block'])\n",
    "\n",
    "    to_process = [(idx_start, fasta_list[idx_start:idx_end], settings) for idx_start, idx_end in  blocks]\n",
    "\n",
    "    spectra = []\n",
    "    pept_dicts = []\n",
    "    with Pool(n_processes) as p:\n",
    "        max_ = len(to_process)\n",
    "        for i, _ in enumerate(p.imap_unordered(digest_fasta_block, to_process)):\n",
    "            if callback:\n",
    "                callback((i+1)/max_)\n",
    "            spectra.extend(_[0])\n",
    "            pept_dicts.append(_[1])\n",
    "\n",
    "    spectra = sorted(spectra, key=lambda x: x[1])\n",
    "    spectra_set = [spectra[idx] for idx in range(len(spectra)-1) if spectra[idx][1] != spectra[idx+1][1]]\n",
    "    spectra_set.append(spectra[-1])\n",
    "\n",
    "    pept_dict = merge_pept_dicts(pept_dicts)\n",
    "\n",
    "    return spectra_set, pept_dict, fasta_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel search on large files\n",
    "\n",
    "In some cases (i.e., a lot of modifications), it will not be useful to save the database as it will consume too much memory. Here, we use the function `search_parallel` from search. It creates theoretical spectra on the fly and directly searches against them. As we cannot create a pept_dict here, we need to create one from the search results. For this, we group peptides by their FASTA index and generate a lookup dictionary that can be used as a pept_dict.\n",
    "\n",
    "> Note that we are passing the settings argument here. Search results should be stored in the corresponding path in the hdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pept_dict_from_search(settings):\n",
    "    \"\"\"\n",
    "    Generates a peptide dict from a large search\n",
    "    \"\"\"\n",
    "\n",
    "    paths = settings['experiment']['files']\n",
    "\n",
    "    bases = [os.path.splitext(_)[0]+'.hdf' for _ in paths]\n",
    "\n",
    "    all_dfs = []\n",
    "    for _ in bases:\n",
    "        try:\n",
    "            df = pd.read_hdf(_, key='peptide_fdr')\n",
    "        except KeyError:\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "        if df > 0:\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    df = pd.concat(all_dfs)\n",
    "\n",
    "    df['fasta_index'] = df['fasta_index'].str.split(',')\n",
    "\n",
    "    lst_col = 'fasta_index'\n",
    "\n",
    "    df_ = pd.DataFrame({\n",
    "          col:np.repeat(df[col].values, df[lst_col].str.len())\n",
    "          for col in df.columns.drop(lst_col)}\n",
    "        ).assign(**{lst_col:np.concatenate(df[lst_col].values)})[df.columns]\n",
    "\n",
    "    df_['fasta_index'] = df_['fasta_index'].astype('int')\n",
    "    df_grouped = df_.groupby(['sequence'])['fasta_index'].unique()\n",
    "\n",
    "    pept_dict = {}\n",
    "    for keys, vals in zip(df_grouped.index, df_grouped.values):\n",
    "        pept_dict[keys] = vals.tolist()\n",
    "\n",
    "    return pept_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "To save the generated spectra, we rely on NumPy's NPZ format. For this, we create a dictionary and save all the generated elements. The container will contain the following elements:\n",
    "\n",
    "* `precursors`: An array containing the precursor masses\n",
    "* `seqs`: An array containing the peptide sequences for the precursor masses\n",
    "* `pept_dict`: A peptide dictionary to look up the peptides and return their FASTA index\n",
    "* `fasta_dict`: A fasta dictionary to look up the FASTA entry based on a pept_dict index\n",
    "* `fragmasses`: An array containing the fragment masses. Unoccupied cells are filled with -1\n",
    "* `fragtypes:`: An array containg the fragment types. 0 equals b-ions and 1 equals y-ions. Unoccupied cells are filled with -1\n",
    "* `bounds`: An integer array containing the upper bounds for the fragment masses / types array. This is needed to quickly slice the data.\n",
    "\n",
    "All arrays are sorted according to the precursor mass.\n",
    "\n",
    "> To access the dictionaries such as `pept_dict` or `fasta_dict`, one needs to extract them using the `.item()` method like so: `container['pept_dict'].item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import alphapept.io\n",
    "import pandas as pd\n",
    "\n",
    "def save_database(spectra, pept_dict, fasta_dict, database_path, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to save a database to the *.hdf format.\n",
    "    \"\"\"\n",
    "\n",
    "    precmasses, seqs, fragmasses, fragtypes = zip(*spectra)\n",
    "    sortindex = np.argsort(precmasses)\n",
    "    \n",
    "    to_save = {}\n",
    "    \n",
    "    to_save[\"precursors\"] = np.array(precmasses)[sortindex]\n",
    "    to_save[\"seqs\"] = np.array(seqs, dtype=object)[sortindex]\n",
    "    to_save[\"proteins\"] = pd.DataFrame(fasta_dict).T\n",
    "\n",
    "    to_save[\"fragmasses\"] = alphapept.io.list_to_numpy_f32(np.array(fragmasses, dtype='object')[sortindex])\n",
    "    to_save[\"fragtypes\"] = alphapept.io.list_to_numpy_f32(np.array(fragtypes, dtype='object')[sortindex])\n",
    "\n",
    "    to_save[\"bounds\"] = np.sum(to_save['fragmasses']>=0,axis=0).astype(np.int64)\n",
    "\n",
    "    db_file = alphapept.io.HDF_File(database_path, is_new_file=True)\n",
    "    for key, value in to_save.items():\n",
    "        db_file.write(value, dataset_name=key)\n",
    "    \n",
    "    peps = np.array(list(pept_dict), dtype=object)\n",
    "    indices = np.empty(len(peps) + 1, dtype=np.int64)\n",
    "    indices[0] = 0\n",
    "    indices[1:] = np.cumsum([len(pept_dict[i]) for i in peps])\n",
    "    proteins = np.concatenate([pept_dict[i] for i in peps])\n",
    "    \n",
    "    db_file.write(\"peptides\")\n",
    "    db_file.write(\n",
    "        peps,\n",
    "        dataset_name=\"sequences\",\n",
    "        group_name=\"peptides\"\n",
    "    )\n",
    "    db_file.write(\n",
    "        indices,\n",
    "        dataset_name=\"protein_indptr\",\n",
    "        group_name=\"peptides\"\n",
    "    )\n",
    "    db_file.write(\n",
    "        proteins,\n",
    "        dataset_name=\"protein_indices\",\n",
    "        group_name=\"peptides\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import collections\n",
    "\n",
    "def read_database(database_path:str, array_name:str=None):\n",
    "    db_file = alphapept.io.HDF_File(database_path)\n",
    "    if array_name is None:\n",
    "        db_data = {\n",
    "            key: db_file.read(\n",
    "                dataset_name=key\n",
    "            ) for key in db_file.read() if key not in (\n",
    "                \"proteins\",\n",
    "                \"peptides\"\n",
    "            )\n",
    "        }\n",
    "        db_data[\"fasta_dict\"] = np.array(\n",
    "            collections.OrderedDict(db_file.read(dataset_name=\"proteins\").T)\n",
    "        )\n",
    "        peps = db_file.read(dataset_name=\"sequences\", group_name=\"peptides\")\n",
    "        protein_indptr = db_file.read(\n",
    "            dataset_name=\"protein_indptr\",\n",
    "            group_name=\"peptides\"\n",
    "        )\n",
    "        protein_indices = db_file.read(\n",
    "            dataset_name=\"protein_indices\",\n",
    "            group_name=\"peptides\"\n",
    "        )\n",
    "        db_data[\"pept_dict\"] = np.array(\n",
    "            {\n",
    "                pep: (protein_indices[s: e]).tolist() for pep, s, e in zip(\n",
    "                    peps,\n",
    "                    protein_indptr[:-1],\n",
    "                    protein_indptr[1:],\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        db_data[\"seqs\"] = db_data[\"seqs\"].astype(str)\n",
    "    else:\n",
    "        db_data = db_file.read(dataset_name=array_name)\n",
    "    return db_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_settings.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_interface.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-alphapept] *",
   "language": "python",
   "name": "conda-env-.conda-alphapept-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
