# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/09_matching.ipynb (unless otherwise specified).

__all__ = ['calculate_distance', 'calib_table', 'align', 'calculate_deltas', 'align_files', 'align_dfs',
           'align_datasets', 'get_probability', 'match_datasets']

# Cell
import logging
import pandas as pd
from itertools import combinations
import numpy as np
import os
import alphapept.io
import functools
from sklearn.linear_model import LinearRegression


def calculate_distance(table_1, table_2, offset_cols, calib = False):
    """
    Calculate the distance, either relative or absolute
    TODO: We could use a weighting factor
    """

    shared_precursors = list(set(table_1.index).intersection(set(table_2.index)))

    table_1_ = table_1.loc[shared_precursors]
    table_2_ = table_2.loc[shared_precursors]

    table_1_ = table_1_.groupby(table_1_.index).mean()
    table_2_ = table_2_.groupby(table_2_.index).mean()

    deltas = []

    for col in list(offset_cols.keys()):
        if calib:
            col_ = col+'_calib'
        else:
            col_ = col

        if offset_cols[col] == 'absolute':
            deltas.append(np.nanmedian(table_1_[col_] - table_2_[col_]))
        elif offset_cols[col] == 'relative':
            deltas.append(np.nanmedian((table_1_[col_] - table_2_[col_]) / (table_1_[col_] + table_2_[col_]) * 2))
        else:
            raise NotImplementedError(offset_cols[col_])

    return deltas, len(shared_precursors)

def calib_table(table, delta, offset_cols):
    """
    Apply offset to a table
    If not _calib table exist, create a new one.

    """
    for col in list(offset_cols.keys()):

        if (col not in table.columns) and (col+'_apex' in table.columns):
            col_ = col+'_apex'
        else:
            col_ = col

        if offset_cols[col] == 'absolute':
            table[col+'_calib'] =  table[col_]-delta[col]
        elif offset_cols[col] == 'relative':
            table[col+'_calib'] = (1-delta[col_])*table[col]
        else:
            raise NotImplementedError(offset_cols[col])

def align(deltas, filenames, weights=None):
    """
    Solve equation system
    """
    matrix = []

    for i in range(len(deltas)):
        start, end = deltas.index[i]

        start_idx = filenames.index(start)
        end_idx = filenames.index(end)

        lines = np.zeros(len(filenames)-1)
        lines[start_idx:end_idx] = 1
        matrix.append(lines)

    # Remove nan values

    not_nan = ~deltas.isnull().any(axis=1)
    matrix = np.array(matrix)
    matrix = matrix[not_nan]
    deltas_ = deltas[not_nan]

    if len(deltas) < matrix.shape[1]:
        logging.info('Low overlap between datasets detected. Alignment may fail.')

    if weights is not None:
        reg = LinearRegression(fit_intercept=False).fit(matrix, deltas_.values, sample_weight = weights[not_nan])
        score= reg.score(matrix, deltas_.values)
    else:
        reg = LinearRegression(fit_intercept=False).fit(matrix, deltas_.values)
        score= reg.score(matrix, deltas_.values)

    logging.info(f"Regression score is {score}")

    x= reg.predict(np.eye(len(filenames)-1))

    #x = np.linalg.lstsq(matrix, deltas_.values, rcond=None)[0] #Alternative w/o weights

    return x


def calculate_deltas(combos, filenames, splits, calib = False, callback=None):
    """
    Calculate offsets for multiple files
    TODO: Parallelize
    """

    offset_cols = {}

    callback = None

    deltas = pd.DataFrame()
    weights = []

    for i, combo in enumerate(combos):
        df_1 = splits[combo[0]].copy()
        df_2 = splits[combo[1]].copy()

        if not offset_cols:
            offset_cols = {'mz':'relative', 'rt':'absolute'}
            if 'mobility' in df_1.columns:
                logging.info("Also using mobility for calibration.")
                offset_cols['mobility'] = 'relative'
            cols = list(offset_cols.keys())

        if len(deltas) == 0:
             deltas = pd.DataFrame(columns = cols)

        dists, weight = calculate_distance(df_1, df_2, offset_cols, calib = calib)
        deltas = deltas.append(pd.DataFrame([dists], columns = cols, index=[(filenames[combo[0]], filenames[combo[1]])]))

        weights.append(weight)

        if callback:
            callback((i+1)/len(combos))

    return deltas, np.array(weights), offset_cols


def align_files(filenames, alignment, offset_cols):

    for idx, file in enumerate(filenames):

        for column in ['feature_table', 'peptide_fdr']:
            df = alphapept.io.MS_Data_File(file).read(dataset_name=column)
            calib_table(df, alignment.iloc[idx], offset_cols)
            logging.info(f"Saving {file} - {column}.")
            ms_file = alphapept.io.MS_Data_File(file, is_overwritable=True)

            ms_file.write(df, dataset_name=column)


def align_dfs(stack, alignment, offset_cols):

    calib_stack = []

    for idx, df in enumerate(stack):

        df = stack[idx].copy()

        calib_table(df, alignment.iloc[idx], offset_cols)

        calib_stack.append(df)

    return calib_stack


def align_datasets(settings, callback=None):
    filenames = settings['experiment']['file_paths']

    if callback:
        def progress_wrapper(current, step, n_steps):
            callback(step+current/n_steps)

        cb = functools.partial(progress_wrapper, 0, 2)
    else:
        cb = None

    filenames_ = settings['experiment']['file_paths']
    filenames_ = [os.path.splitext(_)[0]+'.ms_data.hdf' for _ in filenames_]

    if len(filenames) > 1:

        df = pd.read_hdf(settings['experiment']['results_path'], 'protein_fdr')
        filenames = list(df['filename'].unique())

    if len(filenames) > 1:

        splits = [df[df['filename'] == _] for _ in filenames]

        splits_indexed = [_.set_index('precursor') for _ in splits]

        combos = list(combinations(range(len(filenames)), 2))

        deltas, weights, offset_cols = calculate_deltas(combos, filenames, splits_indexed, callback=cb)

        cols = list(offset_cols.keys())

        before_sum = deltas.abs().sum().to_dict()
        before_mean = deltas.abs().mean().to_dict()

        logging.info(f'Total deviation before calibration {before_sum}')
        logging.info(f'Mean deviation before calibration {before_mean}')

        logging.info(f'Solving equation system')

        alignment = pd.DataFrame(align(deltas, filenames, weights), columns = cols)
        alignment = pd.concat([pd.DataFrame(np.zeros((1, alignment.shape[1])), columns= cols), alignment])
        alignment -= alignment.mean()

        logging.info(f'Solving equation system complete.')

        logging.info(f'Applying offset')

        align_files(filenames_, -alignment, offset_cols) #Feature_table

        stack = align_dfs(splits, -alignment, offset_cols)

        c = pd.concat(stack).sort_index()
        c.to_hdf(settings['experiment']['results_path'], 'protein_fdr')

        if cb:
            cb = functools.partial(progress_wrapper, 1, 2)

        splits_indexed = [_.set_index('precursor', drop=False) for _ in stack]

        deltas, weights, offset_cols = calculate_deltas(combos, filenames, splits_indexed, calib = True, callback=cb)

        after_sum = deltas.abs().sum().to_dict()
        after_mean = deltas.abs().mean().to_dict()

        logging.info(f'Total deviation after calibration {after_sum}')
        logging.info(f'Mean deviation after calibration {after_mean}')

        change_sum = {k:v/before_sum[k] for k,v in after_sum.items()}
        change_mean = {k:v/before_mean[k] for k,v in after_mean.items()}

        logging.info(f'Change (after/before) total deviation {change_sum}')
        logging.info(f'Change (after/before) mean deviation {change_mean}')

    else:
        logging.info('Only 1 dataset present. Skipping alignment.')

# Cell

from sklearn.neighbors import KDTree
from .utils import assemble_df
from scipy import stats

def get_probability(df, ref, sigma, index):

    sigma = sigma.iloc[index].values
    sigma = sigma*np.eye(len(sigma))

    mu = ref.iloc[index].values

    x = df.iloc[index].values

    try:
        m_dist_x = np.dot((x-mu).transpose(), np.linalg.inv(sigma))
        m_dist_x = np.dot(m_dist_x, (x-mu))
        _ = stats.chi2.cdf(m_dist_x, len(mu))
    except Exception as e:
        _ = np.nan

    return _

def match_datasets(settings, callback = None):

    if len(settings['experiment']['file_paths']) > 2:
        xx = assemble_df(settings, field='protein_fdr')
        cols = ['precursor','mz_calib','rt_calib']

        if 'mobility' in xx.columns:
            cols += ['mobility_calib']
            use_mobility = True
        else:
            use_mobility = False

        grouped = xx[cols].groupby('precursor').mean()
        std_ = xx[cols].groupby('precursor').std()

        group_columns = grouped.columns
        grouped[[_+'_std' for _ in group_columns]] = std_

        std_range = np.nanmedian(std_.values, axis=0)

        min_match_p = settings['matching']['min_match_p']
        min_match_d = settings['matching']['min_match_d']

        filenames = settings['experiment']['file_paths']

        lookup_dict = xx.set_index('precursor')[['protein','protein_group','sequence']].to_dict()

        for idx, filename in enumerate(filenames):
            file = os.path.splitext(filename)[0] + '.ms_data.hdf'

            df = alphapept.io.MS_Data_File(file).read(dataset_name='protein_fdr')
            features = alphapept.io.MS_Data_File(file).read(dataset_name='feature_table')
            features['feature_idx'] = features.index

            matching_set = set(grouped.index) - set(df['precursor'])
            logging.info(f'Trying to match file {file} with database of {len(matching_set):,} unidentified candidates')

            mz_range = std_range[0]
            rt_range = std_range[1]

            tree_points = features[group_columns].values
            tree_points[:,0] = tree_points[:,0]/mz_range
            tree_points[:,1] = tree_points[:,1]/rt_range

            query_points = grouped.loc[matching_set][group_columns].values
            query_points[:,0] = query_points[:,0]/mz_range
            query_points[:,1] = query_points[:,1]/rt_range

            if use_mobility:
                logging.info("Using mobility")
                i_range = std_range[2]

                tree_points[:,2] = tree_points[:,2]/i_range
                query_points[:,2] = query_points[:,2]/i_range

            matching_tree = KDTree(tree_points, metric="minkowski")

            dist, idx = matching_tree.query(query_points, k=1)

            matched = features.iloc[idx[:,0]]

            to_keep = dist < min_match_d

            matched = matched[to_keep]

            ref = grouped.loc[matching_set][group_columns][to_keep]
            sigma = std_.loc[matching_set][to_keep]

            logging.info(f'{len(matched):,} possible features for matching based on distance of {min_match_d}')

            matched['matching_p'] = [get_probability(matched[group_columns], ref, sigma, i) for i in range(len(matched))]
            matched['precursor'] = grouped.loc[matching_set][to_keep].index.values

            matched = matched[matched['matching_p']< min_match_p]

            logging.info(f'{len(matched):,} possible features for matching based on probability of {min_match_p}')

            matched['type'] = 'matched'

            for _ in lookup_dict.keys():
                matched[_] = [lookup_dict[_][x] for x in matched['precursor']]

            df['type'] = 'msms'
            df['matching_p'] = np.nan

            shared_columns = set(matched.columns).intersection(set(df.columns))

            df_ = pd.concat([df, matched[shared_columns]], ignore_index=True)

            logging.info(f"Saving {file} - protein_fdr.")
            ms_file = alphapept.io.MS_Data_File(file, is_overwritable=True)

            ms_file.write(df_, dataset_name='protein_fdr')

            if callback:
                callback((i+1)/len(filenames))
    else:
        logging.info('Less than 3 datasets present. Skipping matching.')