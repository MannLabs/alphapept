# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/12_performance.ipynb (unless otherwise specified).

__all__ = ['COMPILATION_MODE_OPTIONS', 'is_valid_compilation_mode', 'set_worker_count', 'MAX_WORKER_COUNT',
           'set_compilation_mode', 'compile_function', '__copy_func', 'DYNAMIC_COMPILATION_ENABLED',
           'performance_function', 'AlphaPool']

# Cell

COMPILATION_MODE_OPTIONS = [
    "python",
    "python-multithread",
    "numba",
    "numba-multithread",
    "cuda", # Cuda is always multithreaded
]

# Cell

import functools
import math
import os
import logging
import psutil
import ast

# Parallelization
import multiprocessing
import threading

# Compilation
import numpy as np
import numba
from numba import cuda
try:
    import cupy
    cuda.get_current_device()
    __GPU_AVAILABLE = True
except ModuleNotFoundError:
    __GPU_AVAILABLE = False
    cupy = None
    logging.info("Cupy is not available")
except cuda.CudaSupportError:
    __GPU_AVAILABLE = False
    logging.info("Cuda device is not available")

def is_valid_compilation_mode(compilation_mode: str) -> None:
    """Check if the provided string is a valid compilation mode.

    Args:
        compilation_mode (str): The compilation mode to verify.

    Raises:
        ModuleNotFoundError: When trying to use an unavailable GPU.
        NotImplementedError: When the compilation mode is not valid.

    """
    if compilation_mode.startswith("cuda") and not __GPU_AVAILABLE:
        raise ModuleNotFoundError('Cuda functions are not available.')
    if compilation_mode not in COMPILATION_MODE_OPTIONS:
        raise NotImplementedError(
            f"Compilation mode '{compilation_mode}' is not available, "
            "see COMPILATION_MODE_OPTIONS for available options."
        )

# Cell

if __GPU_AVAILABLE:
    COMPILATION_MODE = "cuda"
else:
    COMPILATION_MODE = "numba-multithread"

# Cell

MAX_WORKER_COUNT = psutil.cpu_count()

def set_worker_count(worker_count: int = 1, set_global: bool = True) -> int:
    """Parse and set the (global) number of threads.

    Args:
        worker_count (int): The number of workers.
            If larger than available cores, it is trimmed to the available maximum.
            If 0, it is set to the maximum cores available.
            If negative, it indicates how many cores NOT to use.
            Default is 1
        set_global (bool): If False, the number of workers is only parsed to a valid value.
            If True, the number of workers is saved as a global variable.
            Default is True.

    Returns:
        int: The parsed worker_count.

    """
    max_cpu_count = psutil.cpu_count()
    if worker_count > max_cpu_count:
        worker_count = max_cpu_count
    else:
        while worker_count <= 0:
            worker_count += max_cpu_count
    if set_global:
        global MAX_WORKER_COUNT
        MAX_WORKER_COUNT = worker_count
    return worker_count

# Cell

DYNAMIC_COMPILATION_ENABLED = False

def set_compilation_mode(
    compilation_mode: str = None,
    enable_dynamic_compilation: bool = None,
) -> None:
    """Set the global compilation mode to use.

    Args:
        compilation_mode (str): The compilation mode to use.
            Will be checked with `is_valid_compilation_mode`.
            Default is None
        enable_dynamic_compilation (bool): Enable dynamic compilation.
            If enabled, code will generally be slower and no other functions can
            be called from within a compiled function anymore, as they are compiled at runtime.
            WARNING: Enabling this is strongly disadvised in almost all cases!
            Default is None.

    """
    if enable_dynamic_compilation is not None:
        global DYNAMIC_COMPILATION_ENABLED
        DYNAMIC_COMPILATION_ENABLED = enable_dynamic_compilation
    if compilation_mode is not None:
        is_valid_compilation_mode(compilation_mode)
        global COMPILATION_MODE
        COMPILATION_MODE = compilation_mode


def compile_function(
    _func: callable = None,
    *,
    compilation_mode: str = None,
    **decorator_kwargs,
) -> callable:
    """A decorator to compile a given function.

    Numba functions are by default set to use `nogil=True` and `nopython=True`,
    unless explicitly defined otherwise.
    Cuda functions are by default set to use `device=True`,
    unless explicitly defined otherwise..

    Args:
        compilation_mode (str): The compilation mode to use.
            Will be checked with `is_valid_compilation_mode`.
            If None, the global COMPILATION_MODE will be used as soon as the function is decorated for static compilation.
            If DYNAMIC_COMPILATION_ENABLED, the function will always be compiled at runtime and
            thus by default returns a Python function.
            Static recompilation can be enforced by reimporting a module containing
            the function with importlib.reload(imported_module).
            If COMPILATION_MODE is Python and not DYNAMIC_COMPILATION_ENABLED, no compilation will be used.
            Default is None
        **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.

    Returns:
        callable: A decorated function that is compiled.

    """
    if compilation_mode is None:
        if DYNAMIC_COMPILATION_ENABLED:
            compilation_mode = "dynamic"
        else:
            compilation_mode = COMPILATION_MODE
    elif COMPILATION_MODE.startswith("python"):
        compilation_mode = "python"
    else:
        is_valid_compilation_mode(compilation_mode)
    def parse_compilation(current_compilation_mode, func):
        if current_compilation_mode.startswith("python"):
            compiled_function = __copy_func(func)
        elif current_compilation_mode.startswith("numba"):
            if "nogil" in decorator_kwargs:
                if "nopython" in decorator_kwargs:
                    compiled_function = numba.jit(func, **decorator_kwargs)
                else:
                    compiled_function = numba.jit(func, **decorator_kwargs, nopython=True)
            elif "nopython" in decorator_kwargs:
                compiled_function = numba.jit(func, **decorator_kwargs, nogil=True)
            else:
                compiled_function = numba.jit(func, **decorator_kwargs, nogil=True, nopython=True)
        elif current_compilation_mode.startswith("cuda"):
            if "device" in decorator_kwargs:
                compiled_function = cuda.jit(func, **decorator_kwargs)
            else:
                compiled_function = cuda.jit(func, **decorator_kwargs, device=True)
        return compiled_function
    def decorated_function(func):
        if compilation_mode != "dynamic":
            is_valid_compilation_mode(compilation_mode)
            static_compiled_function = parse_compilation(compilation_mode, func)
            return functools.wraps(func)(static_compiled_function)
        else:
            def dynamic_compiled_function(*func_args, **func_kwargs):
                compiled_function = parse_compilation(COMPILATION_MODE, func)
                return compiled_function(*func_args, **func_kwargs)
            return functools.wraps(func)(dynamic_compiled_function)
    if _func is None:
        return decorated_function
    else:
        return decorated_function(_func)


import types
import functools

def __copy_func(f):
    """Based on http://stackoverflow.com/a/6528148/190597 (Glenn Maynard)"""
    g = types.FunctionType(f.__code__, f.__globals__, name=f.__name__,
                           argdefs=f.__defaults__,
                           closure=f.__closure__)
    g = functools.update_wrapper(g, f)
    g.__kwdefaults__ = f.__kwdefaults__
    return g

# Cell

def performance_function(
    _func: callable = None,
    *,
    worker_count: int = None,
    compilation_mode: str = None,
    **decorator_kwargs,
) -> callable:
    """A decorator to compile a given function and allow multithreading over an multiple indices.

    NOTE This should only be used on functions that are compilable.
    Functions that need to be decorated need to have an `index` argument as first argument.
    If an iterable is provided to the decorated function,
    the original (compiled) function will be applied to all elements of this iterable.
    The most efficient way to provide iterables are with ranges, but numpy arrays work as well.
    Functions can not return values,
    results should be stored in buffer arrays inside thge function instead.

    Args:
        worker_count (int): The number of workers to use for multithreading.
            If None, the global MAX_WORKER_COUNT is used at runtime.
            Default is None.
        compilation_mode (str): The compilation mode to use. Will be forwarded to the `compile_function` decorator.
        **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.

    Returns:
        callable: A decorated function that is compiled and parallelized.

    """
    if worker_count is not None:
        worker_count = set_worker_count(worker_count, set_global=False)
    if compilation_mode is None:
        if DYNAMIC_COMPILATION_ENABLED:
            compilation_mode = "dynamic"
        else:
            compilation_mode = COMPILATION_MODE
    elif COMPILATION_MODE.startswith("python"):
        compilation_mode = "python"
    else:
        is_valid_compilation_mode(compilation_mode)
    def _decorated_function(func):
        if compilation_mode != "dynamic":
            compiled_function = compile_function(
                func,
                compilation_mode=compilation_mode,
                **decorator_kwargs
            )
        def _parallel_python(
            compiled_function,
            iterable,
            start,
            stop,
            step,
            *func_args
        ):
            if start != -1:
                for index in range(start, stop, step):
                    compiled_function(index, *func_args)
            else:
                for index in iterable:
                    compiled_function(index, *func_args)
        _parallel_numba = numba.njit(nogil=True)(_parallel_python)
        def _parallel_cuda(compiled_function, iterable, *func_args):
            cuda_func_dict = {"cuda": cuda, "compiled_function": compiled_function}
            # Cuda functions cannot handle tuple unpacking but need a fixed number of arguments.
            if isinstance(iterable, range):
                func_string = ", ".join(f"arg{i}" for i in range(len(func_args) + 3))
                cuda_string = (
                    f"@cuda.jit\n"
                    f"def cuda_func({func_string}):\n"
                    f"    index = arg0 + arg2 * cuda.grid(1)\n"
                    f"    compiled_function(index, {func_string[18:]})\n"
                )
                exec(cuda_string, cuda_func_dict)
                cuda_func_dict["cuda_func"].forall(len(iterable), 1)(
                    iterable.start,
                    iterable.stop,
                    iterable.step,
                    *func_args
                )
            else:
                func_string = ", ".join(f"arg{i}" for i in range(len(func_args) + 1))
                cuda_string = (
                    f"@cuda.jit\n"
                    f"def cuda_func({func_string}):\n"
                    f"    index = arg0[cuda.grid(1)]\n"
                    f"    compiled_function(index, {func_string[6:]})\n"
                )
                exec(cuda_string, cuda_func_dict)
                cuda_func_dict["cuda_func"].forall(len(iterable), 1)(iterable, *func_args)
        def _performance_function(iterable, *func_args):
            if compilation_mode == "dynamic":
                selected_compilation_mode = COMPILATION_MODE
                _compiled_function = compile_function(
                    func,
                    compilation_mode=selected_compilation_mode,
                    **decorator_kwargs
                )
            else:
                _compiled_function = compiled_function
                selected_compilation_mode = compilation_mode
            try:
                iter(iterable)
            except TypeError:
                iterable = np.array([iterable])
            if worker_count is None:
                selected_worker_count = MAX_WORKER_COUNT
            else:
                selected_worker_count = worker_count
            if selected_compilation_mode == "cuda":
                _parallel_cuda(_compiled_function, iterable, *func_args)
            else:
                if "python" in selected_compilation_mode:
                    parallel_function = _parallel_python
                elif "numba" in selected_compilation_mode:
                    parallel_function = _parallel_numba
                else:
                    raise NotImplementedError(
                        f"Compilation mode {selected_compilation_mode} is not valid. "
                        "This error should not be possible, something is seriously wrong!!!"
                    )
                if (selected_compilation_mode in ["python", "numba"]) or (selected_worker_count == 1):
                    iterable_is_range = isinstance(iterable, range)
                    x = np.empty(0, dtype=np.int64) if iterable_is_range else iterable
                    parallel_function(
                        _compiled_function,
                        np.empty(0, dtype=np.int64) if iterable_is_range else iterable,
                        iterable.start if iterable_is_range else -1,
                        iterable.stop if iterable_is_range else -1,
                        iterable.step if iterable_is_range else -1,
                        *func_args
                    )
                else:
                    workers = []
                    for worker_id in range(selected_worker_count):
                        local_iterable = iterable[worker_id::selected_worker_count]
                        iterable_is_range = isinstance(local_iterable, range)
                        worker = threading.Thread(
                            target=parallel_function,
                            args=(
                                _compiled_function,
                                np.empty(0, dtype=np.int64) if iterable_is_range else local_iterable,
                                local_iterable.start if iterable_is_range else -1,
                                local_iterable.stop if iterable_is_range else -1,
                                local_iterable.step if iterable_is_range else -1,
                                *func_args
                            )
                        )
                        worker.start()
                        workers.append(worker)
                    for worker in workers:
                        worker.join()
                        del worker
        return functools.wraps(func)(_performance_function)
    if _func is None:
        return _decorated_function
    else:
        return _decorated_function(_func)

# Cell
from multiprocessing import Pool

def AlphaPool(process_count: int) -> multiprocessing.Pool:
    """Create a multiprocessing.Pool object.

    Args:
        process_count (int): The number of processes.
            If larger than available cores, it is trimmed to the available maximum.


    Returns:
        multiprocessing.Pool: A Pool object to parallelize functions with multiple processes.

    """
    max_processes = psutil.cpu_count()
    new_max = min(process_count, 50, max_processes)

    if new_max == 0:
        new_max = 1
    logging.info(f"AlphaPool was set to {process_count} processes. Setting max to {new_max}.")

    return Pool(new_max)