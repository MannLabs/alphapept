# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/11_interface.ipynb.

# %% auto 0
__all__ = ['HEADLESS', 'CONTEXT_SETTINGS', 'CLICK_SETTINGS_OPTION', 'tqdm_wrapper', 'check_version_and_hardware',
           'wrapped_partial', 'create_database', 'import_raw_data', 'feature_finding', 'search_data',
           'recalibrate_data', 'score', 'isobaric_labeling', 'protein_grouping', 'align', 'match',
           'read_label_intensity', 'quantification', 'export', 'run_complete_workflow', 'extract_median_unique',
           'get_file_summary', 'get_summary', 'parallel_execute', 'bcolors', 'is_port_in_use', 'run_cli',
           'cli_overview', 'cli_database', 'cli_import', 'cli_feature_finding', 'cli_search', 'cli_recalibrate',
           'cli_score', 'cli_align', 'cli_match', 'cli_quantify', 'cli_export', 'cli_workflow', 'cli_gui']

# %% ../nbs/11_interface.ipynb 4
import alphapept.utils
from .utils import set_logger

import alphapept.performance

import logging
import sys
import numpy as np
import psutil
import tqdm
import warnings
import pandas as pd

warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)

HEADLESS = False

def tqdm_wrapper(pbar, update: float) -> None:
    """Update a qdm progress bar.

    Args:
        pbar (type): a tqd,.tqdm objet.
        update (float): The new value for the progressbar.

    """
    current_value = pbar.n
    delta = update - current_value
    pbar.update(delta)

# %% ../nbs/11_interface.ipynb 5
def check_version_and_hardware(settings: dict) -> dict:
    """Show platform and python information and parse settings.

    Args:
        settings (dict): A dictionary with settings how to process the data.

    Returns:
        dict: The parsed settings.

    """
    import alphapept.utils
    #alphapept.utils.check_hardware()
    #alphapept.utils.check_python_env()
    alphapept.utils.show_platform_info()
    alphapept.utils.show_python_info()


    settings = alphapept.utils.check_settings(settings)
    return settings

# %% ../nbs/11_interface.ipynb 6
def wrapped_partial(func: callable, *args, **kwargs) -> callable:
    """Wrap a function with partial args and kwargs.

    Args:
        func (callable): The function to be wrapped.
        *args (type): Args to be wrapped.
        **kwargs (type): Kwargs to be wrapped.

    Returns:
        callable: The wrapped function.

    """
    partial_func = functools.partial(func, *args, **kwargs)
    functools.update_wrapper(partial_func, func)
    return partial_func

# %% ../nbs/11_interface.ipynb 8
import os
import functools
import copy

def create_database(
    settings: dict,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Create the search database.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    Raises:
        FileNotFoundError: If the FASTA file is not found.

    """
    import alphapept.fasta
    if not logger_set:
        set_logger()
    if not settings_parsed:
        settings = check_version_and_hardware(settings)
    if 'database_path' not in settings['experiment']:
        database_path = ''
    else:
        database_path = settings['experiment']['database_path']

    if database_path is None:
        database_path = ''


    if not settings['fasta']['save_db']: #Do not save DB
        settings['experiment']['database_path'] = None
        logging.info('Not saving Database.')

        return settings


    temp_settings = settings

    if os.path.isfile(database_path):
        logging.info(
            'Database path set and exists. Using {} as database.'.format(
                database_path
            )
        )
    else:
        logging.info(
            'Database path {} is not a file.'.format(database_path)
        )

        if len(settings['experiment']['fasta_paths']) == 0:
            raise FileNotFoundError("No FASTA files set.")

        total_fasta_size = 0

        for fasta_file in settings['experiment']['fasta_paths']:
            if os.path.isfile(fasta_file):

                fasta_size = os.stat(fasta_file).st_size/(1024**2)

                total_fasta_size += fasta_size

                logging.info(
                    'Found FASTA file {} with size {:.2f} Mb.'.format(
                        fasta_file,
                        fasta_size
                    )
                )
            else:
                raise FileNotFoundError(
                    'File {} not found'.format(fasta_file)
                )

        fasta_size_max = settings['fasta']['fasta_size_max']

        if total_fasta_size >= fasta_size_max:
            logging.info(f'Total FASTA size {total_fasta_size:.2f} is larger than the set maximum size of {fasta_size_max:.2f} Mb')

            settings['experiment']['database_path'] = None

            return settings

        logging.info('Creating a new database from FASTA.')

        if not callback:
            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))
        else:
            cb = callback

        (
            spectra,
            pept_dict,
            fasta_dict
        ) = alphapept.fasta.generate_database_parallel(
            temp_settings,
            callback=cb
        )
        logging.info(
            'Digested {:,} proteins and generated {:,} spectra'.format(
                len(fasta_dict),
                len(spectra)
            )
        )

        alphapept.fasta.save_database(
            spectra,
            pept_dict,
            fasta_dict,
            database_path = database_path,
            **settings['fasta']
        )
        logging.info(
            'Database saved to {}. Filesize of database is {:.2f} GB'.format(
                database_path,
                os.stat(database_path).st_size/(1024**3)
            )
        )

        settings['experiment']['database_path'] = database_path

    return settings

# %% ../nbs/11_interface.ipynb 9
def import_raw_data(
    settings: dict,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Import raw data.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    """
    if not logger_set:
        set_logger()
    if not settings_parsed:
        settings = check_version_and_hardware(settings)

    if not callback:
        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))
    else:
        cb = callback

    import alphapept.io

    settings = parallel_execute(settings, alphapept.io.raw_conversion, callback = cb)

    return settings

# %% ../nbs/11_interface.ipynb 10
def feature_finding(
    settings: dict,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Find features.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    """
    if not logger_set:
        set_logger()
    if not settings_parsed:
        settings = check_version_and_hardware(settings)

    import alphapept.feature_finding

    if not callback:
        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))
    else:
        cb = callback

    settings = parallel_execute(settings, alphapept.feature_finding.find_features, callback = cb)

    return settings

# %% ../nbs/11_interface.ipynb 11
def search_data(
    settings: dict,
    first_search: bool = True,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Create the search database.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        first_search (bool): If True, save the intermediary results as `first search`.
            Otherwise, calibrate mz_values are used and results are saved as `second search`.
            Defaults to True.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    Raises:
        FileNotFoundError: If the FASTA file is not found.

    """
    if not logger_set:
        set_logger()
    if not settings_parsed:
        settings = check_version_and_hardware(settings)

    import alphapept.search
    import alphapept.io

    if not callback:
        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))
    else:
        cb = callback

    if first_search:
        logging.info('Starting first search.')
        if settings['experiment']['database_path'] is not None:
            settings = parallel_execute(settings, wrapped_partial(alphapept.search.search_db, first_search = first_search), callback = cb)

            db_data = alphapept.fasta.read_database(settings['experiment']['database_path'])

            fasta_dict = db_data['fasta_dict'].item()
            pept_dict = db_data['pept_dict'].item()


        else:
            ms_files = []
            for _ in settings['experiment']['file_paths']:
                base, ext = os.path.splitext(_)
                ms_files.append(base + '.ms_data.hdf')

            fasta_dict = alphapept.search.search_parallel(
                settings,
                callback=cb
            )
            pept_dict = None

        logging.info('First search complete.')
    else:
        logging.info('Starting second search with DB.')

        if settings['experiment']['database_path'] is not None:
            settings = parallel_execute(settings, wrapped_partial(alphapept.search.search_db, first_search = first_search), callback = cb)

            db_data = alphapept.fasta.read_database(settings['experiment']['database_path'])

            fasta_dict = db_data['fasta_dict'].item()
            pept_dict = db_data['pept_dict'].item()


        else:
            ms_files = []
            for _ in settings['experiment']['file_paths']:
                base, ext = os.path.splitext(_)
                ms_files.append(base + '.ms_data.hdf')

            try:
                offsets = [
                    alphapept.io.MS_Data_File(
                        ms_file_name
                    ).read(
                        dataset_name="corrected_mass",
                        group_name="features",
                        attr_name="estimated_max_precursor_ppm"
                    ) * settings['search']['calibration_std_prec'] for ms_file_name in ms_files
                ]
            except KeyError:
                logging.info('No calibration found.')
                offsets = None

            try:
                frag_tols = [float(
                    alphapept.io.MS_Data_File(
                        ms_file_name
                    ).read(dataset_name="estimated_max_fragment_ppm")[0] * settings['search']['calibration_std_prec']) for ms_file_name in ms_files
                ]
        
            except KeyError:
                logging.info('Fragment tolerance not calibrated found.')
                frag_tols = None
                
                
            logging.info('Starting second search.')

            fasta_dict = alphapept.search.search_parallel(
                settings,
                calibration=offsets,
                fragment_calibration=frag_tols,
                callback=cb
            )
            pept_dict = None

        logging.info('Second search complete.')
    return settings, pept_dict, fasta_dict

# %% ../nbs/11_interface.ipynb 12
def recalibrate_data(
    settings: dict,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Recalibrate mz values.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    """
    if not logger_set:
        set_logger()
    if not settings_parsed:
        settings = check_version_and_hardware(settings)

    import alphapept.recalibration

    if settings['search']['calibrate']:
        if not callback:
            cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))
        else:
            cb = callback

        settings = parallel_execute(settings, alphapept.recalibration.calibrate_hdf, callback = cb)

    return settings

# %% ../nbs/11_interface.ipynb 13
def score(
    settings: dict,
    pept_dict: dict = None,
    fasta_dict: dict = None,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Score PSMs and calculate FDR.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        pept_dict (dict): A dictionary with peptides. Defaults to None.
        fasta_dict (dict): A dictionary with fasta sequences. Defaults to None.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    """
    if not logger_set:
        set_logger()
    if not settings_parsed:
        settings = check_version_and_hardware(settings)

    import alphapept.score
    import alphapept.fasta

    if not callback:
        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))
    else:
        cb = callback

    if fasta_dict is None:

        db_data = alphapept.fasta.read_database(
            settings['experiment']['database_path']
        )
        fasta_dict = db_data['fasta_dict'].item()
        pept_dict = db_data['pept_dict'].item()

    settings = parallel_execute(settings, alphapept.score.score_hdf, callback = cb)

    return settings

# %% ../nbs/11_interface.ipynb 14
def isobaric_labeling(
    settings: dict,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Search for isobaric labels.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    """
    
    if 'isobaric_label' in settings:
        if settings['isobaric_label']['label'] != 'None':    
            if not logger_set:
                set_logger()
            if not settings_parsed:
                settings = check_version_and_hardware(settings)

            import alphapept.label

            if settings['search']['calibrate']:
                if not callback:
                    cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))
                else:
                    cb = callback

                settings = parallel_execute(settings, alphapept.label.find_labels, callback = cb)

    return settings

# %% ../nbs/11_interface.ipynb 15
def protein_grouping(
    settings: dict,
    pept_dict: dict = None,
    fasta_dict: dict = None,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Group peptides into proteins.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        pept_dict (dict): A dictionary with peptides. Defaults to None.
        fasta_dict (dict): A dictionary with fasta sequences. Defaults to None.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    """
    if not logger_set:
        set_logger()
    if not settings_parsed:
        settings = check_version_and_hardware(settings)

    import alphapept.score
    import alphapept.fasta

    if not callback:
        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))
    else:
        cb = callback

    if fasta_dict is None:

        db_data = alphapept.fasta.read_database(
            settings['experiment']['database_path']
        )
        fasta_dict = db_data['fasta_dict'].item()
        pept_dict = db_data['pept_dict'].item()


    if pept_dict is None: #Pept dict extractions needs scored
        pept_dict = alphapept.fasta.pept_dict_from_search(settings)

    logging.info(f'Fasta dict with length {len(fasta_dict):,}, Pept dict with length {len(pept_dict):,}')

    # Protein groups
    logging.info('Extracting protein groups.')

    if not callback:
        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))
    else:
        cb = callback

    alphapept.score.protein_grouping_all(settings, pept_dict, fasta_dict, callback=cb)

    logging.info('Protein groups complete.')

    return settings

# %% ../nbs/11_interface.ipynb 16
def align(
    settings: dict,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Align multiple samples.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    """
    if not logger_set:
        set_logger()
    if not settings_parsed:
        settings = check_version_and_hardware(settings)

    import alphapept.matching

    alphapept.matching.align_datasets(settings, callback = callback)

    return settings

def match(
    settings: dict,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Match datasets.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    """
    if not logger_set:
        set_logger()
    if not settings_parsed:
        settings = check_version_and_hardware(settings)

    import alphapept.matching


    alphapept.matching.match_datasets(settings)

    return settings

# %% ../nbs/11_interface.ipynb 17
from typing import NamedTuple

def read_label_intensity(df : pd.DataFrame, label: NamedTuple) ->  pd.DataFrame:
    """Reads the label intensities from peptides and sums them by protein group.

    Args:
        df (pd.DataFrame): Table with peptide information.
        label (NamedTuple): Label used for the experiment.

    Returns:
        pd.DataFrame: Summary protein table containing proteins and their intensity for each channel."""

    all_channels = []

    for channel in label.channels:

        _ = df[['protein_group', channel]].groupby('protein_group').sum()

        all_channels.append(_)

    protein_table = pd.concat(all_channels, axis=1)

    return protein_table

# %% ../nbs/11_interface.ipynb 18
def quantification(
    settings: dict,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Normalize and quantify datasets.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    """
    if not logger_set:
        set_logger()
    if not settings_parsed:
        settings = check_version_and_hardware(settings)

    import alphapept.quantification
    
    skip = False
    protein_summary = pd.DataFrame()

    field = settings['quantification']['mode']

    if os.path.isfile(settings['experiment']['results_path']):
    
        results_path = settings['experiment']['results_path']
        base, ext = os.path.splitext(results_path)
        
        logging.info('Reading protein_fdr for quantification.')
        df = pd.read_hdf(settings['experiment']['results_path'], 'protein_fdr')
        
        if 'isobaric_label' in settings:
            if settings['isobaric_label']['label'] != 'None':
                from alphapept.constants import label_dict

                label = label_dict[settings['isobaric_label']['label']]
                logging.info('Reading intensities from channels.')
                protein_summary = read_label_intensity(df, label)
                skip = True
        
        if not skip:

            if settings["workflow"]["lfq_quantification"]:

                if field in df.keys():  # Check if the quantification information exists.
                    # We could include another protein fdr in here..

                    samples = df['sample_group'].unique().tolist()

                    if len(samples) > 1:
                        logging.info('Delayed Normalization.')
                        df, normalization = alphapept.quantification.delayed_normalization(
                            df,
                            field
                        )
                        pd.DataFrame(normalization).to_hdf(
                            settings['experiment']['results_path'],
                            'fraction_normalization'
                        )
                        df_grouped = df.groupby(
                            ['sample_group', 'precursor', 'protein_group']
                        )[['{}_dn'.format(field)]].sum().reset_index()
                    else:
                        df_grouped = df.groupby(
                            ['sample_group', 'precursor', 'protein_group']
                        )[field].sum().reset_index()

                    logging.info('Saving protein_groups after delayed normalization to combined_protein_fdr_dn')
                    df.to_hdf(
                        settings['experiment']['results_path'],
                        'combined_protein_fdr_dn'
                    )

                    logging.info('Complete.')
                    logging.info('Starting profile extraction.')

                    if not callback:
                        cb = functools.partial(tqdm_wrapper, tqdm.tqdm(total=1))
                    else:
                        cb = callback

                    protein_table = alphapept.quantification.protein_profile_parallel_ap(
                        settings,
                        df_grouped,
                        callback=cb
                    )
                    logging.info('LFQ complete.')
                    
            else:
                logging.info('Exporting protein intensity.')
                protein_table = df.groupby(['protein_group','sample_group'])[field].sum().unstack()
                    
            protein_table.to_hdf(
                settings['experiment']['results_path'],
                'protein_table'
            )
            protein_table.to_csv(base+'_proteins.csv')
            logging.info('Extracting protein_summary')

            protein_summary = pd.DataFrame(index = df['protein_group'].unique())

            for field in ['sequence','precursor']:
                col_ = 'n_'+ field+' '
                m = df.groupby(['protein_group','sample_group'])[field].count().unstack()
                m.columns = [col_ +_ for _ in m.columns]
                protein_summary.loc[m.index, m.columns] = m.values

            # Add intensity
            new_cols = ['intensity ' + _ if not _.endswith('_LFQ') else 'LFQ intensity '+_[:-4] for _ in protein_table.columns ]
            protein_summary.loc[protein_table.index, new_cols] = protein_table.values
        
        if len(protein_summary) > 0:
            ps_out = base+'_protein_summary.csv'
            protein_summary.to_csv(ps_out)
            logging.info(f'Saved protein_summary of length {len(protein_summary):,} saved to {ps_out}')

            #protein summary
            protein_summary.to_hdf(
            settings['experiment']['results_path'],'protein_summary')
        
    else:
        logging.info('No results.hdf present.')
        df = pd.DataFrame()

    if len(df) > 0:
        results_path = settings['experiment']['results_path']

        logging.info('Updating protein_fdr.') #This now has delayed normalization in it
        
        df.to_hdf(
            results_path,
            'protein_fdr'
        )

        logging.info('Exporting as csv.')
        base, ext = os.path.splitext(results_path)
        df.to_csv(base+'_peptides.csv')
        logging.info(f'Saved df of length {len(df):,} saved to {base}')


    else:
        logging.info(f"No Proteins found.")

    return settings

# %% ../nbs/11_interface.ipynb 19
import yaml

def export(
    settings: dict,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None
) -> dict:
    """Export settings.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: the parsed settings.

    """
    if not logger_set:
        set_logger()
    if not settings_parsed:
        settings = check_version_and_hardware(settings)
    base, ext = os.path.splitext(settings['experiment']['results_path'])
    out_path_settings = base+'.yaml'

    with open(out_path_settings, 'w') as file:
        yaml.dump(settings, file)

    logging.info('Settings saved to {}'.format(out_path_settings))
    logging.info('Analysis complete.')
    return settings

# %% ../nbs/11_interface.ipynb 20
from time import time, sleep
from .__version__ import VERSION_NO
import datetime
import alphapept.utils


def run_complete_workflow(
    settings: dict,
    progress: bool = False,
    logger_set: bool = False,
    settings_parsed: bool = False,
    callback: callable = None,
    callback_overall: callable = None,
    callback_task: callable = None,
    logfile: str = None
) -> dict:
    """Run all AlphaPept steps from a settings dict.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        progress (bool): Track progress. Defaults to False.
        logger_set (bool): If False, reset the default logger. Defaults to False.
        settings_parsed (bool): If True, reparse the settings. Defaults to False.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.
        callback_overall (callable): Same as callback, but for the overall progress. Defaults to None.
        callback_task (callable): Same as callback, but for the task progress. Defaults to None.
        logfile (str): The name of a logfile. Defaults to None.

    Returns:
        dict: the parsed settings.
    """
    if not logger_set:
        set_logger()

    if logfile is not None:
        set_logger(log_file_name=logfile)
    if not settings_parsed:
        settings = check_version_and_hardware(settings)

    steps = []
    
    N_FILES = len(settings['experiment']['file_paths'])

    workflow = settings['workflow']
    
    if "continue_runs" in workflow:
        if not workflow["continue_runs"]:
            for _ in settings['experiment']['file_paths']:
                alphapept.utils.delete_file(os.path.splitext(_)[0]+".ms_data.hdf")
    if workflow["create_database"]:
        steps.append(create_database)
    if workflow["import_raw_data"]:
        steps.append(import_raw_data)
    if workflow["find_features"]:
        steps.append(feature_finding)
    if workflow["search_data"]:
        steps.append(search_data)
    if workflow["recalibrate_data"]:
        steps.append(recalibrate_data)
        steps.append(search_data)
    steps.append(score)
    if workflow["align"] and (N_FILES >1):
        steps.append(align)
    if workflow["match"] and (N_FILES >1):
        if align not in steps:
            steps.append(align)
        steps.append(match)
    steps.append(isobaric_labeling)
    steps.append(protein_grouping)
    steps.append(quantification)
    steps.append(export)

    n_steps = len(steps)
    logging.info(f"Workflow has {n_steps} steps")
    
    if progress:
        logging.info('Setting callback to logger.')


        #log progress to be used
        def cb_logger_o(x):
            logging.info(f"__progress_overall {x:.3f}")
        def cb_logger_c(x):
            logging.info(f"__progress_current {x:.3f}")
        def cb_logger_t(x):
            logging.info(f"__current_task {x}")

        callback = cb_logger_c
        callback_overall = cb_logger_o
        callback_task = cb_logger_t


    def progress_wrapper(step, n_steps, current):
        if callback:
            callback(current)
        if callback_overall:
            callback_overall((step/n_steps)+(current/n_steps))

    pept_dict = None
    fasta_dict = None

    first_search = True

    time_dict = {}

    run_start = time()

    summary = {}
    
    
    # Labels
    
    if 'isobaric_label' in settings:
        if settings['isobaric_label']['label'] != 'None':
            from alphapept.constants import label_dict

            label = label_dict[settings['isobaric_label']['label']]

            for _ in ['mods_fixed_terminal', 'mods_variable']:
                for mod in getattr(label, _):
                    if mod not in settings['fasta'][_]:
                        settings['fasta'][_].append(mod)                   
            
    for idx, step in enumerate(steps):
        logging.info(f'==== {step.__name__} ====')
        if callback_task:
            callback_task(step.__name__)

        start = time()

        if callback_overall:
            progress_wrapper(idx, n_steps, 0)
            cb = functools.partial(progress_wrapper, idx, n_steps)
        elif callback:
            cb = callback
        else:
            cb = None

        if step is search_data:
            settings, pept_dict, fasta_dict = step(settings, first_search=first_search, logger_set = True, settings_parsed = True, callback = cb)

        elif (step is score) or (step is protein_grouping):

            settings = step(settings, pept_dict=pept_dict, fasta_dict=fasta_dict, logger_set = True,  settings_parsed = True, callback = cb)

        else:
            if step is export:
                # Get summary information
                summary = get_summary(settings, summary)
                settings['summary'] = summary

            settings = step(settings, logger_set = True,  settings_parsed = True, callback = cb)

        if step is recalibrate_data:
            first_search = False

        if callback_overall:
            progress_wrapper(idx, n_steps, 1)

        end = time()

        if f"{step.__name__} (min)" in time_dict:
            time_dict[f"{step.__name__}_2 (min)"] = (end-start)/60 #minutes
        else:
            time_dict[f"{step.__name__} (min)"] = (end-start)/60 #minutes

        time_dict['total (min)'] = (end-run_start)/60

        summary['timing'] = time_dict
        summary['version'] = VERSION_NO
        summary['time'] = f"{datetime.datetime.now()}"

        processed_files = []

        for _ in settings['experiment']['file_paths']:
            processed_files.append(os.path.split(_)[1])

        summary['processed_files'] = processed_files

    return settings

# %% ../nbs/11_interface.ipynb 21
from time import time, sleep
from .__version__ import VERSION_NO
import datetime
import alphapept.utils


def extract_median_unique(settings: dict, fields: list, summary_type='filename') -> tuple:
    """Extract the medion protein FDR and number of unique proteins.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        fields (list): A list with colum names to calculate summary statistics.
        summary_type (str): A str of column name used for summarizing ('filename' or 'sample_group')

    Returns:
        tuple: Two arrays with the median protein FDR per file/sample_group and the unique number of protein hits

    """
    protein_fdr = pd.read_hdf(settings['experiment']['results_path'], 'protein_fdr')
    cols = [_ for _ in ['protein','protein_group','precursor','sequence_naked','sequence'] if _ in protein_fdr.columns]
    n_unique = protein_fdr.groupby(summary_type)[cols].nunique()
    cols = [_ for _ in fields if _ in protein_fdr.columns]
    median = protein_fdr[[summary_type]+cols].groupby(summary_type).median()
    if(summary_type=='filename'):
        n_unique.index = [os.path.split(_)[1][:-12] for _ in n_unique.index]
        median.index = [os.path.split(_)[1][:-12] for _ in median.index]

    return median, n_unique


def get_file_summary(ms_data: alphapept.io.MS_Data_File, fields: list) -> dict:
    """Get summarize statitics from an MS_Data file.

    Args:
        ms_data (alphapept.io.MS_Data_File): An MS_Data file which has been fully identified and quantified.
        fields (list): A list with colum names to calculate summary statistics.

    Returns:
        dict: A dictionary with summary statistics.

    """
    f_summary = {}
    
    
    try:
        f_summary['acquisition_date_time'] = ms_data.read(group_name = 'Raw', attr_name = 'acquisition_date_time')
    except KeyError:
        f_summary['acquisition_date_time'] = None
    
    try:
        n_ms2 = ms_data.read(group_name='Raw/MS2_scans', dataset_name='prec_mass_list2', return_dataset_shape=True)[0]
    except KeyError:
        n_ms2 = 0

    for key in ms_data.read():

        if "is_pd_dataframe" in ms_data.read(attr_name="", group_name=key):
            df = ms_data.read(dataset_name=key)

            f_summary[f"{key} (n in table)"] = len(df)

            if key in ['identifications']:
                
                m = df[df["q_value"].gt(0.01)]
                
                f_summary['id_rate (0.01)'] = round(float( m['raw_idx'].nunique() / n_ms2),2)

            if key in ['feature_table','peptide_fdr']:
                for field in fields:
                    if field in df.columns:
                        f_summary[f'{field} ({key}, median)'] = float(df[field].median())

    return f_summary


def get_summary(settings: dict, summary: dict) -> dict:
    """Append file summary statistics to a summary dictionary.

    Args:
        settings (dict): A dictionary with settings how to process the data.
        summary (dict): A dictionary with summary statistics of the experiment.

    Returns:
        dict: The summary in which file summary statistcs are appended.

    """

    summary['file_sizes'] = {}
    
    fields = ['fwhm','ms1_int_sum_area','ms1_int_sum_apex','ms1_int_max_area','ms1_int_max_apex','rt_length','rt_tail','prec_offset_raw_ppm', 'prec_offset_ppm','mobility']

    file_sizes = {}
    for _ in settings['experiment']['file_paths']:

        base, ext = os.path.splitext(_)
        filename = os.path.split(base)[1]
        ms_file_name = os.path.splitext(_)[0] + ".ms_data.hdf"

        file_sizes[ms_file_name] = os.path.getsize(ms_file_name)/1024**2

        ms_data = alphapept.io.MS_Data_File(os.path.splitext(_)[0] + ".ms_data.hdf")

        summary[filename] = get_file_summary(ms_data, fields)

    summary['file_sizes']['files'] = file_sizes
    if os.path.isfile(settings['experiment']['results_path']):
        summary['file_sizes']['results'] = os.path.getsize(settings['experiment']['results_path'])/1024**2

        median, n_unique = extract_median_unique(settings, fields)

        for col in median.columns:
            for _ in range(len(median)):
                summary[median.index[_]][f'{col} (protein_fdr, median)'] = float(median.iloc[_][col])

        for col in n_unique.columns:
            for _ in range(len(n_unique)):
                summary[n_unique.index[_]][f'{col} (protein_fdr, n unique)'] = int(n_unique.iloc[_][col])
                
        if(len(list(dict.fromkeys(settings["experiment"]["fraction"])))>1):
            s_median, s_n_unique = extract_median_unique(settings, fields,summary_type='sample_group')
        
            for _ in range(len(s_median)):
                summary[s_median.index[_]]={}
                for col in s_median.columns:
                    summary[s_median.index[_]][f'{col} (protein_fdr, median)'] = float(s_median.iloc[_][col])

            for _ in range(len(s_n_unique)):
                for col in s_n_unique.columns:
                    summary[s_n_unique.index[_]][f'{col} (protein_fdr, n unique)'] = int(s_n_unique.iloc[_][col])

    return summary

import alphapept.utils
from .utils import set_logger

import logging
import sys
import numpy as np
import psutil


def parallel_execute(
    settings: dict,
    step: callable,
    callback: callable = None,
) -> dict:
    """Generic function to execute worklow steps in parallel on a per-file basis.

    Args:
        settings (dict): The settings for processing the step function.
        step (callable): A function that accepts settings as input parameter.
        callback (callable): A function that accepts a float between 0 and 1 as progress. Defaults to None.

    Returns:
        dict: The settings after processing.

    Raises:
        NotImplementedError: When the step is feature finding on files other then Thermo or Bruker.

    """
    n_processes = alphapept.performance.set_worker_count(
        worker_count=settings['general']['n_processes'],
        set_global=False
    )

    if False: #TODO Make the loop below more elegant with a dict 
        MEMORY_LIMITS = {}
        MEMORY_LIMITS['find_features'] = {}
        MEMORY_LIMITS['find_features']['thermo'] = 8
        MEMORY_LIMITS['find_features']['bruker'] = 25
        MEMORY_LIMITS['raw_conversion']['bruker'] = 8
        MEMORY_LIMITS['raw_conversion']['sciex'] = 30
        MEMORY_LIMITS['search_db'] = 8
    

    files = settings['experiment']['file_paths']
    n_files = len(files)
    logging.info(f'Processing {len(files)} files for step {step.__name__}')

    if 'failed' not in settings:
        settings['failed'] = {}

    to_process = [(i, settings) for i in range(n_files)]

    failed = []
    
    if n_files == 1:
        if not step(to_process[0], callback=callback, parallel=True):
            failed.append(files[0])

    elif n_processes == 1:
        logging.info(f'Proccessing one by one...')

        for i in tqdm.tqdm(range(len(to_process))):
                if not step(to_process[i], callback=callback, parallel=True):
                    failed.append(files[i])
                    
    else:
        #Limit number of processes for Bruker FF
        if step.__name__ == 'find_features':
            base, ext = os.path.splitext(files[0])
            if ext.lower() == '.d':
                memory_available = psutil.virtual_memory().available/1024**3
                n_processes_temp = max((int(memory_available //25 ),1))
                n_processes = min((n_processes, n_processes_temp))
                logging.info(f'Using Bruker Feature Finder. Setting Process limit to {n_processes}.')
            elif ext.lower() in ('.raw','.mzml','.wiff'):
                memory_available = psutil.virtual_memory().available/1024**3
                n_processes_temp = max((int(memory_available //8 ), 1))
                n_processes = min((n_processes, n_processes_temp))
                logging.info(f'Setting Process limit to {n_processes}')
            else:
                raise NotImplementedError('Feature Finding: File extension {} not understood.'.format(ext))

        elif step.__name__ == 'search_db':
            memory_available = psutil.virtual_memory().available/1024**3
            n_processes_temp = max((int(memory_available //8 ), 1)) # 8 gb per file: Todo: make this better
            n_processes = min((n_processes, n_processes_temp))
            n_processes = min((n_processes, n_files)) #not more processes than files.
            logging.info(f'Searching. Setting Process limit to {n_processes}.')

        elif step.__name__ == 'raw_conversion': #alphatims raw conversion needs enough RAM
            base, ext = os.path.splitext(files[0])
            if ext.lower() == '.d':
                memory_available = psutil.virtual_memory().available/1024**3
                n_processes_temp = max((int(memory_available //8 ), 1)) # 8 gb per file: Todo: make this better
                n_processes = min((n_processes, n_processes_temp))
                n_processes = min((n_processes, n_files)) #not more processes than files.
                logging.info(f'Importing Raw data with Bruker. Setting Process limit to {n_processes}.')
            elif ext.lower() == '.wiff':
                memory_available = psutil.virtual_memory().available/1024**3
                n_processes_temp = max((int(memory_available //30 ), 1)) # 8 gb per file: Todo: make this better
                n_processes = min((n_processes, n_processes_temp))
                n_processes = min((n_processes, n_files)) #not more processes than files.
                logging.info(f'Importing Raw data with SciEx. Setting Process limit to {n_processes}.')
        else:
            pass


        failed = []
        rerun = []
        rerun_map = {}
        with alphapept.performance.AlphaPool(n_processes) as p:
            for i, success in enumerate(p.imap(step, to_process)):
                progress = (i+1)/n_files
                if success is not True:
                    failed.append((i, files[i]))
                    logging.error(f'Processing of {files[i]} for step {step.__name__} failed. Exception {success}')
                    rerun_map[len(rerun)] = i
                    rerun.append(to_process[i])
                else:
                    logging.error(f'Processing of {files[i]} for step {step.__name__} succeeded. {progress*100:.2f} %')

                if callback:
                    callback(progress)
        
        n_failed = len(failed)
        if n_failed > 0:
            ## Retry failed with more memory
            n_processes_ = max((1, int(n_processes // 2)))
            logging.info(f'Attempting to rerun failed runs with {n_processes_} processes')

            failed = []
            with alphapept.performance.AlphaPool(n_processes_) as p:
                for i, success in enumerate(p.imap(step, rerun)):
                    progress = (i+1)/n_failed
                    if success is not True:
                        failed.append(files[rerun_map[i]])
                        logging.error(f'Processing of {files[i]} for step {step.__name__} failed. Exception {success}')
                    else:
                        logging.error(f'Processing of {files[i]} for step {step.__name__} succeeded. {progress*100:.2f} %')
                    if callback:
                        callback(progress)

    if step.__name__ not in settings['failed']:
        settings['failed'][step.__name__] = failed
    else:
        settings['failed'][step.__name__+'_2'] = failed

    return settings

# %% ../nbs/11_interface.ipynb 23
class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

import click
import os
import alphapept.settings
from .__version__ import VERSION_NO
from .__version__ import COPYRIGHT
from .__version__ import URL
from .utils import check_github_version

def is_port_in_use(port: int) -> bool:
    import socket
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex(('localhost', port)) == 0

CONTEXT_SETTINGS = dict(help_option_names=['-h', '--help'])
CLICK_SETTINGS_OPTION = click.argument(
    "settings_file",
#     help="A .yaml file with settings.",
    type=click.Path(exists=True, dir_okay=False),
#     default=f"{os.path.dirname(__file__)}/settings_template.yaml"
)


def run_cli() -> None:
    """Run the command line interface."""
    
    from packaging import version

    remote_version = check_github_version()

    version_str = '.{}.'.format(VERSION_NO.center(50))

    if remote_version:
        if version.parse(remote_version) > version.parse(VERSION_NO):
            version_str = f'{VERSION_NO} {bcolors.OKGREEN} -> Update available ({remote_version}){bcolors.ENDC}'
            version_str = f".{version_str.center(50+len(bcolors.ENDC)+len(bcolors.OKGREEN))}."
        elif version.parse(remote_version) < version.parse(VERSION_NO):
            version_str = f'{VERSION_NO} {bcolors.OKCYAN} -> Current is newer than remote ({remote_version}){bcolors.ENDC}'
            version_str = f".{version_str.center(50+len(bcolors.ENDC)+len(bcolors.OKCYAN))}."
        else:
            pass

    print(
        "\n".join(
            [
                "\n",
                r"     ___    __      __          ____             __ ",
                r"    /   |  / /___  / /_  ____  / __ \___  ____  / /_",
                r"   / /| | / / __ \/ __ \/ __ \/ /_/ / _ \/ __ \/ __/",
                r"  / ___ |/ / /_/ / / / / /_/ / ____/ ___/ /_/ / /_  ",
                r" /_/  |_/_/ .___/_/ /_/\__,_/_/    \___/ .___/\__/  ",
                r"         /_/                          /_/           ",
                '.'*52,
                '.{}.'.format(URL.center(50)),
                '.{}.'.format(COPYRIGHT.center(50)),
                version_str,
                '.'*52,
                "\n"
            ]
        )
    )

    cli_overview.add_command(cli_database)
    cli_overview.add_command(cli_import)
    cli_overview.add_command(cli_feature_finding)
    cli_overview.add_command(cli_search)
    cli_overview.add_command(cli_recalibrate)
    cli_overview.add_command(cli_score)
    cli_overview.add_command(cli_quantify)
    cli_overview.add_command(cli_export)
    cli_overview.add_command(cli_workflow)
    cli_overview.add_command(cli_gui)
    cli_overview()


@click.group(
    context_settings=CONTEXT_SETTINGS,
#     help="AlphaPept"
)
def cli_overview():
    pass


@click.command(
    "database",
    help="Create a database from a fasta file.",
    short_help="Create a database from a fasta file."
)
@CLICK_SETTINGS_OPTION
def cli_database(settings_file):
    settings = alphapept.settings.load_settings(settings_file)
    create_database(settings)


@click.command(
    "import",
    help="Import and convert raw data from vendor to `.ms_data.hdf` file with default settings.",
    short_help="Import and convert raw data from vendor to `.ms_data.hdf` file with default settings."
)
@CLICK_SETTINGS_OPTION
def cli_import(settings_file):
    from alphapept.paths import DEFAULT_SETTINGS_PATH
    settings = alphapept.settings.load_settings_as_template(DEFAULT_SETTINGS_PATH)
    settings['experiment']['file_paths'] = [settings_file]
    import_raw_data(settings)


@click.command(
    "features",
    help="Find features in a `.ms_data.hdf` file.",
    short_help="Find features in a `.ms_data.hdf` file."
)
@CLICK_SETTINGS_OPTION
def cli_feature_finding(settings_file):
    settings = alphapept.settings.load_settings(settings_file)
    feature_finding(settings)


@click.command(
    "search",
    help="Search and identify feature in a `.ms_data.hdf` file.",
    short_help="Search and identify feature in a `.ms_data.hdf` file."
)
@CLICK_SETTINGS_OPTION
@click.option(
    '--recalibrated_features',
    '-r',
    'recalibrated',
    help="Use recalibrated features if present",
    is_flag=True,
    default=False,
    show_default=True,
)
def cli_search(settings_file, recalibrated):
    settings = alphapept.settings.load_settings(settings_file)
    search_data(settings, recalibrated)


@click.command(
    "recalibrate",
    help="Recalibrate a `.ms_data.hdf` file.",
    short_help="Recalibrate a `.ms_data.hdf` file."
)
@CLICK_SETTINGS_OPTION
def cli_recalibrate(settings_file):
    settings = alphapept.settings.load_settings(settings_file)
    recalibrate_data(settings)


@click.command(
    "score",
    help="Score PSM from a `.ms_data.hdf` file.",
    short_help="Score PSM from a `.ms_data.hdf` file."
)
@CLICK_SETTINGS_OPTION
def cli_score(settings_file):
    settings = alphapept.settings.load_settings(settings_file)
    score(settings)

@click.command(
    "align",
    help="Align multiple `.ms_data.hdf` files.",
    short_help="Align multiple `.ms_data.hdf` files."
)
@CLICK_SETTINGS_OPTION
def cli_align(settings_file):
    settings = alphapept.settings.load_settings(settings_file)
    align(settings)

@click.command(
    "match",
    help="Perform match between run type analysis on multiple `.ms_data.hdf` files.",
    short_help="Perform match between run type analysis on multiple `.ms_data.hdf` files."
)
@CLICK_SETTINGS_OPTION
def cli_match(settings_file):
    settings = alphapept.settings.load_settings(settings_file)
    align(settings)
    match(settings)

@click.command(
    "quantify",
    help="Quantify and compare multiple `.ms_data.hdf` files.",
    short_help="Quantify and compare multiple `.ms_data.hdf` files."
)
@CLICK_SETTINGS_OPTION
def cli_quantify(settings_file):
    settings = alphapept.settings.load_settings(settings_file)
    quantification(settings)


@click.command(
    "export",
    help="Export protein table from `.ms_data.hdf` files as `.csv`",
    short_help="Export protein table from `.ms_data.hdf` files as `.csv`."
)
@CLICK_SETTINGS_OPTION
def cli_export(settings_file):
    settings = alphapept.settings.load_settings(settings_file)
    export(settings)


@click.command(
    "workflow",
    help="Run the complete AlphaPept workflow.",
    short_help="Run the complete AlphaPept workflow."
)
@click.option(
    "--progress",
    "-p",
    help="Log progress output.",
    is_flag=True,
)
@click.option(
    "--headless",
    help="Disable manual termination requirement upon fail/complete.",
    is_flag=True,
)
@CLICK_SETTINGS_OPTION
def cli_workflow(settings_file, progress, headless):
    global HEADLESS
    HEADLESS = headless
    settings = alphapept.settings.load_settings(settings_file)
    run_complete_workflow(settings, progress = progress)


@click.command(
    "gui",
    help="Start graphical user interface for AlphaPept.",
)
@click.option(
    "--port",
    "-p",
    help="Set port for the streamlit server.",
    type=click.IntRange(1, 49151)
)

def cli_gui(port):
    _this_file = os.path.abspath(__file__)
    _this_directory = os.path.dirname(_this_file)
    
    if not port:
        port = 8505
        
    if is_port_in_use(port):
        
        print(f'AlphaPept port {port} is already in use. Please check if AlphaPept is running at http://127.0.0.1:{port}.')
        print('It is not recommended to start multiple AlphaPept instances but instead to queue experiments.')
        print('Attempting to open existing AlphaPept Sesssion...')
        import webbrowser

        webbrowser.open(f'http://127.0.0.1:{port}')
        
    else:

        file_path = os.path.join(_this_directory, 'webui.py')

        print('Starting AlphaPept Background Process')

        from alphapept.gui.utils import start_process
        from alphapept.gui.status import queue_watcher, check_process

        from alphapept.paths import PROCESS_FILE

        start_process(target=queue_watcher, process_file=PROCESS_FILE, verbose=False)

        running, last_pid, p_name, status, queue_watcher_state = check_process(PROCESS_FILE)

        print('Starting AlphaPept Server')

        #if __name__ == '__main__':
        #    sys.argv = ["streamlit", "run", "webui.py"]
        #    sys.exit(stcli.main())

        #args = '--theme.primaryColor #18212b --theme.backgroundColor #FFFFFF --theme.secondaryBackgroundColor #f0f2f6 --theme.textColor #262730 --theme.font "sans serif"'

        #sys.argv = ["streamlit", "run", file_path, args]

        HOME = os.path.expanduser("~")

        ST_PATH = os.path.join(HOME, ".streamlit")

        for folder in [ST_PATH]:
            if not os.path.isdir(folder):
                os.mkdir(folder)

        #Check if streamlit credentials exists
        ST_CREDENTIALS = os.path.join(ST_PATH, 'credentials.toml')
        if not os.path.isfile(ST_CREDENTIALS):
            with open(ST_CREDENTIALS, 'w') as file:
                file.write("[general]\n")
                file.write('\nemail = ""')


        import sys
        from streamlit.web import cli as stcli

        theme = []

        theme.append("--theme.backgroundColor=#FFFFFF")
        theme.append("--theme.secondaryBackgroundColor=#f0f2f6")
        theme.append("--theme.textColor=#262730")
        theme.append("--theme.font=sans serif")
        theme.append("--theme.primaryColor=#18212b")

        args = ["streamlit", "run", file_path, "--global.developmentMode=false", f"--server.port={port}", "--browser.gatherUsageStats=False"]

        args.extend(theme)

        sys.argv = args

        sys.exit(stcli.main())
