# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_feature_finding.ipynb (unless otherwise specified).

__all__ = ['connect_centroids_unidirection', 'convert_connections_to_array', 'eliminate_overarching_vertex',
           'path_finder', 'find_path_start', 'find_path_length', 'fill_path_matrix', 'find_centroid_connections',
           'get_hills', 'connect_centroids', 'extract_hills', 'fast_minima', 'split', 'split_hills',
           'check_large_hills', 'filter_hills', 'hill_stats', 'remove_duplicates', 'get_hill_data',
           'check_isotope_pattern', 'DELTA_M', 'DELTA_S', 'maximum_offset', 'correlate', 'extract_edge',
           'edge_correlation', 'get_pre_isotope_patterns', 'check_isotope_pattern_directed', 'grow', 'grow_trail',
           'get_trails', 'plot_pattern', 'get_minpos', 'get_local_minima', 'is_local_minima', 'truncate',
           'check_averagine', 'pattern_to_mz', 'cosine_averagine', 'int_list_to_array', 'mz_to_mass', 'get_minpos',
           'get_local_minima', 'is_local_minima', 'truncate', 'M_PROTON', 'isolate_isotope_pattern',
           'get_isotope_patterns', 'report_', 'feature_finder_report', 'plot_isotope_pattern', 'extract_bruker',
           'convert_bruker', 'map_bruker', 'find_features', 'replace_infs', 'map_ms2']

# Cell
import numpy as np
import alphapept.performance

@alphapept.performance.performance_function
def connect_centroids_unidirection(x, row_borders, connections, scores, centroids, max_gap, ppm_tol):
    for gap in range(max_gap + 1):
        y = x + gap + 1
        if y >= row_borders.shape[0]:
            return

        start_index_f = 0
        if x > 0:
            start_index_f = row_borders[x - 1]

        centroids_1 = centroids[start_index_f: row_borders[x]]

        start_index_b = row_borders[y - 1]

        centroids_2 = centroids[start_index_b: row_borders[y]]

        i = 0
        j = 0

        while (i < len(centroids_1)) & (j < len(centroids_2)):
            mz1, mz2 = centroids_1[i], centroids_2[j]
            diff = mz1 - mz2
            mz_sum = mz1 + mz2
            delta = 2 * 1e6 * abs(diff) / mz_sum

            if delta < centroid_tol:

                if scores[x, i, gap] > delta:
                    scores[x, i, gap] = delta
                    connections[x, i, gap] = (connections.shape[1] * y) + j

            if diff > 0:
                j += 1
            else:
                i += 1

@alphapept.performance.performance_function
def convert_connections_to_array(x, from_r, from_c, to_r, to_c, row_borders, out_from_idx, out_to_idx):
    row = from_r[x]
    col = from_c[x]
    start_index_f = 0
    if row > 0:
        start_index_f = row_borders[row - 1]
    out_from_idx[x] = start_index_f + col

    row = to_r[x]
    col = to_c[x]
    start_index_f = 0
    if row > 0:
        start_index_f = row_borders[row - 1]
    out_to_idx[x] = start_index_f + col

@alphapept.performance.performance_function
def eliminate_overarching_vertex(x, from_idx, to_idx):
    if x == 0:
        return

    if from_idx[x - 1] == from_idx[x]:
        to_idx[x] = -1

@alphapept.performance.performance_function
def path_finder(x, from_idx, to_idx, forward, backward):
    fr = from_idx[x]
    to =  to_idx[x]

    forward[fr] = to
    backward[to] = fr

@alphapept.performance.performance_function
def find_path_start(x, forward, backward, path_starts):
    if forward[x] > -1 and backward[x] == -1:
        path_starts[x] = 0

@alphapept.performance.performance_function
def find_path_length(x, path_starts, forward, path_cnt):
    ctr = 1
    idx = path_starts[x]
    while forward[idx] > -1:
        ctr += 1
        idx = forward[idx]
    path_cnt[x] = ctr

@alphapept.performance.performance_function
def fill_path_matrix(x, path_start, forwards, out_hill_data, out_hill_ptr):
    path_position = 0
    idx = path_start[x]
    while idx > -1:
        out_hill_data[out_hill_ptr[x] + path_position] = idx
        idx = forwards[idx]
        path_position += 1

# @alphapept.performance.performance_function
# def convert_to_coord_path(x, out_rows, out_cols, connected_comps, d_width):
#     idx = connected_comps[x]
#     row = idx // d_width
#     col = idx - row * d_width
#     out_rows[x] = row
#     out_cols[x] = col

# Cell

def find_centroid_connections(rowwise_peaks, row_borders, centroids, max_gap, ppm_tol):
    if alphapept.performance.COMPILATION_MODE == "cuda":
        import cupy
        cupy = cupy
    else:
        import numpy
        cupy = numpy

    max_centroids = int(cupy.max(rowwise_peaks))
    spectra_cnt = len(row_borders) - 1

    connections = cupy.full((spectra_cnt, max_centroids, max_gap + 1), -1, dtype=np.int32)
    score = cupy.full((spectra_cnt, max_centroids, max_gap + 1), np.inf)

    connect_centroids_unidirection(range(len(row_borders)),
                                    row_borders,
                                   connections,
                                   score,
                                   centroids,
                                   max_gap,
                                   centroid_tol)

    score = score[cupy.where(score < np.inf)]

    score_median = cupy.median(score)
    score_std = cupy.std(score)

    del score, max_centroids, spectra_cnt

    c_shape = connections.shape
    from_r, from_c, from_g = cupy.where(connections >= 0)
    to_r = connections[from_r, from_c, from_g] // c_shape[1]
    to_c = connections[from_r, from_c, from_g] - to_r * c_shape[1]

    del connections, from_g

    return from_r, from_c, to_r, to_c, score_median, score_std

# Cell

def get_hills(centroids, from_idx, to_idx, min_hill_length=3):
    if alphapept.performance.COMPILATION_MODE == "cuda":
        import cupy
        cupy = cupy
    else:
        import numpy
        cupy = numpy

    forward = cupy.full(centroids.shape[0], -1)
    backward = cupy.full(centroids.shape[0], -1)
    path_starts = cupy.full(centroids.shape[0], -1)

    path_finder(range(len(from_idx)), from_idx, to_idx, forward, backward)
    find_path_start(range(len(forward)), forward, backward, path_starts)

    # path_starts will now container the first index of all connected centroids
    path_starts = cupy.where(path_starts == 0)[0]

    path_node_cnt = cupy.full(path_starts.shape[0], -1)
    find_path_length(range(len(path_starts)), path_starts, forward, path_node_cnt)

    relavant_path_node = cupy.where(path_node_cnt >= hill_length_min)[0]
    path_starts = cupy.take(path_starts, relavant_path_node)
    path_node_cnt = cupy.take(path_node_cnt, relavant_path_node)
    del relavant_path_node

    # Generate the hill matix indice ptr data
    hill_ptrs = cupy.empty((path_starts.shape[0] + 1), dtype=cupy.int32)

    hill_ptrs[0] = 0
    hill_ptrs[1:] = path_node_cnt.cumsum()
    hill_data = cupy.empty((int(hill_ptrs[-1])), np.int32)

    fill_path_matrix(range(len(path_starts)), path_starts, forward, hill_data, hill_ptrs)

    del from_idx, to_idx, path_starts, forward, backward
    return hill_ptrs, hill_data, path_node_cnt

# Cell
def connect_centroids(rowwise_peaks, row_borders, centroids, max_gap, ppm_tol):
    if alphapept.performance.COMPILATION_MODE == "cuda":
        import cupy
        cupy = cupy
    else:
        import numpy
        cupy = numpy

    from_r, from_c, to_r, to_c, score_median, score_std = find_centroid_connections(rowwise_peaks,
                                                           row_borders,
                                                           centroids,
                                                           max_gap,
                                                           ppm_tol)

    from_idx = cupy.zeros(len(from_r), np.int32)
    to_idx = cupy.zeros(len(from_r), np.int32)

    convert_connections_to_array(range(len(from_r)),
                                    from_r,
                                 from_c,
                                 to_r,
                                 to_c,
                                 row_borders,
                                 from_idx,
                                 to_idx)

    eliminate_overarching_vertex(range(len(from_idx)), from_idx, to_idx)

    relavent_idx = cupy.where(to_idx >= 0)
    from_idx = cupy.take(from_idx, relavent_idx)[0]
    to_idx = cupy.take(to_idx, relavent_idx)[0]

    del from_r, from_c, to_r, to_c, relavent_idx
    return from_idx, to_idx, score_median, score_std


# Cell
def extract_hills(query_data, max_gap, ppm_tol):

    if alphapept.performance.COMPILATION_MODE == "cuda":
        import cupy
        cupy = cupy
    else:
        import numpy
        cupy = numpy

    indices = cupy.array(query_data['indices_ms1'])
    mass_data = cupy.array(query_data['mass_list_ms1'])

    rowwise_peaks = indices[1:] - indices[:-1]
    row_borders = indices[1:]

    from_idx, to_idx, score_median, score_std = connect_centroids(rowwise_peaks, row_borders, mass_data, max_gap, ppm_tol)


    hill_ptrs, hill_data, path_node_cnt = get_hills(mass_data, from_idx, to_idx)

    del mass_data
    del indices

    if cupy.__name__ != 'numpy':
        hill_ptrs = hill_ptrs.get()
        hill_data = hill_data.get()
        path_node_cnt = path_node_cnt.get()

        score_median = score_median.get()
        score_std = score_std.get()

    return hill_ptrs, hill_data, path_node_cnt, score_median, score_std

# Cell
@alphapept.performance.compile_function(compilation_mode="numba")
def fast_minima(y):
    minima = np.zeros(len(y))

    start = 0
    end = len(y)

    for i in range(start + 2, end - 2):
        if ((y[i - 1] > y[i]) & (y[i + 1] > y[i])) \
            or ((y[i - 1] > y[i]) & (y[i + 1] == y[i]) & (y[i + 2] > y[i])) \
            or ((y[i - 2] > y[i]) & (y[i - 1] == y[i]) & (y[i + 1] > y[i])) \
            or (((y[i - 2] > y[i]) & (y[i - 1] == y[i]) & (y[i + 1] == y[i]) & \
                (y[i + 2] > y[i]))):
            minima[i] = 1

    minima = minima.nonzero()[0]

    return minima

@alphapept.performance.performance_function(compilation_mode="numba-multithread")
def split(k, hill_ptrs, int_data, hill_data, splits, split_level, window):
# def split(idx, hill_ptrs_range, hill_ptrs, int_data, hill_data, splits, split_level, window):

#     k = hill_ptrs_range[idx]

    start = hill_ptrs[k]
    end = hill_ptrs[k + 1]

    int_idx = hill_data[start:end] #index to hill data

    int_trace = int_data[int_idx]

    for i in range(len(int_idx)):
        min_index = max(0, i - window)
        max_index = min(len(int_idx), i + window + 1)
        int_trace[i] = np.median(int_trace[min_index:max_index])

    for i in range(len(int_idx)):
        min_index = max(0, i - window)
        max_index = min(len(int_idx), i + window + 1)
        int_trace[i] = np.mean(int_trace[min_index:max_index])

    #minima = (np.diff(np.sign(np.diff(int_trace))) > 0).nonzero()[0] + 1 #This works also but is slower

    minima = fast_minima(int_trace)

    sorted_minima = np.argsort(int_trace[minima])

    minima = minima[sorted_minima]

    for min_ in minima:

        minval = int_trace[min_]

        left_max = max(int_trace[:min_])
        right_max = max(int_trace[min_:])

        min_max = min(left_max, right_max)

        if (minval == 0) or ((min_max / minval) > hill_split_level):
            splits[k] = start+min_
            break # Split only once per iteration

def split_hills(hill_ptrs, hill_data, int_data, split_level, window):

    splits = np.zeros(len(int_data), dtype=np.int32)
    to_check = np.arange(len(hill_ptrs)-1)

    while len(to_check) > 0:
#         print(to_check)
#         print(len(hill_ptrs), len(to_check), len(splits))
#         print(
#             type(to_check),
#             type(hill_ptrs),
#             type(int_data),
#             type(hill_data),
#             type(splits),
#             type(split_level),
#             type(window)
#         )
#         print(
#             to_check.dtype,
#             hill_ptrs.dtype,
#             int_data.dtype,
#             hill_data.dtype,
#             splits.dtype,
#         )
#         split(range(len(to_check)), to_check, hill_ptrs, int_data, hill_data, splits, split_level, window)
        split(to_check, hill_ptrs, int_data, hill_data, splits, split_level, window)
        splitpoints = splits.nonzero()[0]

        to_check = np.zeros(len(hill_ptrs))
        to_check[splitpoints] = 1

        to_check = np.insert(to_check, splitpoints+1, np.ones(len(splitpoints))).nonzero()[0] #array, index, what
        hill_ptrs = np.insert(hill_ptrs, splitpoints+1, splits[splitpoints]) #array, index, what

        splits = np.zeros(len(hill_ptrs), dtype=cupy.int32)

    return hill_ptrs

# Cell
@alphapept.performance.performance_function(compilation_mode="numba-multithread")
def check_large_hills(idx, large_peaks, hill_ptrs, hill_data, int_data, to_remove, large_peak = 40, hill_peak_factor = 2, window=1):

    k = large_peaks[idx]

    start = hill_ptrs[k]
    end = hill_ptrs[k + 1]

    int_idx = hill_data[start:end] #index to hill data

    int_smooth_ = int_data[int_idx]

    for i in range(len(int_idx)):
        min_index = max(0, i - window)
        max_index = min(len(int_idx), i + window + 1)
        int_smooth_[i] = np.median(int_smooth_[min_index:max_index])

    for i in range(len(int_idx)):
        min_index = max(0, i - window)
        max_index = min(len(int_idx), i + window + 1)
        int_smooth_[i] = np.mean(int_smooth_[min_index:max_index])

    int_ = int_data[int_idx]

    max_ = np.max(int_)

    if (max_ / int_smooth_[0] > hill_peak_factor) & (max_ / int_smooth_[-1] > hill_peak_factor):
        to_remove[idx] = 0


def filter_hills(hill_data, hill_ptrs, int_data, hill_check_large =40, window = 1):

    large_peaks = np.where(np.diff(hill_ptrs)>=hill_check_large)[0]

    to_remove = np.ones(len(large_peaks), dtype=np.int32)
    check_large_hills(range(len(large_peaks)), large_peaks, hill_ptrs, hill_data, int_data, to_remove, window)

    idx_ = np.ones(len(hill_data), dtype = np.int32)
    keep = np.ones(len(hill_ptrs)-1, dtype = np.int32)

    to_remove = to_remove.nonzero()[0]

    for _ in to_remove:
        idx_[hill_ptrs[_]:hill_ptrs[_+1]] = 0
        keep[_] = 0

    hill_lens = np.diff(hill_ptrs)
    keep_ = hill_lens[keep.nonzero()[0]]

    hill_data_ = hill_data[idx_.nonzero()[0]]
    hill_ptrs_ = np.empty((len(keep_) + 1), dtype=np.int32)
    hill_ptrs_[0] = 0
    hill_ptrs_[1:] = keep_.cumsum()

    return hill_data_, hill_ptrs_


# Cell

@alphapept.performance.performance_function(compilation_mode="numba-multithread")
def hill_stats(idx, hill_range, hill_ptrs, hill_data, int_data, mass_data, rt_, rt_idx, stats, hill_nboot_max, hill_nboot):
    np.random.seed(42)

    start = hill_ptrs[idx]
    end = hill_ptrs[idx + 1]

    idx_ = hill_data[start:end]

    int_ = int_data[idx_]
    mz_ = mass_data[idx_]

    int_sum = np.sum(int_)
    int_area = np.trapz(rt_[rt_idx[idx_]], int_) #Area

    rt_min = rt_[rt_idx[idx_]].min()
    rt_max = rt_[rt_idx[idx_]].max()

    if len(idx_) > hill_nboot_max:
        bootsize = hill_nboot_max
    else:
        bootsize = len(idx_)

    averages = np.zeros(hill_nboot)
    average = 0

    for i in range(hill_nboot):
        boot = np.random.choice(len(int_), bootsize, replace=True)
        boot_mz = np.sum((mz_[boot] * int_[boot])) / np.sum(int_[boot])
        averages[i] = boot_mz
        average += boot_mz

    average_mz = average/hill_nboot

    delta = 0
    for i in range(hill_nboot):
        delta += (average_mz - averages[i]) ** 2 #maybe easier?
    delta_m = np.sqrt(delta / (hill_nboot - 1))

    stats[idx,0] = average_mz
    stats[idx,1] = delta_m
    stats[idx,2] = int_sum
    stats[idx,3] = int_area
    stats[idx,4] = rt_min
    stats[idx,5] = rt_max

def remove_duplicates(stats, hill_data, hill_ptrs):
    dups = pd.DataFrame(stats).duplicated() #all duplicated hills

    idx_ = np.ones(len(hill_data), dtype = np.int32) #keep all
    keep = np.ones(len(hill_ptrs)-1, dtype = np.int32)

    for _ in np.arange(len(stats))[dups]: #duplicates will be assigned zeros
        idx_[hill_ptrs[_]:hill_ptrs[_+1]] = 0
        keep[_] = 0

    hill_lens = np.diff(hill_ptrs)
    keep_ = hill_lens[keep.nonzero()[0]]

    hill_data_ = hill_data[idx_.nonzero()[0]]
    hill_ptrs_ = np.empty((len(keep_) + 1), dtype=np.int32)
    hill_ptrs_[0] = 0
    hill_ptrs_[1:] = keep_.cumsum()

    return hill_data_, hill_ptrs_, stats[~dups]

def get_hill_data(query_data, hill_ptrs, hill_data, hill_nboot_max = 300, hill_nboot = 150):

    indices_ = np.array(query_data['indices_ms1'])
    rt_ = np.array(query_data['rt_list_ms1'])
    mass_data = np.array(query_data['mass_list_ms1'])
    scan_idx = np.searchsorted(indices_, np.arange(len(mass_data)), side='right') - 1
    int_data = np.array(query_data['int_list_ms1'])

    stats = np.zeros((len(hill_ptrs)-1, 6)) #mz, delta, rt_min, rt_max, sum_max
    hill_stats(range(len(hill_ptrs)-1), np.arange(len(hill_ptrs)-1), hill_ptrs, hill_data, int_data, mass_data, rt_, scan_idx, stats, hill_nboot_max, hill_nboot)

    # sort the stats
    sortindex = np.argsort(stats[:,4]) #Sorted by rt_min
    stats = stats[sortindex,:]
    idxs_upper = stats[:,4].searchsorted(stats[:,5], side="right")
    sortindex_ = np.arange(len(sortindex))[sortindex]

    return stats, sortindex_, idxs_upper, scan_idx, hill_data, hill_ptrs

# Cell
from .constants import mass_dict

DELTA_M = mass_dict['delta_M']
DELTA_S = mass_dict['delta_S']
maximum_offset = DELTA_M + DELTA_S

@alphapept.performance.compile_function(compilation_mode="numba")
def check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge, mass_range = 5):
    """
    Check if two masses could belong to the same isotope pattern
    """
    delta_mass1 = delta_mass1 * iso_mass_range
    delta_mass2 = delta_mass2 * iso_mass_range

    delta_mass = np.abs(mass1 - mass2)

    left_side = np.abs(delta_mass - DELTA_M / charge)
    right_side = np.sqrt((DELTA_S / charge) ** 2 + delta_mass1 ** 2 + delta_mass2 ** 2)

    return left_side <= right_side

# Cell

@alphapept.performance.compile_function(compilation_mode="numba")
def correlate(scans_, scans_2, int_, int_2):

    min_one, max_one = scans_[0], scans_[-1]
    min_two, max_two = scans_2[0], scans_2[-1]

    if min_one + 3 > max_two:  # at least an overlap of 3 elements
        corr = 0
    elif min_two + 3 > max_one:
        corr = 0
    else:
        min_s = min(min_one, min_two)
        max_s = max(max_one, max_two)

        int_one_scaled = np.zeros(int(max_s - min_s + 1))
        int_two_scaled = np.zeros(int(max_s - min_s + 1))

        int_one_scaled[scans_ - min_s] = int_
        int_two_scaled[scans_2 - min_s] = int_2

        corr = np.sum(int_one_scaled * int_two_scaled) / np.sqrt(
            np.sum(int_one_scaled ** 2) * np.sum(int_two_scaled ** 2)
        )

    return corr

# Cell
@alphapept.performance.compile_function(compilation_mode="numba")
def extract_edge(stats, idxs_upper, runner, max_index, maximum_offset,  min_charge = 1, max_charge = 6, mass_range=5):
    edges = []

    mass1 = stats[runner, 0]
    delta_mass1 = stats[runner, 1]

    for j in range(runner+1, idxs_upper[runner]):
        mass2 = stats[j, 0]
        if np.abs(mass2 - mass1) <= maximum_offset:
            delta_mass2 = stats[j, 1]
            for charge in range(iso_charge_min, iso_charge_max + 1):
                if check_isotope_pattern(mass1, mass2, delta_mass1, delta_mass2, charge, iso_mass_range):
                    edges.append((runner, j))
                    break

    return edges

@alphapept.performance.performance_function(compilation_mode="numba-multithread")
def edge_correlation(idx, to_keep, sortindex_, pre_edges, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff):

    edge = pre_edges[idx,:]

    y = sortindex_[edge[0]]
    start = hill_ptrs[y]
    end = hill_ptrs[y + 1]
    idx_ = hill_data[start:end]
    int_ = int_data[idx_]
    scans_ = scan_idx[idx_]

    con = sortindex_[edge[1]]
    start = hill_ptrs[con]
    end = hill_ptrs[con + 1]
    idx_2 = hill_data[start:end]
    int_2 = int_data[idx_2]
    scans_2 = scan_idx[idx_2]

    if correlate(scans_, scans_2, int_, int_2) > cc_cutoff:
        to_keep[idx] = 1

# Cell
import networkx as nx

def get_pre_isotope_patterns(stats, idxs_upper, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, maximum_offset, iso_charge_min=1, iso_charge_max=6, iso_mass_range=5, cc_cutoff=0.6):
    pre_edges = []

    # Step 1
    for runner in range(len(stats)):
        pre_edges.extend(extract_edge(stats, idxs_upper, runner, idxs_upper[runner], maximum_offset, iso_charge_min, iso_charge_max, iso_mass_range))

    to_keep = np.zeros(len(pre_edges), dtype='int')
    pre_edges = np.array(pre_edges)
    edge_correlation(range(len(to_keep)), to_keep, sortindex_, pre_edges, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)
    edges = pre_edges[to_keep.nonzero()]

    G2 = nx.Graph()
    for i in range(len(edges)):
        G2.add_edge(edges[i][0], edges[i][1])

    pre_isotope_patterns = [
        sorted(list(c))
        for c in sorted(nx.connected_components(G2), key=len, reverse=True)
    ]

    return pre_isotope_patterns


# Cell

@alphapept.performance.compile_function(compilation_mode="numba")
def check_isotope_pattern_directed(mass1, mass2, delta_mass1, delta_mass2, charge, index, mass_range):
    """
    Check if two masses could belong to the same isotope pattern

    """
    delta_mass1 = delta_mass1 * iso_mass_range
    delta_mass2 = delta_mass2 * iso_mass_range

    left_side = np.abs(mass1 - mass2 - index * DELTA_M / charge)
    right_side = np.sqrt((DELTA_S / charge) ** 2 + delta_mass1 ** 2 + delta_mass2 ** 2)

    return left_side <= right_side


@alphapept.performance.compile_function(compilation_mode="numba")
def grow(trail, seed, direction, relative_pos, index, stats, pattern, charge, mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff):
    """
    Grows isotope pattern based on a seed and direction

    """
    x = pattern[seed]  # This is the seed
    mass1 = stats[x,0]
    delta_mass1 = stats[x,1]

    k = sortindex_[x]
    start = hill_ptrs[k]
    end = hill_ptrs[k + 1]
    idx_ = hill_data[start:end]
    int_ = int_data[idx_]
    scans_ = scan_idx[idx_]

    growing = True

    while growing:
        if direction == 1:
            if seed + relative_pos == len(pattern):
                growing = False
                break
        else:
            if seed + relative_pos < 0:
                growing = False
                break

        y = pattern[seed + relative_pos]  # This is a reference peak

        l = sortindex_[y]

        mass2 = stats[y,0]
        delta_mass2 = stats[y,1]

        start = hill_ptrs[l]
        end = hill_ptrs[l + 1]
        idx_ = hill_data[start:end]
        int_2 = int_data[idx_]
        scans_2 = scan_idx[idx_]

        if correlate(scans_, scans_2, int_, int_2) > cc_cutoff:
            if check_isotope_pattern_directed(mass1, mass2, delta_mass1, delta_mass2, charge, -direction * index, iso_mass_range):
                if direction == 1:
                    trail.append(y)
                else:
                    trail.insert(0, y)
                index += (
                    1
                )  # Greedy matching: Only one edge for a specific distance, will not affect the following matches

        delta_mass = np.abs(mass1 - mass2)

        if (delta_mass > (DELTA_M+DELTA_S) * index):  # the pattern is sorted so there is a maximum to look back
            break

        relative_pos += direction

    return trail

@alphapept.performance.compile_function(compilation_mode="numba")
def grow_trail(seed, pattern, stats, charge, mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff):
    """
    Wrapper to grow an isotope pattern to the left and right side
    """
    x = pattern[seed]
    trail = List()
    trail.append(x)
    trail = grow(trail, seed, -1, -1, 1, stats, pattern, charge, iso_mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)
    trail = grow(trail, seed, 1, 1, 1, stats, pattern, charge, iso_mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)

    return trail


@alphapept.performance.compile_function(compilation_mode="numba")
def get_trails(seed, pattern, stats, charge_range, mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff):
    """
    Wrapper to extract trails for a given charge range
    """
    trails = []
    for charge in charge_range:
        trail = grow_trail(seed, pattern, stats, charge, iso_mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)

        trails.append(trail)

    return trails

# Cell

def plot_pattern(pattern, sorted_hills, centroids, hill_data):

    """
    Helper function to plot a pattern
    """
    f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,10))
    centroid_dtype = [("mz", float), ("int", float), ("scan_no", int), ("rt", float)]

    mzs = []
    rts = []
    ints = []
    for entry in pattern:
        hill = sorted_hills[entry]
        hill_data = np.array([centroids[_[0]][_[1]] for _ in hill], dtype=centroid_dtype)

        int_profile = hill_data["int"]
        ax1.plot(hill_data["rt"], hill_data["int"])
        ax2.scatter(hill_data["rt"], hill_data["mz"], s = hill_data["int"]/5e5 )


    ax1.set_title('Pattern')
    ax1.set_xlabel('RT (min)')
    ax1.set_ylabel('Intensity')

    ax2.set_xlabel('RT (min)')
    ax2.set_ylabel('m/z')

    plt.show()

# Cell

@alphapept.performance.compile_function(compilation_mode="numba")
def get_minpos(y, split=5):
    """
    Function to get a list of minima in a trace.
    A minimum is returned if the ratio of lower of the surrounding maxima to the minimum is larger than the splitting factor.
    """
    minima = get_local_minima(y)
    minima_list = List()

    for minpos in minima:

        minval = y[minpos]

        left_max = (y[:minpos]).max()
        right_max = (y[minpos:]).max()

        minimum_max = min(left_max, right_max)

        if minimum_max / minval >= iso_split_level:
            minima_list.append(minpos)

    return minima_list

@alphapept.performance.compile_function(compilation_mode="numba")
def get_local_minima(y):
    """
    Function to return all local minima of a array
    """
    minima = List()
    for i in range(1, len(y) - 1):
        if is_local_minima(y, i):
            minima.append(i)
    return minima


@alphapept.performance.compile_function(compilation_mode="numba")
def is_local_minima(y, i):
    return (y[i - 1] > y[i]) & (y[i + 1] > y[i])


@alphapept.performance.compile_function(compilation_mode="numba")
def truncate(array, intensity_profile, seedpos):
    """
    Function to truncate an intensity profile around its seedposition
    """
    minima = int_list_to_array(get_minpos(intensity_profile, iso_split_level))

    if len(minima) > 0:
        left_minima = minima[minima < seedpos]
        right_minima = minima[minima > seedpos]

        # If the minimum is smaller than the seed
        if len(left_minima) > 0:
            minpos = left_minima[-1]
        else:
            minpos = 0

        if len(right_minima) > 0:
            maxpos = right_minima[0]
        else:
            maxpos = len(array)

        array = array[minpos:maxpos+1]

    return array, intensity_profile

# Cell
from .chem import mass_to_dist
from .constants import averagine_aa, isotopes

@alphapept.performance.compile_function(compilation_mode="numba")
def check_averagine(stats, pattern, charge, averagine_aa, isotopes):

    masses, intensity = pattern_to_mz(stats, pattern, charge)

    spec_one = np.floor(masses).astype(np.int64)
    int_one = intensity

    spec_two, int_two = mass_to_dist(np.min(masses), averagine_aa, isotopes) # maybe change to no rounded version

    spec_two = np.floor(spec_two).astype(np.int64)

    return cosine_averagine(int_one, int_two, spec_one, spec_two)

@alphapept.performance.compile_function(compilation_mode="numba")
def pattern_to_mz(stats, pattern, charge):
    """
    Function to calculate masses and intensities from pattern for a given charge
    """
    mzs = np.zeros(len(pattern))
    ints = np.zeros(len(pattern))

    for i in range(len(pattern)):
        entry = pattern[i]
        mzs[i] = mz_to_mass(stats[entry,0], charge)
        ints[i] = stats[entry,2]

    sortindex = np.argsort(mzs)

    masses = mzs[sortindex]
    intensity = ints[sortindex]

    return masses, intensity

@alphapept.performance.compile_function(compilation_mode="numba")
def cosine_averagine(int_one, int_two, spec_one, spec_two):

    min_one, max_one = spec_one[0], spec_one[-1]
    min_two, max_two = spec_two[0], spec_two[-1]

    min_s = np.min(np.array([min_one, min_two]))
    max_s = np.max(np.array([max_one, max_two]))

    int_one_scaled = np.zeros(int(max_s - min_s + 1))
    int_two_scaled = np.zeros(int(max_s - min_s + 1))

    int_one_scaled[spec_one - min_s] = int_one
    int_two_scaled[spec_two - min_s] = int_two

    corr = np.sum(int_one_scaled * int_two_scaled) / np.sqrt(
        np.sum(int_one_scaled ** 2) * np.sum(int_two_scaled ** 2)
    )

    return corr



@alphapept.performance.compile_function(compilation_mode="numba")
def int_list_to_array(numba_list):
    """
    Numba compatbilte function to convert a numba list with integers to a numpy array
    """
    array = np.zeros(len(numba_list), dtype=np.int64)

    for i in range(len(array)):

        array[i] = numba_list[i]

    return array

M_PROTON = mass_dict['Proton']

@alphapept.performance.compile_function(compilation_mode="numba")
def mz_to_mass(mz, charge):
    """
    Function to calculate the mass from a mz value.
    """
    if charge < 0:
        raise NotImplementedError("Negative Charges not implemented.")

    mass = mz * charge - charge * M_PROTON

    return mass


@alphapept.performance.compile_function(compilation_mode="numba")
def get_minpos(y, split=5):
    """
    Function to get a list of minima in a trace.
    A minimum is returned if the ratio of lower of the surrounding maxima to the minimum is larger than the splitting factor.
    """
    minima = get_local_minima(y)
    minima_list = List()

    for minpos in minima:

        minval = y[minpos]
        left_side = y[:minpos]
        right_side = y[minpos:]

        left_max = np.max(left_side)
        right_max = np.max(right_side)

        minimum_max = np.min(np.array((left_max, right_max)))

        if minimum_max / minval > split:
            minima_list.append(minpos)

    return minima_list

@alphapept.performance.compile_function(compilation_mode="numba")
def get_local_minima(y):
    """
    Function to return all local minima of a array
    """
    minima = List()
    for i in range(1, len(y) - 1):
        if is_local_minima(y, i):
            minima.append(i)
    return minima



@alphapept.performance.compile_function(compilation_mode="numba")
def is_local_minima(y, i):
    return (y[i - 1] > y[i]) & (y[i + 1] > y[i])



@alphapept.performance.compile_function(compilation_mode="numba")
def truncate(array, intensity_profile, seedpos):
    """
    Function to truncate an intensity profile around its seedposition
    """
    minima = int_list_to_array(get_minpos(intensity_profile))

    if len(minima) > 0:
        left_minima = minima[minima < seedpos]
        right_minima = minima[minima > seedpos]

        # If the minimum is smaller than the seed
        if len(left_minima) > 0:
            minpos = left_minima[-1]
        else:
            minpos = 0

        if len(right_minima) > 0:
            maxpos = right_minima[0]
        else:
            maxpos = len(array)

        array = array[minpos:maxpos+1]
        intensity_profile = intensity_profile[minpos:maxpos+1]

    return array, intensity_profile

# Cell
@alphapept.performance.compile_function(compilation_mode="numba")
def isolate_isotope_pattern(pre_pattern, hill_ptrs, hill_data, int_data, scan_idx, stats, sortindex_, mass_range, charge_range, averagine_aa, isotopes, seed_masses, cc_cutoff):
    """
    Isolate isotope patterns
    """

    longest_trace = 0
    champion_trace = None
    champion_charge = 0

    # Sort patterns by mass

    sortindex = np.argsort(stats[pre_pattern][:,0]) #intensity
    sorted_pattern = pre_pattern[sortindex]
    massindex = np.argsort(stats[sorted_pattern][:,2])[::-1][:iso_n_seeds]

    # Use all the elements in the pre_pattern as seed

    for seed in massindex:  # Loop through all seeds
        seed_global = sorted_pattern[seed]

        trails = get_trails(seed, sorted_pattern, stats, charge_range, iso_mass_range, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, cc_cutoff)

        for index, trail in enumerate(trails):
            if len(trail) > longest_trace:  # Needs to be longer than the current champion

                arr = int_list_to_array(trail)
                intensity_profile = stats[arr][:,2]
                seedpos = np.nonzero(arr==seed_global)[0][0]

                # truncate around the seed...
                arr, intensity_profile = truncate(arr, intensity_profile, seedpos, iso_split_level)

                # Remove lower masses:
                # Take the index of the maximum and remove all masses on the left side
                if charge_range[index] * stats[seed_global, 0] < 1000:
                    maxpos = np.argmax(intensity_profile)
                    arr = arr[maxpos:]

                if len(arr) > longest_trace:
                    # Averagine check
                    cc = check_averagine(stats, arr, charge_range[index], averagine_aa, isotopes)
                    if cc > 0.6:
                        # Update the champion
                        champion_trace = arr
                        champion_charge = charge_range[index]
                        longest_trace = len(arr)

    return champion_trace, champion_charge

# Cell

from numba.typed import List

def get_isotope_patterns(pre_isotope_patterns, hill_ptrs, hill_data, int_data, scan_idx, stats, sortindex_,  averagine_aa, isotopes, iso_charge_min = 1, iso_charge_max = 6, iso_mass_range = 5, iso_n_seeds = 100, cc_cutoff=0.6, iso_split_level = 1.3, callback=None):
    """
    Wrapper function to iterate over pre_isotope_patterns
    """

    isotope_patterns = []
    isotope_charges = []

    charge_range = List()

    for i in range(iso_charge_min, iso_charge_max + 1):
        charge_range.append(i)

    isotope_patterns = []
    isotope_charges = []

    for idx, pre_pattern in enumerate(pre_isotope_patterns):
        extract = True
        while extract:
            isotope_pattern, isotope_charge = isolate_isotope_pattern(np.array(pre_pattern), hill_ptrs, hill_data, int_data, scan_idx, stats, sortindex_, iso_mass_range, charge_range, averagine_aa, isotopes, iso_n_seeds, cc_cutoff, iso_split_level)
            if isotope_pattern is None:
                length = 0
            else:
                length = len(isotope_pattern)

            if length > 1:
                isotope_charges.append(isotope_charge)
                isotope_patterns.append(isotope_pattern)

                pre_pattern = [_ for _ in pre_pattern if _ not in isotope_pattern]

                if len(pre_pattern) <= 1:
                    extract = False
            else:
                extract = False


        if callback:
            callback((idx+1)/len(pre_isotope_patterns))


    iso_patterns = np.zeros(sum([len(_) for _ in isotope_patterns]), dtype=np.int64)

    iso_idx = np.zeros(len(isotope_patterns)+1, dtype='int')


    start = 0
    for idx, _ in enumerate(isotope_patterns):
        iso_patterns[start:start+len(_)] = _
        start += len(_)
        iso_idx[idx+1] = start



    return iso_patterns, iso_idx, np.array(isotope_charges)

# Cell
@alphapept.performance.performance_function(compilation_mode="numba-multithread")
def report_(idx, isotope_charges, isotope_patterns, iso_idx, stats, sortindex_, hill_ptrs, hill_data, int_data, rt_, rt_idx, results):

    pattern = isotope_patterns[iso_idx[idx]:iso_idx[idx+1]]
    isotope_data = stats[pattern]

    mz = np.min(isotope_data[:, 0])
    mz_std = np.mean(isotope_data[:, 1])
    charge = isotope_charges[idx]
    mass = mz_to_mass(mz, charge)
    int_max_idx = np.argmax(isotope_data[:, 2])
    mz_most_abundant = isotope_data[:, 0][int_max_idx]

    int_max = isotope_data[:,2][int_max_idx]

    rt_start = isotope_data[int_max_idx, 4] # This is the start of the most abundant trace
    rt_end = isotope_data[int_max_idx, 5]

    int_sum = np.sum(isotope_data[:, 2])

    # better measurement of the peak with interpolation

    rt_min_ = min(isotope_data[:, 4])
    rt_max_ = max(isotope_data[:, 5])

    rt_range = np.linspace(rt_min_, rt_max_, 100)
    trace_sum = np.zeros_like(rt_range)

    for k in pattern:
        x = sortindex_[k]

        start = hill_ptrs[x]
        end = hill_ptrs[x + 1]
        idx_ = hill_data[start:end]
        int_ = int_data[idx_]
        rts = rt_[rt_idx[idx_]]

        interpolation = np.interp(rt_range, rts, int_)

        #Filter

        interpolation[:(rt_range < rts[0]).sum()] = 0

        right_cut = (rt_range > rts[-1]).sum()
        if right_cut > 0:
            interpolation[-right_cut:]= 0

        trace_sum += interpolation

    rt_apex_idx = trace_sum.argmax()
    rt_apex = rt_range[rt_apex_idx]

    trace = trace_sum
    half_max = trace.max()/2

    if rt_apex_idx == 0:
        left_apex = 0
    else:
        left_apex = np.abs(trace[:rt_apex_idx]-half_max).argmin()
    right_apex = np.abs(trace[rt_apex_idx:]-half_max).argmin()+rt_apex_idx

    int_apex = trace_sum[rt_apex_idx]
    fwhm = rt_range[right_apex] - rt_range[left_apex]

    n_isotopes = len(pattern)

    rt_cutoff = 0.95 #5%
    if rt_apex_idx == 0:
        rt_min_idx = 0
    else:
        rt_min_idx = np.abs(trace[:rt_apex_idx]-trace.max()*(1-rt_cutoff)).argmin()
    rt_max_idx = np.abs(trace[rt_apex_idx:]-trace.max()*(1-rt_cutoff)).argmin()+rt_apex_idx

    #plt.xlabel('rt')
    #plt.ylabel('int')
    #plt.show()
    #plt.plot(rt_range, trace_sum)

    #plt.plot([rt_range[left_apex], rt_range[right_apex]], [(trace[left_apex] + trace[right_apex])/2]*2, 'k:')

    #plt.plot(rt_range[rt_apex_idx], trace[rt_apex_idx], 'k*')
    #plt.plot(rt_range[rt_min_idx], trace[rt_min_idx], 'k*')
    #plt.plot(rt_range[rt_max_idx], trace[rt_max_idx], 'k*')

    #plt.show()

    rt_start = rt_range[rt_min_idx]
    rt_end = rt_range[rt_max_idx]

    int_area = np.trapz(trace_sum[rt_min_idx:rt_max_idx], rt_range[rt_min_idx:rt_max_idx])
    int_sum = trace_sum.sum()

    results[idx,:] = np.array([mz, mz_std, mz_most_abundant, charge, rt_start, rt_apex, rt_end, fwhm, n_isotopes, mass, int_apex, int_area, int_sum])

# Cell
import pandas as pd

def feature_finder_report(query_data, isotope_patterns, isotope_charges, iso_idx, stats, sortindex_, hill_ptrs, hill_data,):
    rt_ = np.array(query_data['rt_list_ms1'])
    indices_ = np.array(query_data['indices_ms1'])
    mass_data = np.array(query_data['mass_list_ms1'])
    rt_idx = np.searchsorted(indices_, np.arange(len(mass_data)), side='right') - 1

    int_data = np.array(query_data['int_list_ms1'])

    results = np.zeros((len(isotope_charges), 13))

    report_(range(len(isotope_charges)), isotope_charges, isotope_patterns, iso_idx, stats, sortindex_, hill_ptrs, hill_data, int_data, rt_, rt_idx, results)

    df = pd.DataFrame(results, columns = ['mz','mz_std','mz_most_abundant','charge','rt_start','rt_apex','rt_end','fwhm','n_isotopes','mass','int_apex','int_area', 'int_sum'])

    #Test
    #df_ = df.copy()
    #df_['mz'] -= 1.00286864/df_['charge']
    #df_['mass'] = df_['mz'] * df_['charge'] - df_['charge'] * 1.00727646687
    #df = pd.concat([df,df_])

    df.sort_values(['rt_start','mz'])

    return df


# Cell
def plot_isotope_pattern(index, df, sorted_stats, centroids, scan_range=100, mz_range=2, plot_hills = False):
    """
    Plot an isotope pattern in its local environment
    """

    markersize = 10
    plot_offset_mz = 1
    plot_offset_rt = 2

    feature =  df.loc[index]

    scan = rt_dict[feature['rt_apex']]

    start_scan = scan-scan_range
    end_scan = scan+scan_range

    mz_min = feature['mz']-mz_range-plot_offset_mz
    mz_max = feature['mz']+mz_range+plot_offset_mz

    sub_data = np.hstack(centroids[start_scan:end_scan])

    selection = sub_data[(sub_data['mz']>mz_min) & (sub_data['mz']<mz_max)]

    min_rt = selection['rt'].min() - plot_offset_rt
    max_rt = selection['rt'].max() + plot_offset_rt

    hill_selection = sorted_stats[(sorted_stats['mz_avg']>mz_min) & (sorted_stats['mz_avg']<mz_max) & (sorted_stats['rt_max']<max_rt) & (sorted_stats['rt_min']>min_rt)]

    plt.style.use('dark_background')

    plt.figure(figsize=(15,15))
    plt.scatter(selection['rt'], selection['mz'], c= np.log(selection['int']), marker='s', s=markersize, alpha=0.9)
    plt.colorbar()
    plt.grid(False)
    plt.xlabel('RT (min)')
    plt.ylabel('m/z')

    box_height = mz_range/50

    if plot_hills:
        for hill in hill_selection:
            bbox = [hill['rt_min'], hill['mz_avg']-box_height, hill['rt_max'], hill['mz_avg']+box_height]

            rect = plt.Rectangle((bbox[0], bbox[1]),
                                      bbox[2] - bbox[0],
                                      bbox[3] - bbox[1], fill=False,
                                      edgecolor='w', linewidth=1, alpha = 0.3)
            plt.gca().add_patch(rect)


    feature_selection = df[(df['mz']>mz_min) & (df['mz']<mz_max) & (df['rt_end']<max_rt) & (df['rt_start']>min_rt)]

    for f_idx in feature_selection.index:
        for c_idx in range(len(sorted_stats[isotope_patterns[f_idx]])-1):

            start = sorted_stats[isotope_patterns[f_idx]][c_idx]
            end = sorted_stats[isotope_patterns[f_idx]][c_idx+1]

            start_mass = start['mz_avg']
            start_rt = (start['rt_min']+start['rt_max'])/2

            end_mass = end['mz_avg']
            end_rt = (end['rt_min']+end['rt_max'])/2

            plt.plot([start_rt, end_rt], [start_mass, end_mass], '+', color='y')
            plt.plot([start_rt, end_rt], [start_mass, end_mass], ':', color='y')

        if plot_hills:
            for hill_idx in isotope_patterns[f_idx]:

                hill = sorted_stats[hill_idx]
                bbox = [hill['rt_min'], hill['mz_avg']-box_height, hill['rt_max'], hill['mz_avg']+box_height]

                rect = plt.Rectangle((bbox[0], bbox[1]),
                                          bbox[2] - bbox[0],
                                          bbox[3] - bbox[1], fill=False,
                                          edgecolor='g', linewidth=1, alpha = 0.8)
                plt.gca().add_patch(rect)


    plt.xlim([min_rt+plot_offset_rt, max_rt-plot_offset_rt])
    plt.ylim([mz_min+plot_offset_mz, mz_max-plot_offset_mz])
    plt.title('Pattern')
    plt.show()

    plt.style.use('ggplot')

# Cell
import subprocess
import os
import platform


def extract_bruker(file, base_dir = "ext/bruker/FF", config = "proteomics_4d.config"):
    """
    Call Bruker Feautre Finder via subprocess
    """

    feature_path = file + '/'+ os.path.split(file)[-1] + '.features'

    base_dir = os.path.join(os.path.dirname(__file__), base_dir)

    operating_system = platform.system()

    if operating_system == 'Linux':
        ff_dir = os.path.join(base_dir, 'linux64','uff-cmdline2')
        logging.info('Using Linux FF')
    elif operating_system == 'Windows':
        ff_dir = os.path.join(base_dir, 'win64','uff-cmdline2.exe')
        logging.info('Using Windows FF')
    else:
        raise NotImplementedError(f"System {operating_system} not supported.")

    if os.path.exists(feature_path):
        return feature_path
    else:
        if not os.path.isfile(ff_dir):
            raise FileNotFoundError(f'Bruker feature finder cmd not found here {ff_dir}.')

        config_path = base_dir + '/'+ config

        if not os.path.isfile(config_path):
            raise FileNotFoundError(f'Config file not found here {config_path}.')

        if operating_system == 'Windows':
            FF_parameters = [ff_dir,'--ff 4d',f'--readconfig "{config_path}"', f'--analysisDirectory "{file}"']

            process = subprocess.Popen(' '.join(FF_parameters), stdout=subprocess.PIPE)
            for line in iter(process.stdout.readline, b''):
                logtxt = line.decode('utf8')
                logging.info(logtxt[48:].rstrip()) #Remove logging info from FF
        elif operating_system == 'Linux':
            FF_parameters = [
                ff_dir,
                '--ff',
                '4d',
                '--readconfig',
                config_path,
                '--analysisDirectory',
                file
            ]
            process = subprocess.run(FF_parameters, stdout=subprocess.PIPE)

        if os.path.exists(feature_path):
            return feature_path
        else:
            raise FileNotFoundError(f"Feature file {feature_path} does not exist.")


import sqlalchemy as db

def convert_bruker(feature_path):
    """
    Reads feature table and converts to feature table to be used with AlphaPept

    """
    engine_featurefile = db.create_engine('sqlite:///{}'.format(feature_path))
    feature_table = pd.read_sql_table('LcTimsMsFeature', engine_featurefile)

    from .constants import mass_dict

    M_PROTON = mass_dict['Proton']
    feature_table['Mass'] = feature_table['MZ'].values * feature_table['Charge'].values - feature_table['Charge'].values*M_PROTON
    feature_table = feature_table.rename(columns={"MZ": "mz","Mass": "mass", "RT": "rt_apex", "RT_lower":"rt_start", "RT_upper":"rt_end", "Mobility": "mobility", "Mobility_lower": "mobility_lower", "Mobility_upper": "mobility_upper", "Charge":"charge","Intensity":'int_sum',"ClusterCount":'n_isotopes'})
    feature_table['rt_apex'] = feature_table['rt_apex']/60
    feature_table['rt_start'] = feature_table['rt_start']/60
    feature_table['rt_end'] = feature_table['rt_end']/60

    return feature_table


def map_bruker(feature_path, feature_table, query_data):
    """
    Map Ms1 to Ms2 via Table FeaturePrecursorMapping from Bruker FF
    """
    engine_featurefile = db.create_engine('sqlite:///{}'.format(feature_path))

    mapping = pd.read_sql_table('FeaturePrecursorMapping', engine_featurefile)
    mapping = mapping.set_index('PrecursorId')
    feature_table= feature_table.set_index('Id')


    query_prec_id = query_data['prec_id']

    #Now look up the feature for each precursor

    mass_matched = []
    mz_matched = []
    rt_matched = []
    query_idx = []
    f_idx = []

    for idx, prec_id in tqdm(enumerate(query_prec_id)):
        try:
            f_id = mapping.loc[prec_id]['FeatureId']
            all_matches = feature_table.loc[f_id]
            if type(f_id) == np.int64:
                match = all_matches
                mz_matched.append(match['mz'])
                rt_matched.append(match['rt_apex'])
                mass_matched.append(match['mass'])
                query_idx.append(idx)
                f_idx.append(match['FeatureId'])

            else:
                for k in range(len(all_matches)):
                    match = all_matches.iloc[k]
                    mz_matched.append(match['mz'])
                    rt_matched.append(match['rt_apex'])
                    mass_matched.append(match['mass'])
                    query_idx.append(idx)
                    f_idx.append(match['FeatureId'])

        except KeyError:
            pass

    features = pd.DataFrame(np.array([mass_matched, mz_matched, rt_matched, query_idx, f_idx]).T, columns = ['mass_matched', 'mz_matched', 'rt_matched', 'query_idx', 'feature_idx'])

    features['query_idx'] = features['query_idx'].astype('int')

    return features

# Cell
import numpy as np

import logging
import os
from .search import query_data_to_features
import alphapept.io
import functools


def find_features(to_process, callback = None, parallel = False):
    """
    Wrapper for feature finding
    """
    try:
        index, settings = to_process
        file_name = settings['experiment']['file_paths'][index]

        base, ext = os.path.splitext(file_name)

        if ext.lower() == '.raw':
            datatype='thermo'
        elif ext.lower() == '.d':
            datatype='bruker'
        else:
            raise NotImplementedError('File extension {} not understood.'.format(ext))

        out_file = f"{base}.ms_data.hdf"

        skip = True
        if os.path.isfile(out_file):
            try:
                alphapept.io.MS_Data_File(
                    out_file
                ).read(dataset_name="features")
                logging.info(
                    'Found *.hdf with features for {}'.format(out_file)
                )
            except KeyError:

                logging.info(
                    'No *.hdf file with features found for {}. Adding to feature finding list.'.format(out_file)
                )
                skip = False

        if not skip:
            ms_file = alphapept.io.MS_Data_File(out_file, is_read_only=False)
            query_data = ms_file.read_DDA_query_data()

            if not settings['workflow']["find_features"]:
                features = query_data_to_features(query_data)
            else:
                if datatype == 'thermo':

                    from .constants import averagine_aa, isotopes

                    f_settings = settings['features']
                    max_gap = f_settings['max_gap']
                    centroid_tol = f_settings['centroid_tol']
                    hill_split_level = f_settings['hill_split_level']
                    iso_split_level = f_settings['iso_split_level']


                    window = f_settings['hill_smoothing']
                    hill_check_large = f_settings['hill_check_large']

                    iso_charge_min = f_settings['iso_charge_min']
                    iso_charge_max = f_settings['iso_charge_max']
                    iso_n_seeds = f_settings['iso_n_seeds']

                    hill_nboot_max = f_settings['hill_nboot_max']
                    hill_nboot = f_settings['hill_nboot']

                    iso_mass_range = f_settings['iso_mass_range']

                    iso_corr_min = f_settings['iso_corr_min']

                    logging.info('Feature finding on {}'.format(file_name))

                    logging.info(f'Hill extraction with ppm_tol {ppm_tol} and max_gap {max_gap}')

                    hill_ptrs, hill_data, path_node_cnt, score_median, score_std = extract_hills(query_data, max_gap, ppm_tol)
                    logging.info(f'Number of hills {len(hill_ptrs):,}, len = {np.mean(path_node_cnt):.2f}')

                    logging.info(f'Repeating hill extraction with centroid_tol {score_median+score_std*3:.2f}')

                    hill_ptrs, hill_data, path_node_cnt, score_median, score_std = extract_hills(query_data, max_gap, score_median+score_std*3)
                    logging.info(f'Number of hills {len(hill_ptrs):,}, len = {np.mean(path_node_cnt):.2f}')

                    int_data = np.array(query_data['int_list_ms1'])

                    hill_ptrs = split_hills(hill_ptrs, hill_data, int_data, hill_split_level=hill_split_level, window = window) #hill lenght is inthere already
                    logging.info(f'After split hill_ptrs {len(hill_ptrs):,}')

                    hill_data, hill_ptrs = filter_hills(hill_data, hill_ptrs, int_data, hill_check_large =hill_check_large, window=window)

                    logging.info(f'After filter hill_ptrs {len(hill_ptrs):,}')

                    stats, sortindex_, idxs_upper, scan_idx, hill_data, hill_ptrs = get_hill_data(query_data, hill_ptrs, hill_data, hill_nboot_max = hill_nboot_max, hill_nboot = hill_nboot)
                    logging.info('Extracting hill stats complete')

                    pre_isotope_patterns = get_pre_isotope_patterns(stats, idxs_upper, sortindex_, hill_ptrs, hill_data, int_data, scan_idx, maximum_offset, iso_charge_min=iso_charge_min, iso_charge_max=iso_charge_max, iso_mass_range=iso_mass_range, cc_cutoff=iso_corr_min)
                    logging.info('Found {:,} pre isotope patterns.'.format(len(pre_isotope_patterns)))

                    isotope_patterns, iso_idx, isotope_charges = get_isotope_patterns(pre_isotope_patterns, hill_ptrs, hill_data, int_data, scan_idx, stats, sortindex_, averagine_aa, isotopes, iso_charge_min = iso_charge_min, iso_charge_max = iso_charge_max, iso_mass_range = iso_mass_range, iso_n_seeds = iso_n_seeds, cc_cutoff = iso_corr_min, iso_split_level=iso_split_level, callback=None)
                    logging.info('Extracted {:,} isotope patterns.'.format(len(isotope_charges)))

                    feature_table = feature_finder_report(query_data, isotope_patterns, isotope_charges, iso_idx, stats, sortindex_, hill_ptrs, hill_data)

                    logging.info('Report complete.')

                elif datatype == 'bruker':
                    logging.info('Feature finding on {}'.format(file_name))
                    feature_path = extract_bruker(file_name)
                    feature_table = convert_bruker(feature_path)
                    logging.info('Bruker featurer finder complete. Extracted {:,} features.'.format(len(feature_table)))

                # Calculate additional params
                feature_table['rt_length'] = feature_table['rt_end'] - feature_table['rt_start']
                feature_table['rt_right'] = feature_table['rt_end'] - feature_table['rt_apex']
                feature_table['rt_left'] = feature_table['rt_apex'] - feature_table['rt_start']
                feature_table['rt_tail'] = feature_table['rt_right'] / feature_table['rt_left']

                logging.info('Matching features to query data.')
                features = map_ms2(feature_table, query_data, **settings['features'])

                logging.info('Saving feature table.')
                ms_file.write(feature_table, dataset_name="feature_table")
                logging.info('Feature table saved to {}'.format(out_file))


            logging.info('Saving features.')
            ms_file.write(features, dataset_name="features")
            logging.info(f'Feature finding of file {file_name} complete.')
        return True
    except Exception as e:
        logging.error(f'Feature finding of file {file_name} failed. Exception {e}')
        return f"{e}" #Can't return exception object, cast as string

# Cell

from sklearn.neighbors import KDTree
import pandas as pd
import numpy as np


def replace_infs(array):
    """
    Replace nans and infs with 0
    """
    array[array == -np.inf] = 0
    array[array == np.inf] = 0
    array[np.isnan(array)] = 0

    return array

def map_ms2(feature_table, query_data, map_mz_range = 1, map_rt_range = 0.5, map_mob_range = 0.3, map_n_neighbors=5, search_unidentified = False, **kwargs):
    """
    Map MS1 features to MS2 based on rt and mz
    if ccs is included also add
    """
    feature_table['rt'] = feature_table['rt_apex']

    range_dict = {}
    range_dict['mz'] = ('mono_mzs2', map_mz_range)
    range_dict['rt'] = ('rt_list_ms2', map_rt_range)
    range_dict['mobility'] = ('mobility', map_mob_range)

    query_dict = {}
    query_dict['rt'] = 'rt_list_ms2'
    query_dict['mass'] = 'prec_mass_list2'
    query_dict['mz'] = 'mono_mzs2'
    query_dict['charge'] = 'charge2'
    query_dict['mobility'] = 'mobility'

    if 'mobility' not in feature_table.columns:
        del range_dict['mobility']
        del query_dict['mobility']
        use_mob = False
    else:
        use_mob = True

    tree_points = feature_table[list(range_dict.keys())].values

    for i, key in enumerate(range_dict):
        tree_points[:,i] = tree_points[:,i]/range_dict[key][1]

    matching_tree = KDTree(tree_points, metric="minkowski")
    ref_points = np.array([query_data[range_dict[_][0]] / range_dict[_][1] for _ in range_dict]).T
    ref_points = replace_infs(ref_points)

    dist, idx = matching_tree.query(ref_points, k=map_n_neighbors)
    ref_matched = np.zeros(ref_points.shape[0], dtype=np.bool_)

    all_df = []
    for neighbor in range(map_n_neighbors):

        ref_df = pd.DataFrame(np.array([query_data[query_dict[_]] for _ in query_dict]).T, columns = query_dict.keys())

        for _ in query_dict:
            ref_df[_+'_matched'] = feature_table.iloc[idx[:,neighbor]][_].values
            ref_df[_+'_offset'] = ref_df[_+'_matched'] - ref_df[_]

        ref_df['query_idx'] = ref_df.index
        ref_df['feature_idx'] = idx[:,neighbor]

        for field in ['int_sum','int_apex','rt_start','rt_apex','rt_end','fwhm','mobility_lower','mobility_upper']:
            if field in feature_table.keys():
                ref_df[field] = feature_table.iloc[idx[:,neighbor]][field].values

        rt_check = (ref_df['rt_start'] <= ref_df['rt']) & (ref_df['rt'] <= ref_df['rt_end'])

        # check isolation window (win=3)
        mass_check = np.abs(ref_df['mz_offset'].values) <= 3

        _check = rt_check & mass_check
        if use_mob:
            mob_check = (ref_df['mobility_lower'] <= ref_df['mobility']) & (ref_df['mobility'] <= ref_df['mobility_upper'])
            _check &= mob_check

        ref_matched |= _check
        ref_df['dist'] = dist[:,neighbor]
        ref_df = ref_df[_check]

        all_df.append(ref_df)


    if search_unidentified:
        if use_mob:
            unmatched_ref = pd.DataFrame(np.array([query_data['rt_list_ms2'], query_data['prec_mass_list2'], query_data['mono_mzs2'], query_data['charge2'], query_data['mobility']]).T, columns=['rt', 'mass', 'mz', 'charge','mobility'])
        else:
            unmatched_ref = pd.DataFrame(np.array([query_data['rt_list_ms2'], query_data['prec_mass_list2'], query_data['mono_mzs2'], query_data['charge2']]).T, columns=['rt', 'mass', 'mz', 'charge'])
        unmatched_ref = unmatched_ref[~ref_matched]
        unmatched_ref['mass_matched'] = unmatched_ref['mass']
        unmatched_ref['mass_offset'] = 0
        unmatched_ref['rt_matched'] = unmatched_ref['rt']
        unmatched_ref['rt_offset']  = 0
        unmatched_ref['mz_matched'] = unmatched_ref['mz']
        unmatched_ref['mz_offset'] = 0
        unmatched_ref['charge_matched'] = unmatched_ref['charge']
        unmatched_ref['query_idx'] = unmatched_ref.index
        unmatched_ref['feature_idx'] = np.nan

        if use_mob:
            ref_df['mobility_matched'] = unmatched_ref['mobility']
            ref_df['mobility_offset'] = np.nan

        for field in ['int_sum','int_apex','rt_start','rt_apex','rt_end','fwhm']:
            if field in feature_table.keys():
                unmatched_ref[field] = np.nan
        unmatched_ref['dist'] = np.nan

        all_df.append(unmatched_ref)

    features = pd.concat(all_df)

    features = features.sort_values('mass_matched', ascending=True)
    features = features.reset_index(drop=True)

    return features