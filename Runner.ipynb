{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner\n",
    "\n",
    "This notebook shows how to perform a maxquant run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "# Create logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create STDERR handler\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "# ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter and add it to the handler\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)-s - %(message)s', \"%Y-%m-%d %H:%M:%S\")\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Set STDERR handler as the only handler \n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "def check_enviroment():\n",
    "    import numba \n",
    "    if float('.'.join(numba.__version__.split('.')[0:2])) < 0.46:\n",
    "        raise RuntimeError('Numba version {} not sufficient'.format(numba.__version__))\n",
    "\n",
    "def check_settings(settings):\n",
    "\n",
    "    \"\"\"\n",
    "    Consistency check for settings.\n",
    "\n",
    "    \"\"\"\n",
    "    check_enviroment()\n",
    "    \n",
    "    logging.info('Checking raw path.')\n",
    "\n",
    "    #Check if a valid raw file is provided. If a npz file is also provided do not convert\n",
    "\n",
    "    if os.path.isfile(settings[\"raw\"][\"raw_path_npz\"]):\n",
    "        if settings[\"general\"][\"convert_raw\"]:\n",
    "            settings[\"general\"][\"convert_raw\"] = False\n",
    "            logging.info('NPZ for raw file present. Skipping conversion step.')\n",
    "    else:\n",
    "        if os.path.isfile(settings[\"raw\"][\"raw_path\"]):\n",
    "            logging.info('No NPZ for raw present. Performing conversion step.')\n",
    "            settings[\"general\"][\"convert_raw\"] = True\n",
    "        else:\n",
    "            raise FileNotFoundError('No raw or converted raw file provided')\n",
    "\n",
    "    logging.info('Raw path okay.')\n",
    "\n",
    "    #Check library file\n",
    "    logging.info('Checking library path.')\n",
    "\n",
    "    if os.path.isfile(settings[\"fasta\"][\"library_path\"]):\n",
    "        if settings[\"general\"][\"create_library\"]:\n",
    "            settings[\"general\"][\"create_library\"] = False\n",
    "            logging.info('NPZ for library file present. Skipping library creation step.')\n",
    "    else:\n",
    "        if os.path.isfile(settings[\"fasta\"][\"fasta_path\"]):\n",
    "            logging.info('No NPZ for library present. Creating library from FASTA.')\n",
    "            settings[\"general\"][\"create_library\"] = True\n",
    "        else:\n",
    "            raise FileNotFoundError('No FASTA or library file provided')\n",
    "\n",
    "    logging.info('Library path okay.')\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "For callback, we employ the following callback strategy:\n",
    "\n",
    "* `CURRENT_TASK`\n",
    "* `CURRENT_PROGRESS`\n",
    "* `OVERALL_PROGESS`\n",
    "\n",
    "\n",
    "How can we estimate how long a task will take? Maybe we don't need this. Or later we can have some good estimates\n",
    "\n",
    "General Tasks:\n",
    "\n",
    "* File Conversion - 10%\n",
    "* Library Generation - 10%\n",
    "* Feature Finding - 40%\n",
    "* Search, Recalibration, Search - 30%\n",
    "* Output Tables - 10%\n",
    "\n",
    "\n",
    "At a later stage we should be able to add things\n",
    "\n",
    "\n",
    "*ToDo fix long progess things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from alphapept.settings import settings\n",
    "\n",
    "#settings['raw']['raw_path_npz'] = 'F:/rawdata/04_hela_testrun/20190402_QX1_SeVW_MA_HeLa_500ng_LC11.npz'\n",
    "settings['raw']['raw_path'] = 'F:/rawdata/04_hela_testrun/20190402_QX1_SeVW_MA_HeLa_500ng_LC11.raw'\n",
    "#settings['fasta']['library_path'] = 'F:/rawdata/zz.database/uniprot_Human_reviewed_March_2019_manual.npz'\n",
    "settings['fasta']['fasta_path'] = 'F:/rawdata/zz.database/uniprot_Human_reviewed_March_2019_manual.fasta'\n",
    "\n",
    "settings['general']['create_library'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-bd7d4d5dcfd0>:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  with tqdm(total=100, desc='Current Progress', unit='%') as current_progress:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10d76a98bc140a681df7f391f37e901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current Progress', style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-bd7d4d5dcfd0>:26: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  with tqdm(total=100, desc='Overall Progress', unit='%') as overall_progress:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e226e37ead049509b45a32ec2c7dd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Overall Progress', style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:13:34 INFO - Checking raw path.\n",
      "2020-04-20 00:13:34 INFO - No NPZ for raw present. Performing conversion step.\n",
      "2020-04-20 00:13:34 INFO - Raw path okay.\n",
      "2020-04-20 00:13:34 INFO - Checking library path.\n",
      "2020-04-20 00:13:34 INFO - No NPZ for library present. Creating library from FASTA.\n",
      "2020-04-20 00:13:34 INFO - Library path okay.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Settings\n",
      "Digesting FASTA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:15:01 INFO - Digested 20,418 proteins and generated 5,331,318 peptides\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Spectra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:17:13 INFO - Generated 5,331,318 spectra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving library\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:17:50 INFO - Database saved to F:/rawdata/zz.database/uniprot_Human_reviewed_March_2019_manual.npz. Filesize 3.31 Gb\n",
      "2020-04-20 00:17:50 INFO - Imported existing <module 'comtypes.gen' from 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\alphap\\\\lib\\\\site-packages\\\\comtypes\\\\gen\\\\__init__.py'>\n",
      "2020-04-20 00:17:50 INFO - Using writeable comtypes cache directory: 'C:\\ProgramData\\Anaconda3\\envs\\alphap\\lib\\site-packages\\comtypes\\gen'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting raw files\n",
      "Raw File saved to F:/rawdata/04_hela_testrun/20190402_QX1_SeVW_MA_HeLa_500ng_LC11.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:19:33 INFO - Raw file(s) saved to ['F:/rawdata/04_hela_testrun/20190402_QX1_SeVW_MA_HeLa_500ng_LC11.npz']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting centroids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:19:50 INFO - Loaded 13,230 centroids.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exctracting hills\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\alphap\\lib\\site-packages\\numba\\ir_utils.py:2041: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'centroids' of function 'connect_centroids_forward'.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"alphapept\\feature_finding.py\", line 189:\u001b[0m\n",
      "\u001b[1m@njit\n",
      "\u001b[1mdef connect_centroids_forward(centroids, max_centroids, max_gap, ppm_tol):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\alphap\\lib\\site-packages\\numba\\ir_utils.py:2041: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'centroids' of function 'connect_centroids_backward'.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"alphapept\\feature_finding.py\", line 232:\u001b[0m\n",
      "\u001b[1m@njit\n",
      "\u001b[1mdef connect_centroids_backward(centroids, max_centroids, max_gap, ppm_tol):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "2020-04-20 00:25:04 INFO - A total of 992,051 hills extracted. Average hill length 16.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting hills\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:26:37 INFO - Split 992,051 hills into 1,239,960 hills\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refining hills\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:27:05 INFO - Filtered 1,239,960 hills. Remaining 1,221,467 hills\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating hill statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\alphap\\lib\\site-packages\\numba\\ir_utils.py:2041: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'hill_data' of function 'get_hill_data_numba'.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"alphapept\\feature_finding.py\", line 581:\u001b[0m\n",
      "\u001b[1m@njit\n",
      "\u001b[1mdef get_hill_data_numba(hill_data):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "2020-04-20 00:31:13 INFO - Extracting hill stats complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting pre isotope patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:32:42 INFO - Found 200012 pre isotope patterns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deisotope patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:34:47 INFO - Extracted 218786 isotope patterns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating feature statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:35:31 INFO - Report complete.\n",
      "F:\\projects\\alphapept\\alphapept\\matching.py:23: RuntimeWarning: divide by zero encountered in log\n",
      "  query_mz = np.log(query_data['mono_mzs2'])*1e6/ppm_range\n",
      "2020-04-20 00:35:32 INFO - Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2020-04-20 00:35:32 INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running first search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:36:06 INFO - First search complete. Compared 52,615,866 spectra and found 225,653 psms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting columns for scoring.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:36:38 INFO - Extracted columns for 225,653 psms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring psms.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:36:39 INFO - Scoring complete. For 0.01 FDR found 56,515 targets and 565 decoys.\n",
      "2020-04-20 00:36:39 INFO - Precursor Offset (PPM) is -3.81 (mean), 5.11 (std)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\projects\\alphapept\\alphapept\\recalibration.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sub['o_mass_ppm_offset'] = f2\n",
      "F:\\projects\\alphapept\\alphapept\\recalibration.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sub['o_mass_ppm_calib'] = (df_sub['o_mass_ppm']-df_sub['o_mass_ppm_offset'])\n",
      "2020-04-20 00:37:12 INFO - Calibration complete. Precursor Offset (PPM) is 0.01 (mean), 1.74 (std)\n",
      "2020-04-20 00:37:12 INFO - Adjusting search bound to 5.22 ppm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running second search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:37:24 INFO - Second search complete. Compared 14,653,853 spectra and found 116,850 psms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting columns for scoring.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:37:43 INFO - Extracted columns for 116,850 psms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring psms.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:37:44 INFO - Scoring complete. For 0.01 FDR found 53,821 targets and 538 decoys.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring\n",
      "FDR control on peptides\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:37:45 INFO - Scoring peptides complete. For 0.01 FDR found 53,366 targets and 451 decoys.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform protein grouping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:39:18 INFO - Scoring proteins complete. For 0.01 FDR found 51,463 targets and 72 decoys. A total of 5,829 proteins found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 00:39:20 INFO - Saved to F:/rawdata/04_hela_testrun/20190402_QX1_SeVW_MA_HeLa_500ng_LC11_ap.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "COMPLETE\n"
     ]
    }
   ],
   "source": [
    "if False: #Uncomment to run\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from functools import partial\n",
    "    import pandas as pd\n",
    "\n",
    "    CURRENT_TASK = print\n",
    "\n",
    "    time_dict = {}\n",
    "\n",
    "    time_dict['generate_library'] = 2\n",
    "    time_dict['generate_spectra'] = 3\n",
    "    time_dict['raw_to_npz'] = 5\n",
    "    time_dict['raw_to_centroid'] = 1\n",
    "    time_dict['get_hills'] = 1\n",
    "    time_dict['split_hills'] = 1\n",
    "    time_dict['filter_hills'] = 1\n",
    "    time_dict['get_hill_data'] = 1\n",
    "    time_dict['get_edges'] = 1\n",
    "    time_dict['get_isotope_patterns'] = 1\n",
    "    time_dict['feature_finder_report'] = 1\n",
    "    time_dict['get_psms'] = 5\n",
    "\n",
    "\n",
    "\n",
    "    with tqdm(total=100, desc='Current Progress', unit='%') as current_progress:\n",
    "        with tqdm(total=100, desc='Overall Progress', unit='%') as overall_progress:\n",
    "\n",
    "            progress = 0\n",
    "\n",
    "            def progress_wrapper(current, delta=1):\n",
    "                \"\"\"\n",
    "                Wrapper function to change the overall progress with the current progress\n",
    "                \"\"\"\n",
    "\n",
    "                global progress\n",
    "\n",
    "                to_update = np.round(current*100-current_progress.n, 2)\n",
    "                current_progress.update(to_update)\n",
    "\n",
    "                overall = progress + current*delta\n",
    "                to_update = np.round(overall-overall_progress.n, 2)\n",
    "\n",
    "                overall_progress.update(to_update)\n",
    "\n",
    "                if current == 1.0:\n",
    "                    progress += delta\n",
    "\n",
    "\n",
    "            from alphapept.constants import mass_dict\n",
    "\n",
    "\n",
    "            CURRENT_TASK('Checking Settings')\n",
    "            settings = check_settings(settings)\n",
    "\n",
    "            if settings[\"general\"][\"create_library\"]:\n",
    "                from alphapept.fasta import generate_library, generate_spectra, save_library\n",
    "\n",
    "                CURRENT_TASK('Digesting FASTA')\n",
    "                to_add, pept_dict, fasta_dict = generate_library(mass_dict, callback = partial(progress_wrapper, delta=time_dict['generate_library']), **settings['fasta'])\n",
    "                logging.info('Digested {:,} proteins and generated {:,} peptides'.format(len(fasta_dict), len(to_add)))\n",
    "\n",
    "                CURRENT_TASK('Generating Spectra')\n",
    "                spectra = generate_spectra(to_add, mass_dict, callback = partial(progress_wrapper, delta=time_dict['generate_spectra']))\n",
    "                logging.info('Generated {:,} spectra'.format(len(spectra)))\n",
    "\n",
    "                CURRENT_TASK('Saving library')\n",
    "                base, ext = os.path.splitext(settings['fasta']['fasta_path'])\n",
    "                settings['fasta']['library_path'] = base + '.npz'\n",
    "                library_path = save_library(spectra, pept_dict, fasta_dict, **settings['fasta'])             \n",
    "                logging.info('Database saved to {}. Filesize {:.2f} Gb'.format(library_path, os.stat(library_path).st_size/(1024**3)))\n",
    "\n",
    "\n",
    "            if settings[\"general\"][\"convert_raw\"]:\n",
    "                from alphapept.io import raw_to_npz\n",
    "\n",
    "                CURRENT_TASK('Converting raw files')\n",
    "                out_path = raw_to_npz(settings[\"raw\"], callback=partial(progress_wrapper, delta=time_dict['raw_to_npz']))\n",
    "                settings[\"raw\"][\"query_path\"] = out_path[0]\n",
    "                logging.info('Raw file(s) saved to {}'.format(out_path))\n",
    "            else:\n",
    "                settings[\"raw\"][\"query_path\"] = settings[\"raw\"][\"raw_path_npz\"]\n",
    "\n",
    "            db_data = np.load(settings[\"fasta\"][\"library_path\"], allow_pickle=True)\n",
    "            query_data = np.load(settings[\"raw\"][\"query_path\"], allow_pickle=True)\n",
    "\n",
    "\n",
    "            #Feature Finding Part\n",
    "\n",
    "            from alphapept.feature_finding import raw_to_centroid, get_hills, split_hills, filter_hills, get_hill_data, get_edges, get_isotope_patterns, feature_finder_report\n",
    "            from alphapept.constants import averagine_aa, isotopes\n",
    "\n",
    "\n",
    "            CURRENT_TASK('Converting centroids')\n",
    "            centroids = raw_to_centroid(query_data, callback=partial(progress_wrapper, delta=time_dict['raw_to_centroid']))\n",
    "            logging.info('Loaded {:,} centroids.'.format(len(centroids)))\n",
    "\n",
    "            CURRENT_TASK('Exctracting hills')\n",
    "            completed_hills = get_hills(centroids, callback=partial(progress_wrapper, delta=time_dict['get_hills']))\n",
    "            logging.info('A total of {:,} hills extracted. Average hill length {:.2f}'.format(len(completed_hills), np.mean([len(_) for _ in completed_hills])))\n",
    "\n",
    "            CURRENT_TASK('Splitting hills')\n",
    "            splitted_hills = split_hills(completed_hills, centroids, smoothing=1, callback=partial(progress_wrapper, delta=time_dict['split_hills']))\n",
    "            logging.info('Split {:,} hills into {:,} hills'.format(len(completed_hills), len(splitted_hills)))\n",
    "\n",
    "            CURRENT_TASK('Refining hills')\n",
    "            filtered_hills = filter_hills(splitted_hills, centroids, callback=partial(progress_wrapper, delta=time_dict['filter_hills']))\n",
    "            logging.info('Filtered {:,} hills. Remaining {:,} hills'.format(len(splitted_hills), len(filtered_hills)))\n",
    "\n",
    "            CURRENT_TASK('Calculating hill statistics')\n",
    "            sorted_hills, sorted_stats, sorted_data = get_hill_data(filtered_hills, centroids, callback=partial(progress_wrapper, delta=time_dict['get_hill_data']))\n",
    "            logging.info('Extracting hill stats complete')\n",
    "\n",
    "            CURRENT_TASK('Connecting pre isotope patterns')\n",
    "            pre_isotope_patterns = get_edges(sorted_stats, sorted_data, callback=partial(progress_wrapper, delta=time_dict['get_edges']))\n",
    "            logging.info('Found {} pre isotope patterns.'.format(len(pre_isotope_patterns)))\n",
    "\n",
    "            CURRENT_TASK('Deisotope patterns')\n",
    "            isotope_patterns, isotope_charges = get_isotope_patterns(pre_isotope_patterns, sorted_stats, sorted_data, averagine_aa, isotopes, callback=partial(progress_wrapper, delta=time_dict['get_isotope_patterns']))\n",
    "            logging.info('Extracted {} isotope patterns.'.format(len(isotope_patterns)))\n",
    "\n",
    "            CURRENT_TASK('Calculating feature statistics')\n",
    "            feature_table = feature_finder_report(isotope_patterns, isotope_charges, sorted_stats, sorted_data, sorted_hills, query_data, callback=partial(progress_wrapper, delta=time_dict['feature_finder_report']))\n",
    "            logging.info('Report complete.')\n",
    "\n",
    "            from alphapept.matching import match_ms2\n",
    "            features = match_ms2(feature_table, query_data)\n",
    "\n",
    "\n",
    "            # Search part\n",
    "\n",
    "            from alphapept.search import get_psms, get_score_columns\n",
    "            from alphapept.score import score_x_tandem\n",
    "\n",
    "            CURRENT_TASK('Running first search.')\n",
    "            psms, num_specs_compared = get_psms(query_data, db_data, features, callback=partial(progress_wrapper, delta=time_dict['get_psms']), **settings[\"search\"])\n",
    "            logging.info('First search complete. Compared {:,} spectra and found {:,} psms.'.format(num_specs_compared, len(psms)))\n",
    "\n",
    "            CURRENT_TASK('Extracting columns for scoring.')\n",
    "            psms, num_specs_scored = get_score_columns(psms, query_data, db_data, features, **settings[\"search\"])\n",
    "            logging.info('Extracted columns for {:,} psms.'.format(num_specs_scored))\n",
    "\n",
    "\n",
    "            CURRENT_TASK('Scoring psms.')\n",
    "            df = score_x_tandem(pd.DataFrame(psms), plot=False, verbose=False, **settings[\"search\"])\n",
    "            logging.info('Scoring complete. For {} FDR found {:,} targets and {:,} decoys.'.format(settings[\"search\"][\"peptide_fdr\"], df['target'].sum(), df['decoy'].sum()) )\n",
    "\n",
    "\n",
    "            if settings['search']['calibrate']:\n",
    "\n",
    "                from alphapept.recalibration import get_calibration\n",
    "\n",
    "                CURRENT_TASK('Calibrating features.')\n",
    "                logging.info('Precursor Offset (PPM) is {:.2f} (mean), {:.2f} (std)'.format(df['o_mass_ppm'].mean(), df['o_mass_ppm'].std()))\n",
    "                'Calibrating MS1 spectra'\n",
    "                features_calib, df_sub = get_calibration(df, features, **settings[\"calibration\"])\n",
    "\n",
    "                o_mass_ppm_mean = df_sub['o_mass_ppm_calib'].mean()\n",
    "                o_mass_ppm_std = df_sub['o_mass_ppm_calib'].std()\n",
    "\n",
    "                logging.info('Calibration complete. Precursor Offset (PPM) is {:.2f} (mean), {:.2f} (std)'.format(o_mass_ppm_mean, o_mass_ppm_std))\n",
    "\n",
    "                logging.info('Adjusting search bound to {:.2f} ppm.'.format(3*o_mass_ppm_std))\n",
    "\n",
    "                settings[\"search\"][\"m_offset\"] = 3*o_mass_ppm_std\n",
    "\n",
    "                CURRENT_TASK('Running second search.')\n",
    "                psms, num_specs_compared = get_psms(query_data, db_data, features_calib, callback=partial(progress_wrapper, delta=time_dict['get_psms']), **settings[\"search\"])\n",
    "                logging.info('Second search complete. Compared {:,} spectra and found {:,} psms.'.format(num_specs_compared, len(psms)))\n",
    "\n",
    "                CURRENT_TASK('Extracting columns for scoring.')\n",
    "                psms, num_specs_scored = get_score_columns(psms, query_data, db_data, features_calib, **settings[\"search\"])\n",
    "                logging.info('Extracted columns for {:,} psms.'.format(num_specs_scored))\n",
    "\n",
    "                CURRENT_TASK('Scoring psms.')\n",
    "                df = score_x_tandem(pd.DataFrame(psms), plot=False, verbose=False, **settings[\"search\"])\n",
    "                logging.info('Scoring complete. For {} FDR found {:,} targets and {:,} decoys.'.format(settings[\"search\"][\"peptide_fdr\"], df['target'].sum(), df['decoy'].sum()) )\n",
    "\n",
    "            ## Protein Groups and FDR control\n",
    "\n",
    "            from alphapept.score import cut_global_fdr, perform_protein_grouping, cut_global_fdr, get_x_tandem_score, filter_score\n",
    "\n",
    "            CURRENT_TASK('Scoring')\n",
    "            df = pd.DataFrame(psms)\n",
    "            df['score'] = get_x_tandem_score(df)\n",
    "            df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "            df = filter_score(df)\n",
    "\n",
    "            CURRENT_TASK('FDR control on peptides')\n",
    "            df = cut_global_fdr(df, analyte_level='sequence',  plot=False, verbose=False)\n",
    "            logging.info('Scoring peptides complete. For {} FDR found {:,} targets and {:,} decoys.'.format(settings[\"search\"][\"peptide_fdr\"], df['target'].sum(), df['decoy'].sum()) )\n",
    "\n",
    "\n",
    "            CURRENT_TASK('Perform protein grouping')\n",
    "            df = perform_protein_grouping(df, db_data['pept_dict'].item(), db_data['fasta_dict'].item())\n",
    "            df = cut_global_fdr(df, analyte_level='protein',  plot=False, verbose=False)\n",
    "            logging.info('Scoring proteins complete. For {} FDR found {:,} targets and {:,} decoys. A total of {:,} proteins found.'.format(settings[\"search\"][\"protein_fdr\"], df['target'].sum(), df['decoy'].sum(), len(set(df['protein']))))\n",
    "\n",
    "            CURRENT_TASK('Saving')\n",
    "\n",
    "            base, ext = os.path.splitext(settings['raw']['query_path'])\n",
    "            out_path = base+'_ap.csv'\n",
    "            df.to_csv(out_path, index = False)\n",
    "            logging.info('Saved to {}'.format(out_path))\n",
    "\n",
    "\n",
    "    CURRENT_TASK('COMPLETE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
