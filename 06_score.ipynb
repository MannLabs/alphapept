{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score\n",
    "\n",
    "> Functions related to the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all functions related to the scoring of peptide-spectrum-matches (PSMS).\n",
    "\n",
    "Current ToDo here:\n",
    "\n",
    "- Most of the functions are not very well described yet\n",
    "- Introductory text to give an overview as well as the relevant papers would be nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def filter_seq(df):\n",
    "    \"\"\"\n",
    "    Filter df by sequence\n",
    "\n",
    "    \"\"\"\n",
    "    df[\"rank_sequence\"] = (\n",
    "        df.groupby(\"sequence\")[\"score\"].rank(\"dense\", ascending=False).astype(\"int\")\n",
    "    )\n",
    "    df_filtered = df[df[\"rank_sequence\"] == 1]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def filter_score(df):\n",
    "    \"\"\"\n",
    "    Filter df by score\n",
    "    TODO: PSMS could get the same score when having modifications. Only keep one\n",
    "\n",
    "    \"\"\"\n",
    "    df[\"rank\"] = df.groupby(\"query_idx\")[\"score\"].rank(\"dense\", ascending=False).astype(\"int\")\n",
    "\n",
    "    df_filtered = df[df[\"rank\"] == 1]\n",
    "    # in case two hits have the same score and therfore rank only accept the first one\n",
    "    df_filtered = df_filtered.drop_duplicates(\"query_idx\")\n",
    "\n",
    "    # TOD: this needs to be sorted out, for modifications -> What if we have MoxM -> oxMM, this will screw up with the filter sequence part\n",
    "    return df_filtered\n",
    "\n",
    "@njit\n",
    "def get_q_values(fdr_values):\n",
    "    \"\"\"\n",
    "    Calculate q values from fdr_values\n",
    "    \"\"\"\n",
    "    q_values = np.zeros_like(fdr_values)\n",
    "    min_q_value = np.max(fdr_values)\n",
    "    for i in range(len(fdr_values) - 1, -1, -1):\n",
    "        fdr = fdr_values[i]\n",
    "        if fdr < min_q_value:\n",
    "            min_q_value = fdr\n",
    "        q_values[i] = min_q_value\n",
    "\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR\n",
    "\n",
    "The employed FDR strategy is based on classical target-decoy competition approach. The FDR is approximated by dividing the number of decoy discoveries by the number of target discoveries at agiven score threshold ( #decoys / #targets )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cut_fdr(df, fdr_level=0.01, plot=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Cuts a dataframe with a given fdr level\n",
    "\n",
    "    Args:\n",
    "        fdr_level: fdr level that should be used\n",
    "        plot: flag to enable plot\n",
    "        verbose: flag to enable printing of results\n",
    "\n",
    "    Returns:\n",
    "        cutoff: df with psms within fdr\n",
    "        cutoff_value: numerical value of score cutoff\n",
    "\n",
    "    Raises:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"target\"] = ~df[\"decoy\"]\n",
    "\n",
    "    df = df.sort_values(by=[\"score\",\"decoy\"], ascending=False)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df[\"target_cum\"] = np.cumsum(df[\"target\"])\n",
    "    df[\"decoys_cum\"] = np.cumsum(df[\"decoy\"])\n",
    "\n",
    "    df[\"fdr\"] = df[\"decoys_cum\"] / df[\"target_cum\"]\n",
    "    df[\"q_value\"] = get_q_values(df[\"fdr\"].values)\n",
    "\n",
    "    last_q_value = df[\"q_value\"].iloc[-1]\n",
    "    first_q_value = df[\"q_value\"].iloc[0]\n",
    "\n",
    "    if last_q_value < fdr_level:\n",
    "        if verbose:\n",
    "            print('Last q_value {:.3f} of dataset is smaller than fdr_level {:.3f}'.format(last_q_value, fdr_level))\n",
    "        cutoff_index = len(df)-1\n",
    "\n",
    "    elif first_q_value > fdr_level:\n",
    "        if verbose:\n",
    "            print('First q_value {:.3f} of dataset is larger than fdr_level {:.3f}'.format(last_q_value, fdr_level))\n",
    "        cutoff_index = 0\n",
    "\n",
    "    else:\n",
    "        cutoff_index = df[df[\"q_value\"].gt(fdr_level)].index[0] - 1\n",
    "\n",
    "    cutoff_value = df.loc[cutoff_index][\"score\"]\n",
    "    cutoff = df[df[\"score\"] >= cutoff_value]\n",
    "\n",
    "    targets = df.loc[cutoff_index, \"target_cum\"]\n",
    "    decoy = df.loc[cutoff_index, \"decoys_cum\"]\n",
    "\n",
    "    fdr = df.loc[cutoff_index, \"fdr\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"{:,} target ({:,} decoy) of {} PSM. fdr {:.6f} for a cutoff of {:.2f} \".format(\n",
    "                targets, decoy, len(df), fdr, cutoff_value\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(df[\"score\"], df[\"fdr\"])\n",
    "        plt.axhline(0.01, color=\"k\", linestyle=\"--\")\n",
    "\n",
    "        plt.axvline(cutoff_value, color=\"r\", linestyle=\"--\")\n",
    "        plt.title(\"fdr vs Cutoff value\")\n",
    "        plt.xlabel(\"Score\")\n",
    "        plt.ylabel(\"fdr\")\n",
    "        # plt.savefig('fdr.png')\n",
    "        plt.show()\n",
    "\n",
    "        bins = np.linspace(np.min(df[\"score\"]), np.max(df[\"score\"]), 100)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.distplot(df[df[\"decoy\"]][\"score\"].values, label=\"decoy\", bins=bins)\n",
    "        sns.distplot(df[~df[\"decoy\"]][\"score\"].values, label=\"target\", bins=bins)\n",
    "        plt.xlabel(\"Score\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Score vs Class\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    cutoff = cutoff.reset_index(drop=True)\n",
    "    return cutoff_value, cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global FDR\n",
    "\n",
    "The global FDR model is based on the concept of selecting the best scoring analyte across the dataset and estimating the FDR by target-decoy competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def cut_global_fdr(data, analyte_level='sequence', fdr_level=0.01, plot=True, verbose=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to estimate and filter by global peptide or protein fdr\n",
    "\n",
    "    \"\"\"\n",
    "    data_sub = data[[analyte_level,'score','decoy']]\n",
    "    data_sub_unique = data_sub.groupby([analyte_level,'decoy'], as_index=False).agg({\"score\": \"max\"})\n",
    "    #print(data_sub_unique)\n",
    "\n",
    "    if analyte_level=='sequence':\n",
    "        agg_score = data_sub_unique.groupby([analyte_level,'decoy'])['score'].max().reset_index()\n",
    "    elif analyte_level=='protein':\n",
    "        agg_score = data_sub_unique.groupby([analyte_level,'decoy'])['score'].sum().reset_index()\n",
    "    else:\n",
    "        raise Exception('analyte_level should be either sequence or protein. The selected analyte_level was: {}'.format(analyte_level))\n",
    "\n",
    "    agg_cval, agg_cutoff = cut_fdr(agg_score, fdr_level=fdr_level, plot=plot, verbose=verbose)\n",
    "    #print(agg_cval)\n",
    "    agg_report = pd.merge(data,\n",
    "                          agg_cutoff,\n",
    "                          how = 'inner',\n",
    "                          on = [analyte_level,'decoy'],\n",
    "                          suffixes=('', '_'+analyte_level),\n",
    "                          validate=\"many_to_one\")\n",
    "    return agg_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def score_psms(db_masses, db_frags, db_bounds, db_seqs, frag_types, plot=True, verbose=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to search and score one MS run by the X!Tandem approach.\n",
    "    \"\"\"\n",
    "    print('Loading query data from {}.'.format(kwargs['query_path'] ))\n",
    "    with np.load(kwargs['query_path'] , allow_pickle=True) as data:\n",
    "        query_masses = data['prec_mass_list2']\n",
    "        query_frags = data['mass_list_ms2']\n",
    "        query_ints = data['int_list_ms2']\n",
    "\n",
    "        query_bounds = np.sum(query_frags>=0,axis=0).astype(np.int64)\n",
    "\n",
    "    n_query = len(query_masses)\n",
    "    print(\"{:,} queries loaded.\".format(n_query))\n",
    "\n",
    "    psms, num_specs_compared = get_psms(query_masses, query_frags, query_bounds, db_masses, db_frags, db_bounds, **kwargs)\n",
    "\n",
    "    psms, num_specs_scored = get_score_columns(psms, query_masses, query_frags, query_bounds, query_ints, db_masses, db_frags, db_bounds, db_seqs, frag_types, **kwargs)\n",
    "\n",
    "    df = pd.DataFrame(psms)\n",
    "    df.head()\n",
    "\n",
    "    df['b_hits'] = df['b_hits'].astype('int')\n",
    "    df['y_hits'] = df['y_hits'].astype('int')\n",
    "\n",
    "    df['b_hits_fac'] = df['b_hits'].apply(lambda x: np.math.factorial(x))\n",
    "    df['y_hits_fac'] = df['y_hits'].apply(lambda x: np.math.factorial(x))\n",
    "\n",
    "    df['score'] = df['matched_int']*df['b_hits_fac']*df['y_hits_fac']\n",
    "    df['score'] = df['score'].apply(lambda x: np.log(x))\n",
    "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "\n",
    "    df = filter_score(df)\n",
    "    df = filter_seq(df)\n",
    "\n",
    "    cval, cutoff = cut_fdr(df, plot=plot, verbose=verbose)\n",
    "    return cutoff\n",
    "\n",
    "def get_x_tandem_score(df):\n",
    "    df['b_hits_int'] = df['b_hits'].astype('int')\n",
    "    df['y_hits_int'] = df['y_hits'].astype('int')\n",
    "\n",
    "    df['b_hits_fac'] = df['b_hits_int'].apply(lambda x: np.math.factorial(x))\n",
    "    df['y_hits_fac'] = df['y_hits_int'].apply(lambda x: np.math.factorial(x))\n",
    "\n",
    "    df['x_tandem'] = df['matched_int']*df['b_hits_fac']*df['y_hits_fac']\n",
    "    df['x_tandem'] = df['x_tandem'].apply(lambda x: np.log(x))\n",
    "\n",
    "    return df.x_tandem.values\n",
    "\n",
    "def score_x_tandem(df, fdr_level = 0.01, plot = True, verbose=True, **kwargs):\n",
    "    df['score'] = get_x_tandem_score(df)\n",
    "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "\n",
    "    df = filter_score(df)\n",
    "    df = filter_seq(df)\n",
    "\n",
    "    cval, cutoff = cut_fdr(df, fdr_level, plot, verbose)\n",
    "\n",
    "    return cutoff\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning based scoring\n",
    "\n",
    "The score_RF function trains a random forest classifier to score psms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def score_RF(df, \n",
    "             features,\n",
    "             fdr_level = 0.01,\n",
    "             ini_score = 'y_hits',\n",
    "             n_iterations = 5,  \n",
    "             n_train = 5000,\n",
    "             max_depth = [5,20,50],\n",
    "             max_leaf_nodes = [20,50, 100],\n",
    "             plot = True, \n",
    "             verbose = True, \n",
    "             **kwargs):\n",
    "    \n",
    "    # Setup ML pipeline\n",
    "    scaler = StandardScaler()\n",
    "    rfc = RandomForestClassifier(class_weight={False:1,True:5}, random_state=22)\n",
    "    ## Initiate scaling + classification pipeline\n",
    "    pipeline = Pipeline([('scaler', scaler), ('clf', rfc)]) \n",
    "    parameters = {'clf__max_depth':(max_depth), 'clf__max_leaf_nodes': (max_leaf_nodes)}\n",
    "    ## Setup grid search framework for parameter selection and internal cross validation\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters, cv=3) # Create grid search for hyperparameter optimization of the pipeline\n",
    "    \n",
    "    # Append extra features \n",
    "    # @ToDo only do so if these features are requested in the input\n",
    "    df['abs_delta_m_ppm'] = np.abs(df['delta_m_ppm'])\n",
    "    df['nakedSequence'] = df['sequence'].str.replace('[a-z]|_', '')\n",
    "    df['nAA']= df['nakedSequence'].str.len() \n",
    "    df['nMissed'] = df['sequence'].str.count('K') + df['sequence'].str.count('R') - 1\n",
    "    df = pd.get_dummies(df, columns=['charge'])\n",
    "    count_seq = df.groupby('sequence')['sequence'].count()\n",
    "    df['lnSequence'] = np.log(count_seq[df['sequence']].values)\n",
    "    df['xTandem'] = get_x_tandem_score(df)\n",
    "    \n",
    "    # Prepare target and decoy df\n",
    "    df['decoy'] = df['sequence'].str[-1].str.islower()\n",
    "    df['score'] = df[ini_score]\n",
    "    dfT = df[~df.decoy]\n",
    "    dfD = df[df.decoy]\n",
    "\n",
    "    for i in range(0,n_iterations):\n",
    "        if (i == 0):\n",
    "            df_prescore = dfT.append(dfD)\n",
    "        else:\n",
    "            df_prescore[\"score\"] = cv.predict_proba(df_prescore[features])\n",
    "\n",
    "        df_scored = filter_score(df_prescore)\n",
    "        df_scored = filter_seq(df_scored)\n",
    "        scored = cut_fdr(df_scored, fdr_level, plot=False, verbose=False)[1]\n",
    "        highT = scored[scored.decoy==False]\n",
    "        dfT_high = dfT[dfT['query_idx'].isin(highT.query_idx)]\n",
    "        df_training = dfT_high.sample(n=n_train,random_state=22).append(dfD.sample(n=n_train,random_state=22))\n",
    "\n",
    "        X = df_training[features]\n",
    "\n",
    "        y = df_training['decoy']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=41, stratify=y.values)\n",
    "\n",
    "        cv.fit(X_train,y_train)\n",
    "        if verbose:\n",
    "            print('Best parameters for iteration {} were {}'.format(i, cv.best_params_))\n",
    "            print('Best test score was {}'.format(cv.score(X_test, y_test)))\n",
    "\n",
    "    if plot:\n",
    "        feature_importances=cv.best_estimator_.named_steps['clf'].feature_importances_\n",
    "        indices = np.argsort(feature_importances)[::-1][:40]\n",
    "        g = sns.barplot(y=X.columns[indices][:40],x = feature_importances[indices][:40] , orient='h', palette='RdBu')\n",
    "        g.set_xlabel(\"Relative importance\",fontsize=12)\n",
    "        g.set_ylabel(\"Features\",fontsize=12)\n",
    "        g.tick_params(labelsize=9)\n",
    "        g.set_title(\"Feature importance\")\n",
    "        plt.show()\n",
    "\n",
    "    df_new = df.copy()\n",
    "    df_new['score'] = cv.predict_proba(df_new[features])[:,0]\n",
    "    df_new = filter_score(df_new)\n",
    "    df_new = filter_seq(df_new)\n",
    "    cval, cutoff = cut_fdr(df_new, fdr_level, plot, verbose)\n",
    "    \n",
    "    return cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_protein_groups(data, pept_dict, fasta_dict, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to perform protein grouping by razor approach\n",
    "    \"\"\"\n",
    "    G=nx.Graph()\n",
    "\n",
    "    found_proteins = {}\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        line = data.iloc[i]\n",
    "        seq = line['sequence']\n",
    "        score = line['score']\n",
    "        if seq in pept_dict:\n",
    "            proteins = pept_dict[seq]\n",
    "            if len(proteins) > 1:\n",
    "                for protein in proteins:\n",
    "                    G.add_edge(i, str(protein), score=score)\n",
    "            else: #if there is only one PSM just add to this protein\n",
    "                if proteins[0] in found_proteins.keys():\n",
    "                    found_proteins[proteins[0]] = found_proteins[proteins[0]] + [i]\n",
    "                else:\n",
    "                    found_proteins[proteins[0]] = [i]\n",
    "\n",
    "    print('A total of {:,} proteins with unique PSMs found'.format(len(found_proteins)))\n",
    "\n",
    "    connected_groups = np.array([list(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)])\n",
    "    n_groups = len(connected_groups)\n",
    "    print('A total of {} ambigious proteins'.format(len(connected_groups)))\n",
    "\n",
    "    #Solving with razor:\n",
    "    found_proteins_razor = {}\n",
    "    for a in tqdm(connected_groups):\n",
    "\n",
    "\n",
    "        H = G.subgraph(a)\n",
    "\n",
    "        nodes = list(np.array(a)[np.array(list(isinstance(i, str) for i in a))])\n",
    "\n",
    "        removed = []\n",
    "\n",
    "\n",
    "        while len(nodes) > 0:\n",
    "\n",
    "            neighbors_list = []\n",
    "\n",
    "            for node in nodes:\n",
    "                neighbors = list(H.neighbors(node))\n",
    "                n_neigbhors = len(neighbors)\n",
    "\n",
    "                if node in G:\n",
    "                    if node in found_proteins.keys():\n",
    "                        n_neigbhors+= len(found_proteins[_])\n",
    "\n",
    "                neighbors_list.append((n_neigbhors, node, neighbors))\n",
    "\n",
    "            neighbors_list.sort()\n",
    "\n",
    "            #Remove the last entry:\n",
    "\n",
    "            count, node, psms = neighbors_list[-1]\n",
    "\n",
    "            nodes.remove(node)\n",
    "\n",
    "            psms = [_ for _ in psms if _ not in removed]\n",
    "\n",
    "            removed+= psms\n",
    "\n",
    "            found_proteins_razor[node] = psms\n",
    "\n",
    "    #Put back in Df\n",
    "    report = data.copy()\n",
    "    report['protein'] = ''\n",
    "\n",
    "    for protein in tqdm(found_proteins.keys()):\n",
    "        indexes = found_proteins[protein]\n",
    "        report.loc[indexes, 'protein'] = fasta_dict[protein]['name']\n",
    "\n",
    "    report['Razor'] = False\n",
    "    for protein in tqdm(found_proteins_razor.keys()):\n",
    "        indexes = found_proteins_razor[protein]\n",
    "        report.loc[indexes, 'protein'] = fasta_dict[int(protein)]['name']\n",
    "        report.loc[indexes, 'Razor'] = True\n",
    "\n",
    "    return report\n",
    "\n",
    "def perform_protein_grouping(data, pept_dict, fasta_dict, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper function to perform protein grouping by razor approach\n",
    "\n",
    "    \"\"\"\n",
    "    data_sub = data[['sequence','score','decoy']]\n",
    "    data_sub_unique = data_sub.groupby(['sequence','decoy'], as_index=False).agg({\"score\": \"max\"})\n",
    "\n",
    "    targets = data_sub_unique[data_sub_unique.decoy == False]\n",
    "    protein_targets = get_protein_groups(targets, pept_dict, fasta_dict, **kwargs)\n",
    "\n",
    "    decoys = data_sub_unique[data_sub_unique.decoy == True]\n",
    "    protein_decoys = get_protein_groups(decoys, pept_dict, fasta_dict, **kwargs)\n",
    "\n",
    "    protein_groups = protein_targets.append(protein_decoys)\n",
    "    protein_groups_app = protein_groups[['sequence','decoy','protein','Razor']]\n",
    "    protein_report = pd.merge(data,\n",
    "                                protein_groups_app,\n",
    "                                how = 'inner',\n",
    "                                on = ['sequence','decoy'],\n",
    "                                validate=\"many_to_one\")\n",
    "    return protein_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_cut_fdr():\n",
    "    import random\n",
    "    import string\n",
    "    from collections import Counter\n",
    "    # Generate dummy data\n",
    "    n_samples = 10000\n",
    "    test_data = np.random.rand(n_samples)\n",
    "    df = pd.DataFrame(test_data, columns=['score'])\n",
    "    df['decoy'] = (np.random.rand(n_samples) + df['score']) < 0.5\n",
    "\n",
    "    df['filename'] = np.repeat(['file1','file2','file3','file4'], 2500)\n",
    "\n",
    "    sequences = []\n",
    "    i = 0\n",
    "    while i < 5000:\n",
    "        i += 1\n",
    "        sequences.append(''.join(random.choices(string.ascii_uppercase, k=50)))\n",
    "    df['sequence'] = np.random.choice(sequences, 10000, replace=True)\n",
    "\n",
    "    proteins = []\n",
    "    i = 0\n",
    "    while i < 500:\n",
    "        i += 1\n",
    "        proteins.append(''.join(random.choices(string.ascii_uppercase, k=50)))\n",
    "    df['protein'] = np.random.choice(proteins, 10000, replace=True)\n",
    "\n",
    "    for fdr_level in [0.01, 0.02, 0.05, 0.1, 0.2, 0.4]:\n",
    "        cutoff_value, cutoff = cut_fdr(df,fdr_level = fdr_level, verbose=False, plot=False)\n",
    "        assert cutoff.iloc[-1]['fdr'] <= fdr_level\n",
    "        count_fdr = len(cutoff[cutoff.decoy])/len(cutoff[cutoff.target])\n",
    "        assert  count_fdr <= fdr_level\n",
    "        sequence_res = cut_global_fdr(df, plot=False, verbose=False)\n",
    "        sequence_count_fdr = len(np.unique(sequence_res[sequence_res.decoy].sequence))/len(np.unique(sequence_res[~ sequence_res.decoy].sequence))\n",
    "        assert len(np.unique(sequence_res.filename)) == 4\n",
    "        assert Counter(sequence_res.sequence).most_common(1)[0][1] > 1\n",
    "        assert sequence_count_fdr <= fdr_level\n",
    "        protein_res = cut_global_fdr(df, analyte_level=\"protein\", verbose=False, plot=False)\n",
    "        protein_count_fdr = len(np.unique(protein_res[protein_res.decoy].protein))/len(np.unique(protein_res[~ protein_res.decoy].protein))\n",
    "        assert len(np.unique(protein_res.filename)) == 4\n",
    "        assert protein_count_fdr <= fdr_level\n",
    "        assert Counter(protein_res.sequence).most_common(1)[0][1] > 1\n",
    "        \n",
    "test_cut_fdr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_report_as_npz(\n",
    "    df,\n",
    "    fasta_dict,\n",
    "    pept_dict,\n",
    "    report_path_npz\n",
    "):\n",
    "\n",
    "    to_save = {}\n",
    "    to_save[\"df\"] = df\n",
    "    to_save[\"fasta_dict\"] = fasta_dict\n",
    "    to_save[\"pept_dict\"] = np.array(pept_dict)\n",
    "\n",
    "    np.savez(report_path_npz, **to_save)\n",
    "\n",
    "    print(\"Raw File saved to {}\".format(report_path_npz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_chem.ipynb.\n",
      "Converted 02_io.ipynb.\n",
      "Converted 03_fasta.ipynb.\n",
      "Converted 04_feature_finding.ipynb.\n",
      "Converted 05_search.ipynb.\n",
      "Converted 06_score.ipynb.\n",
      "Converted 07_recalibration.ipynb.\n",
      "Converted 08_quantification.ipynb.\n",
      "Converted 09_matching.ipynb.\n",
      "Converted 10_constants.ipynb.\n",
      "Converted 11_settings.ipynb.\n",
      "Converted 12_settings_template.ipynb.\n",
      "Converted Runner.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
