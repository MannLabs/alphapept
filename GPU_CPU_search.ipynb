{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU/CPU Optimization on search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphapept.search import get_psms\n",
    "\n",
    "import alphapept.io\n",
    "\n",
    "ms_file = 'E:/test_temp/thermo_HeLa.ms_data.hdf'\n",
    "ms_file_ = alphapept.io.MS_Data_File(f\"{ms_file}\")\n",
    "\n",
    "\n",
    "db_data_path = 'E:/test_temp/database.hdf'\n",
    "\n",
    "#         TODO calibrated_fragments should be included in settings\n",
    "query_data = ms_file_.read_DDA_query_data(\n",
    "    calibrated_fragments=True,\n",
    "    database_file_name=db_data_path\n",
    ")\n",
    "\n",
    "features = ms_file_.read(dataset_name=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "psms, num_specs_compared = get_psms(query_data, db_data_path, features, parallel = True, m_tol = 20, m_offset=20, ppm=True, min_frag_hits = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphapept.fasta import read_database\n",
    "import numpy as np\n",
    "from alphapept.search import get_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphapept.search import compare_specs_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "# A decorator for writing GPU/CPU agnostic code\n",
    "import multiprocessing\n",
    "import threading\n",
    "import functools\n",
    "import math\n",
    "import numba as numba_\n",
    "numba = numba_\n",
    "import numpy as np\n",
    "from numba import cuda as cuda_\n",
    "cuda = cuda_\n",
    "from numba import njit\n",
    "\n",
    "try:\n",
    "    import cupy\n",
    "    jit_fun = cuda.jit(device=True) #Device Function\n",
    "except ModuleNotFoundError:\n",
    "    import numpy as cupy\n",
    "    jit_fun = njit\n",
    "    \n",
    "@numba.njit\n",
    "def grid_1d(x): return -1\n",
    "@numba.njit\n",
    "def grid_2d(x): return -1, -1\n",
    "\n",
    "\n",
    "def set_cuda_grid(dimensions=0):\n",
    "    global cuda\n",
    "    if dimensions == 0:\n",
    "        cuda = cuda_\n",
    "        cuda.grid = cuda_.grid\n",
    "    if dimensions == 1:\n",
    "        cuda = numba_\n",
    "        cuda.grid = grid_1d\n",
    "    if dimensions == 2:\n",
    "        cuda = numba_\n",
    "        cuda.grid = grid_2d\n",
    "      \n",
    "def parallel_compiled_func(\n",
    "    _func=None,\n",
    "    *,\n",
    "    cpu_threads=None,\n",
    "    dimensions=1,\n",
    "):\n",
    "    set_cuda_grid()\n",
    "    if dimensions not in (1, 2):\n",
    "        raise ValueError(\"Only 1D and 2D are supported\")\n",
    "    if cpu_threads is not None:\n",
    "        use_gpu = False\n",
    "    else:\n",
    "        try:\n",
    "            cuda.get_current_device()\n",
    "        except cuda.CudaSupportError:\n",
    "            use_gpu = False\n",
    "            cpu_threads = 0\n",
    "        else:\n",
    "            use_gpu = True\n",
    "        try:\n",
    "            import cupy\n",
    "        except ModuleNotFoundError:\n",
    "            use_gpu = False\n",
    "            cpu_threads = 0\n",
    "\n",
    "    if use_gpu:\n",
    "        set_cuda_grid()\n",
    "        def parallel_compiled_func_inner(func):\n",
    "            cuda_func = cuda.jit(func)\n",
    "            if dimensions == 1:\n",
    "                def wrapper(iterable_1d, *args):\n",
    "                    cuda_func.forall(iterable_1d.shape[0], 1)(\n",
    "                        -1,\n",
    "                        iterable_1d,\n",
    "                        *args\n",
    "                    )\n",
    "            elif dimensions == 2:\n",
    "                def wrapper(iterable_2d, *args):\n",
    "                    threadsperblock = (\n",
    "                        min(iterable_2d.shape[0], 16),\n",
    "                        min(iterable_2d.shape[0], 16)\n",
    "                    )\n",
    "                    blockspergrid_x = math.ceil(\n",
    "                        iterable_2d.shape[0] / threadsperblock[0]\n",
    "                    )\n",
    "                    blockspergrid_y = math.ceil(\n",
    "                        iterable_2d.shape[1] / threadsperblock[1]\n",
    "                    )\n",
    "                    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "                    cuda_func[blockspergrid, threadsperblock](\n",
    "                        -1,\n",
    "                        -1,\n",
    "                        iterable_2d,\n",
    "                        *args\n",
    "                    )\n",
    "            return functools.wraps(func)(wrapper)\n",
    "    else:\n",
    "        set_cuda_grid(dimensions)\n",
    "        if cpu_threads <= 0:\n",
    "            cpu_threads = multiprocessing.cpu_count()\n",
    "        def parallel_compiled_func_inner(func):\n",
    "            numba_func = numba.njit(nogil=True)(func)\n",
    "            if dimensions == 1:\n",
    "                def numba_func_parallel(\n",
    "                    thread,\n",
    "                    iterable_1d,\n",
    "                    *args\n",
    "                ):\n",
    "                    for i in range(\n",
    "                        thread,\n",
    "                        len(iterable_1d),\n",
    "                        cpu_threads\n",
    "                    ):\n",
    "                        numba_func(i, iterable_1d, *args)\n",
    "            elif dimensions == 2:\n",
    "                def numba_func_parallel(\n",
    "                    thread,\n",
    "                    iterable_2d,\n",
    "                    *args\n",
    "                ):\n",
    "                    for i in range(\n",
    "                        thread,\n",
    "                        iterable_2d.shape[0],\n",
    "                        cpu_threads\n",
    "                    ):\n",
    "                        for j in range(iterable_2d.shape[1]):\n",
    "                            numba_func(i, j, iterable_2d, *args)\n",
    "            numba_func_parallel = numba.njit(nogil=True)(numba_func_parallel)\n",
    "            def wrapper(iterable, *args):\n",
    "                threads = []\n",
    "                for thread_id in range(cpu_threads):\n",
    "                    t = threading.Thread(\n",
    "                        target=numba_func_parallel,\n",
    "                        args=(thread_id, iterable, *args)\n",
    "                    )\n",
    "                    t.start()\n",
    "                    threads.append(t)\n",
    "                for t in threads:\n",
    "                    t.join()\n",
    "                    del t\n",
    "            return functools.wraps(func)(wrapper)\n",
    "    if _func is None:\n",
    "        return parallel_compiled_func_inner\n",
    "    else:\n",
    "        return parallel_compiled_func_inner(_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit_fun\n",
    "def compare_frags(query_frag, db_frag, mtol, ppm=False):\n",
    "    \"\"\"\n",
    "    Compare query and database frags and find hits\n",
    "    \"\"\"\n",
    "    q_max = len(query_frag)\n",
    "    d_max = len(db_frag)\n",
    "    hits = np.zeros(d_max, dtype=np.int16)\n",
    "    q, d = 0, 0  # q > query, d > database\n",
    "    while q < q_max and d < d_max:\n",
    "        mass1 = query_frag[q]\n",
    "        mass2 = db_frag[d]\n",
    "        delta_mass = mass1 - mass2\n",
    "\n",
    "        if ppm:\n",
    "            sum_mass = mass1 + mass2\n",
    "            mass_difference = 2 * delta_mass / sum_mass * 1e6\n",
    "        else:\n",
    "            mass_difference = delta_mass\n",
    "\n",
    "        if abs(mass_difference) <= mtol:\n",
    "            hits[d] = q + 1  # Save query position +1 (zero-indexing)\n",
    "            d += 1\n",
    "            q += 1  # Only one query for each db element\n",
    "        elif delta_mass < 0:\n",
    "            q += 1\n",
    "        elif delta_mass > 0:\n",
    "            d += 1\n",
    "\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@parallel_compiled_func\n",
    "def search(idx, search_idx, query_indices, query_frags, db_frags, db_bounds, query_masses, frag_hits, mtol = 20, ppm=True):\n",
    "    if idx == -1:\n",
    "        x = cuda.grid(1)\n",
    "    else:\n",
    "        x = idx\n",
    "        \n",
    "    query_idx, db_idx = search_idx[x]\n",
    "         \n",
    "    query_idx_start = query_indices[query_idx]\n",
    "    query_idx_end = query_indices[query_idx + 1]\n",
    "    query_frag = query_frags[query_idx_start:query_idx_end]\n",
    "    db_frag = db_frags[:, db_idx] [: db_bounds[db_idx] ]\n",
    "    o_mass = query_masses[query_idx]  - db_masses[db_idx]\n",
    "    hits = compare_frags(query_frag, db_frag, mtol, ppm)\n",
    "    frag_hits[query_idx, db_idx - idxs_lower[query_idx] ] = cupy.sum(hits > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit_fun\n",
    "def get_search_idx(idxs_higher, idxs_lower, query_masses):\n",
    "    n_comparisons = cupy.zeros((cupy.sum(idxs_higher-idxs_lower),2))\n",
    "    i = 0\n",
    "    for query_idx in range(len(query_masses)):\n",
    "        for db_idx in range(idxs_lower[query_idx] , idxs_higher[query_idx] ):\n",
    "            n_comparisons[i,0] = query_idx\n",
    "            n_comparisons[i,1] = db_idx\n",
    "            i+=1\n",
    "    return n_comparisons\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_data = db_data_path\n",
    "m_offset_calibrated = False\n",
    "\n",
    "m_offset = 20\n",
    "m_tol = 20\n",
    "ppm = True\n",
    "\n",
    "\n",
    "if isinstance(db_data, str):\n",
    "    db_masses = read_database(db_data, array_name = 'precursors')\n",
    "    db_frags = read_database(db_data, array_name = 'fragmasses')\n",
    "    db_bounds = read_database(db_data, array_name = 'bounds')\n",
    "else:\n",
    "    db_masses = db_data['precursors']\n",
    "    db_frags = db_data['fragmasses']\n",
    "    db_bounds = db_data['bounds']\n",
    "\n",
    "query_indices = query_data[\"indices_ms2\"]\n",
    "query_bounds = query_data['bounds']\n",
    "query_frags = query_data['mass_list_ms2']\n",
    "\n",
    "if features is not None:\n",
    "    if m_offset_calibrated:\n",
    "        m_offset = m_offset_calibrated\n",
    "        query_masses = features['corrected_mass'].values\n",
    "    else:\n",
    "        query_masses = features['mass_matched'].values\n",
    "    query_mz = features['mz_matched'].values\n",
    "    query_rt = features['rt_matched'].values\n",
    "    query_bounds = query_bounds[features['query_idx'].values]\n",
    "    query_selection = features['query_idx'].values\n",
    "    indices = np.zeros(len(query_selection) + 1, np.int64)\n",
    "    indices[1:] = np.diff(query_indices)[query_selection]\n",
    "    indices = np.cumsum(indices)\n",
    "    query_frags = np.concatenate(\n",
    "        [\n",
    "            query_frags[s: e] for s, e in zip(\n",
    "                query_indices[query_selection], query_indices[query_selection + 1]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    query_indices = indices\n",
    "else:\n",
    "    if m_offset_calibrated:\n",
    "        m_offset = m_offset_calibrated\n",
    "    query_masses = query_data['prec_mass_list2']\n",
    "    query_mz = query_data['mono_mzs2']\n",
    "    query_rt = query_data['rt_list_ms2']\n",
    "\n",
    "#     idxs_lower, idxs_higher = get_idxs(db_masses, query_masses, m_offset, ppm)\n",
    "idxs_lower, idxs_higher = get_idxs(\n",
    "    db_masses,\n",
    "    query_masses,\n",
    "    m_offset,\n",
    "    ppm\n",
    ")\n",
    "frag_hits = np.zeros(\n",
    "    (len(query_masses), np.max(idxs_higher - idxs_lower)), dtype=int\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psms_(\n",
    "    query_data,\n",
    "    db_data,\n",
    "    features,\n",
    "    m_tol,\n",
    "    m_offset,\n",
    "    ppm,\n",
    "    min_frag_hits,\n",
    "    callback = None,\n",
    "    m_offset_calibrated = None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper function to extract psms from dataset\n",
    "\n",
    "    Args:\n",
    "        db_masses: database precursor masses\n",
    "        query_masses: query precursor masses\n",
    "        m_offset: mass offset in dalton or ppm\n",
    "        ppm: flag for ppm or dalton\n",
    "        callback: Callback function, e.g. for progress bar\n",
    "    Returns:\n",
    "        idxs_lower: lower search range\n",
    "        idxs_higher: upper search range\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(db_data, str):\n",
    "        db_masses = read_database(db_data, array_name = 'precursors')\n",
    "        db_frags = read_database(db_data, array_name = 'fragmasses')\n",
    "        db_bounds = read_database(db_data, array_name = 'bounds')\n",
    "    else:\n",
    "        db_masses = db_data['precursors']\n",
    "        db_frags = db_data['fragmasses']\n",
    "        db_bounds = db_data['bounds']\n",
    "    \n",
    "    query_indices = query_data[\"indices_ms2\"]\n",
    "    query_bounds = query_data['bounds']\n",
    "    query_frags = query_data['mass_list_ms2']\n",
    "\n",
    "    if features is not None:\n",
    "        if m_offset_calibrated:\n",
    "            m_offset = m_offset_calibrated\n",
    "            query_masses = features['corrected_mass'].values\n",
    "        else:\n",
    "            query_masses = features['mass_matched'].values\n",
    "        query_mz = features['mz_matched'].values\n",
    "        query_rt = features['rt_matched'].values\n",
    "        query_bounds = query_bounds[features['query_idx'].values]\n",
    "        query_selection = features['query_idx'].values\n",
    "        indices = np.zeros(len(query_selection) + 1, np.int64)\n",
    "        indices[1:] = np.diff(query_indices)[query_selection]\n",
    "        indices = np.cumsum(indices)\n",
    "        query_frags = np.concatenate(\n",
    "            [\n",
    "                query_frags[s: e] for s, e in zip(\n",
    "                    query_indices[query_selection], query_indices[query_selection + 1]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        query_indices = indices\n",
    "    else:\n",
    "        if m_offset_calibrated:\n",
    "            m_offset = m_offset_calibrated\n",
    "        query_masses = query_data['prec_mass_list2']\n",
    "        query_mz = query_data['mono_mzs2']\n",
    "        query_rt = query_data['rt_list_ms2']\n",
    "    \n",
    "#     idxs_lower, idxs_higher = get_idxs(db_masses, query_masses, m_offset, ppm)\n",
    "    idxs_lower, idxs_higher = get_idxs(\n",
    "        db_masses,\n",
    "        query_masses,\n",
    "        m_offset,\n",
    "        ppm\n",
    "    )\n",
    "    frag_hits = np.zeros(\n",
    "        (len(query_masses), np.max(idxs_higher - idxs_lower)), dtype=int\n",
    "    )\n",
    "\n",
    "    #logging.info(f'Performing search on {len(query_masses):,} query and {len(db_masses):,} db entries with m_tol = {m_tol:.2f} and m_offset = {m_offset:.2f}.')\n",
    "\n",
    "    search_idx = get_search_idx(idxs_higher, idxs_lower, query_masses).astype(cupy.int)\n",
    "\n",
    "    search(search_idx, query_indices, query_frags, db_frags, db_bounds, query_masses, frag_hits)\n",
    "\n",
    "    hit_query, hit_db = cupy.where(frag_hits >= min_frag_hits)\n",
    "    hits = frag_hits[hit_query, hit_db]\n",
    "    hit_db += idxs_lower[hit_query]\n",
    "\n",
    "    psms = cupy.array(\n",
    "        list(zip(hit_query, hit_db, hits)), dtype=[(\"query_idx\", int), (\"db_idx\", int), (\"hits\", int)]\n",
    "    )\n",
    "\n",
    "    return psms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_frag_hits = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([(  2753,      14, 6), (  2753,      15, 6), (  2840,      49, 6),\n",
       "       ..., (113350, 8958238, 6), (113350, 8958239, 6),\n",
       "       (113411, 8959373, 7)],\n",
       "      dtype=[('query_idx', '<i4'), ('db_idx', '<i4'), ('hits', '<i4')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_psms_(\n",
    "    query_data,\n",
    "    db_data,\n",
    "    features,\n",
    "    m_tol,\n",
    "    m_offset,\n",
    "    ppm,\n",
    "    min_frag_hits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphapept] *",
   "language": "python",
   "name": "conda-env-alphapept-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
