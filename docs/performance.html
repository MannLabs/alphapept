---

title: Performance


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/13_performance.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/13_performance.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>AlphaPept deals with high-throughput data. As this can be computationally intensive, we try to make all functions as performant as possible. To do so, we rely on two principles:</p>
<ul>
<li><strong>Compilation</strong></li>
<li><strong>Parallelization</strong></li>
</ul>
<p>A first step of <strong>compilation</strong> can be achieved by using NumPy arrays which are already heavily c-optimized. Net we consider three kinds of compilation:</p>
<ul>
<li><strong>Python</strong> This allows to use no compilation</li>
<li><strong>Numba</strong> This allows to use just-in-time (JIT) compilation.</li>
<li><strong>Cuda</strong> This allows compilation on the GPU.</li>
</ul>
<p>All of these compilation approaches can be combined with <strong>parallelization</strong> approaches. We consider the following possibilities:</p>
<ul>
<li><strong>No parallelization</strong> Not all functionality can be parallelized.</li>
<li><strong>Multithreading</strong> This is only performant when Python's global interpreter lock (GIL) is released or when mostly using input-/output (IO) functions.</li>
<li><strong>GPU</strong> This is only available if an NVIDIA GPU is available and properly configured.</li>
</ul>
<p>Note that not all compilation approaches can sensibly be combined with all parallelization approaches.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we import all libraries, taking into account that not every machine has a GPU (with NVidia cuda cores) available:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="is_valid_compilation_mode" class="doc_header"><code>is_valid_compilation_mode</code><a href="https://github.com/mannlabs/alphapept/tree/master/alphapept/performance.py#L46" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>is_valid_compilation_mode</code>(<strong><code>compilation_mode</code></strong>:<code>str</code>)</p>
</blockquote>
<p>Check if the provided string is a valid compilation mode.</p>
<p>Args:
    compilation_mode (str): The compilation mode to verify.</p>
<p>Raises:
    ModuleNotFoundError: When trying to use an unavailable GPU.
    NotImplementedError: When the compilation mode is not valid.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By default, we will use <a href="/speed.html#cuda"><code>cuda</code></a> if it is available. If not, <code>numba-multithread</code> will be used as default.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To consistently use multiple threads or processes, we can set a global MAX_WORKER_COUNT parameter.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="set_worker_count" class="doc_header"><code>set_worker_count</code><a href="https://github.com/mannlabs/alphapept/tree/master/alphapept/performance.py#L76" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>set_worker_count</code>(<strong><code>worker_count</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>set_global</code></strong>:<code>bool</code>=<em><code>True</code></em>)</p>
</blockquote>
<p>Parse and set the (global) number of threads.</p>
<p>Args:
    worker_count (int): The number of workers.
        If larger than available cores, it is trimmed to the available maximum.
        If 0, it is set to the maximum cores available.
        If negative, it indicates how many cores NOT to use.
        Default is 1
    set_global (bool): If False, the number of workers is only parsed to a valid value.
        If True, the number of workers is saved as a global variable.
        Default is True.</p>
<p>Returns:
    int: The parsed worker_count.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Compiled functions are intended to be very fast. However, they do not have the same flexibility as pure Python functions. In general, we recommend to use staticly defined compilation functions for optimal performance. We provide the option to define a default compilation mode for decorated functions, while also allowing to define the compilation mode for each individual function.</p>
<p><strong>NOTE</strong>: Compiled functions are by default expected to be performed on a single thread. Thus, 'cuda' funtions are always assumed to be device functions which makes them callable from within the GPU, unless explicitly stated otherwise. Similarly, 'numba' functions are always assumed to bo 'nopython' and 'nogil'.</p>
<p><strong>NOTE</strong> If the global compilation mode is set to Python, all decorators default to python, even if a specific compilation_mode is provided.</p>
<p>In addition, we allow to enable dynamic compilation, meaning the compilation mode of functions can be changed at runtime. Do note that this comes at the cost of some performance, as compilation needs to be done at runtime as well. Moreover, functions that are defined with dynamic compilation can not be called from within other compiled functions (with the exception of 'python' compilation, which means no compilation is actually performed).</p>
<p><strong>NOTE</strong>: Dynamic compilation must be enabled before functions are decorated to take effect at runtime, otherwise they are statically compiled with the current settings at the time they are defined! Alternatively, statically compiled functions of a an 'imported_module' can reloaded (and thus statically be recompiled) with the commands:</p>

<pre><code>import importlib
importlib.reload(imported_module)</code></pre>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="set_compilation_mode" class="doc_header"><code>set_compilation_mode</code><a href="https://github.com/mannlabs/alphapept/tree/master/alphapept/performance.py#L108" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>set_compilation_mode</code>(<strong><code>compilation_mode</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>enable_dynamic_compilation</code></strong>:<code>bool</code>=<em><code>None</code></em>)</p>
</blockquote>
<p>Set the global compilation mode to use.</p>
<p>Args:
    compilation_mode (str): The compilation mode to use.
        Will be checked with <a href="/performance.html#is_valid_compilation_mode"><code>is_valid_compilation_mode</code></a>.
        Default is None
    enable_dynamic_compilation (bool): Enable dynamic compilation.
        If enabled, code will generally be slower and no other functions can
        be called from within a compiled function anymore, as they are compiled at runtime.
        WARNING: Enabling this is strongly disadvised in almost all cases!
        Default is None.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="compile_function" class="doc_header"><code>compile_function</code><a href="https://github.com/mannlabs/alphapept/tree/master/alphapept/performance.py#L134" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>compile_function</code>(<strong><code>_func</code></strong>:<code>callable</code>=<em><code>None</code></em>, <strong><code>compilation_mode</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong>**<code>decorator_kwargs</code></strong>)</p>
</blockquote>
<p>A decorator to compile a given function.</p>
<p>Numba functions are by default set to use <code>nogil=True</code> and <code>nopython=True</code>,
unless explicitly defined otherwise.
Cuda functions are by default set to use <code>device=True</code>,
unless explicitly defined otherwise..</p>
<p>Args:
    compilation_mode (str): The compilation mode to use.
        Will be checked with <a href="/performance.html#is_valid_compilation_mode"><code>is_valid_compilation_mode</code></a>.
        If None, the global COMPILATION_MODE will be used as soon as the function is decorated for static compilation.
        If DYNAMIC_COMPILATION_ENABLED, the function will always be compiled at runtime and
        thus by default returns a Python function.
        Static recompilation can be enforced by reimporting a module containing
        the function with importlib.reload(imported_module).
        If COMPILATION_MODE is Python and not DYNAMIC_COMPILATION_ENABLED, no compilation will be used.
        Default is None
    **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.</p>
<p>Returns:
    callable: A decorated function that is compiled.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="__copy_func" class="doc_header"><code>__copy_func</code><a href="https://github.com/mannlabs/alphapept/tree/master/alphapept/performance.py#L210" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>__copy_func</code>(<strong><code>f</code></strong>)</p>
</blockquote>
<p>Based on <a href="http://stackoverflow.com/a/6528148/190597">http://stackoverflow.com/a/6528148/190597</a> (Glenn Maynard)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Testing yields the expected results:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">types</span>

<span class="n">set_compilation_mode</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;numba-multithread&quot;</span><span class="p">)</span>

<span class="nd">@compile_function</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_func_python</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Docstring test&quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    
<span class="nd">@compile_function</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;numba&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_func_numba</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Docstring test&quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">set_compilation_mode</span><span class="p">(</span><span class="n">enable_dynamic_compilation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nd">@compile_function</span>
<span class="k">def</span> <span class="nf">test_func_dynamic_runtime</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Docstring test&quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">set_compilation_mode</span><span class="p">(</span><span class="n">enable_dynamic_compilation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;numba-multithread&quot;</span><span class="p">)</span>

<span class="nd">@compile_function</span>
<span class="k">def</span> <span class="nf">test_func_static_runtime_numba</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Docstring test&quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">test_func_python</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">))</span>
<span class="n">test_func_python</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">test_func_numba</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">registry</span><span class="o">.</span><span class="n">CPUDispatcher</span><span class="p">))</span>
<span class="n">test_func_numba</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

<span class="k">if</span> <span class="n">__GPU_AVAILABLE</span><span class="p">:</span>
    <span class="nd">@compile_function</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">test_func_cuda</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Docstring test&quot;&quot;&quot;</span>
        <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Cuda function cannot be tested from outside the GPU</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">test_func_cuda</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">Dispatcher</span><span class="p">))</span>
    <span class="n">test_func_cuda</span><span class="o">.</span><span class="n">forall</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

<span class="n">set_compilation_mode</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">test_func_static_runtime_numba</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">registry</span><span class="o">.</span><span class="n">CPUDispatcher</span><span class="p">))</span>
<span class="n">test_func_static_runtime_numba</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

<span class="n">set_compilation_mode</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">test_func_dynamic_runtime</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">))</span>
<span class="n">test_func_dynamic_runtime</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

<span class="n">set_compilation_mode</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;numba&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">test_func_dynamic_runtime</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">))</span>
<span class="n">test_func_dynamic_runtime</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

<span class="c1"># # Cuda function cannot be tested from outside the GPU</span>
<span class="c1"># set_compilation_mode(compilation_mode=&quot;cuda&quot;)</span>
<span class="c1"># a = np.zeros(1)</span>
<span class="c1"># assert(isinstance(test_func_dynamic_runtime, types.FunctionType))</span>
<span class="c1"># test_func_dynamic_runtime.forall(1,1)(a)</span>
<span class="c1"># assert(np.all(a == np.ones(1)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>&lt;numba.cuda.compiler.Dispatcher object at 0x000001747EBCA670&gt;
&lt;class &#39;numba.cuda.compiler.Dispatcher&#39;&gt;
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we define the 'performance_function' decorator to take full advantage of both compilation and parallelization for maximal performance. Note that a 'performance_function' can not return values. Instead, it should store results in provided buffer arrays.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="performance_function" class="doc_header"><code>performance_function</code><a href="https://github.com/mannlabs/alphapept/tree/master/alphapept/performance.py#L221" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>performance_function</code>(<strong><code>_func</code></strong>:<code>callable</code>=<em><code>None</code></em>, <strong><code>worker_count</code></strong>:<code>int</code>=<em><code>None</code></em>, <strong><code>compilation_mode</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong>**<code>decorator_kwargs</code></strong>)</p>
</blockquote>
<p>A decorator to compile a given function and allow multithreading over an multiple indices.</p>
<p>NOTE This should only be used on functions that are compilable.
Functions that need to be decorated need to have an <code>index</code> argument as first argument.
If an iterable is provided to the decorated function,
the original (compiled) function will be applied to all elements of this iterable.
The most efficient way to provide iterables are with ranges, but numpy arrays work as well.
Functions can not return values,
results should be stored in buffer arrays inside thge function instead.</p>
<p>Args:
    worker_count (int): The number of workers to use for multithreading.
        If None, the global MAX_WORKER_COUNT is used at runtime.
        Default is None.
    compilation_mode (str): The compilation mode to use. Will be forwarded to the <a href="/performance.html#compile_function"><code>compile_function</code></a> decorator.
    **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.</p>
<p>Returns:
    callable: A decorated function that is compiled and parallelized.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We test this function with a simple smoothing algorithm.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">smooth_func</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">in_array</span><span class="p">,</span> <span class="n">out_array</span><span class="p">,</span> <span class="n">window_size</span><span class="p">):</span>
    <span class="n">min_index</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">index</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">max_index</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_array</span><span class="p">))</span>
    <span class="n">smooth_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_index</span><span class="p">,</span> <span class="n">max_index</span><span class="p">):</span>
        <span class="n">smooth_value</span> <span class="o">+=</span> <span class="n">in_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">out_array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+=</span> <span class="n">smooth_value</span> <span class="o">/</span> <span class="p">(</span><span class="n">max_index</span> <span class="o">-</span> <span class="n">min_index</span><span class="p">)</span>


<span class="n">set_compilation_mode</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;numba-multithread&quot;</span><span class="p">)</span>
<span class="n">set_worker_count</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">array_size</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">6</span>
<span class="n">smooth_factor</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">4</span>

<span class="c1"># python test</span>
<span class="n">in_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">array_size</span><span class="p">)</span>
<span class="n">out_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">in_array</span><span class="p">)</span>

<span class="n">func</span> <span class="o">=</span> <span class="n">performance_function</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">)(</span><span class="n">smooth_func</span><span class="p">)</span>
<span class="o">%</span><span class="k">time</span> func(range(in_array[::100].shape[0]), in_array[::100], out_array[::100], smooth_factor//10) #too slow to test otherwise

<span class="c1"># numba test</span>
<span class="n">in_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">array_size</span><span class="p">)</span>
<span class="n">out_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">in_array</span><span class="p">)</span>

<span class="n">func</span> <span class="o">=</span> <span class="n">performance_function</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;numba&quot;</span><span class="p">)(</span><span class="n">smooth_func</span><span class="p">)</span>
<span class="o">%</span><span class="k">time</span> func(range(in_array.shape[0]), in_array, out_array, smooth_factor)

<span class="c1"># numba-multithread test</span>
<span class="n">in_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">array_size</span><span class="p">)</span>
<span class="n">out_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">in_array</span><span class="p">)</span>

<span class="n">func</span> <span class="o">=</span> <span class="n">performance_function</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;numba-multithread&quot;</span><span class="p">)(</span><span class="n">smooth_func</span><span class="p">)</span>
<span class="o">%</span><span class="k">time</span> func(range(in_array.shape[0]), in_array, out_array, smooth_factor)

<span class="c1"># cuda test</span>
<span class="k">if</span> <span class="n">__GPU_AVAILABLE</span><span class="p">:</span>
    <span class="n">in_array</span> <span class="o">=</span> <span class="n">cupy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">array_size</span><span class="p">)</span>
    <span class="n">out_array</span> <span class="o">=</span> <span class="n">cupy</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">in_array</span><span class="p">)</span>

    <span class="n">func</span> <span class="o">=</span> <span class="n">performance_function</span><span class="p">(</span><span class="n">compilation_mode</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)(</span><span class="n">smooth_func</span><span class="p">)</span>
    <span class="o">%</span><span class="k">time</span> func(range(in_array.shape[0]), in_array, out_array, smooth_factor)
    <span class="o">%</span><span class="k">time</span> tmp = out_array.get()
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Wall time: 9.54 s
Wall time: 10.7 s
Wall time: 959 ms
Wall time: 631 ms
Wall time: 1.41 s
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

